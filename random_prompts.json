{
  "<controllable_image_generation>": [
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <controllable_image_generation>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: runwayml/stable-diffusion-v1-5\n  description: ### ‚ö†Ô∏è This repository is a mirror of the now deprecated `ruwnayml/stable-diffusion-v1-5`, this repository or organization are not affiliated in any way with RunwayML.\nModifications to the original model card are in <span style=\"color:crimson\">red</span> or <span style=\"color:darkgreen\">green</span>\n\nStable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.\nFor more information about how Stable Diffusion functions, please have a look at [ü§ó's Stable Diffusion blog](https://huggingface.co/blog/stable_diffusion).\n\nThe **Stable-Diffusion-v1-5** checkpoint was initialized with the weights of the [Stable-Diffusion-v1-2](https:/steps/huggingface.co/CompVis/stable-diffusion-v1-2) \ncheckpoint and subsequently fine-tuned on 595k steps at resolution 512x512 on \"laion-aesthetics v2 5+\" and 10% dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598).\n\nYou can use this both with the [üß®Diffusers library](https://github.com/huggingface/diffusers) and [RunwayML GitHub repository](https://github.com/runwayml/stable-diffusion) (<span style=\"color:crimson\">now deprecated</span>), <span style=\"color:darkgreen\">ComfyUI, Automatic1111, SD.Next, InvokeAI</span>.\n\n### Use with Diffusers\n```py\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = \"sd-legacy/stable-diffusion-v1-5\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]  \n    \nimage.save(\"astronaut_rides_horse.png\")\n```\nFor more detailed instructions, use-cases and examples in JAX follow the instructions [here](https://github.com/huggingface/diffusers#text-to-image-generation-with-stable-diffusion)\n\n### Use with GitHub Repository <span style=\"color:crimson\">(now deprecated)</span>, <span style=\"color:darkgreen\">ComfyUI or Automatic1111</span>\n\n1. Download the weights \n   - [v1-5-pruned-emaonly.safetensors](https://huggingface.co/sd-legacy/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.safetensors) - ema-only weight. uses less VRAM - suitable for inference\n   - [v1-5-pruned.safetensors](https://huggingface.co/sd-legacy/stable-diffusion-v1-5/resolve/main/v1-5-pruned.safetensors) - ema+non-ema weights. uses more VRAM - suitable for fine-tuning\n\n2. Follow instructions [here](https://github.com/runwayml/stable-diffusion). <span style=\"color:crimson\">(now deprecated)</span>\n\n3. <span style=\"color:darkgreen\">Use locally with <a href=\"https://github.com/comfyanonymous/ComfyUI\">ComfyUI</a>, <a href=\"https://github.com/AUTOMATIC1111/stable-diffusion-webui\">AUTOMATIC1111</a>, <a href=\"https://github.com/vladmandic/automatic\">SD.Next</a>, <a href=\"https://github.com/invoke-ai/InvokeAI\">InvokeAI</a></span>\n created at: 2024-08-30\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: lodestones/P.A.W.F.E.C.T-Alpha\n  description: ## Discord and Help\n[furry diffusion discord](https://discord.gg/furrydiffusion), in the \"custom-models\" channel\n\n[lodestone rock's server](https://discord.gg/SQVcWVbqKx)\n created at: 2023-07-02\n- model_name: kandinsky-community/kandinsky-2-2-controlnet-depth\n  description: Kandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.\n\nIt uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\n\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov)\n created at: 2023-06-28\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: kandinsky-community/kandinsky-2-2-prior\n  description: Kandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.\n\nIt uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\n\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov)\n created at: 2023-06-09\n- model_name: kandinsky-community/kandinsky-2-2-decoder\n  description: Kandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.\n\nIt uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\n\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov)\n created at: 2023-06-09\n- model_name: kandinsky-community/kandinsky-2-1\n  description: Kandinsky 2.1 inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.\n\nIt uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\n\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov)\n created at: 2023-05-24\n- model_name: logo-wizard/logo-diffusion-checkpoint\n  description: # Best practices\nWe recommend using this model with the following prompt template:  \n**positive:** f\"a logo of {company industry}, {some objects}, {colors}, modern, minimalism, vector art, 2d, best quality, centered\"  \n**negative:** \"low quality, worst quality, bad composition, extra digit, fewer digits, text, inscription, watermark, label, asymmetric\"  \n  \nSome other recommendations:  \n**num_inference_steps** = *30*  \n**guidance_scale** = *7.5*  \n**height** = *768*  \n**width** = *768*  \n**scheduler** = diffusers.EulerAncestralDiscreteScheduler (used by default)\n\n![results](./results.png)\n created at: 2023-05-05\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: stablediffusionapi/edge-of-realism\n  description: ![generated from modelslab.com](https://assets.modelslab.com/generations/d3d3f607-e8c6-4758-903a-17804fb4002b-0.png)\n## Get API Key\n\nGet API key from [ModelsLab](https://modelslab.com/), No Payment needed. \n\nReplace Key in below code, change **model_id**  to \"edge-of-realism\"\n\nCoding in PHP/Node/Java etc? Have a look at docs for more code examples: [View docs](https://stablediffusionapi.com/docs)\n\nModel link: [View model](https://stablediffusionapi.com/models/edge-of-realism)\n\nCredits: [View credits](https://civitai.com/?query=model_search)\n\nView all models: [View Models](https://stablediffusionapi.com/models)\n\n    import requests  \n    import json  \n      \n    url =  \"https://stablediffusionapi.com/api/v3/dreambooth\"  \n      \n    payload = json.dumps({  \n    \"key\":  \"\",  \n    \"model_id\":  \"edge-of-realism\",  \n    \"prompt\":  \"actual 8K portrait photo of gareth person, portrait, happy colors, bright eyes, clear eyes, warm smile, smooth soft skin, big dreamy eyes, beautiful intricate colored hair, symmetrical, anime wide eyes, soft lighting, detailed face, by makoto shinkai, stanley artgerm lau, wlop, rossdraws, concept art, digital painting, looking into camera\",  \n    \"negative_prompt\":  \"painting, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, deformed, ugly, blurry, bad anatomy, bad proportions, extra limbs, cloned face, skinny, glitchy, double torso, extra arms, extra hands, mangled fingers, missing lips, ugly face, distorted face, extra legs, anime\",  \n    \"width\":  \"512\",  \n    \"height\":  \"512\",  \n    \"samples\":  \"1\",  \n    \"num_inference_steps\":  \"30\",  \n    \"safety_checker\":  \"no\",  \n    \"enhance_prompt\":  \"yes\",  \n    \"seed\":  None,  \n    \"guidance_scale\":  7.5,  \n    \"multi_lingual\":  \"no\",  \n    \"panorama\":  \"no\",  \n    \"self_attention\":  \"no\",  \n    \"upscale\":  \"no\",  \n    \"embeddings\":  \"embeddings_model_id\",  \n    \"lora\":  \"lora_model_id\",  \n    \"webhook\":  None,  \n    \"track_id\":  None  \n    })  \n      \n    headers =  {  \n    'Content-Type':  'application/json'  \n    }  \n      \n    response = requests.request(\"POST\", url, headers=headers, data=payload)  \n      \n    print(response.text)\n\n> Use this coupon code to get 25% off **DMGG0RBN**\n created at: 2023-04-22\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: xyn-ai/anything-v4.0\n  description: Welcome to Anything V4 - a latent diffusion model for weebs. The newest version of Anything. This model is intended to produce high-quality, highly detailed anime style with just a few prompts. Like other anime-style Stable Diffusion models, it also supports danbooru tags to generate images.\n\ne.g. **_1girl, white hair, golden eyes, beautiful eyes, detail, flower meadow, cumulonimbus clouds, lighting, detailed sky, garden_** \n\nI think the V4.5 version better though, it's in this repo. feel free 2 try it.\n created at: 2023-03-23\n- model_name: SG161222/Realistic_Vision_V2.0\n  description: None\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: stabilityai/stable-diffusion-2-1-unclip\n  description: This `stable-diffusion-2-1-unclip` is a finetuned version of Stable Diffusion 2.1, modified to accept (noisy) CLIP image embedding in addition to the text prompt, and can be used to create image variations (Examples) or can be chained with text-to-image CLIP priors. The amount of noise added to the image embedding can be specified via the noise_level (0 means no noise, 1000 full noise).\n\n- Use it with üß® [`diffusers`](#examples)\n created at: 2023-03-20\n- model_name: black-forest-labs/FLUX.1-Kontext-dev\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-05-28T22:23:43+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-kontext-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of editing images based on text instructions. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-06-28T22:57:58+00:00\n- model_name: bullerwins/FLUX.1-Kontext-dev-GGUF\n  description: created with example workflow added in files `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-26T16:26:45+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-kontext-dev\n  description: this repository contains nunchaku-quantized versions of , capable of editing images based on text instructions. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:52:52+00:00\n- model_name: unsloth/FLUX.1-Kontext-dev-GGUF\n  description: gguf files for . original license still applies. a huge thanks to and for help with conversion. ggufs were created with and . to run, use comfyui-gguf and place the model files in the `comfyui/models/unet` directory. for more instructions, go to the readme. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-30T10:10:09+00:00\n- model_name: Owen777/Kontext-Style-Loras\n  description: this repository provides a collection of 20+ style lora adapters for the flux.1 kontext model, enabling a wide range of artistic and cartoon styles for high-quality image-to-image generation. these loras are trained on high-quality paired data generated by gpt-4o. srouce training code:\n created at: 2025-07-04T06:54:56+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-fill-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of filling areas in existing images based on a text description. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:19:13+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-fill-dev\n  description: this repository contains nunchaku-quantized versions of , capable of filling areas in existing images based on a text description. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:43:28+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-canny-dev\n  description: this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:48:02+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-depth-dev\n  description: this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:45:46+00:00\n- model_name: fuliucansheng/FLUX.1-Kontext-dev-diffusers\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-28T19:05:56+00:00\n- model_name: AIDC-AI/Ovis-U1-3B\n  description: building on the foundation of the ovis series, ovis-u1 is a 3-billion-parameter unified model that seamlessly integrates multimodal understanding, text-to-image generation, and image editing within a single powerful framework. the overall architecture of ovis-u1 cf. fig.2 in our report. ovis-u1 has been tested with python 3.10, torch 2.4.0, transformers 4.51.3, and deepspeed 0.15.4. for a comprehensive list of package dependencies, please consult the requirements.txt file.\n created at: 2025-06-28T12:00:29+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-canny-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:07:43+00:00\n- model_name: fofr/kontext-make-person-real\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on using: you should use `make this person look real` as part of the prompt instruction for your image-to-image editing.\n created at: 2025-07-19T14:35:21+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-depth-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:11:47+00:00\n- model_name: xiaolxl/HXHY-Asian-style-realism-Realistic-KontextLoRA\n  description: :nsfw,kontext,,„ÄÇ model statement: the training material does not contain any nsfw or minor characters, and the training material is only used to adjust the effect of the original kontext model. when you use this model, the output results will be generated based on the images you use, so please do not use it for any illegal or violation purposes.\n created at: 2025-07-07T02:38:04+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-schnell-pix2pix-turbo\n  description: this repository contains loras for both original and nunchaku-quantized to translate sketch to images from user prompts. - developed by: nunchaku team, cmu generative intelligence lab - model type: image-to-image - developed by: nunchaku team, cmu generative intelligence lab - model type: image-to-image - license: apache-2.0 - quantized from model:\n created at: 2025-07-21T04:53:10+00:00\n- model_name: fofr/flux-kontext-dev-ps1-lora\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on . you should use `render this image like a ps1 game no ui` to trigger the image generation.\n created at: 2025-07-12T19:02:05+00:00\n- model_name: AlekseyCalvin/QWEN_IMAGE_nf4_w_AbliteratedTE_Diffusers\n  description: this repo contains a variant of qwen's , the state-of-the-art generative model with extensive and image/text-to-image &/or instruction/control-editing capabilities. to make these cutting edge capabilities more accessible to those constrained to low-end consumer-grade hardware, we've quantized the dit diffusion transformer component of qwen-image to the 4-bit nf4 format using the bits&bytes toolkit. this optimization was derived by us directly from the bf16 base model weights released on 08/04/2025, with no other mix-ins or modifications to the dit component.\n created at: 2025-08-05T12:00:18+00:00\n- model_name: lucataco/kontext-realearth\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on using: you should use `make this a realistic drone photo` as part of the prompt instruction for your image-to-image editing.\n created at: 2025-07-22T20:28:00+00:00\n\nExamples:\n- example user query: I am a digital artist. I want to transform my artworks to a different style or representation.\n  model_name: GreeneryScenery/SheepsControlV5\n- example user query: We need a new anime character sketch for our story. The character should be a warrior girl in the woods.\n  model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n- example user query: A social media platform wants to create AI-generated profile pictures for its users based on their description. We need to provide a way to generate these images based on their descriptions.\n  model_name: lllyasviel/control_v11p_sd15_normalbae\n- example user query: Create a program that produces a new image by reimagining the input image. Modify the input image with guidance like \"add more trees\" or \"make the sky darker.\"\n  model_name: GreeneryScenery/SheepsControlV3\n- example user query: We want to create a visual scene from this story snippet: \"In the enchanted forest, a magical unicorn grazes near a crystal-clear pond.\"\n  model_name: lllyasviel/control_v11p_sd15_scribble\n- example user query: I need a model to generate images based on text descriptions with additional input conditions, like containing soft edges.\n  model_name: lllyasviel/control_v11p_sd15_softedge\n\nUser prompt:\nIn relation to the provided depth image and the concept 'a winter wonderland', imagine a new creation.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <controllable_image_generation>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: runwayml/stable-diffusion-v1-5\n  description: ### ‚ö†Ô∏è This repository is a mirror of the now deprecated `ruwnayml/stable-diffusion-v1-5`, this repository or organization are not affiliated in any way with RunwayML.\nModifications to the original model card are in <span style=\"color:crimson\">red</span> or <span style=\"color:darkgreen\">green</span>\n\nStable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.\nFor more information about how Stable Diffusion functions, please have a look at [ü§ó's Stable Diffusion blog](https://huggingface.co/blog/stable_diffusion).\n\nThe **Stable-Diffusion-v1-5** checkpoint was initialized with the weights of the [Stable-Diffusion-v1-2](https:/steps/huggingface.co/CompVis/stable-diffusion-v1-2) \ncheckpoint and subsequently fine-tuned on 595k steps at resolution 512x512 on \"laion-aesthetics v2 5+\" and 10% dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598).\n\nYou can use this both with the [üß®Diffusers library](https://github.com/huggingface/diffusers) and [RunwayML GitHub repository](https://github.com/runwayml/stable-diffusion) (<span style=\"color:crimson\">now deprecated</span>), <span style=\"color:darkgreen\">ComfyUI, Automatic1111, SD.Next, InvokeAI</span>.\n\n### Use with Diffusers\n```py\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = \"sd-legacy/stable-diffusion-v1-5\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]  \n    \nimage.save(\"astronaut_rides_horse.png\")\n```\nFor more detailed instructions, use-cases and examples in JAX follow the instructions [here](https://github.com/huggingface/diffusers#text-to-image-generation-with-stable-diffusion)\n\n### Use with GitHub Repository <span style=\"color:crimson\">(now deprecated)</span>, <span style=\"color:darkgreen\">ComfyUI or Automatic1111</span>\n\n1. Download the weights \n   - [v1-5-pruned-emaonly.safetensors](https://huggingface.co/sd-legacy/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.safetensors) - ema-only weight. uses less VRAM - suitable for inference\n   - [v1-5-pruned.safetensors](https://huggingface.co/sd-legacy/stable-diffusion-v1-5/resolve/main/v1-5-pruned.safetensors) - ema+non-ema weights. uses more VRAM - suitable for fine-tuning\n\n2. Follow instructions [here](https://github.com/runwayml/stable-diffusion). <span style=\"color:crimson\">(now deprecated)</span>\n\n3. <span style=\"color:darkgreen\">Use locally with <a href=\"https://github.com/comfyanonymous/ComfyUI\">ComfyUI</a>, <a href=\"https://github.com/AUTOMATIC1111/stable-diffusion-webui\">AUTOMATIC1111</a>, <a href=\"https://github.com/vladmandic/automatic\">SD.Next</a>, <a href=\"https://github.com/invoke-ai/InvokeAI\">InvokeAI</a></span>\n created at: 2024-08-30\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: lodestones/P.A.W.F.E.C.T-Alpha\n  description: ## Discord and Help\n[furry diffusion discord](https://discord.gg/furrydiffusion), in the \"custom-models\" channel\n\n[lodestone rock's server](https://discord.gg/SQVcWVbqKx)\n created at: 2023-07-02\n- model_name: kandinsky-community/kandinsky-2-2-controlnet-depth\n  description: Kandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.\n\nIt uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\n\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov)\n created at: 2023-06-28\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: kandinsky-community/kandinsky-2-2-prior\n  description: Kandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.\n\nIt uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\n\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov)\n created at: 2023-06-09\n- model_name: kandinsky-community/kandinsky-2-2-decoder\n  description: Kandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.\n\nIt uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\n\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov)\n created at: 2023-06-09\n- model_name: kandinsky-community/kandinsky-2-1\n  description: Kandinsky 2.1 inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.\n\nIt uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\n\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov)\n created at: 2023-05-24\n- model_name: logo-wizard/logo-diffusion-checkpoint\n  description: # Best practices\nWe recommend using this model with the following prompt template:  \n**positive:** f\"a logo of {company industry}, {some objects}, {colors}, modern, minimalism, vector art, 2d, best quality, centered\"  \n**negative:** \"low quality, worst quality, bad composition, extra digit, fewer digits, text, inscription, watermark, label, asymmetric\"  \n  \nSome other recommendations:  \n**num_inference_steps** = *30*  \n**guidance_scale** = *7.5*  \n**height** = *768*  \n**width** = *768*  \n**scheduler** = diffusers.EulerAncestralDiscreteScheduler (used by default)\n\n![results](./results.png)\n created at: 2023-05-05\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: stablediffusionapi/edge-of-realism\n  description: ![generated from modelslab.com](https://assets.modelslab.com/generations/d3d3f607-e8c6-4758-903a-17804fb4002b-0.png)\n## Get API Key\n\nGet API key from [ModelsLab](https://modelslab.com/), No Payment needed. \n\nReplace Key in below code, change **model_id**  to \"edge-of-realism\"\n\nCoding in PHP/Node/Java etc? Have a look at docs for more code examples: [View docs](https://stablediffusionapi.com/docs)\n\nModel link: [View model](https://stablediffusionapi.com/models/edge-of-realism)\n\nCredits: [View credits](https://civitai.com/?query=model_search)\n\nView all models: [View Models](https://stablediffusionapi.com/models)\n\n    import requests  \n    import json  \n      \n    url =  \"https://stablediffusionapi.com/api/v3/dreambooth\"  \n      \n    payload = json.dumps({  \n    \"key\":  \"\",  \n    \"model_id\":  \"edge-of-realism\",  \n    \"prompt\":  \"actual 8K portrait photo of gareth person, portrait, happy colors, bright eyes, clear eyes, warm smile, smooth soft skin, big dreamy eyes, beautiful intricate colored hair, symmetrical, anime wide eyes, soft lighting, detailed face, by makoto shinkai, stanley artgerm lau, wlop, rossdraws, concept art, digital painting, looking into camera\",  \n    \"negative_prompt\":  \"painting, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, deformed, ugly, blurry, bad anatomy, bad proportions, extra limbs, cloned face, skinny, glitchy, double torso, extra arms, extra hands, mangled fingers, missing lips, ugly face, distorted face, extra legs, anime\",  \n    \"width\":  \"512\",  \n    \"height\":  \"512\",  \n    \"samples\":  \"1\",  \n    \"num_inference_steps\":  \"30\",  \n    \"safety_checker\":  \"no\",  \n    \"enhance_prompt\":  \"yes\",  \n    \"seed\":  None,  \n    \"guidance_scale\":  7.5,  \n    \"multi_lingual\":  \"no\",  \n    \"panorama\":  \"no\",  \n    \"self_attention\":  \"no\",  \n    \"upscale\":  \"no\",  \n    \"embeddings\":  \"embeddings_model_id\",  \n    \"lora\":  \"lora_model_id\",  \n    \"webhook\":  None,  \n    \"track_id\":  None  \n    })  \n      \n    headers =  {  \n    'Content-Type':  'application/json'  \n    }  \n      \n    response = requests.request(\"POST\", url, headers=headers, data=payload)  \n      \n    print(response.text)\n\n> Use this coupon code to get 25% off **DMGG0RBN**\n created at: 2023-04-22\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: xyn-ai/anything-v4.0\n  description: Welcome to Anything V4 - a latent diffusion model for weebs. The newest version of Anything. This model is intended to produce high-quality, highly detailed anime style with just a few prompts. Like other anime-style Stable Diffusion models, it also supports danbooru tags to generate images.\n\ne.g. **_1girl, white hair, golden eyes, beautiful eyes, detail, flower meadow, cumulonimbus clouds, lighting, detailed sky, garden_** \n\nI think the V4.5 version better though, it's in this repo. feel free 2 try it.\n created at: 2023-03-23\n- model_name: SG161222/Realistic_Vision_V2.0\n  description: None\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: stabilityai/stable-diffusion-2-1-unclip\n  description: This `stable-diffusion-2-1-unclip` is a finetuned version of Stable Diffusion 2.1, modified to accept (noisy) CLIP image embedding in addition to the text prompt, and can be used to create image variations (Examples) or can be chained with text-to-image CLIP priors. The amount of noise added to the image embedding can be specified via the noise_level (0 means no noise, 1000 full noise).\n\n- Use it with üß® [`diffusers`](#examples)\n created at: 2023-03-20\n- model_name: black-forest-labs/FLUX.1-Kontext-dev\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-05-28T22:23:43+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-kontext-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of editing images based on text instructions. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-06-28T22:57:58+00:00\n- model_name: bullerwins/FLUX.1-Kontext-dev-GGUF\n  description: created with example workflow added in files `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-26T16:26:45+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-kontext-dev\n  description: this repository contains nunchaku-quantized versions of , capable of editing images based on text instructions. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:52:52+00:00\n- model_name: unsloth/FLUX.1-Kontext-dev-GGUF\n  description: gguf files for . original license still applies. a huge thanks to and for help with conversion. ggufs were created with and . to run, use comfyui-gguf and place the model files in the `comfyui/models/unet` directory. for more instructions, go to the readme. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-30T10:10:09+00:00\n- model_name: Owen777/Kontext-Style-Loras\n  description: this repository provides a collection of 20+ style lora adapters for the flux.1 kontext model, enabling a wide range of artistic and cartoon styles for high-quality image-to-image generation. these loras are trained on high-quality paired data generated by gpt-4o. srouce training code:\n created at: 2025-07-04T06:54:56+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-fill-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of filling areas in existing images based on a text description. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:19:13+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-fill-dev\n  description: this repository contains nunchaku-quantized versions of , capable of filling areas in existing images based on a text description. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:43:28+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-canny-dev\n  description: this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:48:02+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-depth-dev\n  description: this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:45:46+00:00\n- model_name: fuliucansheng/FLUX.1-Kontext-dev-diffusers\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-28T19:05:56+00:00\n- model_name: AIDC-AI/Ovis-U1-3B\n  description: building on the foundation of the ovis series, ovis-u1 is a 3-billion-parameter unified model that seamlessly integrates multimodal understanding, text-to-image generation, and image editing within a single powerful framework. the overall architecture of ovis-u1 cf. fig.2 in our report. ovis-u1 has been tested with python 3.10, torch 2.4.0, transformers 4.51.3, and deepspeed 0.15.4. for a comprehensive list of package dependencies, please consult the requirements.txt file.\n created at: 2025-06-28T12:00:29+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-canny-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:07:43+00:00\n- model_name: fofr/kontext-make-person-real\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on using: you should use `make this person look real` as part of the prompt instruction for your image-to-image editing.\n created at: 2025-07-19T14:35:21+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-depth-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:11:47+00:00\n- model_name: xiaolxl/HXHY-Asian-style-realism-Realistic-KontextLoRA\n  description: :nsfw,kontext,,„ÄÇ model statement: the training material does not contain any nsfw or minor characters, and the training material is only used to adjust the effect of the original kontext model. when you use this model, the output results will be generated based on the images you use, so please do not use it for any illegal or violation purposes.\n created at: 2025-07-07T02:38:04+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-schnell-pix2pix-turbo\n  description: this repository contains loras for both original and nunchaku-quantized to translate sketch to images from user prompts. - developed by: nunchaku team, cmu generative intelligence lab - model type: image-to-image - developed by: nunchaku team, cmu generative intelligence lab - model type: image-to-image - license: apache-2.0 - quantized from model:\n created at: 2025-07-21T04:53:10+00:00\n- model_name: fofr/flux-kontext-dev-ps1-lora\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on . you should use `render this image like a ps1 game no ui` to trigger the image generation.\n created at: 2025-07-12T19:02:05+00:00\n- model_name: AlekseyCalvin/QWEN_IMAGE_nf4_w_AbliteratedTE_Diffusers\n  description: this repo contains a variant of qwen's , the state-of-the-art generative model with extensive and image/text-to-image &/or instruction/control-editing capabilities. to make these cutting edge capabilities more accessible to those constrained to low-end consumer-grade hardware, we've quantized the dit diffusion transformer component of qwen-image to the 4-bit nf4 format using the bits&bytes toolkit. this optimization was derived by us directly from the bf16 base model weights released on 08/04/2025, with no other mix-ins or modifications to the dit component.\n created at: 2025-08-05T12:00:18+00:00\n- model_name: lucataco/kontext-realearth\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on using: you should use `make this a realistic drone photo` as part of the prompt instruction for your image-to-image editing.\n created at: 2025-07-22T20:28:00+00:00\n\nExamples:\n- example user query: I am a digital artist. I want to transform my artworks to a different style or representation.\n  model_name: GreeneryScenery/SheepsControlV5\n- example user query: We need a new anime character sketch for our story. The character should be a warrior girl in the woods.\n  model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n- example user query: A social media platform wants to create AI-generated profile pictures for its users based on their description. We need to provide a way to generate these images based on their descriptions.\n  model_name: lllyasviel/control_v11p_sd15_normalbae\n- example user query: Create a program that produces a new image by reimagining the input image. Modify the input image with guidance like \"add more trees\" or \"make the sky darker.\"\n  model_name: GreeneryScenery/SheepsControlV3\n- example user query: We want to create a visual scene from this story snippet: \"In the enchanted forest, a magical unicorn grazes near a crystal-clear pond.\"\n  model_name: lllyasviel/control_v11p_sd15_scribble\n- example user query: I need a model to generate images based on text descriptions with additional input conditions, like containing soft edges.\n  model_name: lllyasviel/control_v11p_sd15_softedge\n\nUser prompt:\nConstructing an image around the normal picture with the idea 'a vintage car', create a new image.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <controllable_image_generation>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: runwayml/stable-diffusion-v1-5\n  description: ### ‚ö†Ô∏è This repository is a mirror of the now deprecated `ruwnayml/stable-diffusion-v1-5`, this repository or organization are not affiliated in any way with RunwayML.\nModifications to the original model card are in <span style=\"color:crimson\">red</span> or <span style=\"color:darkgreen\">green</span>\n\nStable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.\nFor more information about how Stable Diffusion functions, please have a look at [ü§ó's Stable Diffusion blog](https://huggingface.co/blog/stable_diffusion).\n\nThe **Stable-Diffusion-v1-5** checkpoint was initialized with the weights of the [Stable-Diffusion-v1-2](https:/steps/huggingface.co/CompVis/stable-diffusion-v1-2) \ncheckpoint and subsequently fine-tuned on 595k steps at resolution 512x512 on \"laion-aesthetics v2 5+\" and 10% dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598).\n\nYou can use this both with the [üß®Diffusers library](https://github.com/huggingface/diffusers) and [RunwayML GitHub repository](https://github.com/runwayml/stable-diffusion) (<span style=\"color:crimson\">now deprecated</span>), <span style=\"color:darkgreen\">ComfyUI, Automatic1111, SD.Next, InvokeAI</span>.\n\n### Use with Diffusers\n```py\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = \"sd-legacy/stable-diffusion-v1-5\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]  \n    \nimage.save(\"astronaut_rides_horse.png\")\n```\nFor more detailed instructions, use-cases and examples in JAX follow the instructions [here](https://github.com/huggingface/diffusers#text-to-image-generation-with-stable-diffusion)\n\n### Use with GitHub Repository <span style=\"color:crimson\">(now deprecated)</span>, <span style=\"color:darkgreen\">ComfyUI or Automatic1111</span>\n\n1. Download the weights \n   - [v1-5-pruned-emaonly.safetensors](https://huggingface.co/sd-legacy/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.safetensors) - ema-only weight. uses less VRAM - suitable for inference\n   - [v1-5-pruned.safetensors](https://huggingface.co/sd-legacy/stable-diffusion-v1-5/resolve/main/v1-5-pruned.safetensors) - ema+non-ema weights. uses more VRAM - suitable for fine-tuning\n\n2. Follow instructions [here](https://github.com/runwayml/stable-diffusion). <span style=\"color:crimson\">(now deprecated)</span>\n\n3. <span style=\"color:darkgreen\">Use locally with <a href=\"https://github.com/comfyanonymous/ComfyUI\">ComfyUI</a>, <a href=\"https://github.com/AUTOMATIC1111/stable-diffusion-webui\">AUTOMATIC1111</a>, <a href=\"https://github.com/vladmandic/automatic\">SD.Next</a>, <a href=\"https://github.com/invoke-ai/InvokeAI\">InvokeAI</a></span>\n created at: 2024-08-30\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: lodestones/P.A.W.F.E.C.T-Alpha\n  description: ## Discord and Help\n[furry diffusion discord](https://discord.gg/furrydiffusion), in the \"custom-models\" channel\n\n[lodestone rock's server](https://discord.gg/SQVcWVbqKx)\n created at: 2023-07-02\n- model_name: kandinsky-community/kandinsky-2-2-controlnet-depth\n  description: Kandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.\n\nIt uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\n\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov)\n created at: 2023-06-28\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: kandinsky-community/kandinsky-2-2-prior\n  description: Kandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.\n\nIt uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\n\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov)\n created at: 2023-06-09\n- model_name: kandinsky-community/kandinsky-2-2-decoder\n  description: Kandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.\n\nIt uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\n\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov)\n created at: 2023-06-09\n- model_name: kandinsky-community/kandinsky-2-1\n  description: Kandinsky 2.1 inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.\n\nIt uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\n\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov)\n created at: 2023-05-24\n- model_name: logo-wizard/logo-diffusion-checkpoint\n  description: # Best practices\nWe recommend using this model with the following prompt template:  \n**positive:** f\"a logo of {company industry}, {some objects}, {colors}, modern, minimalism, vector art, 2d, best quality, centered\"  \n**negative:** \"low quality, worst quality, bad composition, extra digit, fewer digits, text, inscription, watermark, label, asymmetric\"  \n  \nSome other recommendations:  \n**num_inference_steps** = *30*  \n**guidance_scale** = *7.5*  \n**height** = *768*  \n**width** = *768*  \n**scheduler** = diffusers.EulerAncestralDiscreteScheduler (used by default)\n\n![results](./results.png)\n created at: 2023-05-05\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: stablediffusionapi/edge-of-realism\n  description: ![generated from modelslab.com](https://assets.modelslab.com/generations/d3d3f607-e8c6-4758-903a-17804fb4002b-0.png)\n## Get API Key\n\nGet API key from [ModelsLab](https://modelslab.com/), No Payment needed. \n\nReplace Key in below code, change **model_id**  to \"edge-of-realism\"\n\nCoding in PHP/Node/Java etc? Have a look at docs for more code examples: [View docs](https://stablediffusionapi.com/docs)\n\nModel link: [View model](https://stablediffusionapi.com/models/edge-of-realism)\n\nCredits: [View credits](https://civitai.com/?query=model_search)\n\nView all models: [View Models](https://stablediffusionapi.com/models)\n\n    import requests  \n    import json  \n      \n    url =  \"https://stablediffusionapi.com/api/v3/dreambooth\"  \n      \n    payload = json.dumps({  \n    \"key\":  \"\",  \n    \"model_id\":  \"edge-of-realism\",  \n    \"prompt\":  \"actual 8K portrait photo of gareth person, portrait, happy colors, bright eyes, clear eyes, warm smile, smooth soft skin, big dreamy eyes, beautiful intricate colored hair, symmetrical, anime wide eyes, soft lighting, detailed face, by makoto shinkai, stanley artgerm lau, wlop, rossdraws, concept art, digital painting, looking into camera\",  \n    \"negative_prompt\":  \"painting, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, deformed, ugly, blurry, bad anatomy, bad proportions, extra limbs, cloned face, skinny, glitchy, double torso, extra arms, extra hands, mangled fingers, missing lips, ugly face, distorted face, extra legs, anime\",  \n    \"width\":  \"512\",  \n    \"height\":  \"512\",  \n    \"samples\":  \"1\",  \n    \"num_inference_steps\":  \"30\",  \n    \"safety_checker\":  \"no\",  \n    \"enhance_prompt\":  \"yes\",  \n    \"seed\":  None,  \n    \"guidance_scale\":  7.5,  \n    \"multi_lingual\":  \"no\",  \n    \"panorama\":  \"no\",  \n    \"self_attention\":  \"no\",  \n    \"upscale\":  \"no\",  \n    \"embeddings\":  \"embeddings_model_id\",  \n    \"lora\":  \"lora_model_id\",  \n    \"webhook\":  None,  \n    \"track_id\":  None  \n    })  \n      \n    headers =  {  \n    'Content-Type':  'application/json'  \n    }  \n      \n    response = requests.request(\"POST\", url, headers=headers, data=payload)  \n      \n    print(response.text)\n\n> Use this coupon code to get 25% off **DMGG0RBN**\n created at: 2023-04-22\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: xyn-ai/anything-v4.0\n  description: Welcome to Anything V4 - a latent diffusion model for weebs. The newest version of Anything. This model is intended to produce high-quality, highly detailed anime style with just a few prompts. Like other anime-style Stable Diffusion models, it also supports danbooru tags to generate images.\n\ne.g. **_1girl, white hair, golden eyes, beautiful eyes, detail, flower meadow, cumulonimbus clouds, lighting, detailed sky, garden_** \n\nI think the V4.5 version better though, it's in this repo. feel free 2 try it.\n created at: 2023-03-23\n- model_name: SG161222/Realistic_Vision_V2.0\n  description: None\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: stabilityai/stable-diffusion-2-1-unclip\n  description: This `stable-diffusion-2-1-unclip` is a finetuned version of Stable Diffusion 2.1, modified to accept (noisy) CLIP image embedding in addition to the text prompt, and can be used to create image variations (Examples) or can be chained with text-to-image CLIP priors. The amount of noise added to the image embedding can be specified via the noise_level (0 means no noise, 1000 full noise).\n\n- Use it with üß® [`diffusers`](#examples)\n created at: 2023-03-20\n- model_name: black-forest-labs/FLUX.1-Kontext-dev\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-05-28T22:23:43+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-kontext-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of editing images based on text instructions. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-06-28T22:57:58+00:00\n- model_name: bullerwins/FLUX.1-Kontext-dev-GGUF\n  description: created with example workflow added in files `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-26T16:26:45+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-kontext-dev\n  description: this repository contains nunchaku-quantized versions of , capable of editing images based on text instructions. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:52:52+00:00\n- model_name: unsloth/FLUX.1-Kontext-dev-GGUF\n  description: gguf files for . original license still applies. a huge thanks to and for help with conversion. ggufs were created with and . to run, use comfyui-gguf and place the model files in the `comfyui/models/unet` directory. for more instructions, go to the readme. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-30T10:10:09+00:00\n- model_name: Owen777/Kontext-Style-Loras\n  description: this repository provides a collection of 20+ style lora adapters for the flux.1 kontext model, enabling a wide range of artistic and cartoon styles for high-quality image-to-image generation. these loras are trained on high-quality paired data generated by gpt-4o. srouce training code:\n created at: 2025-07-04T06:54:56+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-fill-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of filling areas in existing images based on a text description. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:19:13+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-fill-dev\n  description: this repository contains nunchaku-quantized versions of , capable of filling areas in existing images based on a text description. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:43:28+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-canny-dev\n  description: this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:48:02+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-depth-dev\n  description: this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:45:46+00:00\n- model_name: fuliucansheng/FLUX.1-Kontext-dev-diffusers\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-28T19:05:56+00:00\n- model_name: AIDC-AI/Ovis-U1-3B\n  description: building on the foundation of the ovis series, ovis-u1 is a 3-billion-parameter unified model that seamlessly integrates multimodal understanding, text-to-image generation, and image editing within a single powerful framework. the overall architecture of ovis-u1 cf. fig.2 in our report. ovis-u1 has been tested with python 3.10, torch 2.4.0, transformers 4.51.3, and deepspeed 0.15.4. for a comprehensive list of package dependencies, please consult the requirements.txt file.\n created at: 2025-06-28T12:00:29+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-canny-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:07:43+00:00\n- model_name: fofr/kontext-make-person-real\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on using: you should use `make this person look real` as part of the prompt instruction for your image-to-image editing.\n created at: 2025-07-19T14:35:21+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-depth-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:11:47+00:00\n- model_name: xiaolxl/HXHY-Asian-style-realism-Realistic-KontextLoRA\n  description: :nsfw,kontext,,„ÄÇ model statement: the training material does not contain any nsfw or minor characters, and the training material is only used to adjust the effect of the original kontext model. when you use this model, the output results will be generated based on the images you use, so please do not use it for any illegal or violation purposes.\n created at: 2025-07-07T02:38:04+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-schnell-pix2pix-turbo\n  description: this repository contains loras for both original and nunchaku-quantized to translate sketch to images from user prompts. - developed by: nunchaku team, cmu generative intelligence lab - model type: image-to-image - developed by: nunchaku team, cmu generative intelligence lab - model type: image-to-image - license: apache-2.0 - quantized from model:\n created at: 2025-07-21T04:53:10+00:00\n- model_name: fofr/flux-kontext-dev-ps1-lora\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on . you should use `render this image like a ps1 game no ui` to trigger the image generation.\n created at: 2025-07-12T19:02:05+00:00\n- model_name: AlekseyCalvin/QWEN_IMAGE_nf4_w_AbliteratedTE_Diffusers\n  description: this repo contains a variant of qwen's , the state-of-the-art generative model with extensive and image/text-to-image &/or instruction/control-editing capabilities. to make these cutting edge capabilities more accessible to those constrained to low-end consumer-grade hardware, we've quantized the dit diffusion transformer component of qwen-image to the 4-bit nf4 format using the bits&bytes toolkit. this optimization was derived by us directly from the bf16 base model weights released on 08/04/2025, with no other mix-ins or modifications to the dit component.\n created at: 2025-08-05T12:00:18+00:00\n- model_name: lucataco/kontext-realearth\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on using: you should use `make this a realistic drone photo` as part of the prompt instruction for your image-to-image editing.\n created at: 2025-07-22T20:28:00+00:00\n\nExamples:\n- example user query: I am a digital artist. I want to transform my artworks to a different style or representation.\n  model_name: GreeneryScenery/SheepsControlV5\n- example user query: We need a new anime character sketch for our story. The character should be a warrior girl in the woods.\n  model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n- example user query: A social media platform wants to create AI-generated profile pictures for its users based on their description. We need to provide a way to generate these images based on their descriptions.\n  model_name: lllyasviel/control_v11p_sd15_normalbae\n- example user query: Create a program that produces a new image by reimagining the input image. Modify the input image with guidance like \"add more trees\" or \"make the sky darker.\"\n  model_name: GreeneryScenery/SheepsControlV3\n- example user query: We want to create a visual scene from this story snippet: \"In the enchanted forest, a magical unicorn grazes near a crystal-clear pond.\"\n  model_name: lllyasviel/control_v11p_sd15_scribble\n- example user query: I need a model to generate images based on text descriptions with additional input conditions, like containing soft edges.\n  model_name: lllyasviel/control_v11p_sd15_softedge\n\nUser prompt:\nDocumenting the story told by the segmentation image and the description 'a busy marketplace', create an image.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <controllable_image_generation>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: runwayml/stable-diffusion-v1-5\n  description: ### ‚ö†Ô∏è This repository is a mirror of the now deprecated `ruwnayml/stable-diffusion-v1-5`, this repository or organization are not affiliated in any way with RunwayML.\nModifications to the original model card are in <span style=\"color:crimson\">red</span> or <span style=\"color:darkgreen\">green</span>\n\nStable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.\nFor more information about how Stable Diffusion functions, please have a look at [ü§ó's Stable Diffusion blog](https://huggingface.co/blog/stable_diffusion).\n\nThe **Stable-Diffusion-v1-5** checkpoint was initialized with the weights of the [Stable-Diffusion-v1-2](https:/steps/huggingface.co/CompVis/stable-diffusion-v1-2) \ncheckpoint and subsequently fine-tuned on 595k steps at resolution 512x512 on \"laion-aesthetics v2 5+\" and 10% dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598).\n\nYou can use this both with the [üß®Diffusers library](https://github.com/huggingface/diffusers) and [RunwayML GitHub repository](https://github.com/runwayml/stable-diffusion) (<span style=\"color:crimson\">now deprecated</span>), <span style=\"color:darkgreen\">ComfyUI, Automatic1111, SD.Next, InvokeAI</span>.\n\n### Use with Diffusers\n```py\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = \"sd-legacy/stable-diffusion-v1-5\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]  \n    \nimage.save(\"astronaut_rides_horse.png\")\n```\nFor more detailed instructions, use-cases and examples in JAX follow the instructions [here](https://github.com/huggingface/diffusers#text-to-image-generation-with-stable-diffusion)\n\n### Use with GitHub Repository <span style=\"color:crimson\">(now deprecated)</span>, <span style=\"color:darkgreen\">ComfyUI or Automatic1111</span>\n\n1. Download the weights \n   - [v1-5-pruned-emaonly.safetensors](https://huggingface.co/sd-legacy/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.safetensors) - ema-only weight. uses less VRAM - suitable for inference\n   - [v1-5-pruned.safetensors](https://huggingface.co/sd-legacy/stable-diffusion-v1-5/resolve/main/v1-5-pruned.safetensors) - ema+non-ema weights. uses more VRAM - suitable for fine-tuning\n\n2. Follow instructions [here](https://github.com/runwayml/stable-diffusion). <span style=\"color:crimson\">(now deprecated)</span>\n\n3. <span style=\"color:darkgreen\">Use locally with <a href=\"https://github.com/comfyanonymous/ComfyUI\">ComfyUI</a>, <a href=\"https://github.com/AUTOMATIC1111/stable-diffusion-webui\">AUTOMATIC1111</a>, <a href=\"https://github.com/vladmandic/automatic\">SD.Next</a>, <a href=\"https://github.com/invoke-ai/InvokeAI\">InvokeAI</a></span>\n created at: 2024-08-30\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: lodestones/P.A.W.F.E.C.T-Alpha\n  description: ## Discord and Help\n[furry diffusion discord](https://discord.gg/furrydiffusion), in the \"custom-models\" channel\n\n[lodestone rock's server](https://discord.gg/SQVcWVbqKx)\n created at: 2023-07-02\n- model_name: kandinsky-community/kandinsky-2-2-controlnet-depth\n  description: Kandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.\n\nIt uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\n\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov)\n created at: 2023-06-28\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: kandinsky-community/kandinsky-2-2-prior\n  description: Kandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.\n\nIt uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\n\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov)\n created at: 2023-06-09\n- model_name: kandinsky-community/kandinsky-2-2-decoder\n  description: Kandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.\n\nIt uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\n\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov)\n created at: 2023-06-09\n- model_name: kandinsky-community/kandinsky-2-1\n  description: Kandinsky 2.1 inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.\n\nIt uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\n\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov)\n created at: 2023-05-24\n- model_name: logo-wizard/logo-diffusion-checkpoint\n  description: # Best practices\nWe recommend using this model with the following prompt template:  \n**positive:** f\"a logo of {company industry}, {some objects}, {colors}, modern, minimalism, vector art, 2d, best quality, centered\"  \n**negative:** \"low quality, worst quality, bad composition, extra digit, fewer digits, text, inscription, watermark, label, asymmetric\"  \n  \nSome other recommendations:  \n**num_inference_steps** = *30*  \n**guidance_scale** = *7.5*  \n**height** = *768*  \n**width** = *768*  \n**scheduler** = diffusers.EulerAncestralDiscreteScheduler (used by default)\n\n![results](./results.png)\n created at: 2023-05-05\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: stablediffusionapi/edge-of-realism\n  description: ![generated from modelslab.com](https://assets.modelslab.com/generations/d3d3f607-e8c6-4758-903a-17804fb4002b-0.png)\n## Get API Key\n\nGet API key from [ModelsLab](https://modelslab.com/), No Payment needed. \n\nReplace Key in below code, change **model_id**  to \"edge-of-realism\"\n\nCoding in PHP/Node/Java etc? Have a look at docs for more code examples: [View docs](https://stablediffusionapi.com/docs)\n\nModel link: [View model](https://stablediffusionapi.com/models/edge-of-realism)\n\nCredits: [View credits](https://civitai.com/?query=model_search)\n\nView all models: [View Models](https://stablediffusionapi.com/models)\n\n    import requests  \n    import json  \n      \n    url =  \"https://stablediffusionapi.com/api/v3/dreambooth\"  \n      \n    payload = json.dumps({  \n    \"key\":  \"\",  \n    \"model_id\":  \"edge-of-realism\",  \n    \"prompt\":  \"actual 8K portrait photo of gareth person, portrait, happy colors, bright eyes, clear eyes, warm smile, smooth soft skin, big dreamy eyes, beautiful intricate colored hair, symmetrical, anime wide eyes, soft lighting, detailed face, by makoto shinkai, stanley artgerm lau, wlop, rossdraws, concept art, digital painting, looking into camera\",  \n    \"negative_prompt\":  \"painting, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, deformed, ugly, blurry, bad anatomy, bad proportions, extra limbs, cloned face, skinny, glitchy, double torso, extra arms, extra hands, mangled fingers, missing lips, ugly face, distorted face, extra legs, anime\",  \n    \"width\":  \"512\",  \n    \"height\":  \"512\",  \n    \"samples\":  \"1\",  \n    \"num_inference_steps\":  \"30\",  \n    \"safety_checker\":  \"no\",  \n    \"enhance_prompt\":  \"yes\",  \n    \"seed\":  None,  \n    \"guidance_scale\":  7.5,  \n    \"multi_lingual\":  \"no\",  \n    \"panorama\":  \"no\",  \n    \"self_attention\":  \"no\",  \n    \"upscale\":  \"no\",  \n    \"embeddings\":  \"embeddings_model_id\",  \n    \"lora\":  \"lora_model_id\",  \n    \"webhook\":  None,  \n    \"track_id\":  None  \n    })  \n      \n    headers =  {  \n    'Content-Type':  'application/json'  \n    }  \n      \n    response = requests.request(\"POST\", url, headers=headers, data=payload)  \n      \n    print(response.text)\n\n> Use this coupon code to get 25% off **DMGG0RBN**\n created at: 2023-04-22\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: xyn-ai/anything-v4.0\n  description: Welcome to Anything V4 - a latent diffusion model for weebs. The newest version of Anything. This model is intended to produce high-quality, highly detailed anime style with just a few prompts. Like other anime-style Stable Diffusion models, it also supports danbooru tags to generate images.\n\ne.g. **_1girl, white hair, golden eyes, beautiful eyes, detail, flower meadow, cumulonimbus clouds, lighting, detailed sky, garden_** \n\nI think the V4.5 version better though, it's in this repo. feel free 2 try it.\n created at: 2023-03-23\n- model_name: SG161222/Realistic_Vision_V2.0\n  description: None\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: stabilityai/stable-diffusion-2-1-unclip\n  description: This `stable-diffusion-2-1-unclip` is a finetuned version of Stable Diffusion 2.1, modified to accept (noisy) CLIP image embedding in addition to the text prompt, and can be used to create image variations (Examples) or can be chained with text-to-image CLIP priors. The amount of noise added to the image embedding can be specified via the noise_level (0 means no noise, 1000 full noise).\n\n- Use it with üß® [`diffusers`](#examples)\n created at: 2023-03-20\n- model_name: black-forest-labs/FLUX.1-Kontext-dev\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-05-28T22:23:43+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-kontext-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of editing images based on text instructions. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-06-28T22:57:58+00:00\n- model_name: bullerwins/FLUX.1-Kontext-dev-GGUF\n  description: created with example workflow added in files `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-26T16:26:45+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-kontext-dev\n  description: this repository contains nunchaku-quantized versions of , capable of editing images based on text instructions. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:52:52+00:00\n- model_name: unsloth/FLUX.1-Kontext-dev-GGUF\n  description: gguf files for . original license still applies. a huge thanks to and for help with conversion. ggufs were created with and . to run, use comfyui-gguf and place the model files in the `comfyui/models/unet` directory. for more instructions, go to the readme. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-30T10:10:09+00:00\n- model_name: Owen777/Kontext-Style-Loras\n  description: this repository provides a collection of 20+ style lora adapters for the flux.1 kontext model, enabling a wide range of artistic and cartoon styles for high-quality image-to-image generation. these loras are trained on high-quality paired data generated by gpt-4o. srouce training code:\n created at: 2025-07-04T06:54:56+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-fill-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of filling areas in existing images based on a text description. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:19:13+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-fill-dev\n  description: this repository contains nunchaku-quantized versions of , capable of filling areas in existing images based on a text description. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:43:28+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-canny-dev\n  description: this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:48:02+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-depth-dev\n  description: this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:45:46+00:00\n- model_name: fuliucansheng/FLUX.1-Kontext-dev-diffusers\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-28T19:05:56+00:00\n- model_name: AIDC-AI/Ovis-U1-3B\n  description: building on the foundation of the ovis series, ovis-u1 is a 3-billion-parameter unified model that seamlessly integrates multimodal understanding, text-to-image generation, and image editing within a single powerful framework. the overall architecture of ovis-u1 cf. fig.2 in our report. ovis-u1 has been tested with python 3.10, torch 2.4.0, transformers 4.51.3, and deepspeed 0.15.4. for a comprehensive list of package dependencies, please consult the requirements.txt file.\n created at: 2025-06-28T12:00:29+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-canny-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:07:43+00:00\n- model_name: fofr/kontext-make-person-real\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on using: you should use `make this person look real` as part of the prompt instruction for your image-to-image editing.\n created at: 2025-07-19T14:35:21+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-depth-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:11:47+00:00\n- model_name: xiaolxl/HXHY-Asian-style-realism-Realistic-KontextLoRA\n  description: :nsfw,kontext,,„ÄÇ model statement: the training material does not contain any nsfw or minor characters, and the training material is only used to adjust the effect of the original kontext model. when you use this model, the output results will be generated based on the images you use, so please do not use it for any illegal or violation purposes.\n created at: 2025-07-07T02:38:04+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-schnell-pix2pix-turbo\n  description: this repository contains loras for both original and nunchaku-quantized to translate sketch to images from user prompts. - developed by: nunchaku team, cmu generative intelligence lab - model type: image-to-image - developed by: nunchaku team, cmu generative intelligence lab - model type: image-to-image - license: apache-2.0 - quantized from model:\n created at: 2025-07-21T04:53:10+00:00\n- model_name: fofr/flux-kontext-dev-ps1-lora\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on . you should use `render this image like a ps1 game no ui` to trigger the image generation.\n created at: 2025-07-12T19:02:05+00:00\n- model_name: AlekseyCalvin/QWEN_IMAGE_nf4_w_AbliteratedTE_Diffusers\n  description: this repo contains a variant of qwen's , the state-of-the-art generative model with extensive and image/text-to-image &/or instruction/control-editing capabilities. to make these cutting edge capabilities more accessible to those constrained to low-end consumer-grade hardware, we've quantized the dit diffusion transformer component of qwen-image to the 4-bit nf4 format using the bits&bytes toolkit. this optimization was derived by us directly from the bf16 base model weights released on 08/04/2025, with no other mix-ins or modifications to the dit component.\n created at: 2025-08-05T12:00:18+00:00\n- model_name: lucataco/kontext-realearth\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on using: you should use `make this a realistic drone photo` as part of the prompt instruction for your image-to-image editing.\n created at: 2025-07-22T20:28:00+00:00\n\nExamples:\n- example user query: I am a digital artist. I want to transform my artworks to a different style or representation.\n  model_name: GreeneryScenery/SheepsControlV5\n- example user query: We need a new anime character sketch for our story. The character should be a warrior girl in the woods.\n  model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n- example user query: A social media platform wants to create AI-generated profile pictures for its users based on their description. We need to provide a way to generate these images based on their descriptions.\n  model_name: lllyasviel/control_v11p_sd15_normalbae\n- example user query: Create a program that produces a new image by reimagining the input image. Modify the input image with guidance like \"add more trees\" or \"make the sky darker.\"\n  model_name: GreeneryScenery/SheepsControlV3\n- example user query: We want to create a visual scene from this story snippet: \"In the enchanted forest, a magical unicorn grazes near a crystal-clear pond.\"\n  model_name: lllyasviel/control_v11p_sd15_scribble\n- example user query: I need a model to generate images based on text descriptions with additional input conditions, like containing soft edges.\n  model_name: lllyasviel/control_v11p_sd15_softedge\n\nUser prompt:\nDocumenting the story told by the depth imagery and the prompt 'a cozy cabin', generate an image.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <controllable_image_generation>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: runwayml/stable-diffusion-v1-5\n  description: ### ‚ö†Ô∏è This repository is a mirror of the now deprecated `ruwnayml/stable-diffusion-v1-5`, this repository or organization are not affiliated in any way with RunwayML.\nModifications to the original model card are in <span style=\"color:crimson\">red</span> or <span style=\"color:darkgreen\">green</span>\n\nStable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.\nFor more information about how Stable Diffusion functions, please have a look at [ü§ó's Stable Diffusion blog](https://huggingface.co/blog/stable_diffusion).\n\nThe **Stable-Diffusion-v1-5** checkpoint was initialized with the weights of the [Stable-Diffusion-v1-2](https:/steps/huggingface.co/CompVis/stable-diffusion-v1-2) \ncheckpoint and subsequently fine-tuned on 595k steps at resolution 512x512 on \"laion-aesthetics v2 5+\" and 10% dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598).\n\nYou can use this both with the [üß®Diffusers library](https://github.com/huggingface/diffusers) and [RunwayML GitHub repository](https://github.com/runwayml/stable-diffusion) (<span style=\"color:crimson\">now deprecated</span>), <span style=\"color:darkgreen\">ComfyUI, Automatic1111, SD.Next, InvokeAI</span>.\n\n### Use with Diffusers\n```py\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = \"sd-legacy/stable-diffusion-v1-5\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]  \n    \nimage.save(\"astronaut_rides_horse.png\")\n```\nFor more detailed instructions, use-cases and examples in JAX follow the instructions [here](https://github.com/huggingface/diffusers#text-to-image-generation-with-stable-diffusion)\n\n### Use with GitHub Repository <span style=\"color:crimson\">(now deprecated)</span>, <span style=\"color:darkgreen\">ComfyUI or Automatic1111</span>\n\n1. Download the weights \n   - [v1-5-pruned-emaonly.safetensors](https://huggingface.co/sd-legacy/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.safetensors) - ema-only weight. uses less VRAM - suitable for inference\n   - [v1-5-pruned.safetensors](https://huggingface.co/sd-legacy/stable-diffusion-v1-5/resolve/main/v1-5-pruned.safetensors) - ema+non-ema weights. uses more VRAM - suitable for fine-tuning\n\n2. Follow instructions [here](https://github.com/runwayml/stable-diffusion). <span style=\"color:crimson\">(now deprecated)</span>\n\n3. <span style=\"color:darkgreen\">Use locally with <a href=\"https://github.com/comfyanonymous/ComfyUI\">ComfyUI</a>, <a href=\"https://github.com/AUTOMATIC1111/stable-diffusion-webui\">AUTOMATIC1111</a>, <a href=\"https://github.com/vladmandic/automatic\">SD.Next</a>, <a href=\"https://github.com/invoke-ai/InvokeAI\">InvokeAI</a></span>\n created at: 2024-08-30\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: lodestones/P.A.W.F.E.C.T-Alpha\n  description: ## Discord and Help\n[furry diffusion discord](https://discord.gg/furrydiffusion), in the \"custom-models\" channel\n\n[lodestone rock's server](https://discord.gg/SQVcWVbqKx)\n created at: 2023-07-02\n- model_name: kandinsky-community/kandinsky-2-2-controlnet-depth\n  description: Kandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.\n\nIt uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\n\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov)\n created at: 2023-06-28\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: kandinsky-community/kandinsky-2-2-prior\n  description: Kandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.\n\nIt uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\n\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov)\n created at: 2023-06-09\n- model_name: kandinsky-community/kandinsky-2-2-decoder\n  description: Kandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.\n\nIt uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\n\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov)\n created at: 2023-06-09\n- model_name: kandinsky-community/kandinsky-2-1\n  description: Kandinsky 2.1 inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.\n\nIt uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\n\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov)\n created at: 2023-05-24\n- model_name: logo-wizard/logo-diffusion-checkpoint\n  description: # Best practices\nWe recommend using this model with the following prompt template:  \n**positive:** f\"a logo of {company industry}, {some objects}, {colors}, modern, minimalism, vector art, 2d, best quality, centered\"  \n**negative:** \"low quality, worst quality, bad composition, extra digit, fewer digits, text, inscription, watermark, label, asymmetric\"  \n  \nSome other recommendations:  \n**num_inference_steps** = *30*  \n**guidance_scale** = *7.5*  \n**height** = *768*  \n**width** = *768*  \n**scheduler** = diffusers.EulerAncestralDiscreteScheduler (used by default)\n\n![results](./results.png)\n created at: 2023-05-05\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: stablediffusionapi/edge-of-realism\n  description: ![generated from modelslab.com](https://assets.modelslab.com/generations/d3d3f607-e8c6-4758-903a-17804fb4002b-0.png)\n## Get API Key\n\nGet API key from [ModelsLab](https://modelslab.com/), No Payment needed. \n\nReplace Key in below code, change **model_id**  to \"edge-of-realism\"\n\nCoding in PHP/Node/Java etc? Have a look at docs for more code examples: [View docs](https://stablediffusionapi.com/docs)\n\nModel link: [View model](https://stablediffusionapi.com/models/edge-of-realism)\n\nCredits: [View credits](https://civitai.com/?query=model_search)\n\nView all models: [View Models](https://stablediffusionapi.com/models)\n\n    import requests  \n    import json  \n      \n    url =  \"https://stablediffusionapi.com/api/v3/dreambooth\"  \n      \n    payload = json.dumps({  \n    \"key\":  \"\",  \n    \"model_id\":  \"edge-of-realism\",  \n    \"prompt\":  \"actual 8K portrait photo of gareth person, portrait, happy colors, bright eyes, clear eyes, warm smile, smooth soft skin, big dreamy eyes, beautiful intricate colored hair, symmetrical, anime wide eyes, soft lighting, detailed face, by makoto shinkai, stanley artgerm lau, wlop, rossdraws, concept art, digital painting, looking into camera\",  \n    \"negative_prompt\":  \"painting, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, deformed, ugly, blurry, bad anatomy, bad proportions, extra limbs, cloned face, skinny, glitchy, double torso, extra arms, extra hands, mangled fingers, missing lips, ugly face, distorted face, extra legs, anime\",  \n    \"width\":  \"512\",  \n    \"height\":  \"512\",  \n    \"samples\":  \"1\",  \n    \"num_inference_steps\":  \"30\",  \n    \"safety_checker\":  \"no\",  \n    \"enhance_prompt\":  \"yes\",  \n    \"seed\":  None,  \n    \"guidance_scale\":  7.5,  \n    \"multi_lingual\":  \"no\",  \n    \"panorama\":  \"no\",  \n    \"self_attention\":  \"no\",  \n    \"upscale\":  \"no\",  \n    \"embeddings\":  \"embeddings_model_id\",  \n    \"lora\":  \"lora_model_id\",  \n    \"webhook\":  None,  \n    \"track_id\":  None  \n    })  \n      \n    headers =  {  \n    'Content-Type':  'application/json'  \n    }  \n      \n    response = requests.request(\"POST\", url, headers=headers, data=payload)  \n      \n    print(response.text)\n\n> Use this coupon code to get 25% off **DMGG0RBN**\n created at: 2023-04-22\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: xyn-ai/anything-v4.0\n  description: Welcome to Anything V4 - a latent diffusion model for weebs. The newest version of Anything. This model is intended to produce high-quality, highly detailed anime style with just a few prompts. Like other anime-style Stable Diffusion models, it also supports danbooru tags to generate images.\n\ne.g. **_1girl, white hair, golden eyes, beautiful eyes, detail, flower meadow, cumulonimbus clouds, lighting, detailed sky, garden_** \n\nI think the V4.5 version better though, it's in this repo. feel free 2 try it.\n created at: 2023-03-23\n- model_name: SG161222/Realistic_Vision_V2.0\n  description: None\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: stabilityai/stable-diffusion-2-1-unclip\n  description: This `stable-diffusion-2-1-unclip` is a finetuned version of Stable Diffusion 2.1, modified to accept (noisy) CLIP image embedding in addition to the text prompt, and can be used to create image variations (Examples) or can be chained with text-to-image CLIP priors. The amount of noise added to the image embedding can be specified via the noise_level (0 means no noise, 1000 full noise).\n\n- Use it with üß® [`diffusers`](#examples)\n created at: 2023-03-20\n- model_name: black-forest-labs/FLUX.1-Kontext-dev\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-05-28T22:23:43+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-kontext-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of editing images based on text instructions. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-06-28T22:57:58+00:00\n- model_name: bullerwins/FLUX.1-Kontext-dev-GGUF\n  description: created with example workflow added in files `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-26T16:26:45+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-kontext-dev\n  description: this repository contains nunchaku-quantized versions of , capable of editing images based on text instructions. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:52:52+00:00\n- model_name: unsloth/FLUX.1-Kontext-dev-GGUF\n  description: gguf files for . original license still applies. a huge thanks to and for help with conversion. ggufs were created with and . to run, use comfyui-gguf and place the model files in the `comfyui/models/unet` directory. for more instructions, go to the readme. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-30T10:10:09+00:00\n- model_name: Owen777/Kontext-Style-Loras\n  description: this repository provides a collection of 20+ style lora adapters for the flux.1 kontext model, enabling a wide range of artistic and cartoon styles for high-quality image-to-image generation. these loras are trained on high-quality paired data generated by gpt-4o. srouce training code:\n created at: 2025-07-04T06:54:56+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-fill-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of filling areas in existing images based on a text description. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:19:13+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-fill-dev\n  description: this repository contains nunchaku-quantized versions of , capable of filling areas in existing images based on a text description. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:43:28+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-canny-dev\n  description: this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:48:02+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-depth-dev\n  description: this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:45:46+00:00\n- model_name: fuliucansheng/FLUX.1-Kontext-dev-diffusers\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-28T19:05:56+00:00\n- model_name: AIDC-AI/Ovis-U1-3B\n  description: building on the foundation of the ovis series, ovis-u1 is a 3-billion-parameter unified model that seamlessly integrates multimodal understanding, text-to-image generation, and image editing within a single powerful framework. the overall architecture of ovis-u1 cf. fig.2 in our report. ovis-u1 has been tested with python 3.10, torch 2.4.0, transformers 4.51.3, and deepspeed 0.15.4. for a comprehensive list of package dependencies, please consult the requirements.txt file.\n created at: 2025-06-28T12:00:29+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-canny-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:07:43+00:00\n- model_name: fofr/kontext-make-person-real\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on using: you should use `make this person look real` as part of the prompt instruction for your image-to-image editing.\n created at: 2025-07-19T14:35:21+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-depth-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:11:47+00:00\n- model_name: xiaolxl/HXHY-Asian-style-realism-Realistic-KontextLoRA\n  description: :nsfw,kontext,,„ÄÇ model statement: the training material does not contain any nsfw or minor characters, and the training material is only used to adjust the effect of the original kontext model. when you use this model, the output results will be generated based on the images you use, so please do not use it for any illegal or violation purposes.\n created at: 2025-07-07T02:38:04+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-schnell-pix2pix-turbo\n  description: this repository contains loras for both original and nunchaku-quantized to translate sketch to images from user prompts. - developed by: nunchaku team, cmu generative intelligence lab - model type: image-to-image - developed by: nunchaku team, cmu generative intelligence lab - model type: image-to-image - license: apache-2.0 - quantized from model:\n created at: 2025-07-21T04:53:10+00:00\n- model_name: fofr/flux-kontext-dev-ps1-lora\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on . you should use `render this image like a ps1 game no ui` to trigger the image generation.\n created at: 2025-07-12T19:02:05+00:00\n- model_name: AlekseyCalvin/QWEN_IMAGE_nf4_w_AbliteratedTE_Diffusers\n  description: this repo contains a variant of qwen's , the state-of-the-art generative model with extensive and image/text-to-image &/or instruction/control-editing capabilities. to make these cutting edge capabilities more accessible to those constrained to low-end consumer-grade hardware, we've quantized the dit diffusion transformer component of qwen-image to the 4-bit nf4 format using the bits&bytes toolkit. this optimization was derived by us directly from the bf16 base model weights released on 08/04/2025, with no other mix-ins or modifications to the dit component.\n created at: 2025-08-05T12:00:18+00:00\n- model_name: lucataco/kontext-realearth\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on using: you should use `make this a realistic drone photo` as part of the prompt instruction for your image-to-image editing.\n created at: 2025-07-22T20:28:00+00:00\n\nExamples:\n- example user query: I am a digital artist. I want to transform my artworks to a different style or representation.\n  model_name: GreeneryScenery/SheepsControlV5\n- example user query: We need a new anime character sketch for our story. The character should be a warrior girl in the woods.\n  model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n- example user query: A social media platform wants to create AI-generated profile pictures for its users based on their description. We need to provide a way to generate these images based on their descriptions.\n  model_name: lllyasviel/control_v11p_sd15_normalbae\n- example user query: Create a program that produces a new image by reimagining the input image. Modify the input image with guidance like \"add more trees\" or \"make the sky darker.\"\n  model_name: GreeneryScenery/SheepsControlV3\n- example user query: We want to create a visual scene from this story snippet: \"In the enchanted forest, a magical unicorn grazes near a crystal-clear pond.\"\n  model_name: lllyasviel/control_v11p_sd15_scribble\n- example user query: I need a model to generate images based on text descriptions with additional input conditions, like containing soft edges.\n  model_name: lllyasviel/control_v11p_sd15_softedge\n\nUser prompt:\nInspired by the depth image and the description 'an underwater scene', create an image.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <controllable_image_generation>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: runwayml/stable-diffusion-v1-5\n  description: ### ‚ö†Ô∏è This repository is a mirror of the now deprecated `ruwnayml/stable-diffusion-v1-5`, this repository or organization are not affiliated in any way with RunwayML.\nModifications to the original model card are in <span style=\"color:crimson\">red</span> or <span style=\"color:darkgreen\">green</span>\n\nStable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.\nFor more information about how Stable Diffusion functions, please have a look at [ü§ó's Stable Diffusion blog](https://huggingface.co/blog/stable_diffusion).\n\nThe **Stable-Diffusion-v1-5** checkpoint was initialized with the weights of the [Stable-Diffusion-v1-2](https:/steps/huggingface.co/CompVis/stable-diffusion-v1-2) \ncheckpoint and subsequently fine-tuned on 595k steps at resolution 512x512 on \"laion-aesthetics v2 5+\" and 10% dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598).\n\nYou can use this both with the [üß®Diffusers library](https://github.com/huggingface/diffusers) and [RunwayML GitHub repository](https://github.com/runwayml/stable-diffusion) (<span style=\"color:crimson\">now deprecated</span>), <span style=\"color:darkgreen\">ComfyUI, Automatic1111, SD.Next, InvokeAI</span>.\n\n### Use with Diffusers\n```py\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = \"sd-legacy/stable-diffusion-v1-5\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]  \n    \nimage.save(\"astronaut_rides_horse.png\")\n```\nFor more detailed instructions, use-cases and examples in JAX follow the instructions [here](https://github.com/huggingface/diffusers#text-to-image-generation-with-stable-diffusion)\n\n### Use with GitHub Repository <span style=\"color:crimson\">(now deprecated)</span>, <span style=\"color:darkgreen\">ComfyUI or Automatic1111</span>\n\n1. Download the weights \n   - [v1-5-pruned-emaonly.safetensors](https://huggingface.co/sd-legacy/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.safetensors) - ema-only weight. uses less VRAM - suitable for inference\n   - [v1-5-pruned.safetensors](https://huggingface.co/sd-legacy/stable-diffusion-v1-5/resolve/main/v1-5-pruned.safetensors) - ema+non-ema weights. uses more VRAM - suitable for fine-tuning\n\n2. Follow instructions [here](https://github.com/runwayml/stable-diffusion). <span style=\"color:crimson\">(now deprecated)</span>\n\n3. <span style=\"color:darkgreen\">Use locally with <a href=\"https://github.com/comfyanonymous/ComfyUI\">ComfyUI</a>, <a href=\"https://github.com/AUTOMATIC1111/stable-diffusion-webui\">AUTOMATIC1111</a>, <a href=\"https://github.com/vladmandic/automatic\">SD.Next</a>, <a href=\"https://github.com/invoke-ai/InvokeAI\">InvokeAI</a></span>\n created at: 2024-08-30\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: lodestones/P.A.W.F.E.C.T-Alpha\n  description: ## Discord and Help\n[furry diffusion discord](https://discord.gg/furrydiffusion), in the \"custom-models\" channel\n\n[lodestone rock's server](https://discord.gg/SQVcWVbqKx)\n created at: 2023-07-02\n- model_name: kandinsky-community/kandinsky-2-2-controlnet-depth\n  description: Kandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.\n\nIt uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\n\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov)\n created at: 2023-06-28\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: kandinsky-community/kandinsky-2-2-prior\n  description: Kandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.\n\nIt uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\n\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov)\n created at: 2023-06-09\n- model_name: kandinsky-community/kandinsky-2-2-decoder\n  description: Kandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.\n\nIt uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\n\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov)\n created at: 2023-06-09\n- model_name: kandinsky-community/kandinsky-2-1\n  description: Kandinsky 2.1 inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.\n\nIt uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\n\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov)\n created at: 2023-05-24\n- model_name: logo-wizard/logo-diffusion-checkpoint\n  description: # Best practices\nWe recommend using this model with the following prompt template:  \n**positive:** f\"a logo of {company industry}, {some objects}, {colors}, modern, minimalism, vector art, 2d, best quality, centered\"  \n**negative:** \"low quality, worst quality, bad composition, extra digit, fewer digits, text, inscription, watermark, label, asymmetric\"  \n  \nSome other recommendations:  \n**num_inference_steps** = *30*  \n**guidance_scale** = *7.5*  \n**height** = *768*  \n**width** = *768*  \n**scheduler** = diffusers.EulerAncestralDiscreteScheduler (used by default)\n\n![results](./results.png)\n created at: 2023-05-05\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: stablediffusionapi/edge-of-realism\n  description: ![generated from modelslab.com](https://assets.modelslab.com/generations/d3d3f607-e8c6-4758-903a-17804fb4002b-0.png)\n## Get API Key\n\nGet API key from [ModelsLab](https://modelslab.com/), No Payment needed. \n\nReplace Key in below code, change **model_id**  to \"edge-of-realism\"\n\nCoding in PHP/Node/Java etc? Have a look at docs for more code examples: [View docs](https://stablediffusionapi.com/docs)\n\nModel link: [View model](https://stablediffusionapi.com/models/edge-of-realism)\n\nCredits: [View credits](https://civitai.com/?query=model_search)\n\nView all models: [View Models](https://stablediffusionapi.com/models)\n\n    import requests  \n    import json  \n      \n    url =  \"https://stablediffusionapi.com/api/v3/dreambooth\"  \n      \n    payload = json.dumps({  \n    \"key\":  \"\",  \n    \"model_id\":  \"edge-of-realism\",  \n    \"prompt\":  \"actual 8K portrait photo of gareth person, portrait, happy colors, bright eyes, clear eyes, warm smile, smooth soft skin, big dreamy eyes, beautiful intricate colored hair, symmetrical, anime wide eyes, soft lighting, detailed face, by makoto shinkai, stanley artgerm lau, wlop, rossdraws, concept art, digital painting, looking into camera\",  \n    \"negative_prompt\":  \"painting, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, deformed, ugly, blurry, bad anatomy, bad proportions, extra limbs, cloned face, skinny, glitchy, double torso, extra arms, extra hands, mangled fingers, missing lips, ugly face, distorted face, extra legs, anime\",  \n    \"width\":  \"512\",  \n    \"height\":  \"512\",  \n    \"samples\":  \"1\",  \n    \"num_inference_steps\":  \"30\",  \n    \"safety_checker\":  \"no\",  \n    \"enhance_prompt\":  \"yes\",  \n    \"seed\":  None,  \n    \"guidance_scale\":  7.5,  \n    \"multi_lingual\":  \"no\",  \n    \"panorama\":  \"no\",  \n    \"self_attention\":  \"no\",  \n    \"upscale\":  \"no\",  \n    \"embeddings\":  \"embeddings_model_id\",  \n    \"lora\":  \"lora_model_id\",  \n    \"webhook\":  None,  \n    \"track_id\":  None  \n    })  \n      \n    headers =  {  \n    'Content-Type':  'application/json'  \n    }  \n      \n    response = requests.request(\"POST\", url, headers=headers, data=payload)  \n      \n    print(response.text)\n\n> Use this coupon code to get 25% off **DMGG0RBN**\n created at: 2023-04-22\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: xyn-ai/anything-v4.0\n  description: Welcome to Anything V4 - a latent diffusion model for weebs. The newest version of Anything. This model is intended to produce high-quality, highly detailed anime style with just a few prompts. Like other anime-style Stable Diffusion models, it also supports danbooru tags to generate images.\n\ne.g. **_1girl, white hair, golden eyes, beautiful eyes, detail, flower meadow, cumulonimbus clouds, lighting, detailed sky, garden_** \n\nI think the V4.5 version better though, it's in this repo. feel free 2 try it.\n created at: 2023-03-23\n- model_name: SG161222/Realistic_Vision_V2.0\n  description: None\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: stabilityai/stable-diffusion-2-1-unclip\n  description: This `stable-diffusion-2-1-unclip` is a finetuned version of Stable Diffusion 2.1, modified to accept (noisy) CLIP image embedding in addition to the text prompt, and can be used to create image variations (Examples) or can be chained with text-to-image CLIP priors. The amount of noise added to the image embedding can be specified via the noise_level (0 means no noise, 1000 full noise).\n\n- Use it with üß® [`diffusers`](#examples)\n created at: 2023-03-20\n- model_name: black-forest-labs/FLUX.1-Kontext-dev\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-05-28T22:23:43+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-kontext-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of editing images based on text instructions. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-06-28T22:57:58+00:00\n- model_name: bullerwins/FLUX.1-Kontext-dev-GGUF\n  description: created with example workflow added in files `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-26T16:26:45+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-kontext-dev\n  description: this repository contains nunchaku-quantized versions of , capable of editing images based on text instructions. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:52:52+00:00\n- model_name: unsloth/FLUX.1-Kontext-dev-GGUF\n  description: gguf files for . original license still applies. a huge thanks to and for help with conversion. ggufs were created with and . to run, use comfyui-gguf and place the model files in the `comfyui/models/unet` directory. for more instructions, go to the readme. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-30T10:10:09+00:00\n- model_name: Owen777/Kontext-Style-Loras\n  description: this repository provides a collection of 20+ style lora adapters for the flux.1 kontext model, enabling a wide range of artistic and cartoon styles for high-quality image-to-image generation. these loras are trained on high-quality paired data generated by gpt-4o. srouce training code:\n created at: 2025-07-04T06:54:56+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-fill-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of filling areas in existing images based on a text description. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:19:13+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-fill-dev\n  description: this repository contains nunchaku-quantized versions of , capable of filling areas in existing images based on a text description. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:43:28+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-canny-dev\n  description: this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:48:02+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-depth-dev\n  description: this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:45:46+00:00\n- model_name: fuliucansheng/FLUX.1-Kontext-dev-diffusers\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-28T19:05:56+00:00\n- model_name: AIDC-AI/Ovis-U1-3B\n  description: building on the foundation of the ovis series, ovis-u1 is a 3-billion-parameter unified model that seamlessly integrates multimodal understanding, text-to-image generation, and image editing within a single powerful framework. the overall architecture of ovis-u1 cf. fig.2 in our report. ovis-u1 has been tested with python 3.10, torch 2.4.0, transformers 4.51.3, and deepspeed 0.15.4. for a comprehensive list of package dependencies, please consult the requirements.txt file.\n created at: 2025-06-28T12:00:29+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-canny-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:07:43+00:00\n- model_name: fofr/kontext-make-person-real\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on using: you should use `make this person look real` as part of the prompt instruction for your image-to-image editing.\n created at: 2025-07-19T14:35:21+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-depth-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:11:47+00:00\n- model_name: xiaolxl/HXHY-Asian-style-realism-Realistic-KontextLoRA\n  description: :nsfw,kontext,,„ÄÇ model statement: the training material does not contain any nsfw or minor characters, and the training material is only used to adjust the effect of the original kontext model. when you use this model, the output results will be generated based on the images you use, so please do not use it for any illegal or violation purposes.\n created at: 2025-07-07T02:38:04+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-schnell-pix2pix-turbo\n  description: this repository contains loras for both original and nunchaku-quantized to translate sketch to images from user prompts. - developed by: nunchaku team, cmu generative intelligence lab - model type: image-to-image - developed by: nunchaku team, cmu generative intelligence lab - model type: image-to-image - license: apache-2.0 - quantized from model:\n created at: 2025-07-21T04:53:10+00:00\n- model_name: fofr/flux-kontext-dev-ps1-lora\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on . you should use `render this image like a ps1 game no ui` to trigger the image generation.\n created at: 2025-07-12T19:02:05+00:00\n- model_name: AlekseyCalvin/QWEN_IMAGE_nf4_w_AbliteratedTE_Diffusers\n  description: this repo contains a variant of qwen's , the state-of-the-art generative model with extensive and image/text-to-image &/or instruction/control-editing capabilities. to make these cutting edge capabilities more accessible to those constrained to low-end consumer-grade hardware, we've quantized the dit diffusion transformer component of qwen-image to the 4-bit nf4 format using the bits&bytes toolkit. this optimization was derived by us directly from the bf16 base model weights released on 08/04/2025, with no other mix-ins or modifications to the dit component.\n created at: 2025-08-05T12:00:18+00:00\n- model_name: lucataco/kontext-realearth\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on using: you should use `make this a realistic drone photo` as part of the prompt instruction for your image-to-image editing.\n created at: 2025-07-22T20:28:00+00:00\n\nExamples:\n- example user query: I am a digital artist. I want to transform my artworks to a different style or representation.\n  model_name: GreeneryScenery/SheepsControlV5\n- example user query: We need a new anime character sketch for our story. The character should be a warrior girl in the woods.\n  model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n- example user query: A social media platform wants to create AI-generated profile pictures for its users based on their description. We need to provide a way to generate these images based on their descriptions.\n  model_name: lllyasviel/control_v11p_sd15_normalbae\n- example user query: Create a program that produces a new image by reimagining the input image. Modify the input image with guidance like \"add more trees\" or \"make the sky darker.\"\n  model_name: GreeneryScenery/SheepsControlV3\n- example user query: We want to create a visual scene from this story snippet: \"In the enchanted forest, a magical unicorn grazes near a crystal-clear pond.\"\n  model_name: lllyasviel/control_v11p_sd15_scribble\n- example user query: I need a model to generate images based on text descriptions with additional input conditions, like containing soft edges.\n  model_name: lllyasviel/control_v11p_sd15_softedge\n\nUser prompt:\nConjuring a new image from the pose image and the description 'a lively park', create an image.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <controllable_image_generation>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: runwayml/stable-diffusion-v1-5\n  description: ### ‚ö†Ô∏è This repository is a mirror of the now deprecated `ruwnayml/stable-diffusion-v1-5`, this repository or organization are not affiliated in any way with RunwayML.\nModifications to the original model card are in <span style=\"color:crimson\">red</span> or <span style=\"color:darkgreen\">green</span>\n\nStable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.\nFor more information about how Stable Diffusion functions, please have a look at [ü§ó's Stable Diffusion blog](https://huggingface.co/blog/stable_diffusion).\n\nThe **Stable-Diffusion-v1-5** checkpoint was initialized with the weights of the [Stable-Diffusion-v1-2](https:/steps/huggingface.co/CompVis/stable-diffusion-v1-2) \ncheckpoint and subsequently fine-tuned on 595k steps at resolution 512x512 on \"laion-aesthetics v2 5+\" and 10% dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598).\n\nYou can use this both with the [üß®Diffusers library](https://github.com/huggingface/diffusers) and [RunwayML GitHub repository](https://github.com/runwayml/stable-diffusion) (<span style=\"color:crimson\">now deprecated</span>), <span style=\"color:darkgreen\">ComfyUI, Automatic1111, SD.Next, InvokeAI</span>.\n\n### Use with Diffusers\n```py\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = \"sd-legacy/stable-diffusion-v1-5\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]  \n    \nimage.save(\"astronaut_rides_horse.png\")\n```\nFor more detailed instructions, use-cases and examples in JAX follow the instructions [here](https://github.com/huggingface/diffusers#text-to-image-generation-with-stable-diffusion)\n\n### Use with GitHub Repository <span style=\"color:crimson\">(now deprecated)</span>, <span style=\"color:darkgreen\">ComfyUI or Automatic1111</span>\n\n1. Download the weights \n   - [v1-5-pruned-emaonly.safetensors](https://huggingface.co/sd-legacy/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.safetensors) - ema-only weight. uses less VRAM - suitable for inference\n   - [v1-5-pruned.safetensors](https://huggingface.co/sd-legacy/stable-diffusion-v1-5/resolve/main/v1-5-pruned.safetensors) - ema+non-ema weights. uses more VRAM - suitable for fine-tuning\n\n2. Follow instructions [here](https://github.com/runwayml/stable-diffusion). <span style=\"color:crimson\">(now deprecated)</span>\n\n3. <span style=\"color:darkgreen\">Use locally with <a href=\"https://github.com/comfyanonymous/ComfyUI\">ComfyUI</a>, <a href=\"https://github.com/AUTOMATIC1111/stable-diffusion-webui\">AUTOMATIC1111</a>, <a href=\"https://github.com/vladmandic/automatic\">SD.Next</a>, <a href=\"https://github.com/invoke-ai/InvokeAI\">InvokeAI</a></span>\n created at: 2024-08-30\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: lodestones/P.A.W.F.E.C.T-Alpha\n  description: ## Discord and Help\n[furry diffusion discord](https://discord.gg/furrydiffusion), in the \"custom-models\" channel\n\n[lodestone rock's server](https://discord.gg/SQVcWVbqKx)\n created at: 2023-07-02\n- model_name: kandinsky-community/kandinsky-2-2-controlnet-depth\n  description: Kandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.\n\nIt uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\n\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov)\n created at: 2023-06-28\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: kandinsky-community/kandinsky-2-2-prior\n  description: Kandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.\n\nIt uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\n\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov)\n created at: 2023-06-09\n- model_name: kandinsky-community/kandinsky-2-2-decoder\n  description: Kandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.\n\nIt uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\n\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov)\n created at: 2023-06-09\n- model_name: kandinsky-community/kandinsky-2-1\n  description: Kandinsky 2.1 inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.\n\nIt uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\n\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov)\n created at: 2023-05-24\n- model_name: logo-wizard/logo-diffusion-checkpoint\n  description: # Best practices\nWe recommend using this model with the following prompt template:  \n**positive:** f\"a logo of {company industry}, {some objects}, {colors}, modern, minimalism, vector art, 2d, best quality, centered\"  \n**negative:** \"low quality, worst quality, bad composition, extra digit, fewer digits, text, inscription, watermark, label, asymmetric\"  \n  \nSome other recommendations:  \n**num_inference_steps** = *30*  \n**guidance_scale** = *7.5*  \n**height** = *768*  \n**width** = *768*  \n**scheduler** = diffusers.EulerAncestralDiscreteScheduler (used by default)\n\n![results](./results.png)\n created at: 2023-05-05\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: stablediffusionapi/edge-of-realism\n  description: ![generated from modelslab.com](https://assets.modelslab.com/generations/d3d3f607-e8c6-4758-903a-17804fb4002b-0.png)\n## Get API Key\n\nGet API key from [ModelsLab](https://modelslab.com/), No Payment needed. \n\nReplace Key in below code, change **model_id**  to \"edge-of-realism\"\n\nCoding in PHP/Node/Java etc? Have a look at docs for more code examples: [View docs](https://stablediffusionapi.com/docs)\n\nModel link: [View model](https://stablediffusionapi.com/models/edge-of-realism)\n\nCredits: [View credits](https://civitai.com/?query=model_search)\n\nView all models: [View Models](https://stablediffusionapi.com/models)\n\n    import requests  \n    import json  \n      \n    url =  \"https://stablediffusionapi.com/api/v3/dreambooth\"  \n      \n    payload = json.dumps({  \n    \"key\":  \"\",  \n    \"model_id\":  \"edge-of-realism\",  \n    \"prompt\":  \"actual 8K portrait photo of gareth person, portrait, happy colors, bright eyes, clear eyes, warm smile, smooth soft skin, big dreamy eyes, beautiful intricate colored hair, symmetrical, anime wide eyes, soft lighting, detailed face, by makoto shinkai, stanley artgerm lau, wlop, rossdraws, concept art, digital painting, looking into camera\",  \n    \"negative_prompt\":  \"painting, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, deformed, ugly, blurry, bad anatomy, bad proportions, extra limbs, cloned face, skinny, glitchy, double torso, extra arms, extra hands, mangled fingers, missing lips, ugly face, distorted face, extra legs, anime\",  \n    \"width\":  \"512\",  \n    \"height\":  \"512\",  \n    \"samples\":  \"1\",  \n    \"num_inference_steps\":  \"30\",  \n    \"safety_checker\":  \"no\",  \n    \"enhance_prompt\":  \"yes\",  \n    \"seed\":  None,  \n    \"guidance_scale\":  7.5,  \n    \"multi_lingual\":  \"no\",  \n    \"panorama\":  \"no\",  \n    \"self_attention\":  \"no\",  \n    \"upscale\":  \"no\",  \n    \"embeddings\":  \"embeddings_model_id\",  \n    \"lora\":  \"lora_model_id\",  \n    \"webhook\":  None,  \n    \"track_id\":  None  \n    })  \n      \n    headers =  {  \n    'Content-Type':  'application/json'  \n    }  \n      \n    response = requests.request(\"POST\", url, headers=headers, data=payload)  \n      \n    print(response.text)\n\n> Use this coupon code to get 25% off **DMGG0RBN**\n created at: 2023-04-22\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: xyn-ai/anything-v4.0\n  description: Welcome to Anything V4 - a latent diffusion model for weebs. The newest version of Anything. This model is intended to produce high-quality, highly detailed anime style with just a few prompts. Like other anime-style Stable Diffusion models, it also supports danbooru tags to generate images.\n\ne.g. **_1girl, white hair, golden eyes, beautiful eyes, detail, flower meadow, cumulonimbus clouds, lighting, detailed sky, garden_** \n\nI think the V4.5 version better though, it's in this repo. feel free 2 try it.\n created at: 2023-03-23\n- model_name: SG161222/Realistic_Vision_V2.0\n  description: None\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: stabilityai/stable-diffusion-2-1-unclip\n  description: This `stable-diffusion-2-1-unclip` is a finetuned version of Stable Diffusion 2.1, modified to accept (noisy) CLIP image embedding in addition to the text prompt, and can be used to create image variations (Examples) or can be chained with text-to-image CLIP priors. The amount of noise added to the image embedding can be specified via the noise_level (0 means no noise, 1000 full noise).\n\n- Use it with üß® [`diffusers`](#examples)\n created at: 2023-03-20\n- model_name: black-forest-labs/FLUX.1-Kontext-dev\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-05-28T22:23:43+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-kontext-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of editing images based on text instructions. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-06-28T22:57:58+00:00\n- model_name: bullerwins/FLUX.1-Kontext-dev-GGUF\n  description: created with example workflow added in files `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-26T16:26:45+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-kontext-dev\n  description: this repository contains nunchaku-quantized versions of , capable of editing images based on text instructions. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:52:52+00:00\n- model_name: unsloth/FLUX.1-Kontext-dev-GGUF\n  description: gguf files for . original license still applies. a huge thanks to and for help with conversion. ggufs were created with and . to run, use comfyui-gguf and place the model files in the `comfyui/models/unet` directory. for more instructions, go to the readme. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-30T10:10:09+00:00\n- model_name: Owen777/Kontext-Style-Loras\n  description: this repository provides a collection of 20+ style lora adapters for the flux.1 kontext model, enabling a wide range of artistic and cartoon styles for high-quality image-to-image generation. these loras are trained on high-quality paired data generated by gpt-4o. srouce training code:\n created at: 2025-07-04T06:54:56+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-fill-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of filling areas in existing images based on a text description. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:19:13+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-fill-dev\n  description: this repository contains nunchaku-quantized versions of , capable of filling areas in existing images based on a text description. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:43:28+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-canny-dev\n  description: this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:48:02+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-depth-dev\n  description: this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:45:46+00:00\n- model_name: fuliucansheng/FLUX.1-Kontext-dev-diffusers\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-28T19:05:56+00:00\n- model_name: AIDC-AI/Ovis-U1-3B\n  description: building on the foundation of the ovis series, ovis-u1 is a 3-billion-parameter unified model that seamlessly integrates multimodal understanding, text-to-image generation, and image editing within a single powerful framework. the overall architecture of ovis-u1 cf. fig.2 in our report. ovis-u1 has been tested with python 3.10, torch 2.4.0, transformers 4.51.3, and deepspeed 0.15.4. for a comprehensive list of package dependencies, please consult the requirements.txt file.\n created at: 2025-06-28T12:00:29+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-canny-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:07:43+00:00\n- model_name: fofr/kontext-make-person-real\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on using: you should use `make this person look real` as part of the prompt instruction for your image-to-image editing.\n created at: 2025-07-19T14:35:21+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-depth-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:11:47+00:00\n- model_name: xiaolxl/HXHY-Asian-style-realism-Realistic-KontextLoRA\n  description: :nsfw,kontext,,„ÄÇ model statement: the training material does not contain any nsfw or minor characters, and the training material is only used to adjust the effect of the original kontext model. when you use this model, the output results will be generated based on the images you use, so please do not use it for any illegal or violation purposes.\n created at: 2025-07-07T02:38:04+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-schnell-pix2pix-turbo\n  description: this repository contains loras for both original and nunchaku-quantized to translate sketch to images from user prompts. - developed by: nunchaku team, cmu generative intelligence lab - model type: image-to-image - developed by: nunchaku team, cmu generative intelligence lab - model type: image-to-image - license: apache-2.0 - quantized from model:\n created at: 2025-07-21T04:53:10+00:00\n- model_name: fofr/flux-kontext-dev-ps1-lora\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on . you should use `render this image like a ps1 game no ui` to trigger the image generation.\n created at: 2025-07-12T19:02:05+00:00\n- model_name: AlekseyCalvin/QWEN_IMAGE_nf4_w_AbliteratedTE_Diffusers\n  description: this repo contains a variant of qwen's , the state-of-the-art generative model with extensive and image/text-to-image &/or instruction/control-editing capabilities. to make these cutting edge capabilities more accessible to those constrained to low-end consumer-grade hardware, we've quantized the dit diffusion transformer component of qwen-image to the 4-bit nf4 format using the bits&bytes toolkit. this optimization was derived by us directly from the bf16 base model weights released on 08/04/2025, with no other mix-ins or modifications to the dit component.\n created at: 2025-08-05T12:00:18+00:00\n- model_name: lucataco/kontext-realearth\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on using: you should use `make this a realistic drone photo` as part of the prompt instruction for your image-to-image editing.\n created at: 2025-07-22T20:28:00+00:00\n\nExamples:\n- example user query: I am a digital artist. I want to transform my artworks to a different style or representation.\n  model_name: GreeneryScenery/SheepsControlV5\n- example user query: We need a new anime character sketch for our story. The character should be a warrior girl in the woods.\n  model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n- example user query: A social media platform wants to create AI-generated profile pictures for its users based on their description. We need to provide a way to generate these images based on their descriptions.\n  model_name: lllyasviel/control_v11p_sd15_normalbae\n- example user query: Create a program that produces a new image by reimagining the input image. Modify the input image with guidance like \"add more trees\" or \"make the sky darker.\"\n  model_name: GreeneryScenery/SheepsControlV3\n- example user query: We want to create a visual scene from this story snippet: \"In the enchanted forest, a magical unicorn grazes near a crystal-clear pond.\"\n  model_name: lllyasviel/control_v11p_sd15_scribble\n- example user query: I need a model to generate images based on text descriptions with additional input conditions, like containing soft edges.\n  model_name: lllyasviel/control_v11p_sd15_softedge\n\nUser prompt:\nCreating layers of meaning from the normal imagery and the prompt 'a cozy cabin', generate an image.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <controllable_image_generation>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: runwayml/stable-diffusion-v1-5\n  description: ### ‚ö†Ô∏è This repository is a mirror of the now deprecated `ruwnayml/stable-diffusion-v1-5`, this repository or organization are not affiliated in any way with RunwayML.\nModifications to the original model card are in <span style=\"color:crimson\">red</span> or <span style=\"color:darkgreen\">green</span>\n\nStable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.\nFor more information about how Stable Diffusion functions, please have a look at [ü§ó's Stable Diffusion blog](https://huggingface.co/blog/stable_diffusion).\n\nThe **Stable-Diffusion-v1-5** checkpoint was initialized with the weights of the [Stable-Diffusion-v1-2](https:/steps/huggingface.co/CompVis/stable-diffusion-v1-2) \ncheckpoint and subsequently fine-tuned on 595k steps at resolution 512x512 on \"laion-aesthetics v2 5+\" and 10% dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598).\n\nYou can use this both with the [üß®Diffusers library](https://github.com/huggingface/diffusers) and [RunwayML GitHub repository](https://github.com/runwayml/stable-diffusion) (<span style=\"color:crimson\">now deprecated</span>), <span style=\"color:darkgreen\">ComfyUI, Automatic1111, SD.Next, InvokeAI</span>.\n\n### Use with Diffusers\n```py\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = \"sd-legacy/stable-diffusion-v1-5\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]  \n    \nimage.save(\"astronaut_rides_horse.png\")\n```\nFor more detailed instructions, use-cases and examples in JAX follow the instructions [here](https://github.com/huggingface/diffusers#text-to-image-generation-with-stable-diffusion)\n\n### Use with GitHub Repository <span style=\"color:crimson\">(now deprecated)</span>, <span style=\"color:darkgreen\">ComfyUI or Automatic1111</span>\n\n1. Download the weights \n   - [v1-5-pruned-emaonly.safetensors](https://huggingface.co/sd-legacy/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.safetensors) - ema-only weight. uses less VRAM - suitable for inference\n   - [v1-5-pruned.safetensors](https://huggingface.co/sd-legacy/stable-diffusion-v1-5/resolve/main/v1-5-pruned.safetensors) - ema+non-ema weights. uses more VRAM - suitable for fine-tuning\n\n2. Follow instructions [here](https://github.com/runwayml/stable-diffusion). <span style=\"color:crimson\">(now deprecated)</span>\n\n3. <span style=\"color:darkgreen\">Use locally with <a href=\"https://github.com/comfyanonymous/ComfyUI\">ComfyUI</a>, <a href=\"https://github.com/AUTOMATIC1111/stable-diffusion-webui\">AUTOMATIC1111</a>, <a href=\"https://github.com/vladmandic/automatic\">SD.Next</a>, <a href=\"https://github.com/invoke-ai/InvokeAI\">InvokeAI</a></span>\n created at: 2024-08-30\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: lodestones/P.A.W.F.E.C.T-Alpha\n  description: ## Discord and Help\n[furry diffusion discord](https://discord.gg/furrydiffusion), in the \"custom-models\" channel\n\n[lodestone rock's server](https://discord.gg/SQVcWVbqKx)\n created at: 2023-07-02\n- model_name: kandinsky-community/kandinsky-2-2-controlnet-depth\n  description: Kandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.\n\nIt uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\n\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov)\n created at: 2023-06-28\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: kandinsky-community/kandinsky-2-2-prior\n  description: Kandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.\n\nIt uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\n\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov)\n created at: 2023-06-09\n- model_name: kandinsky-community/kandinsky-2-2-decoder\n  description: Kandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.\n\nIt uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\n\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov)\n created at: 2023-06-09\n- model_name: kandinsky-community/kandinsky-2-1\n  description: Kandinsky 2.1 inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.\n\nIt uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\n\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov)\n created at: 2023-05-24\n- model_name: logo-wizard/logo-diffusion-checkpoint\n  description: # Best practices\nWe recommend using this model with the following prompt template:  \n**positive:** f\"a logo of {company industry}, {some objects}, {colors}, modern, minimalism, vector art, 2d, best quality, centered\"  \n**negative:** \"low quality, worst quality, bad composition, extra digit, fewer digits, text, inscription, watermark, label, asymmetric\"  \n  \nSome other recommendations:  \n**num_inference_steps** = *30*  \n**guidance_scale** = *7.5*  \n**height** = *768*  \n**width** = *768*  \n**scheduler** = diffusers.EulerAncestralDiscreteScheduler (used by default)\n\n![results](./results.png)\n created at: 2023-05-05\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: stablediffusionapi/edge-of-realism\n  description: ![generated from modelslab.com](https://assets.modelslab.com/generations/d3d3f607-e8c6-4758-903a-17804fb4002b-0.png)\n## Get API Key\n\nGet API key from [ModelsLab](https://modelslab.com/), No Payment needed. \n\nReplace Key in below code, change **model_id**  to \"edge-of-realism\"\n\nCoding in PHP/Node/Java etc? Have a look at docs for more code examples: [View docs](https://stablediffusionapi.com/docs)\n\nModel link: [View model](https://stablediffusionapi.com/models/edge-of-realism)\n\nCredits: [View credits](https://civitai.com/?query=model_search)\n\nView all models: [View Models](https://stablediffusionapi.com/models)\n\n    import requests  \n    import json  \n      \n    url =  \"https://stablediffusionapi.com/api/v3/dreambooth\"  \n      \n    payload = json.dumps({  \n    \"key\":  \"\",  \n    \"model_id\":  \"edge-of-realism\",  \n    \"prompt\":  \"actual 8K portrait photo of gareth person, portrait, happy colors, bright eyes, clear eyes, warm smile, smooth soft skin, big dreamy eyes, beautiful intricate colored hair, symmetrical, anime wide eyes, soft lighting, detailed face, by makoto shinkai, stanley artgerm lau, wlop, rossdraws, concept art, digital painting, looking into camera\",  \n    \"negative_prompt\":  \"painting, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, deformed, ugly, blurry, bad anatomy, bad proportions, extra limbs, cloned face, skinny, glitchy, double torso, extra arms, extra hands, mangled fingers, missing lips, ugly face, distorted face, extra legs, anime\",  \n    \"width\":  \"512\",  \n    \"height\":  \"512\",  \n    \"samples\":  \"1\",  \n    \"num_inference_steps\":  \"30\",  \n    \"safety_checker\":  \"no\",  \n    \"enhance_prompt\":  \"yes\",  \n    \"seed\":  None,  \n    \"guidance_scale\":  7.5,  \n    \"multi_lingual\":  \"no\",  \n    \"panorama\":  \"no\",  \n    \"self_attention\":  \"no\",  \n    \"upscale\":  \"no\",  \n    \"embeddings\":  \"embeddings_model_id\",  \n    \"lora\":  \"lora_model_id\",  \n    \"webhook\":  None,  \n    \"track_id\":  None  \n    })  \n      \n    headers =  {  \n    'Content-Type':  'application/json'  \n    }  \n      \n    response = requests.request(\"POST\", url, headers=headers, data=payload)  \n      \n    print(response.text)\n\n> Use this coupon code to get 25% off **DMGG0RBN**\n created at: 2023-04-22\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: xyn-ai/anything-v4.0\n  description: Welcome to Anything V4 - a latent diffusion model for weebs. The newest version of Anything. This model is intended to produce high-quality, highly detailed anime style with just a few prompts. Like other anime-style Stable Diffusion models, it also supports danbooru tags to generate images.\n\ne.g. **_1girl, white hair, golden eyes, beautiful eyes, detail, flower meadow, cumulonimbus clouds, lighting, detailed sky, garden_** \n\nI think the V4.5 version better though, it's in this repo. feel free 2 try it.\n created at: 2023-03-23\n- model_name: SG161222/Realistic_Vision_V2.0\n  description: None\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: stabilityai/stable-diffusion-2-1-unclip\n  description: This `stable-diffusion-2-1-unclip` is a finetuned version of Stable Diffusion 2.1, modified to accept (noisy) CLIP image embedding in addition to the text prompt, and can be used to create image variations (Examples) or can be chained with text-to-image CLIP priors. The amount of noise added to the image embedding can be specified via the noise_level (0 means no noise, 1000 full noise).\n\n- Use it with üß® [`diffusers`](#examples)\n created at: 2023-03-20\n- model_name: black-forest-labs/FLUX.1-Kontext-dev\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-05-28T22:23:43+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-kontext-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of editing images based on text instructions. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-06-28T22:57:58+00:00\n- model_name: bullerwins/FLUX.1-Kontext-dev-GGUF\n  description: created with example workflow added in files `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-26T16:26:45+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-kontext-dev\n  description: this repository contains nunchaku-quantized versions of , capable of editing images based on text instructions. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:52:52+00:00\n- model_name: unsloth/FLUX.1-Kontext-dev-GGUF\n  description: gguf files for . original license still applies. a huge thanks to and for help with conversion. ggufs were created with and . to run, use comfyui-gguf and place the model files in the `comfyui/models/unet` directory. for more instructions, go to the readme. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-30T10:10:09+00:00\n- model_name: Owen777/Kontext-Style-Loras\n  description: this repository provides a collection of 20+ style lora adapters for the flux.1 kontext model, enabling a wide range of artistic and cartoon styles for high-quality image-to-image generation. these loras are trained on high-quality paired data generated by gpt-4o. srouce training code:\n created at: 2025-07-04T06:54:56+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-fill-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of filling areas in existing images based on a text description. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:19:13+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-fill-dev\n  description: this repository contains nunchaku-quantized versions of , capable of filling areas in existing images based on a text description. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:43:28+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-canny-dev\n  description: this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:48:02+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-depth-dev\n  description: this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:45:46+00:00\n- model_name: fuliucansheng/FLUX.1-Kontext-dev-diffusers\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-28T19:05:56+00:00\n- model_name: AIDC-AI/Ovis-U1-3B\n  description: building on the foundation of the ovis series, ovis-u1 is a 3-billion-parameter unified model that seamlessly integrates multimodal understanding, text-to-image generation, and image editing within a single powerful framework. the overall architecture of ovis-u1 cf. fig.2 in our report. ovis-u1 has been tested with python 3.10, torch 2.4.0, transformers 4.51.3, and deepspeed 0.15.4. for a comprehensive list of package dependencies, please consult the requirements.txt file.\n created at: 2025-06-28T12:00:29+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-canny-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:07:43+00:00\n- model_name: fofr/kontext-make-person-real\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on using: you should use `make this person look real` as part of the prompt instruction for your image-to-image editing.\n created at: 2025-07-19T14:35:21+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-depth-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:11:47+00:00\n- model_name: xiaolxl/HXHY-Asian-style-realism-Realistic-KontextLoRA\n  description: :nsfw,kontext,,„ÄÇ model statement: the training material does not contain any nsfw or minor characters, and the training material is only used to adjust the effect of the original kontext model. when you use this model, the output results will be generated based on the images you use, so please do not use it for any illegal or violation purposes.\n created at: 2025-07-07T02:38:04+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-schnell-pix2pix-turbo\n  description: this repository contains loras for both original and nunchaku-quantized to translate sketch to images from user prompts. - developed by: nunchaku team, cmu generative intelligence lab - model type: image-to-image - developed by: nunchaku team, cmu generative intelligence lab - model type: image-to-image - license: apache-2.0 - quantized from model:\n created at: 2025-07-21T04:53:10+00:00\n- model_name: fofr/flux-kontext-dev-ps1-lora\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on . you should use `render this image like a ps1 game no ui` to trigger the image generation.\n created at: 2025-07-12T19:02:05+00:00\n- model_name: AlekseyCalvin/QWEN_IMAGE_nf4_w_AbliteratedTE_Diffusers\n  description: this repo contains a variant of qwen's , the state-of-the-art generative model with extensive and image/text-to-image &/or instruction/control-editing capabilities. to make these cutting edge capabilities more accessible to those constrained to low-end consumer-grade hardware, we've quantized the dit diffusion transformer component of qwen-image to the 4-bit nf4 format using the bits&bytes toolkit. this optimization was derived by us directly from the bf16 base model weights released on 08/04/2025, with no other mix-ins or modifications to the dit component.\n created at: 2025-08-05T12:00:18+00:00\n- model_name: lucataco/kontext-realearth\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on using: you should use `make this a realistic drone photo` as part of the prompt instruction for your image-to-image editing.\n created at: 2025-07-22T20:28:00+00:00\n\nExamples:\n- example user query: I am a digital artist. I want to transform my artworks to a different style or representation.\n  model_name: GreeneryScenery/SheepsControlV5\n- example user query: We need a new anime character sketch for our story. The character should be a warrior girl in the woods.\n  model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n- example user query: A social media platform wants to create AI-generated profile pictures for its users based on their description. We need to provide a way to generate these images based on their descriptions.\n  model_name: lllyasviel/control_v11p_sd15_normalbae\n- example user query: Create a program that produces a new image by reimagining the input image. Modify the input image with guidance like \"add more trees\" or \"make the sky darker.\"\n  model_name: GreeneryScenery/SheepsControlV3\n- example user query: We want to create a visual scene from this story snippet: \"In the enchanted forest, a magical unicorn grazes near a crystal-clear pond.\"\n  model_name: lllyasviel/control_v11p_sd15_scribble\n- example user query: I need a model to generate images based on text descriptions with additional input conditions, like containing soft edges.\n  model_name: lllyasviel/control_v11p_sd15_softedge\n\nUser prompt:\nTracing back to the provided canny image and the concept 'a lively concert', imagine a new creation.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <controllable_image_generation>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: runwayml/stable-diffusion-v1-5\n  description: ### ‚ö†Ô∏è This repository is a mirror of the now deprecated `ruwnayml/stable-diffusion-v1-5`, this repository or organization are not affiliated in any way with RunwayML.\nModifications to the original model card are in <span style=\"color:crimson\">red</span> or <span style=\"color:darkgreen\">green</span>\n\nStable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.\nFor more information about how Stable Diffusion functions, please have a look at [ü§ó's Stable Diffusion blog](https://huggingface.co/blog/stable_diffusion).\n\nThe **Stable-Diffusion-v1-5** checkpoint was initialized with the weights of the [Stable-Diffusion-v1-2](https:/steps/huggingface.co/CompVis/stable-diffusion-v1-2) \ncheckpoint and subsequently fine-tuned on 595k steps at resolution 512x512 on \"laion-aesthetics v2 5+\" and 10% dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598).\n\nYou can use this both with the [üß®Diffusers library](https://github.com/huggingface/diffusers) and [RunwayML GitHub repository](https://github.com/runwayml/stable-diffusion) (<span style=\"color:crimson\">now deprecated</span>), <span style=\"color:darkgreen\">ComfyUI, Automatic1111, SD.Next, InvokeAI</span>.\n\n### Use with Diffusers\n```py\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = \"sd-legacy/stable-diffusion-v1-5\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]  \n    \nimage.save(\"astronaut_rides_horse.png\")\n```\nFor more detailed instructions, use-cases and examples in JAX follow the instructions [here](https://github.com/huggingface/diffusers#text-to-image-generation-with-stable-diffusion)\n\n### Use with GitHub Repository <span style=\"color:crimson\">(now deprecated)</span>, <span style=\"color:darkgreen\">ComfyUI or Automatic1111</span>\n\n1. Download the weights \n   - [v1-5-pruned-emaonly.safetensors](https://huggingface.co/sd-legacy/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.safetensors) - ema-only weight. uses less VRAM - suitable for inference\n   - [v1-5-pruned.safetensors](https://huggingface.co/sd-legacy/stable-diffusion-v1-5/resolve/main/v1-5-pruned.safetensors) - ema+non-ema weights. uses more VRAM - suitable for fine-tuning\n\n2. Follow instructions [here](https://github.com/runwayml/stable-diffusion). <span style=\"color:crimson\">(now deprecated)</span>\n\n3. <span style=\"color:darkgreen\">Use locally with <a href=\"https://github.com/comfyanonymous/ComfyUI\">ComfyUI</a>, <a href=\"https://github.com/AUTOMATIC1111/stable-diffusion-webui\">AUTOMATIC1111</a>, <a href=\"https://github.com/vladmandic/automatic\">SD.Next</a>, <a href=\"https://github.com/invoke-ai/InvokeAI\">InvokeAI</a></span>\n created at: 2024-08-30\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: lodestones/P.A.W.F.E.C.T-Alpha\n  description: ## Discord and Help\n[furry diffusion discord](https://discord.gg/furrydiffusion), in the \"custom-models\" channel\n\n[lodestone rock's server](https://discord.gg/SQVcWVbqKx)\n created at: 2023-07-02\n- model_name: kandinsky-community/kandinsky-2-2-controlnet-depth\n  description: Kandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.\n\nIt uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\n\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov)\n created at: 2023-06-28\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: kandinsky-community/kandinsky-2-2-prior\n  description: Kandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.\n\nIt uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\n\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov)\n created at: 2023-06-09\n- model_name: kandinsky-community/kandinsky-2-2-decoder\n  description: Kandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.\n\nIt uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\n\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov)\n created at: 2023-06-09\n- model_name: kandinsky-community/kandinsky-2-1\n  description: Kandinsky 2.1 inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.\n\nIt uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\n\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov)\n created at: 2023-05-24\n- model_name: logo-wizard/logo-diffusion-checkpoint\n  description: # Best practices\nWe recommend using this model with the following prompt template:  \n**positive:** f\"a logo of {company industry}, {some objects}, {colors}, modern, minimalism, vector art, 2d, best quality, centered\"  \n**negative:** \"low quality, worst quality, bad composition, extra digit, fewer digits, text, inscription, watermark, label, asymmetric\"  \n  \nSome other recommendations:  \n**num_inference_steps** = *30*  \n**guidance_scale** = *7.5*  \n**height** = *768*  \n**width** = *768*  \n**scheduler** = diffusers.EulerAncestralDiscreteScheduler (used by default)\n\n![results](./results.png)\n created at: 2023-05-05\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: stablediffusionapi/edge-of-realism\n  description: ![generated from modelslab.com](https://assets.modelslab.com/generations/d3d3f607-e8c6-4758-903a-17804fb4002b-0.png)\n## Get API Key\n\nGet API key from [ModelsLab](https://modelslab.com/), No Payment needed. \n\nReplace Key in below code, change **model_id**  to \"edge-of-realism\"\n\nCoding in PHP/Node/Java etc? Have a look at docs for more code examples: [View docs](https://stablediffusionapi.com/docs)\n\nModel link: [View model](https://stablediffusionapi.com/models/edge-of-realism)\n\nCredits: [View credits](https://civitai.com/?query=model_search)\n\nView all models: [View Models](https://stablediffusionapi.com/models)\n\n    import requests  \n    import json  \n      \n    url =  \"https://stablediffusionapi.com/api/v3/dreambooth\"  \n      \n    payload = json.dumps({  \n    \"key\":  \"\",  \n    \"model_id\":  \"edge-of-realism\",  \n    \"prompt\":  \"actual 8K portrait photo of gareth person, portrait, happy colors, bright eyes, clear eyes, warm smile, smooth soft skin, big dreamy eyes, beautiful intricate colored hair, symmetrical, anime wide eyes, soft lighting, detailed face, by makoto shinkai, stanley artgerm lau, wlop, rossdraws, concept art, digital painting, looking into camera\",  \n    \"negative_prompt\":  \"painting, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, deformed, ugly, blurry, bad anatomy, bad proportions, extra limbs, cloned face, skinny, glitchy, double torso, extra arms, extra hands, mangled fingers, missing lips, ugly face, distorted face, extra legs, anime\",  \n    \"width\":  \"512\",  \n    \"height\":  \"512\",  \n    \"samples\":  \"1\",  \n    \"num_inference_steps\":  \"30\",  \n    \"safety_checker\":  \"no\",  \n    \"enhance_prompt\":  \"yes\",  \n    \"seed\":  None,  \n    \"guidance_scale\":  7.5,  \n    \"multi_lingual\":  \"no\",  \n    \"panorama\":  \"no\",  \n    \"self_attention\":  \"no\",  \n    \"upscale\":  \"no\",  \n    \"embeddings\":  \"embeddings_model_id\",  \n    \"lora\":  \"lora_model_id\",  \n    \"webhook\":  None,  \n    \"track_id\":  None  \n    })  \n      \n    headers =  {  \n    'Content-Type':  'application/json'  \n    }  \n      \n    response = requests.request(\"POST\", url, headers=headers, data=payload)  \n      \n    print(response.text)\n\n> Use this coupon code to get 25% off **DMGG0RBN**\n created at: 2023-04-22\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: xyn-ai/anything-v4.0\n  description: Welcome to Anything V4 - a latent diffusion model for weebs. The newest version of Anything. This model is intended to produce high-quality, highly detailed anime style with just a few prompts. Like other anime-style Stable Diffusion models, it also supports danbooru tags to generate images.\n\ne.g. **_1girl, white hair, golden eyes, beautiful eyes, detail, flower meadow, cumulonimbus clouds, lighting, detailed sky, garden_** \n\nI think the V4.5 version better though, it's in this repo. feel free 2 try it.\n created at: 2023-03-23\n- model_name: SG161222/Realistic_Vision_V2.0\n  description: None\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: stabilityai/stable-diffusion-2-1-unclip\n  description: This `stable-diffusion-2-1-unclip` is a finetuned version of Stable Diffusion 2.1, modified to accept (noisy) CLIP image embedding in addition to the text prompt, and can be used to create image variations (Examples) or can be chained with text-to-image CLIP priors. The amount of noise added to the image embedding can be specified via the noise_level (0 means no noise, 1000 full noise).\n\n- Use it with üß® [`diffusers`](#examples)\n created at: 2023-03-20\n- model_name: black-forest-labs/FLUX.1-Kontext-dev\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-05-28T22:23:43+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-kontext-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of editing images based on text instructions. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-06-28T22:57:58+00:00\n- model_name: bullerwins/FLUX.1-Kontext-dev-GGUF\n  description: created with example workflow added in files `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-26T16:26:45+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-kontext-dev\n  description: this repository contains nunchaku-quantized versions of , capable of editing images based on text instructions. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:52:52+00:00\n- model_name: unsloth/FLUX.1-Kontext-dev-GGUF\n  description: gguf files for . original license still applies. a huge thanks to and for help with conversion. ggufs were created with and . to run, use comfyui-gguf and place the model files in the `comfyui/models/unet` directory. for more instructions, go to the readme. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-30T10:10:09+00:00\n- model_name: Owen777/Kontext-Style-Loras\n  description: this repository provides a collection of 20+ style lora adapters for the flux.1 kontext model, enabling a wide range of artistic and cartoon styles for high-quality image-to-image generation. these loras are trained on high-quality paired data generated by gpt-4o. srouce training code:\n created at: 2025-07-04T06:54:56+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-fill-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of filling areas in existing images based on a text description. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:19:13+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-fill-dev\n  description: this repository contains nunchaku-quantized versions of , capable of filling areas in existing images based on a text description. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:43:28+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-canny-dev\n  description: this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:48:02+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-depth-dev\n  description: this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:45:46+00:00\n- model_name: fuliucansheng/FLUX.1-Kontext-dev-diffusers\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-28T19:05:56+00:00\n- model_name: AIDC-AI/Ovis-U1-3B\n  description: building on the foundation of the ovis series, ovis-u1 is a 3-billion-parameter unified model that seamlessly integrates multimodal understanding, text-to-image generation, and image editing within a single powerful framework. the overall architecture of ovis-u1 cf. fig.2 in our report. ovis-u1 has been tested with python 3.10, torch 2.4.0, transformers 4.51.3, and deepspeed 0.15.4. for a comprehensive list of package dependencies, please consult the requirements.txt file.\n created at: 2025-06-28T12:00:29+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-canny-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:07:43+00:00\n- model_name: fofr/kontext-make-person-real\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on using: you should use `make this person look real` as part of the prompt instruction for your image-to-image editing.\n created at: 2025-07-19T14:35:21+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-depth-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:11:47+00:00\n- model_name: xiaolxl/HXHY-Asian-style-realism-Realistic-KontextLoRA\n  description: :nsfw,kontext,,„ÄÇ model statement: the training material does not contain any nsfw or minor characters, and the training material is only used to adjust the effect of the original kontext model. when you use this model, the output results will be generated based on the images you use, so please do not use it for any illegal or violation purposes.\n created at: 2025-07-07T02:38:04+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-schnell-pix2pix-turbo\n  description: this repository contains loras for both original and nunchaku-quantized to translate sketch to images from user prompts. - developed by: nunchaku team, cmu generative intelligence lab - model type: image-to-image - developed by: nunchaku team, cmu generative intelligence lab - model type: image-to-image - license: apache-2.0 - quantized from model:\n created at: 2025-07-21T04:53:10+00:00\n- model_name: fofr/flux-kontext-dev-ps1-lora\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on . you should use `render this image like a ps1 game no ui` to trigger the image generation.\n created at: 2025-07-12T19:02:05+00:00\n- model_name: AlekseyCalvin/QWEN_IMAGE_nf4_w_AbliteratedTE_Diffusers\n  description: this repo contains a variant of qwen's , the state-of-the-art generative model with extensive and image/text-to-image &/or instruction/control-editing capabilities. to make these cutting edge capabilities more accessible to those constrained to low-end consumer-grade hardware, we've quantized the dit diffusion transformer component of qwen-image to the 4-bit nf4 format using the bits&bytes toolkit. this optimization was derived by us directly from the bf16 base model weights released on 08/04/2025, with no other mix-ins or modifications to the dit component.\n created at: 2025-08-05T12:00:18+00:00\n- model_name: lucataco/kontext-realearth\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on using: you should use `make this a realistic drone photo` as part of the prompt instruction for your image-to-image editing.\n created at: 2025-07-22T20:28:00+00:00\n\nExamples:\n- example user query: I am a digital artist. I want to transform my artworks to a different style or representation.\n  model_name: GreeneryScenery/SheepsControlV5\n- example user query: We need a new anime character sketch for our story. The character should be a warrior girl in the woods.\n  model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n- example user query: A social media platform wants to create AI-generated profile pictures for its users based on their description. We need to provide a way to generate these images based on their descriptions.\n  model_name: lllyasviel/control_v11p_sd15_normalbae\n- example user query: Create a program that produces a new image by reimagining the input image. Modify the input image with guidance like \"add more trees\" or \"make the sky darker.\"\n  model_name: GreeneryScenery/SheepsControlV3\n- example user query: We want to create a visual scene from this story snippet: \"In the enchanted forest, a magical unicorn grazes near a crystal-clear pond.\"\n  model_name: lllyasviel/control_v11p_sd15_scribble\n- example user query: I need a model to generate images based on text descriptions with additional input conditions, like containing soft edges.\n  model_name: lllyasviel/control_v11p_sd15_softedge\n\nUser prompt:\nInfluenced by the segmentation picture with the idea 'a charming village', create a new image.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <controllable_image_generation>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: runwayml/stable-diffusion-v1-5\n  description: ### ‚ö†Ô∏è This repository is a mirror of the now deprecated `ruwnayml/stable-diffusion-v1-5`, this repository or organization are not affiliated in any way with RunwayML.\nModifications to the original model card are in <span style=\"color:crimson\">red</span> or <span style=\"color:darkgreen\">green</span>\n\nStable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.\nFor more information about how Stable Diffusion functions, please have a look at [ü§ó's Stable Diffusion blog](https://huggingface.co/blog/stable_diffusion).\n\nThe **Stable-Diffusion-v1-5** checkpoint was initialized with the weights of the [Stable-Diffusion-v1-2](https:/steps/huggingface.co/CompVis/stable-diffusion-v1-2) \ncheckpoint and subsequently fine-tuned on 595k steps at resolution 512x512 on \"laion-aesthetics v2 5+\" and 10% dropping of the text-conditioning to improve [classifier-free guidance sampling](https://arxiv.org/abs/2207.12598).\n\nYou can use this both with the [üß®Diffusers library](https://github.com/huggingface/diffusers) and [RunwayML GitHub repository](https://github.com/runwayml/stable-diffusion) (<span style=\"color:crimson\">now deprecated</span>), <span style=\"color:darkgreen\">ComfyUI, Automatic1111, SD.Next, InvokeAI</span>.\n\n### Use with Diffusers\n```py\nfrom diffusers import StableDiffusionPipeline\nimport torch\n\nmodel_id = \"sd-legacy/stable-diffusion-v1-5\"\npipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\n\nprompt = \"a photo of an astronaut riding a horse on mars\"\nimage = pipe(prompt).images[0]  \n    \nimage.save(\"astronaut_rides_horse.png\")\n```\nFor more detailed instructions, use-cases and examples in JAX follow the instructions [here](https://github.com/huggingface/diffusers#text-to-image-generation-with-stable-diffusion)\n\n### Use with GitHub Repository <span style=\"color:crimson\">(now deprecated)</span>, <span style=\"color:darkgreen\">ComfyUI or Automatic1111</span>\n\n1. Download the weights \n   - [v1-5-pruned-emaonly.safetensors](https://huggingface.co/sd-legacy/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.safetensors) - ema-only weight. uses less VRAM - suitable for inference\n   - [v1-5-pruned.safetensors](https://huggingface.co/sd-legacy/stable-diffusion-v1-5/resolve/main/v1-5-pruned.safetensors) - ema+non-ema weights. uses more VRAM - suitable for fine-tuning\n\n2. Follow instructions [here](https://github.com/runwayml/stable-diffusion). <span style=\"color:crimson\">(now deprecated)</span>\n\n3. <span style=\"color:darkgreen\">Use locally with <a href=\"https://github.com/comfyanonymous/ComfyUI\">ComfyUI</a>, <a href=\"https://github.com/AUTOMATIC1111/stable-diffusion-webui\">AUTOMATIC1111</a>, <a href=\"https://github.com/vladmandic/automatic\">SD.Next</a>, <a href=\"https://github.com/invoke-ai/InvokeAI\">InvokeAI</a></span>\n created at: 2024-08-30\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: lodestones/P.A.W.F.E.C.T-Alpha\n  description: ## Discord and Help\n[furry diffusion discord](https://discord.gg/furrydiffusion), in the \"custom-models\" channel\n\n[lodestone rock's server](https://discord.gg/SQVcWVbqKx)\n created at: 2023-07-02\n- model_name: kandinsky-community/kandinsky-2-2-controlnet-depth\n  description: Kandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.\n\nIt uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\n\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov)\n created at: 2023-06-28\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: kandinsky-community/kandinsky-2-2-prior\n  description: Kandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.\n\nIt uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\n\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov)\n created at: 2023-06-09\n- model_name: kandinsky-community/kandinsky-2-2-decoder\n  description: Kandinsky inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.\n\nIt uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\n\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov)\n created at: 2023-06-09\n- model_name: kandinsky-community/kandinsky-2-1\n  description: Kandinsky 2.1 inherits best practices from Dall-E 2 and Latent diffusion while introducing some new ideas.\n\nIt uses the CLIP model as a text and image encoder,  and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.\n\nThe Kandinsky model is created by [Arseniy Shakhmatov](https://github.com/cene555), [Anton Razzhigaev](https://github.com/razzant), [Aleksandr Nikolich](https://github.com/AlexWortega), [Igor Pavlov](https://github.com/boomb0om), [Andrey Kuznetsov](https://github.com/kuznetsoffandrey) and [Denis Dimitrov](https://github.com/denndimitrov)\n created at: 2023-05-24\n- model_name: logo-wizard/logo-diffusion-checkpoint\n  description: # Best practices\nWe recommend using this model with the following prompt template:  \n**positive:** f\"a logo of {company industry}, {some objects}, {colors}, modern, minimalism, vector art, 2d, best quality, centered\"  \n**negative:** \"low quality, worst quality, bad composition, extra digit, fewer digits, text, inscription, watermark, label, asymmetric\"  \n  \nSome other recommendations:  \n**num_inference_steps** = *30*  \n**guidance_scale** = *7.5*  \n**height** = *768*  \n**width** = *768*  \n**scheduler** = diffusers.EulerAncestralDiscreteScheduler (used by default)\n\n![results](./results.png)\n created at: 2023-05-05\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: stablediffusionapi/edge-of-realism\n  description: ![generated from modelslab.com](https://assets.modelslab.com/generations/d3d3f607-e8c6-4758-903a-17804fb4002b-0.png)\n## Get API Key\n\nGet API key from [ModelsLab](https://modelslab.com/), No Payment needed. \n\nReplace Key in below code, change **model_id**  to \"edge-of-realism\"\n\nCoding in PHP/Node/Java etc? Have a look at docs for more code examples: [View docs](https://stablediffusionapi.com/docs)\n\nModel link: [View model](https://stablediffusionapi.com/models/edge-of-realism)\n\nCredits: [View credits](https://civitai.com/?query=model_search)\n\nView all models: [View Models](https://stablediffusionapi.com/models)\n\n    import requests  \n    import json  \n      \n    url =  \"https://stablediffusionapi.com/api/v3/dreambooth\"  \n      \n    payload = json.dumps({  \n    \"key\":  \"\",  \n    \"model_id\":  \"edge-of-realism\",  \n    \"prompt\":  \"actual 8K portrait photo of gareth person, portrait, happy colors, bright eyes, clear eyes, warm smile, smooth soft skin, big dreamy eyes, beautiful intricate colored hair, symmetrical, anime wide eyes, soft lighting, detailed face, by makoto shinkai, stanley artgerm lau, wlop, rossdraws, concept art, digital painting, looking into camera\",  \n    \"negative_prompt\":  \"painting, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, deformed, ugly, blurry, bad anatomy, bad proportions, extra limbs, cloned face, skinny, glitchy, double torso, extra arms, extra hands, mangled fingers, missing lips, ugly face, distorted face, extra legs, anime\",  \n    \"width\":  \"512\",  \n    \"height\":  \"512\",  \n    \"samples\":  \"1\",  \n    \"num_inference_steps\":  \"30\",  \n    \"safety_checker\":  \"no\",  \n    \"enhance_prompt\":  \"yes\",  \n    \"seed\":  None,  \n    \"guidance_scale\":  7.5,  \n    \"multi_lingual\":  \"no\",  \n    \"panorama\":  \"no\",  \n    \"self_attention\":  \"no\",  \n    \"upscale\":  \"no\",  \n    \"embeddings\":  \"embeddings_model_id\",  \n    \"lora\":  \"lora_model_id\",  \n    \"webhook\":  None,  \n    \"track_id\":  None  \n    })  \n      \n    headers =  {  \n    'Content-Type':  'application/json'  \n    }  \n      \n    response = requests.request(\"POST\", url, headers=headers, data=payload)  \n      \n    print(response.text)\n\n> Use this coupon code to get 25% off **DMGG0RBN**\n created at: 2023-04-22\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: xyn-ai/anything-v4.0\n  description: Welcome to Anything V4 - a latent diffusion model for weebs. The newest version of Anything. This model is intended to produce high-quality, highly detailed anime style with just a few prompts. Like other anime-style Stable Diffusion models, it also supports danbooru tags to generate images.\n\ne.g. **_1girl, white hair, golden eyes, beautiful eyes, detail, flower meadow, cumulonimbus clouds, lighting, detailed sky, garden_** \n\nI think the V4.5 version better though, it's in this repo. feel free 2 try it.\n created at: 2023-03-23\n- model_name: SG161222/Realistic_Vision_V2.0\n  description: None\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: stabilityai/stable-diffusion-2-1-unclip\n  description: This `stable-diffusion-2-1-unclip` is a finetuned version of Stable Diffusion 2.1, modified to accept (noisy) CLIP image embedding in addition to the text prompt, and can be used to create image variations (Examples) or can be chained with text-to-image CLIP priors. The amount of noise added to the image embedding can be specified via the noise_level (0 means no noise, 1000 full noise).\n\n- Use it with üß® [`diffusers`](#examples)\n created at: 2023-03-20\n- model_name: black-forest-labs/FLUX.1-Kontext-dev\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-05-28T22:23:43+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-kontext-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of editing images based on text instructions. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-06-28T22:57:58+00:00\n- model_name: bullerwins/FLUX.1-Kontext-dev-GGUF\n  description: created with example workflow added in files `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-26T16:26:45+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-kontext-dev\n  description: this repository contains nunchaku-quantized versions of , capable of editing images based on text instructions. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:52:52+00:00\n- model_name: unsloth/FLUX.1-Kontext-dev-GGUF\n  description: gguf files for . original license still applies. a huge thanks to and for help with conversion. ggufs were created with and . to run, use comfyui-gguf and place the model files in the `comfyui/models/unet` directory. for more instructions, go to the readme. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-30T10:10:09+00:00\n- model_name: Owen777/Kontext-Style-Loras\n  description: this repository provides a collection of 20+ style lora adapters for the flux.1 kontext model, enabling a wide range of artistic and cartoon styles for high-quality image-to-image generation. these loras are trained on high-quality paired data generated by gpt-4o. srouce training code:\n created at: 2025-07-04T06:54:56+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-fill-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of filling areas in existing images based on a text description. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:19:13+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-fill-dev\n  description: this repository contains nunchaku-quantized versions of , capable of filling areas in existing images based on a text description. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:43:28+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-canny-dev\n  description: this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:48:02+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-depth-dev\n  description: this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:45:46+00:00\n- model_name: fuliucansheng/FLUX.1-Kontext-dev-diffusers\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-28T19:05:56+00:00\n- model_name: AIDC-AI/Ovis-U1-3B\n  description: building on the foundation of the ovis series, ovis-u1 is a 3-billion-parameter unified model that seamlessly integrates multimodal understanding, text-to-image generation, and image editing within a single powerful framework. the overall architecture of ovis-u1 cf. fig.2 in our report. ovis-u1 has been tested with python 3.10, torch 2.4.0, transformers 4.51.3, and deepspeed 0.15.4. for a comprehensive list of package dependencies, please consult the requirements.txt file.\n created at: 2025-06-28T12:00:29+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-canny-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:07:43+00:00\n- model_name: fofr/kontext-make-person-real\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on using: you should use `make this person look real` as part of the prompt instruction for your image-to-image editing.\n created at: 2025-07-19T14:35:21+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-depth-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:11:47+00:00\n- model_name: xiaolxl/HXHY-Asian-style-realism-Realistic-KontextLoRA\n  description: :nsfw,kontext,,„ÄÇ model statement: the training material does not contain any nsfw or minor characters, and the training material is only used to adjust the effect of the original kontext model. when you use this model, the output results will be generated based on the images you use, so please do not use it for any illegal or violation purposes.\n created at: 2025-07-07T02:38:04+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-schnell-pix2pix-turbo\n  description: this repository contains loras for both original and nunchaku-quantized to translate sketch to images from user prompts. - developed by: nunchaku team, cmu generative intelligence lab - model type: image-to-image - developed by: nunchaku team, cmu generative intelligence lab - model type: image-to-image - license: apache-2.0 - quantized from model:\n created at: 2025-07-21T04:53:10+00:00\n- model_name: fofr/flux-kontext-dev-ps1-lora\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on . you should use `render this image like a ps1 game no ui` to trigger the image generation.\n created at: 2025-07-12T19:02:05+00:00\n- model_name: AlekseyCalvin/QWEN_IMAGE_nf4_w_AbliteratedTE_Diffusers\n  description: this repo contains a variant of qwen's , the state-of-the-art generative model with extensive and image/text-to-image &/or instruction/control-editing capabilities. to make these cutting edge capabilities more accessible to those constrained to low-end consumer-grade hardware, we've quantized the dit diffusion transformer component of qwen-image to the 4-bit nf4 format using the bits&bytes toolkit. this optimization was derived by us directly from the bf16 base model weights released on 08/04/2025, with no other mix-ins or modifications to the dit component.\n created at: 2025-08-05T12:00:18+00:00\n- model_name: lucataco/kontext-realearth\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on using: you should use `make this a realistic drone photo` as part of the prompt instruction for your image-to-image editing.\n created at: 2025-07-22T20:28:00+00:00\n\nExamples:\n- example user query: I am a digital artist. I want to transform my artworks to a different style or representation.\n  model_name: GreeneryScenery/SheepsControlV5\n- example user query: We need a new anime character sketch for our story. The character should be a warrior girl in the woods.\n  model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n- example user query: A social media platform wants to create AI-generated profile pictures for its users based on their description. We need to provide a way to generate these images based on their descriptions.\n  model_name: lllyasviel/control_v11p_sd15_normalbae\n- example user query: Create a program that produces a new image by reimagining the input image. Modify the input image with guidance like \"add more trees\" or \"make the sky darker.\"\n  model_name: GreeneryScenery/SheepsControlV3\n- example user query: We want to create a visual scene from this story snippet: \"In the enchanted forest, a magical unicorn grazes near a crystal-clear pond.\"\n  model_name: lllyasviel/control_v11p_sd15_scribble\n- example user query: I need a model to generate images based on text descriptions with additional input conditions, like containing soft edges.\n  model_name: lllyasviel/control_v11p_sd15_softedge\n\nUser prompt:\nTaking cues from the pose picture with the idea 'a bustling harbor', create a new image.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n"
  ],
  "<controllable_video_generation>": [
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <controllable_video_generation>\n\nCandidate Models:\n- model_name: camenduru/text2-video-zero\n  description: This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.\n created at: 2023-03-29\n- model_name: chavinlo/TempoFunk\n  description: A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text.\n created at: 2023-02-07\n- model_name: damo-vilab/text-to-video-ms-1.7b\n  description: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. The overall model parameters are about 1.7 billion. Currently, it only supports English input.\n created at: 2023-03-22\n- model_name: nitrosocke/redshift-diffusion\n  description: Tune-A-Video - Redshift is a text-to-video generation model based on the nitrosocke/redshift-diffusion model. It generates videos based on textual prompts, such as 'a man is skiing' or '(redshift style) spider man is skiing'.\n created at: 2022-11-06\n- model_name: nitrosocke/mo-di-diffusion\n  description: Tune-A-Video is a text-to-video generation model based on the Hugging Face framework. The model generates videos based on textual prompts in a modern Disney style.\n created at: 2022-10-27\n- model_name: ImRma/Brucelee\n  description: A Hugging Face model for converting Persian and English text into video.\n created at: 2023-04-23\n- model_name: damo-vilab/text-to-video-ms-1.7b-legacy\n  description: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.\n created at: 2023-03-22\n- model_name: duncan93/video\n  description: A text-to-video model trained on OpenAssistant/oasst1 dataset.\n created at: 2023-05-02\n- model_name: cerspense/zeroscope_v2_576w\n  description: zeroscope_v2_576w uses 7.9gb of vram when rendering 30 frames at 576x320\n\n### Using it with the 1111 text2video extension\n\n1. Download files in the zs2_576w folder.\n2. Replace the respective files in the 'stable-diffusion-webui\\models\\ModelScope\\t2v' directory.\n\n### Upscaling recommendations\n\nFor upscaling, it's recommended to use [zeroscope_v2_XL](https://huggingface.co/cerspense/zeroscope_v2_XL) via vid2vid in the 1111 extension. It works best at 1024x576 with a denoise strength between 0.66 and 0.85. Remember to use the same prompt that was used to generate the original clip. <br />\n\n### Usage in üß® Diffusers\n\nLet's first install the libraries required:\n\n```bash\n$ pip install diffusers transformers accelerate torch\n```\n\nNow, generate a video:\n\n```py\nimport torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_video\n\npipe = DiffusionPipeline.from_pretrained(\"cerspense/zeroscope_v2_576w\", torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n\nprompt = \"Darth Vader is surfing on waves\"\nvideo_frames = pipe(prompt, num_inference_steps=40, height=320, width=576, num_frames=24).frames\nvideo_path = export_to_video(video_frames)\n```\n\nHere are some results:\n\n<table>\n    <tr>\n        Darth vader is surfing on waves.\n        <br>\n        <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/darthvader_cerpense.gif\"\n            alt=\"Darth vader surfing in waves.\"\n            style=\"width: 576;\" />\n        </center></td>\n    </tr>\n</table>\n\n### Known issues\n\nLower resolutions or fewer frames could lead to suboptimal output. <br />\n\nThanks to [camenduru](https://github.com/camenduru), [kabachuha](https://github.com/kabachuha), [ExponentialML](https://github.com/ExponentialML), [dotsimulate](https://www.instagram.com/dotsimulate/), [VANYA](https://twitter.com/veryVANYA), [polyware](https://twitter.com/polyware_ai), [tin2tin](https://github.com/tin2tin)<br />\n created at: 2023-06-21\n- model_name: Searchium-ai/clip4clip-webvid150k\n  description: A CLIP4Clip video-text retrieval model trained on a subset of the WebVid dataset. \nThe model and training method are described in the paper [\"Clip4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval\"](https://arxiv.org/pdf/2104.08860.pdf) by Lou et el, and implemented in the accompanying [GitHub repository](https://github.com/ArrowLuo/CLIP4Clip).\n\nThe training process utilized the [WebVid Dataset](https://m-bain.github.io/webvid-dataset/), a comprehensive collection of short videos with corresponding textual descriptions sourced from the web. \nFor training purposes, a subset consisting of the first 150,000 video-text pairs from the dataset were used.\n\nThis HF model is based on the [clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32) architecture, with weights trained by Daphna Idelson at [Searchium](https://www.searchium.ai).\n\n\n# How to use\n### Extracting Text Embeddings:\n\n```python\nimport numpy as np\nimport torch\nfrom transformers import CLIPTokenizer, CLIPTextModelWithProjection\n\n\nsearch_sentence = \"a basketball player performing a slam dunk\"\n\nmodel = CLIPTextModelWithProjection.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\ntokenizer = CLIPTokenizer.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\n\ninputs = tokenizer(text=search_sentence , return_tensors=\"pt\")\noutputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n\n# Normalize embeddings for retrieval:\nfinal_output = outputs[0] / outputs[0].norm(dim=-1, keepdim=True)\nfinal_output = final_output.cpu().detach().numpy()\nprint(\"final output: \", final_output)\n```\n\n### Extracting Video Embeddings:\n\nAn additional notebook [\"GSI_VideoRetrieval_VideoEmbedding.ipynb\"](https://huggingface.co/Searchium-ai/clip4clip-webvid150k/blob/main/Notebooks/GSI_VideoRetrieval_VideoEmbedding.ipynb), provides instructions for extracting video embeddings and includes the necessary tools for preprocessing videos.\n created at: 2023-04-17\n- model_name: PAIR/text2video-zero-controlnet-canny-avatar\n  description: [Text2Video-Zero](https://arxiv.org/abs/2303.13439) is a zero-shot text to video generator. It can perform `zero-shot text-to-video generation`, `Video Instruct Pix2Pix` (instruction-guided video editing), \n`text and pose conditional video generation`, `text and canny-edge conditional video generation`, and \n`text, canny-edge and dreambooth conditional video generation`. For more information about this work, \nplease have a look at our [paper](https://arxiv.org/abs/2303.13439) and our demo: [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/PAIR/Text2Video-Zero)\nOur [code](https://github.com/Picsart-AI-Research/Text2Video-Zero) works with any StableDiffusion base model.\n\nThis model provides [DreamBooth](https://arxiv.org/abs/2208.12242) weights for the `Avatar style` to be used with edge guidance (using [ControlNet](https://arxiv.org/abs/2302.05543)) in text2video zero.\n created at: 2023-03-27\n- model_name: Wan-AI/Wan2.1-T2V-1.3B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-01T08:39:07+00:00\n- model_name: bullerwins/Wan2.2-T2V-A14B-GGUF\n  description: you need to download both a high-noise model and a low-noise model. high noise is used for the first steps and the low-noise for the details. place them on comfyui/models/unet you will need from  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T16:29:01+00:00\n- model_name: Wan-AI/Wan2.2-T2V-A14B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T09:04:28+00:00\n- model_name: genmo/mochi-1-preview\n  description: a state of the art video generation model by . mochi 1 preview is an open state-of-the-art video generation model with high-fidelity motion and strong prompt adherence in preliminary evaluation. this model dramatically closes the gap between closed and open video generation systems. we‚Äôre releasing the model under a permissive apache 2.0 license. try this model for free on .\n created at: 2024-10-22T07:36:03+00:00\n- model_name: Wan-AI/Wan2.1-T2V-14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-02-25T07:26:34+00:00\n- model_name: Wan-AI/Wan2.1-T2V-14B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-01T08:39:52+00:00\n- model_name: QuantStack/Wan2.1_T2V_14B_LightX2V_StepCfgDistill_VACE-GGUF\n  description: this is a gguf conversion of an addon of and scopes. the process involved extracting vace scopes and injecting into the target models, using scripts provided by . all quantized versions were created from the fp16 model using the conversion scripts provided by city96, available at the github repository.\n created at: 2025-06-16T15:13:22+00:00\n- model_name: QuantStack/Wan2.1_14B_VACE-GGUF\n  description: - based on the this is a direct gguf conversion of all quants are created from the fp32 base file, though i only uploaded the q8 0 and less, if you want the f16 or bf16 one i would upload it per request.\n created at: 2025-05-16T14:55:27+00:00\n- model_name: Wan-AI/Wan2.2-TI2V-5B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T09:03:29+00:00\n- model_name: Skywork/SkyReels-V2-DF-14B-720P\n  description: skyreels v2: infinite-length film generative model üìë technical report ¬∑ üëã playground ¬∑ üí¨ discord ¬∑ ü§ó hugging face ¬∑ ü§ñ modelscope ¬∑ üåê github\n created at: 2025-04-18T13:34:52+00:00\n- model_name: Wan-AI/Wan2.2-TI2V-5B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-18T09:35:44+00:00\n- model_name: QuantStack/Wan2.1_T2V_14B_FusionX_VACE-GGUF\n  description: this is a gguf conversion of . all quantized versions were created from the base fp16 model using the conversion scripts provided by city96, available at the github repository. the model files can be used in with the custom node. place the required models in the following folders:\n created at: 2025-06-08T02:59:38+00:00\n- model_name: FastVideo/FastWan2.2-TI2V-5B-FullAttn-Diffusers\n  description: fastvideo team&emsp; hf paper vsa arxiv paper vsa github  we're excited to introduce the fastwan2.2 series a new line of models finetuned with our novel sparse-distill strategy. this approach jointly integrates dmd and vsa in a single training process, combining the benefits of both distillation to shorten diffusion steps and sparse attention to reduce attention computations, enabling even faster video generation. fastwan2.2-ti2v-5b-full-diffusers is built upon wan-ai/wan2.2-ti2v-5b-diffusers. it supports efficient 3-step inference and produces high-quality videos at 121√ó704√ó1280 resolution. for training, we used simulated forward for the generator model, making the process data-free. the current fastwan2.2-ti2v-5b-full-diffusers model is trained using only dmd .\n created at: 2025-08-02T04:49:50+00:00\n- model_name: Wan-AI/Wan2.2-T2V-A14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-24T15:07:15+00:00\n- model_name: magespace/Wan2.2-I2V-A14B-Lightning-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp üîó lightx2v distilled & optimized wan2.2 for fast, high-quality 480p / 720p image-to-video generation   wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-08-14T20:06:59+00:00\n- model_name: Wan-AI/Wan2.1-T2V-1.3B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-02-25T11:13:48+00:00\n- model_name: QuantStack/Wan2.1_T2V_14B_FusionX-GGUF\n  description: this is a gguf conversion of by . all quantized versions were created from the base t2v fp16 model using the conversion scripts provided by city96, available at the github repository. the model files can be used in with the custom node. place the required models in the following folders:\n created at: 2025-06-07T01:47:02+00:00\n- model_name: DFloat11/Wan2.2-T2V-A14B-DF11\n  description: this is a dfloat11 losslessly compressed version of the original `wan-ai/wan2.2-t2v-a14b` model. it reduces model size by 32% compared to the original bfloat16 model, while maintaining bit-identical outputs and supporting efficient gpu inference . üî•üî•üî• thanks to dfloat11 compression, `wan-ai/wan2.2-t2v-a14b` can now generate a 5-second 720p video on a single 24gb gpu, while maintaining full model quality. üî•üî•üî• model model size peak gpu memory 5-second 720p generation generation time a100 gpu\n created at: 2025-08-01T20:24:13+00:00\n- model_name: nvidia/Cosmos-1.0-Diffusion-7B-Text2World\n  description: cosmos world foundation models : a family of highly performant pre-trained world foundation models purpose-built for generating physics-aware videos and world states for physical ai development. the cosmos diffusion models are a collection of diffusion based world foundation models that generate dynamic, high quality videos from text, image, or video inputs. it can serve as the building block for various applications or research that are related to world generation. the models are ready for commercial use under nvidia open model license agreement.  cosmos world foundation models : a family of highly performant pre-trained world foundation models purpose-built for generating physics-aware videos and world states for physical ai development. the cosmos diffusion models are a collection of diffusion based world foundation models that generate dynamic, high quality videos from text, image, or video inputs. it can serve as the building block for various applications or research that are related to world generation. the models are ready for commercial use under nvidia open model license agreement.  model developer : nvidia\n created at: 2025-01-07T04:52:52+00:00\n- model_name: DFloat11/Wan2.2-T2V-A14B-2-DF11\n  description: this is a dfloat11 losslessly compressed version of the original `wan-ai/wan2.2-t2v-a14b` model. it reduces model size by 32% compared to the original bfloat16 model, while maintaining bit-identical outputs and supporting efficient gpu inference . üî•üî•üî• thanks to dfloat11 compression, `wan-ai/wan2.2-t2v-a14b` can now generate a 5-second 720p video on a single 24gb gpu, while maintaining full model quality. üî•üî•üî• model model size peak gpu memory 5-second 720p generation generation time a100 gpu\n created at: 2025-08-01T20:25:49+00:00\n\nExamples:\n- example user query: Our company aims to build a product that generates videos from text descriptions. We need to create a video of \"Spiderman surfing.\"\n  model_name: damo-vilab/text-to-video-ms-1.7b\n- example user query: Our customer, a children's toy company, wants to create an animated video advertisement based on their story concept: \"A brave teddy bear saving a cat from a tall tree\".\n  model_name: nitrosocke/mo-di-diffusion\n- example user query: As a film producer, I need a tool that could create a short video sequence based on a given script to visualize a specific scene.\n  model_name: chavinlo/TempoFunk\n- example user query: Our user research team wants to know whether we can create video for advertisement by just describing the advertisement.\n  model_name: ImRma/Brucelee\n- example user query: We want to create a short video clip that showcases a superhero-themed pizza commercial. The commercial should involve Spider-Man delivering pizzas to customers in a city setting. The video should be 5 seconds long and 256x256px dimensions.\n  model_name: nitrosocke/redshift-diffusion\n- example user query: I am writing a script for an educational video on biology. Can you generate a video using the text of my script?\n  model_name: duncan93/video\n\nUser prompt:\nGenerating a video based on the provided canny video and a theme around 'a high-quality building with intricate architecture'.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <controllable_video_generation>\n\nCandidate Models:\n- model_name: camenduru/text2-video-zero\n  description: This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.\n created at: 2023-03-29\n- model_name: chavinlo/TempoFunk\n  description: A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text.\n created at: 2023-02-07\n- model_name: damo-vilab/text-to-video-ms-1.7b\n  description: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. The overall model parameters are about 1.7 billion. Currently, it only supports English input.\n created at: 2023-03-22\n- model_name: nitrosocke/redshift-diffusion\n  description: Tune-A-Video - Redshift is a text-to-video generation model based on the nitrosocke/redshift-diffusion model. It generates videos based on textual prompts, such as 'a man is skiing' or '(redshift style) spider man is skiing'.\n created at: 2022-11-06\n- model_name: nitrosocke/mo-di-diffusion\n  description: Tune-A-Video is a text-to-video generation model based on the Hugging Face framework. The model generates videos based on textual prompts in a modern Disney style.\n created at: 2022-10-27\n- model_name: ImRma/Brucelee\n  description: A Hugging Face model for converting Persian and English text into video.\n created at: 2023-04-23\n- model_name: damo-vilab/text-to-video-ms-1.7b-legacy\n  description: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.\n created at: 2023-03-22\n- model_name: duncan93/video\n  description: A text-to-video model trained on OpenAssistant/oasst1 dataset.\n created at: 2023-05-02\n- model_name: cerspense/zeroscope_v2_576w\n  description: zeroscope_v2_576w uses 7.9gb of vram when rendering 30 frames at 576x320\n\n### Using it with the 1111 text2video extension\n\n1. Download files in the zs2_576w folder.\n2. Replace the respective files in the 'stable-diffusion-webui\\models\\ModelScope\\t2v' directory.\n\n### Upscaling recommendations\n\nFor upscaling, it's recommended to use [zeroscope_v2_XL](https://huggingface.co/cerspense/zeroscope_v2_XL) via vid2vid in the 1111 extension. It works best at 1024x576 with a denoise strength between 0.66 and 0.85. Remember to use the same prompt that was used to generate the original clip. <br />\n\n### Usage in üß® Diffusers\n\nLet's first install the libraries required:\n\n```bash\n$ pip install diffusers transformers accelerate torch\n```\n\nNow, generate a video:\n\n```py\nimport torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_video\n\npipe = DiffusionPipeline.from_pretrained(\"cerspense/zeroscope_v2_576w\", torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n\nprompt = \"Darth Vader is surfing on waves\"\nvideo_frames = pipe(prompt, num_inference_steps=40, height=320, width=576, num_frames=24).frames\nvideo_path = export_to_video(video_frames)\n```\n\nHere are some results:\n\n<table>\n    <tr>\n        Darth vader is surfing on waves.\n        <br>\n        <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/darthvader_cerpense.gif\"\n            alt=\"Darth vader surfing in waves.\"\n            style=\"width: 576;\" />\n        </center></td>\n    </tr>\n</table>\n\n### Known issues\n\nLower resolutions or fewer frames could lead to suboptimal output. <br />\n\nThanks to [camenduru](https://github.com/camenduru), [kabachuha](https://github.com/kabachuha), [ExponentialML](https://github.com/ExponentialML), [dotsimulate](https://www.instagram.com/dotsimulate/), [VANYA](https://twitter.com/veryVANYA), [polyware](https://twitter.com/polyware_ai), [tin2tin](https://github.com/tin2tin)<br />\n created at: 2023-06-21\n- model_name: Searchium-ai/clip4clip-webvid150k\n  description: A CLIP4Clip video-text retrieval model trained on a subset of the WebVid dataset. \nThe model and training method are described in the paper [\"Clip4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval\"](https://arxiv.org/pdf/2104.08860.pdf) by Lou et el, and implemented in the accompanying [GitHub repository](https://github.com/ArrowLuo/CLIP4Clip).\n\nThe training process utilized the [WebVid Dataset](https://m-bain.github.io/webvid-dataset/), a comprehensive collection of short videos with corresponding textual descriptions sourced from the web. \nFor training purposes, a subset consisting of the first 150,000 video-text pairs from the dataset were used.\n\nThis HF model is based on the [clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32) architecture, with weights trained by Daphna Idelson at [Searchium](https://www.searchium.ai).\n\n\n# How to use\n### Extracting Text Embeddings:\n\n```python\nimport numpy as np\nimport torch\nfrom transformers import CLIPTokenizer, CLIPTextModelWithProjection\n\n\nsearch_sentence = \"a basketball player performing a slam dunk\"\n\nmodel = CLIPTextModelWithProjection.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\ntokenizer = CLIPTokenizer.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\n\ninputs = tokenizer(text=search_sentence , return_tensors=\"pt\")\noutputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n\n# Normalize embeddings for retrieval:\nfinal_output = outputs[0] / outputs[0].norm(dim=-1, keepdim=True)\nfinal_output = final_output.cpu().detach().numpy()\nprint(\"final output: \", final_output)\n```\n\n### Extracting Video Embeddings:\n\nAn additional notebook [\"GSI_VideoRetrieval_VideoEmbedding.ipynb\"](https://huggingface.co/Searchium-ai/clip4clip-webvid150k/blob/main/Notebooks/GSI_VideoRetrieval_VideoEmbedding.ipynb), provides instructions for extracting video embeddings and includes the necessary tools for preprocessing videos.\n created at: 2023-04-17\n- model_name: PAIR/text2video-zero-controlnet-canny-avatar\n  description: [Text2Video-Zero](https://arxiv.org/abs/2303.13439) is a zero-shot text to video generator. It can perform `zero-shot text-to-video generation`, `Video Instruct Pix2Pix` (instruction-guided video editing), \n`text and pose conditional video generation`, `text and canny-edge conditional video generation`, and \n`text, canny-edge and dreambooth conditional video generation`. For more information about this work, \nplease have a look at our [paper](https://arxiv.org/abs/2303.13439) and our demo: [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/PAIR/Text2Video-Zero)\nOur [code](https://github.com/Picsart-AI-Research/Text2Video-Zero) works with any StableDiffusion base model.\n\nThis model provides [DreamBooth](https://arxiv.org/abs/2208.12242) weights for the `Avatar style` to be used with edge guidance (using [ControlNet](https://arxiv.org/abs/2302.05543)) in text2video zero.\n created at: 2023-03-27\n- model_name: Wan-AI/Wan2.1-T2V-1.3B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-01T08:39:07+00:00\n- model_name: bullerwins/Wan2.2-T2V-A14B-GGUF\n  description: you need to download both a high-noise model and a low-noise model. high noise is used for the first steps and the low-noise for the details. place them on comfyui/models/unet you will need from  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T16:29:01+00:00\n- model_name: Wan-AI/Wan2.2-T2V-A14B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T09:04:28+00:00\n- model_name: genmo/mochi-1-preview\n  description: a state of the art video generation model by . mochi 1 preview is an open state-of-the-art video generation model with high-fidelity motion and strong prompt adherence in preliminary evaluation. this model dramatically closes the gap between closed and open video generation systems. we‚Äôre releasing the model under a permissive apache 2.0 license. try this model for free on .\n created at: 2024-10-22T07:36:03+00:00\n- model_name: Wan-AI/Wan2.1-T2V-14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-02-25T07:26:34+00:00\n- model_name: Wan-AI/Wan2.1-T2V-14B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-01T08:39:52+00:00\n- model_name: QuantStack/Wan2.1_T2V_14B_LightX2V_StepCfgDistill_VACE-GGUF\n  description: this is a gguf conversion of an addon of and scopes. the process involved extracting vace scopes and injecting into the target models, using scripts provided by . all quantized versions were created from the fp16 model using the conversion scripts provided by city96, available at the github repository.\n created at: 2025-06-16T15:13:22+00:00\n- model_name: QuantStack/Wan2.1_14B_VACE-GGUF\n  description: - based on the this is a direct gguf conversion of all quants are created from the fp32 base file, though i only uploaded the q8 0 and less, if you want the f16 or bf16 one i would upload it per request.\n created at: 2025-05-16T14:55:27+00:00\n- model_name: Wan-AI/Wan2.2-TI2V-5B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T09:03:29+00:00\n- model_name: Skywork/SkyReels-V2-DF-14B-720P\n  description: skyreels v2: infinite-length film generative model üìë technical report ¬∑ üëã playground ¬∑ üí¨ discord ¬∑ ü§ó hugging face ¬∑ ü§ñ modelscope ¬∑ üåê github\n created at: 2025-04-18T13:34:52+00:00\n- model_name: Wan-AI/Wan2.2-TI2V-5B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-18T09:35:44+00:00\n- model_name: QuantStack/Wan2.1_T2V_14B_FusionX_VACE-GGUF\n  description: this is a gguf conversion of . all quantized versions were created from the base fp16 model using the conversion scripts provided by city96, available at the github repository. the model files can be used in with the custom node. place the required models in the following folders:\n created at: 2025-06-08T02:59:38+00:00\n- model_name: FastVideo/FastWan2.2-TI2V-5B-FullAttn-Diffusers\n  description: fastvideo team&emsp; hf paper vsa arxiv paper vsa github  we're excited to introduce the fastwan2.2 series a new line of models finetuned with our novel sparse-distill strategy. this approach jointly integrates dmd and vsa in a single training process, combining the benefits of both distillation to shorten diffusion steps and sparse attention to reduce attention computations, enabling even faster video generation. fastwan2.2-ti2v-5b-full-diffusers is built upon wan-ai/wan2.2-ti2v-5b-diffusers. it supports efficient 3-step inference and produces high-quality videos at 121√ó704√ó1280 resolution. for training, we used simulated forward for the generator model, making the process data-free. the current fastwan2.2-ti2v-5b-full-diffusers model is trained using only dmd .\n created at: 2025-08-02T04:49:50+00:00\n- model_name: Wan-AI/Wan2.2-T2V-A14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-24T15:07:15+00:00\n- model_name: magespace/Wan2.2-I2V-A14B-Lightning-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp üîó lightx2v distilled & optimized wan2.2 for fast, high-quality 480p / 720p image-to-video generation   wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-08-14T20:06:59+00:00\n- model_name: Wan-AI/Wan2.1-T2V-1.3B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-02-25T11:13:48+00:00\n- model_name: QuantStack/Wan2.1_T2V_14B_FusionX-GGUF\n  description: this is a gguf conversion of by . all quantized versions were created from the base t2v fp16 model using the conversion scripts provided by city96, available at the github repository. the model files can be used in with the custom node. place the required models in the following folders:\n created at: 2025-06-07T01:47:02+00:00\n- model_name: DFloat11/Wan2.2-T2V-A14B-DF11\n  description: this is a dfloat11 losslessly compressed version of the original `wan-ai/wan2.2-t2v-a14b` model. it reduces model size by 32% compared to the original bfloat16 model, while maintaining bit-identical outputs and supporting efficient gpu inference . üî•üî•üî• thanks to dfloat11 compression, `wan-ai/wan2.2-t2v-a14b` can now generate a 5-second 720p video on a single 24gb gpu, while maintaining full model quality. üî•üî•üî• model model size peak gpu memory 5-second 720p generation generation time a100 gpu\n created at: 2025-08-01T20:24:13+00:00\n- model_name: nvidia/Cosmos-1.0-Diffusion-7B-Text2World\n  description: cosmos world foundation models : a family of highly performant pre-trained world foundation models purpose-built for generating physics-aware videos and world states for physical ai development. the cosmos diffusion models are a collection of diffusion based world foundation models that generate dynamic, high quality videos from text, image, or video inputs. it can serve as the building block for various applications or research that are related to world generation. the models are ready for commercial use under nvidia open model license agreement.  cosmos world foundation models : a family of highly performant pre-trained world foundation models purpose-built for generating physics-aware videos and world states for physical ai development. the cosmos diffusion models are a collection of diffusion based world foundation models that generate dynamic, high quality videos from text, image, or video inputs. it can serve as the building block for various applications or research that are related to world generation. the models are ready for commercial use under nvidia open model license agreement.  model developer : nvidia\n created at: 2025-01-07T04:52:52+00:00\n- model_name: DFloat11/Wan2.2-T2V-A14B-2-DF11\n  description: this is a dfloat11 losslessly compressed version of the original `wan-ai/wan2.2-t2v-a14b` model. it reduces model size by 32% compared to the original bfloat16 model, while maintaining bit-identical outputs and supporting efficient gpu inference . üî•üî•üî• thanks to dfloat11 compression, `wan-ai/wan2.2-t2v-a14b` can now generate a 5-second 720p video on a single 24gb gpu, while maintaining full model quality. üî•üî•üî• model model size peak gpu memory 5-second 720p generation generation time a100 gpu\n created at: 2025-08-01T20:25:49+00:00\n\nExamples:\n- example user query: Our company aims to build a product that generates videos from text descriptions. We need to create a video of \"Spiderman surfing.\"\n  model_name: damo-vilab/text-to-video-ms-1.7b\n- example user query: Our customer, a children's toy company, wants to create an animated video advertisement based on their story concept: \"A brave teddy bear saving a cat from a tall tree\".\n  model_name: nitrosocke/mo-di-diffusion\n- example user query: As a film producer, I need a tool that could create a short video sequence based on a given script to visualize a specific scene.\n  model_name: chavinlo/TempoFunk\n- example user query: Our user research team wants to know whether we can create video for advertisement by just describing the advertisement.\n  model_name: ImRma/Brucelee\n- example user query: We want to create a short video clip that showcases a superhero-themed pizza commercial. The commercial should involve Spider-Man delivering pizzas to customers in a city setting. The video should be 5 seconds long and 256x256px dimensions.\n  model_name: nitrosocke/redshift-diffusion\n- example user query: I am writing a script for an educational video on biology. Can you generate a video using the text of my script?\n  model_name: duncan93/video\n\nUser prompt:\nUsing the this depth video, let‚Äôs transform our ideas into a heartfelt film that highlights 'an intimate concert where the musician connects with the audience through heartfelt melodies', inviting viewers into an emotional journey.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <controllable_video_generation>\n\nCandidate Models:\n- model_name: camenduru/text2-video-zero\n  description: This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.\n created at: 2023-03-29\n- model_name: chavinlo/TempoFunk\n  description: A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text.\n created at: 2023-02-07\n- model_name: damo-vilab/text-to-video-ms-1.7b\n  description: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. The overall model parameters are about 1.7 billion. Currently, it only supports English input.\n created at: 2023-03-22\n- model_name: nitrosocke/redshift-diffusion\n  description: Tune-A-Video - Redshift is a text-to-video generation model based on the nitrosocke/redshift-diffusion model. It generates videos based on textual prompts, such as 'a man is skiing' or '(redshift style) spider man is skiing'.\n created at: 2022-11-06\n- model_name: nitrosocke/mo-di-diffusion\n  description: Tune-A-Video is a text-to-video generation model based on the Hugging Face framework. The model generates videos based on textual prompts in a modern Disney style.\n created at: 2022-10-27\n- model_name: ImRma/Brucelee\n  description: A Hugging Face model for converting Persian and English text into video.\n created at: 2023-04-23\n- model_name: damo-vilab/text-to-video-ms-1.7b-legacy\n  description: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.\n created at: 2023-03-22\n- model_name: duncan93/video\n  description: A text-to-video model trained on OpenAssistant/oasst1 dataset.\n created at: 2023-05-02\n- model_name: cerspense/zeroscope_v2_576w\n  description: zeroscope_v2_576w uses 7.9gb of vram when rendering 30 frames at 576x320\n\n### Using it with the 1111 text2video extension\n\n1. Download files in the zs2_576w folder.\n2. Replace the respective files in the 'stable-diffusion-webui\\models\\ModelScope\\t2v' directory.\n\n### Upscaling recommendations\n\nFor upscaling, it's recommended to use [zeroscope_v2_XL](https://huggingface.co/cerspense/zeroscope_v2_XL) via vid2vid in the 1111 extension. It works best at 1024x576 with a denoise strength between 0.66 and 0.85. Remember to use the same prompt that was used to generate the original clip. <br />\n\n### Usage in üß® Diffusers\n\nLet's first install the libraries required:\n\n```bash\n$ pip install diffusers transformers accelerate torch\n```\n\nNow, generate a video:\n\n```py\nimport torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_video\n\npipe = DiffusionPipeline.from_pretrained(\"cerspense/zeroscope_v2_576w\", torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n\nprompt = \"Darth Vader is surfing on waves\"\nvideo_frames = pipe(prompt, num_inference_steps=40, height=320, width=576, num_frames=24).frames\nvideo_path = export_to_video(video_frames)\n```\n\nHere are some results:\n\n<table>\n    <tr>\n        Darth vader is surfing on waves.\n        <br>\n        <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/darthvader_cerpense.gif\"\n            alt=\"Darth vader surfing in waves.\"\n            style=\"width: 576;\" />\n        </center></td>\n    </tr>\n</table>\n\n### Known issues\n\nLower resolutions or fewer frames could lead to suboptimal output. <br />\n\nThanks to [camenduru](https://github.com/camenduru), [kabachuha](https://github.com/kabachuha), [ExponentialML](https://github.com/ExponentialML), [dotsimulate](https://www.instagram.com/dotsimulate/), [VANYA](https://twitter.com/veryVANYA), [polyware](https://twitter.com/polyware_ai), [tin2tin](https://github.com/tin2tin)<br />\n created at: 2023-06-21\n- model_name: Searchium-ai/clip4clip-webvid150k\n  description: A CLIP4Clip video-text retrieval model trained on a subset of the WebVid dataset. \nThe model and training method are described in the paper [\"Clip4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval\"](https://arxiv.org/pdf/2104.08860.pdf) by Lou et el, and implemented in the accompanying [GitHub repository](https://github.com/ArrowLuo/CLIP4Clip).\n\nThe training process utilized the [WebVid Dataset](https://m-bain.github.io/webvid-dataset/), a comprehensive collection of short videos with corresponding textual descriptions sourced from the web. \nFor training purposes, a subset consisting of the first 150,000 video-text pairs from the dataset were used.\n\nThis HF model is based on the [clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32) architecture, with weights trained by Daphna Idelson at [Searchium](https://www.searchium.ai).\n\n\n# How to use\n### Extracting Text Embeddings:\n\n```python\nimport numpy as np\nimport torch\nfrom transformers import CLIPTokenizer, CLIPTextModelWithProjection\n\n\nsearch_sentence = \"a basketball player performing a slam dunk\"\n\nmodel = CLIPTextModelWithProjection.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\ntokenizer = CLIPTokenizer.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\n\ninputs = tokenizer(text=search_sentence , return_tensors=\"pt\")\noutputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n\n# Normalize embeddings for retrieval:\nfinal_output = outputs[0] / outputs[0].norm(dim=-1, keepdim=True)\nfinal_output = final_output.cpu().detach().numpy()\nprint(\"final output: \", final_output)\n```\n\n### Extracting Video Embeddings:\n\nAn additional notebook [\"GSI_VideoRetrieval_VideoEmbedding.ipynb\"](https://huggingface.co/Searchium-ai/clip4clip-webvid150k/blob/main/Notebooks/GSI_VideoRetrieval_VideoEmbedding.ipynb), provides instructions for extracting video embeddings and includes the necessary tools for preprocessing videos.\n created at: 2023-04-17\n- model_name: PAIR/text2video-zero-controlnet-canny-avatar\n  description: [Text2Video-Zero](https://arxiv.org/abs/2303.13439) is a zero-shot text to video generator. It can perform `zero-shot text-to-video generation`, `Video Instruct Pix2Pix` (instruction-guided video editing), \n`text and pose conditional video generation`, `text and canny-edge conditional video generation`, and \n`text, canny-edge and dreambooth conditional video generation`. For more information about this work, \nplease have a look at our [paper](https://arxiv.org/abs/2303.13439) and our demo: [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/PAIR/Text2Video-Zero)\nOur [code](https://github.com/Picsart-AI-Research/Text2Video-Zero) works with any StableDiffusion base model.\n\nThis model provides [DreamBooth](https://arxiv.org/abs/2208.12242) weights for the `Avatar style` to be used with edge guidance (using [ControlNet](https://arxiv.org/abs/2302.05543)) in text2video zero.\n created at: 2023-03-27\n- model_name: Wan-AI/Wan2.1-T2V-1.3B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-01T08:39:07+00:00\n- model_name: bullerwins/Wan2.2-T2V-A14B-GGUF\n  description: you need to download both a high-noise model and a low-noise model. high noise is used for the first steps and the low-noise for the details. place them on comfyui/models/unet you will need from  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T16:29:01+00:00\n- model_name: Wan-AI/Wan2.2-T2V-A14B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T09:04:28+00:00\n- model_name: genmo/mochi-1-preview\n  description: a state of the art video generation model by . mochi 1 preview is an open state-of-the-art video generation model with high-fidelity motion and strong prompt adherence in preliminary evaluation. this model dramatically closes the gap between closed and open video generation systems. we‚Äôre releasing the model under a permissive apache 2.0 license. try this model for free on .\n created at: 2024-10-22T07:36:03+00:00\n- model_name: Wan-AI/Wan2.1-T2V-14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-02-25T07:26:34+00:00\n- model_name: Wan-AI/Wan2.1-T2V-14B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-01T08:39:52+00:00\n- model_name: QuantStack/Wan2.1_T2V_14B_LightX2V_StepCfgDistill_VACE-GGUF\n  description: this is a gguf conversion of an addon of and scopes. the process involved extracting vace scopes and injecting into the target models, using scripts provided by . all quantized versions were created from the fp16 model using the conversion scripts provided by city96, available at the github repository.\n created at: 2025-06-16T15:13:22+00:00\n- model_name: QuantStack/Wan2.1_14B_VACE-GGUF\n  description: - based on the this is a direct gguf conversion of all quants are created from the fp32 base file, though i only uploaded the q8 0 and less, if you want the f16 or bf16 one i would upload it per request.\n created at: 2025-05-16T14:55:27+00:00\n- model_name: Wan-AI/Wan2.2-TI2V-5B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T09:03:29+00:00\n- model_name: Skywork/SkyReels-V2-DF-14B-720P\n  description: skyreels v2: infinite-length film generative model üìë technical report ¬∑ üëã playground ¬∑ üí¨ discord ¬∑ ü§ó hugging face ¬∑ ü§ñ modelscope ¬∑ üåê github\n created at: 2025-04-18T13:34:52+00:00\n- model_name: Wan-AI/Wan2.2-TI2V-5B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-18T09:35:44+00:00\n- model_name: QuantStack/Wan2.1_T2V_14B_FusionX_VACE-GGUF\n  description: this is a gguf conversion of . all quantized versions were created from the base fp16 model using the conversion scripts provided by city96, available at the github repository. the model files can be used in with the custom node. place the required models in the following folders:\n created at: 2025-06-08T02:59:38+00:00\n- model_name: FastVideo/FastWan2.2-TI2V-5B-FullAttn-Diffusers\n  description: fastvideo team&emsp; hf paper vsa arxiv paper vsa github  we're excited to introduce the fastwan2.2 series a new line of models finetuned with our novel sparse-distill strategy. this approach jointly integrates dmd and vsa in a single training process, combining the benefits of both distillation to shorten diffusion steps and sparse attention to reduce attention computations, enabling even faster video generation. fastwan2.2-ti2v-5b-full-diffusers is built upon wan-ai/wan2.2-ti2v-5b-diffusers. it supports efficient 3-step inference and produces high-quality videos at 121√ó704√ó1280 resolution. for training, we used simulated forward for the generator model, making the process data-free. the current fastwan2.2-ti2v-5b-full-diffusers model is trained using only dmd .\n created at: 2025-08-02T04:49:50+00:00\n- model_name: Wan-AI/Wan2.2-T2V-A14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-24T15:07:15+00:00\n- model_name: magespace/Wan2.2-I2V-A14B-Lightning-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp üîó lightx2v distilled & optimized wan2.2 for fast, high-quality 480p / 720p image-to-video generation   wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-08-14T20:06:59+00:00\n- model_name: Wan-AI/Wan2.1-T2V-1.3B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-02-25T11:13:48+00:00\n- model_name: QuantStack/Wan2.1_T2V_14B_FusionX-GGUF\n  description: this is a gguf conversion of by . all quantized versions were created from the base t2v fp16 model using the conversion scripts provided by city96, available at the github repository. the model files can be used in with the custom node. place the required models in the following folders:\n created at: 2025-06-07T01:47:02+00:00\n- model_name: DFloat11/Wan2.2-T2V-A14B-DF11\n  description: this is a dfloat11 losslessly compressed version of the original `wan-ai/wan2.2-t2v-a14b` model. it reduces model size by 32% compared to the original bfloat16 model, while maintaining bit-identical outputs and supporting efficient gpu inference . üî•üî•üî• thanks to dfloat11 compression, `wan-ai/wan2.2-t2v-a14b` can now generate a 5-second 720p video on a single 24gb gpu, while maintaining full model quality. üî•üî•üî• model model size peak gpu memory 5-second 720p generation generation time a100 gpu\n created at: 2025-08-01T20:24:13+00:00\n- model_name: nvidia/Cosmos-1.0-Diffusion-7B-Text2World\n  description: cosmos world foundation models : a family of highly performant pre-trained world foundation models purpose-built for generating physics-aware videos and world states for physical ai development. the cosmos diffusion models are a collection of diffusion based world foundation models that generate dynamic, high quality videos from text, image, or video inputs. it can serve as the building block for various applications or research that are related to world generation. the models are ready for commercial use under nvidia open model license agreement.  cosmos world foundation models : a family of highly performant pre-trained world foundation models purpose-built for generating physics-aware videos and world states for physical ai development. the cosmos diffusion models are a collection of diffusion based world foundation models that generate dynamic, high quality videos from text, image, or video inputs. it can serve as the building block for various applications or research that are related to world generation. the models are ready for commercial use under nvidia open model license agreement.  model developer : nvidia\n created at: 2025-01-07T04:52:52+00:00\n- model_name: DFloat11/Wan2.2-T2V-A14B-2-DF11\n  description: this is a dfloat11 losslessly compressed version of the original `wan-ai/wan2.2-t2v-a14b` model. it reduces model size by 32% compared to the original bfloat16 model, while maintaining bit-identical outputs and supporting efficient gpu inference . üî•üî•üî• thanks to dfloat11 compression, `wan-ai/wan2.2-t2v-a14b` can now generate a 5-second 720p video on a single 24gb gpu, while maintaining full model quality. üî•üî•üî• model model size peak gpu memory 5-second 720p generation generation time a100 gpu\n created at: 2025-08-01T20:25:49+00:00\n\nExamples:\n- example user query: Our company aims to build a product that generates videos from text descriptions. We need to create a video of \"Spiderman surfing.\"\n  model_name: damo-vilab/text-to-video-ms-1.7b\n- example user query: Our customer, a children's toy company, wants to create an animated video advertisement based on their story concept: \"A brave teddy bear saving a cat from a tall tree\".\n  model_name: nitrosocke/mo-di-diffusion\n- example user query: As a film producer, I need a tool that could create a short video sequence based on a given script to visualize a specific scene.\n  model_name: chavinlo/TempoFunk\n- example user query: Our user research team wants to know whether we can create video for advertisement by just describing the advertisement.\n  model_name: ImRma/Brucelee\n- example user query: We want to create a short video clip that showcases a superhero-themed pizza commercial. The commercial should involve Spider-Man delivering pizzas to customers in a city setting. The video should be 5 seconds long and 256x256px dimensions.\n  model_name: nitrosocke/redshift-diffusion\n- example user query: I am writing a script for an educational video on biology. Can you generate a video using the text of my script?\n  model_name: duncan93/video\n\nUser prompt:\nWould it be possible to conduct a episode based on the pose episode and 'a serene countryside with rolling green hills'?\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <controllable_video_generation>\n\nCandidate Models:\n- model_name: camenduru/text2-video-zero\n  description: This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.\n created at: 2023-03-29\n- model_name: chavinlo/TempoFunk\n  description: A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text.\n created at: 2023-02-07\n- model_name: damo-vilab/text-to-video-ms-1.7b\n  description: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. The overall model parameters are about 1.7 billion. Currently, it only supports English input.\n created at: 2023-03-22\n- model_name: nitrosocke/redshift-diffusion\n  description: Tune-A-Video - Redshift is a text-to-video generation model based on the nitrosocke/redshift-diffusion model. It generates videos based on textual prompts, such as 'a man is skiing' or '(redshift style) spider man is skiing'.\n created at: 2022-11-06\n- model_name: nitrosocke/mo-di-diffusion\n  description: Tune-A-Video is a text-to-video generation model based on the Hugging Face framework. The model generates videos based on textual prompts in a modern Disney style.\n created at: 2022-10-27\n- model_name: ImRma/Brucelee\n  description: A Hugging Face model for converting Persian and English text into video.\n created at: 2023-04-23\n- model_name: damo-vilab/text-to-video-ms-1.7b-legacy\n  description: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.\n created at: 2023-03-22\n- model_name: duncan93/video\n  description: A text-to-video model trained on OpenAssistant/oasst1 dataset.\n created at: 2023-05-02\n- model_name: cerspense/zeroscope_v2_576w\n  description: zeroscope_v2_576w uses 7.9gb of vram when rendering 30 frames at 576x320\n\n### Using it with the 1111 text2video extension\n\n1. Download files in the zs2_576w folder.\n2. Replace the respective files in the 'stable-diffusion-webui\\models\\ModelScope\\t2v' directory.\n\n### Upscaling recommendations\n\nFor upscaling, it's recommended to use [zeroscope_v2_XL](https://huggingface.co/cerspense/zeroscope_v2_XL) via vid2vid in the 1111 extension. It works best at 1024x576 with a denoise strength between 0.66 and 0.85. Remember to use the same prompt that was used to generate the original clip. <br />\n\n### Usage in üß® Diffusers\n\nLet's first install the libraries required:\n\n```bash\n$ pip install diffusers transformers accelerate torch\n```\n\nNow, generate a video:\n\n```py\nimport torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_video\n\npipe = DiffusionPipeline.from_pretrained(\"cerspense/zeroscope_v2_576w\", torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n\nprompt = \"Darth Vader is surfing on waves\"\nvideo_frames = pipe(prompt, num_inference_steps=40, height=320, width=576, num_frames=24).frames\nvideo_path = export_to_video(video_frames)\n```\n\nHere are some results:\n\n<table>\n    <tr>\n        Darth vader is surfing on waves.\n        <br>\n        <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/darthvader_cerpense.gif\"\n            alt=\"Darth vader surfing in waves.\"\n            style=\"width: 576;\" />\n        </center></td>\n    </tr>\n</table>\n\n### Known issues\n\nLower resolutions or fewer frames could lead to suboptimal output. <br />\n\nThanks to [camenduru](https://github.com/camenduru), [kabachuha](https://github.com/kabachuha), [ExponentialML](https://github.com/ExponentialML), [dotsimulate](https://www.instagram.com/dotsimulate/), [VANYA](https://twitter.com/veryVANYA), [polyware](https://twitter.com/polyware_ai), [tin2tin](https://github.com/tin2tin)<br />\n created at: 2023-06-21\n- model_name: Searchium-ai/clip4clip-webvid150k\n  description: A CLIP4Clip video-text retrieval model trained on a subset of the WebVid dataset. \nThe model and training method are described in the paper [\"Clip4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval\"](https://arxiv.org/pdf/2104.08860.pdf) by Lou et el, and implemented in the accompanying [GitHub repository](https://github.com/ArrowLuo/CLIP4Clip).\n\nThe training process utilized the [WebVid Dataset](https://m-bain.github.io/webvid-dataset/), a comprehensive collection of short videos with corresponding textual descriptions sourced from the web. \nFor training purposes, a subset consisting of the first 150,000 video-text pairs from the dataset were used.\n\nThis HF model is based on the [clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32) architecture, with weights trained by Daphna Idelson at [Searchium](https://www.searchium.ai).\n\n\n# How to use\n### Extracting Text Embeddings:\n\n```python\nimport numpy as np\nimport torch\nfrom transformers import CLIPTokenizer, CLIPTextModelWithProjection\n\n\nsearch_sentence = \"a basketball player performing a slam dunk\"\n\nmodel = CLIPTextModelWithProjection.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\ntokenizer = CLIPTokenizer.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\n\ninputs = tokenizer(text=search_sentence , return_tensors=\"pt\")\noutputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n\n# Normalize embeddings for retrieval:\nfinal_output = outputs[0] / outputs[0].norm(dim=-1, keepdim=True)\nfinal_output = final_output.cpu().detach().numpy()\nprint(\"final output: \", final_output)\n```\n\n### Extracting Video Embeddings:\n\nAn additional notebook [\"GSI_VideoRetrieval_VideoEmbedding.ipynb\"](https://huggingface.co/Searchium-ai/clip4clip-webvid150k/blob/main/Notebooks/GSI_VideoRetrieval_VideoEmbedding.ipynb), provides instructions for extracting video embeddings and includes the necessary tools for preprocessing videos.\n created at: 2023-04-17\n- model_name: PAIR/text2video-zero-controlnet-canny-avatar\n  description: [Text2Video-Zero](https://arxiv.org/abs/2303.13439) is a zero-shot text to video generator. It can perform `zero-shot text-to-video generation`, `Video Instruct Pix2Pix` (instruction-guided video editing), \n`text and pose conditional video generation`, `text and canny-edge conditional video generation`, and \n`text, canny-edge and dreambooth conditional video generation`. For more information about this work, \nplease have a look at our [paper](https://arxiv.org/abs/2303.13439) and our demo: [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/PAIR/Text2Video-Zero)\nOur [code](https://github.com/Picsart-AI-Research/Text2Video-Zero) works with any StableDiffusion base model.\n\nThis model provides [DreamBooth](https://arxiv.org/abs/2208.12242) weights for the `Avatar style` to be used with edge guidance (using [ControlNet](https://arxiv.org/abs/2302.05543)) in text2video zero.\n created at: 2023-03-27\n- model_name: Wan-AI/Wan2.1-T2V-1.3B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-01T08:39:07+00:00\n- model_name: bullerwins/Wan2.2-T2V-A14B-GGUF\n  description: you need to download both a high-noise model and a low-noise model. high noise is used for the first steps and the low-noise for the details. place them on comfyui/models/unet you will need from  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T16:29:01+00:00\n- model_name: Wan-AI/Wan2.2-T2V-A14B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T09:04:28+00:00\n- model_name: genmo/mochi-1-preview\n  description: a state of the art video generation model by . mochi 1 preview is an open state-of-the-art video generation model with high-fidelity motion and strong prompt adherence in preliminary evaluation. this model dramatically closes the gap between closed and open video generation systems. we‚Äôre releasing the model under a permissive apache 2.0 license. try this model for free on .\n created at: 2024-10-22T07:36:03+00:00\n- model_name: Wan-AI/Wan2.1-T2V-14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-02-25T07:26:34+00:00\n- model_name: Wan-AI/Wan2.1-T2V-14B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-01T08:39:52+00:00\n- model_name: QuantStack/Wan2.1_T2V_14B_LightX2V_StepCfgDistill_VACE-GGUF\n  description: this is a gguf conversion of an addon of and scopes. the process involved extracting vace scopes and injecting into the target models, using scripts provided by . all quantized versions were created from the fp16 model using the conversion scripts provided by city96, available at the github repository.\n created at: 2025-06-16T15:13:22+00:00\n- model_name: QuantStack/Wan2.1_14B_VACE-GGUF\n  description: - based on the this is a direct gguf conversion of all quants are created from the fp32 base file, though i only uploaded the q8 0 and less, if you want the f16 or bf16 one i would upload it per request.\n created at: 2025-05-16T14:55:27+00:00\n- model_name: Wan-AI/Wan2.2-TI2V-5B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T09:03:29+00:00\n- model_name: Skywork/SkyReels-V2-DF-14B-720P\n  description: skyreels v2: infinite-length film generative model üìë technical report ¬∑ üëã playground ¬∑ üí¨ discord ¬∑ ü§ó hugging face ¬∑ ü§ñ modelscope ¬∑ üåê github\n created at: 2025-04-18T13:34:52+00:00\n- model_name: Wan-AI/Wan2.2-TI2V-5B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-18T09:35:44+00:00\n- model_name: QuantStack/Wan2.1_T2V_14B_FusionX_VACE-GGUF\n  description: this is a gguf conversion of . all quantized versions were created from the base fp16 model using the conversion scripts provided by city96, available at the github repository. the model files can be used in with the custom node. place the required models in the following folders:\n created at: 2025-06-08T02:59:38+00:00\n- model_name: FastVideo/FastWan2.2-TI2V-5B-FullAttn-Diffusers\n  description: fastvideo team&emsp; hf paper vsa arxiv paper vsa github  we're excited to introduce the fastwan2.2 series a new line of models finetuned with our novel sparse-distill strategy. this approach jointly integrates dmd and vsa in a single training process, combining the benefits of both distillation to shorten diffusion steps and sparse attention to reduce attention computations, enabling even faster video generation. fastwan2.2-ti2v-5b-full-diffusers is built upon wan-ai/wan2.2-ti2v-5b-diffusers. it supports efficient 3-step inference and produces high-quality videos at 121√ó704√ó1280 resolution. for training, we used simulated forward for the generator model, making the process data-free. the current fastwan2.2-ti2v-5b-full-diffusers model is trained using only dmd .\n created at: 2025-08-02T04:49:50+00:00\n- model_name: Wan-AI/Wan2.2-T2V-A14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-24T15:07:15+00:00\n- model_name: magespace/Wan2.2-I2V-A14B-Lightning-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp üîó lightx2v distilled & optimized wan2.2 for fast, high-quality 480p / 720p image-to-video generation   wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-08-14T20:06:59+00:00\n- model_name: Wan-AI/Wan2.1-T2V-1.3B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-02-25T11:13:48+00:00\n- model_name: QuantStack/Wan2.1_T2V_14B_FusionX-GGUF\n  description: this is a gguf conversion of by . all quantized versions were created from the base t2v fp16 model using the conversion scripts provided by city96, available at the github repository. the model files can be used in with the custom node. place the required models in the following folders:\n created at: 2025-06-07T01:47:02+00:00\n- model_name: DFloat11/Wan2.2-T2V-A14B-DF11\n  description: this is a dfloat11 losslessly compressed version of the original `wan-ai/wan2.2-t2v-a14b` model. it reduces model size by 32% compared to the original bfloat16 model, while maintaining bit-identical outputs and supporting efficient gpu inference . üî•üî•üî• thanks to dfloat11 compression, `wan-ai/wan2.2-t2v-a14b` can now generate a 5-second 720p video on a single 24gb gpu, while maintaining full model quality. üî•üî•üî• model model size peak gpu memory 5-second 720p generation generation time a100 gpu\n created at: 2025-08-01T20:24:13+00:00\n- model_name: nvidia/Cosmos-1.0-Diffusion-7B-Text2World\n  description: cosmos world foundation models : a family of highly performant pre-trained world foundation models purpose-built for generating physics-aware videos and world states for physical ai development. the cosmos diffusion models are a collection of diffusion based world foundation models that generate dynamic, high quality videos from text, image, or video inputs. it can serve as the building block for various applications or research that are related to world generation. the models are ready for commercial use under nvidia open model license agreement.  cosmos world foundation models : a family of highly performant pre-trained world foundation models purpose-built for generating physics-aware videos and world states for physical ai development. the cosmos diffusion models are a collection of diffusion based world foundation models that generate dynamic, high quality videos from text, image, or video inputs. it can serve as the building block for various applications or research that are related to world generation. the models are ready for commercial use under nvidia open model license agreement.  model developer : nvidia\n created at: 2025-01-07T04:52:52+00:00\n- model_name: DFloat11/Wan2.2-T2V-A14B-2-DF11\n  description: this is a dfloat11 losslessly compressed version of the original `wan-ai/wan2.2-t2v-a14b` model. it reduces model size by 32% compared to the original bfloat16 model, while maintaining bit-identical outputs and supporting efficient gpu inference . üî•üî•üî• thanks to dfloat11 compression, `wan-ai/wan2.2-t2v-a14b` can now generate a 5-second 720p video on a single 24gb gpu, while maintaining full model quality. üî•üî•üî• model model size peak gpu memory 5-second 720p generation generation time a100 gpu\n created at: 2025-08-01T20:25:49+00:00\n\nExamples:\n- example user query: Our company aims to build a product that generates videos from text descriptions. We need to create a video of \"Spiderman surfing.\"\n  model_name: damo-vilab/text-to-video-ms-1.7b\n- example user query: Our customer, a children's toy company, wants to create an animated video advertisement based on their story concept: \"A brave teddy bear saving a cat from a tall tree\".\n  model_name: nitrosocke/mo-di-diffusion\n- example user query: As a film producer, I need a tool that could create a short video sequence based on a given script to visualize a specific scene.\n  model_name: chavinlo/TempoFunk\n- example user query: Our user research team wants to know whether we can create video for advertisement by just describing the advertisement.\n  model_name: ImRma/Brucelee\n- example user query: We want to create a short video clip that showcases a superhero-themed pizza commercial. The commercial should involve Spider-Man delivering pizzas to customers in a city setting. The video should be 5 seconds long and 256x256px dimensions.\n  model_name: nitrosocke/redshift-diffusion\n- example user query: I am writing a script for an educational video on biology. Can you generate a video using the text of my script?\n  model_name: duncan93/video\n\nUser prompt:\nYour objective is to conceive a cinematic piece incorporating a portrayal 'a glowing cloud' and the segmentation film.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <controllable_video_generation>\n\nCandidate Models:\n- model_name: camenduru/text2-video-zero\n  description: This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.\n created at: 2023-03-29\n- model_name: chavinlo/TempoFunk\n  description: A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text.\n created at: 2023-02-07\n- model_name: damo-vilab/text-to-video-ms-1.7b\n  description: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. The overall model parameters are about 1.7 billion. Currently, it only supports English input.\n created at: 2023-03-22\n- model_name: nitrosocke/redshift-diffusion\n  description: Tune-A-Video - Redshift is a text-to-video generation model based on the nitrosocke/redshift-diffusion model. It generates videos based on textual prompts, such as 'a man is skiing' or '(redshift style) spider man is skiing'.\n created at: 2022-11-06\n- model_name: nitrosocke/mo-di-diffusion\n  description: Tune-A-Video is a text-to-video generation model based on the Hugging Face framework. The model generates videos based on textual prompts in a modern Disney style.\n created at: 2022-10-27\n- model_name: ImRma/Brucelee\n  description: A Hugging Face model for converting Persian and English text into video.\n created at: 2023-04-23\n- model_name: damo-vilab/text-to-video-ms-1.7b-legacy\n  description: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.\n created at: 2023-03-22\n- model_name: duncan93/video\n  description: A text-to-video model trained on OpenAssistant/oasst1 dataset.\n created at: 2023-05-02\n- model_name: cerspense/zeroscope_v2_576w\n  description: zeroscope_v2_576w uses 7.9gb of vram when rendering 30 frames at 576x320\n\n### Using it with the 1111 text2video extension\n\n1. Download files in the zs2_576w folder.\n2. Replace the respective files in the 'stable-diffusion-webui\\models\\ModelScope\\t2v' directory.\n\n### Upscaling recommendations\n\nFor upscaling, it's recommended to use [zeroscope_v2_XL](https://huggingface.co/cerspense/zeroscope_v2_XL) via vid2vid in the 1111 extension. It works best at 1024x576 with a denoise strength between 0.66 and 0.85. Remember to use the same prompt that was used to generate the original clip. <br />\n\n### Usage in üß® Diffusers\n\nLet's first install the libraries required:\n\n```bash\n$ pip install diffusers transformers accelerate torch\n```\n\nNow, generate a video:\n\n```py\nimport torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_video\n\npipe = DiffusionPipeline.from_pretrained(\"cerspense/zeroscope_v2_576w\", torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n\nprompt = \"Darth Vader is surfing on waves\"\nvideo_frames = pipe(prompt, num_inference_steps=40, height=320, width=576, num_frames=24).frames\nvideo_path = export_to_video(video_frames)\n```\n\nHere are some results:\n\n<table>\n    <tr>\n        Darth vader is surfing on waves.\n        <br>\n        <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/darthvader_cerpense.gif\"\n            alt=\"Darth vader surfing in waves.\"\n            style=\"width: 576;\" />\n        </center></td>\n    </tr>\n</table>\n\n### Known issues\n\nLower resolutions or fewer frames could lead to suboptimal output. <br />\n\nThanks to [camenduru](https://github.com/camenduru), [kabachuha](https://github.com/kabachuha), [ExponentialML](https://github.com/ExponentialML), [dotsimulate](https://www.instagram.com/dotsimulate/), [VANYA](https://twitter.com/veryVANYA), [polyware](https://twitter.com/polyware_ai), [tin2tin](https://github.com/tin2tin)<br />\n created at: 2023-06-21\n- model_name: Searchium-ai/clip4clip-webvid150k\n  description: A CLIP4Clip video-text retrieval model trained on a subset of the WebVid dataset. \nThe model and training method are described in the paper [\"Clip4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval\"](https://arxiv.org/pdf/2104.08860.pdf) by Lou et el, and implemented in the accompanying [GitHub repository](https://github.com/ArrowLuo/CLIP4Clip).\n\nThe training process utilized the [WebVid Dataset](https://m-bain.github.io/webvid-dataset/), a comprehensive collection of short videos with corresponding textual descriptions sourced from the web. \nFor training purposes, a subset consisting of the first 150,000 video-text pairs from the dataset were used.\n\nThis HF model is based on the [clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32) architecture, with weights trained by Daphna Idelson at [Searchium](https://www.searchium.ai).\n\n\n# How to use\n### Extracting Text Embeddings:\n\n```python\nimport numpy as np\nimport torch\nfrom transformers import CLIPTokenizer, CLIPTextModelWithProjection\n\n\nsearch_sentence = \"a basketball player performing a slam dunk\"\n\nmodel = CLIPTextModelWithProjection.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\ntokenizer = CLIPTokenizer.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\n\ninputs = tokenizer(text=search_sentence , return_tensors=\"pt\")\noutputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n\n# Normalize embeddings for retrieval:\nfinal_output = outputs[0] / outputs[0].norm(dim=-1, keepdim=True)\nfinal_output = final_output.cpu().detach().numpy()\nprint(\"final output: \", final_output)\n```\n\n### Extracting Video Embeddings:\n\nAn additional notebook [\"GSI_VideoRetrieval_VideoEmbedding.ipynb\"](https://huggingface.co/Searchium-ai/clip4clip-webvid150k/blob/main/Notebooks/GSI_VideoRetrieval_VideoEmbedding.ipynb), provides instructions for extracting video embeddings and includes the necessary tools for preprocessing videos.\n created at: 2023-04-17\n- model_name: PAIR/text2video-zero-controlnet-canny-avatar\n  description: [Text2Video-Zero](https://arxiv.org/abs/2303.13439) is a zero-shot text to video generator. It can perform `zero-shot text-to-video generation`, `Video Instruct Pix2Pix` (instruction-guided video editing), \n`text and pose conditional video generation`, `text and canny-edge conditional video generation`, and \n`text, canny-edge and dreambooth conditional video generation`. For more information about this work, \nplease have a look at our [paper](https://arxiv.org/abs/2303.13439) and our demo: [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/PAIR/Text2Video-Zero)\nOur [code](https://github.com/Picsart-AI-Research/Text2Video-Zero) works with any StableDiffusion base model.\n\nThis model provides [DreamBooth](https://arxiv.org/abs/2208.12242) weights for the `Avatar style` to be used with edge guidance (using [ControlNet](https://arxiv.org/abs/2302.05543)) in text2video zero.\n created at: 2023-03-27\n- model_name: Wan-AI/Wan2.1-T2V-1.3B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-01T08:39:07+00:00\n- model_name: bullerwins/Wan2.2-T2V-A14B-GGUF\n  description: you need to download both a high-noise model and a low-noise model. high noise is used for the first steps and the low-noise for the details. place them on comfyui/models/unet you will need from  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T16:29:01+00:00\n- model_name: Wan-AI/Wan2.2-T2V-A14B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T09:04:28+00:00\n- model_name: genmo/mochi-1-preview\n  description: a state of the art video generation model by . mochi 1 preview is an open state-of-the-art video generation model with high-fidelity motion and strong prompt adherence in preliminary evaluation. this model dramatically closes the gap between closed and open video generation systems. we‚Äôre releasing the model under a permissive apache 2.0 license. try this model for free on .\n created at: 2024-10-22T07:36:03+00:00\n- model_name: Wan-AI/Wan2.1-T2V-14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-02-25T07:26:34+00:00\n- model_name: Wan-AI/Wan2.1-T2V-14B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-01T08:39:52+00:00\n- model_name: QuantStack/Wan2.1_T2V_14B_LightX2V_StepCfgDistill_VACE-GGUF\n  description: this is a gguf conversion of an addon of and scopes. the process involved extracting vace scopes and injecting into the target models, using scripts provided by . all quantized versions were created from the fp16 model using the conversion scripts provided by city96, available at the github repository.\n created at: 2025-06-16T15:13:22+00:00\n- model_name: QuantStack/Wan2.1_14B_VACE-GGUF\n  description: - based on the this is a direct gguf conversion of all quants are created from the fp32 base file, though i only uploaded the q8 0 and less, if you want the f16 or bf16 one i would upload it per request.\n created at: 2025-05-16T14:55:27+00:00\n- model_name: Wan-AI/Wan2.2-TI2V-5B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T09:03:29+00:00\n- model_name: Skywork/SkyReels-V2-DF-14B-720P\n  description: skyreels v2: infinite-length film generative model üìë technical report ¬∑ üëã playground ¬∑ üí¨ discord ¬∑ ü§ó hugging face ¬∑ ü§ñ modelscope ¬∑ üåê github\n created at: 2025-04-18T13:34:52+00:00\n- model_name: Wan-AI/Wan2.2-TI2V-5B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-18T09:35:44+00:00\n- model_name: QuantStack/Wan2.1_T2V_14B_FusionX_VACE-GGUF\n  description: this is a gguf conversion of . all quantized versions were created from the base fp16 model using the conversion scripts provided by city96, available at the github repository. the model files can be used in with the custom node. place the required models in the following folders:\n created at: 2025-06-08T02:59:38+00:00\n- model_name: FastVideo/FastWan2.2-TI2V-5B-FullAttn-Diffusers\n  description: fastvideo team&emsp; hf paper vsa arxiv paper vsa github  we're excited to introduce the fastwan2.2 series a new line of models finetuned with our novel sparse-distill strategy. this approach jointly integrates dmd and vsa in a single training process, combining the benefits of both distillation to shorten diffusion steps and sparse attention to reduce attention computations, enabling even faster video generation. fastwan2.2-ti2v-5b-full-diffusers is built upon wan-ai/wan2.2-ti2v-5b-diffusers. it supports efficient 3-step inference and produces high-quality videos at 121√ó704√ó1280 resolution. for training, we used simulated forward for the generator model, making the process data-free. the current fastwan2.2-ti2v-5b-full-diffusers model is trained using only dmd .\n created at: 2025-08-02T04:49:50+00:00\n- model_name: Wan-AI/Wan2.2-T2V-A14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-24T15:07:15+00:00\n- model_name: magespace/Wan2.2-I2V-A14B-Lightning-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp üîó lightx2v distilled & optimized wan2.2 for fast, high-quality 480p / 720p image-to-video generation   wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-08-14T20:06:59+00:00\n- model_name: Wan-AI/Wan2.1-T2V-1.3B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-02-25T11:13:48+00:00\n- model_name: QuantStack/Wan2.1_T2V_14B_FusionX-GGUF\n  description: this is a gguf conversion of by . all quantized versions were created from the base t2v fp16 model using the conversion scripts provided by city96, available at the github repository. the model files can be used in with the custom node. place the required models in the following folders:\n created at: 2025-06-07T01:47:02+00:00\n- model_name: DFloat11/Wan2.2-T2V-A14B-DF11\n  description: this is a dfloat11 losslessly compressed version of the original `wan-ai/wan2.2-t2v-a14b` model. it reduces model size by 32% compared to the original bfloat16 model, while maintaining bit-identical outputs and supporting efficient gpu inference . üî•üî•üî• thanks to dfloat11 compression, `wan-ai/wan2.2-t2v-a14b` can now generate a 5-second 720p video on a single 24gb gpu, while maintaining full model quality. üî•üî•üî• model model size peak gpu memory 5-second 720p generation generation time a100 gpu\n created at: 2025-08-01T20:24:13+00:00\n- model_name: nvidia/Cosmos-1.0-Diffusion-7B-Text2World\n  description: cosmos world foundation models : a family of highly performant pre-trained world foundation models purpose-built for generating physics-aware videos and world states for physical ai development. the cosmos diffusion models are a collection of diffusion based world foundation models that generate dynamic, high quality videos from text, image, or video inputs. it can serve as the building block for various applications or research that are related to world generation. the models are ready for commercial use under nvidia open model license agreement.  cosmos world foundation models : a family of highly performant pre-trained world foundation models purpose-built for generating physics-aware videos and world states for physical ai development. the cosmos diffusion models are a collection of diffusion based world foundation models that generate dynamic, high quality videos from text, image, or video inputs. it can serve as the building block for various applications or research that are related to world generation. the models are ready for commercial use under nvidia open model license agreement.  model developer : nvidia\n created at: 2025-01-07T04:52:52+00:00\n- model_name: DFloat11/Wan2.2-T2V-A14B-2-DF11\n  description: this is a dfloat11 losslessly compressed version of the original `wan-ai/wan2.2-t2v-a14b` model. it reduces model size by 32% compared to the original bfloat16 model, while maintaining bit-identical outputs and supporting efficient gpu inference . üî•üî•üî• thanks to dfloat11 compression, `wan-ai/wan2.2-t2v-a14b` can now generate a 5-second 720p video on a single 24gb gpu, while maintaining full model quality. üî•üî•üî• model model size peak gpu memory 5-second 720p generation generation time a100 gpu\n created at: 2025-08-01T20:25:49+00:00\n\nExamples:\n- example user query: Our company aims to build a product that generates videos from text descriptions. We need to create a video of \"Spiderman surfing.\"\n  model_name: damo-vilab/text-to-video-ms-1.7b\n- example user query: Our customer, a children's toy company, wants to create an animated video advertisement based on their story concept: \"A brave teddy bear saving a cat from a tall tree\".\n  model_name: nitrosocke/mo-di-diffusion\n- example user query: As a film producer, I need a tool that could create a short video sequence based on a given script to visualize a specific scene.\n  model_name: chavinlo/TempoFunk\n- example user query: Our user research team wants to know whether we can create video for advertisement by just describing the advertisement.\n  model_name: ImRma/Brucelee\n- example user query: We want to create a short video clip that showcases a superhero-themed pizza commercial. The commercial should involve Spider-Man delivering pizzas to customers in a city setting. The video should be 5 seconds long and 256x256px dimensions.\n  model_name: nitrosocke/redshift-diffusion\n- example user query: I am writing a script for an educational video on biology. Can you generate a video using the text of my script?\n  model_name: duncan93/video\n\nUser prompt:\nConsider the depth video and a captivating scenario 'a heartfelt reunion between friends at an airport, showcasing emotions and joy in slow motion', and develop a clip that resonates with the audience on multiple levels.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <controllable_video_generation>\n\nCandidate Models:\n- model_name: camenduru/text2-video-zero\n  description: This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.\n created at: 2023-03-29\n- model_name: chavinlo/TempoFunk\n  description: A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text.\n created at: 2023-02-07\n- model_name: damo-vilab/text-to-video-ms-1.7b\n  description: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. The overall model parameters are about 1.7 billion. Currently, it only supports English input.\n created at: 2023-03-22\n- model_name: nitrosocke/redshift-diffusion\n  description: Tune-A-Video - Redshift is a text-to-video generation model based on the nitrosocke/redshift-diffusion model. It generates videos based on textual prompts, such as 'a man is skiing' or '(redshift style) spider man is skiing'.\n created at: 2022-11-06\n- model_name: nitrosocke/mo-di-diffusion\n  description: Tune-A-Video is a text-to-video generation model based on the Hugging Face framework. The model generates videos based on textual prompts in a modern Disney style.\n created at: 2022-10-27\n- model_name: ImRma/Brucelee\n  description: A Hugging Face model for converting Persian and English text into video.\n created at: 2023-04-23\n- model_name: damo-vilab/text-to-video-ms-1.7b-legacy\n  description: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.\n created at: 2023-03-22\n- model_name: duncan93/video\n  description: A text-to-video model trained on OpenAssistant/oasst1 dataset.\n created at: 2023-05-02\n- model_name: cerspense/zeroscope_v2_576w\n  description: zeroscope_v2_576w uses 7.9gb of vram when rendering 30 frames at 576x320\n\n### Using it with the 1111 text2video extension\n\n1. Download files in the zs2_576w folder.\n2. Replace the respective files in the 'stable-diffusion-webui\\models\\ModelScope\\t2v' directory.\n\n### Upscaling recommendations\n\nFor upscaling, it's recommended to use [zeroscope_v2_XL](https://huggingface.co/cerspense/zeroscope_v2_XL) via vid2vid in the 1111 extension. It works best at 1024x576 with a denoise strength between 0.66 and 0.85. Remember to use the same prompt that was used to generate the original clip. <br />\n\n### Usage in üß® Diffusers\n\nLet's first install the libraries required:\n\n```bash\n$ pip install diffusers transformers accelerate torch\n```\n\nNow, generate a video:\n\n```py\nimport torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_video\n\npipe = DiffusionPipeline.from_pretrained(\"cerspense/zeroscope_v2_576w\", torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n\nprompt = \"Darth Vader is surfing on waves\"\nvideo_frames = pipe(prompt, num_inference_steps=40, height=320, width=576, num_frames=24).frames\nvideo_path = export_to_video(video_frames)\n```\n\nHere are some results:\n\n<table>\n    <tr>\n        Darth vader is surfing on waves.\n        <br>\n        <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/darthvader_cerpense.gif\"\n            alt=\"Darth vader surfing in waves.\"\n            style=\"width: 576;\" />\n        </center></td>\n    </tr>\n</table>\n\n### Known issues\n\nLower resolutions or fewer frames could lead to suboptimal output. <br />\n\nThanks to [camenduru](https://github.com/camenduru), [kabachuha](https://github.com/kabachuha), [ExponentialML](https://github.com/ExponentialML), [dotsimulate](https://www.instagram.com/dotsimulate/), [VANYA](https://twitter.com/veryVANYA), [polyware](https://twitter.com/polyware_ai), [tin2tin](https://github.com/tin2tin)<br />\n created at: 2023-06-21\n- model_name: Searchium-ai/clip4clip-webvid150k\n  description: A CLIP4Clip video-text retrieval model trained on a subset of the WebVid dataset. \nThe model and training method are described in the paper [\"Clip4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval\"](https://arxiv.org/pdf/2104.08860.pdf) by Lou et el, and implemented in the accompanying [GitHub repository](https://github.com/ArrowLuo/CLIP4Clip).\n\nThe training process utilized the [WebVid Dataset](https://m-bain.github.io/webvid-dataset/), a comprehensive collection of short videos with corresponding textual descriptions sourced from the web. \nFor training purposes, a subset consisting of the first 150,000 video-text pairs from the dataset were used.\n\nThis HF model is based on the [clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32) architecture, with weights trained by Daphna Idelson at [Searchium](https://www.searchium.ai).\n\n\n# How to use\n### Extracting Text Embeddings:\n\n```python\nimport numpy as np\nimport torch\nfrom transformers import CLIPTokenizer, CLIPTextModelWithProjection\n\n\nsearch_sentence = \"a basketball player performing a slam dunk\"\n\nmodel = CLIPTextModelWithProjection.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\ntokenizer = CLIPTokenizer.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\n\ninputs = tokenizer(text=search_sentence , return_tensors=\"pt\")\noutputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n\n# Normalize embeddings for retrieval:\nfinal_output = outputs[0] / outputs[0].norm(dim=-1, keepdim=True)\nfinal_output = final_output.cpu().detach().numpy()\nprint(\"final output: \", final_output)\n```\n\n### Extracting Video Embeddings:\n\nAn additional notebook [\"GSI_VideoRetrieval_VideoEmbedding.ipynb\"](https://huggingface.co/Searchium-ai/clip4clip-webvid150k/blob/main/Notebooks/GSI_VideoRetrieval_VideoEmbedding.ipynb), provides instructions for extracting video embeddings and includes the necessary tools for preprocessing videos.\n created at: 2023-04-17\n- model_name: PAIR/text2video-zero-controlnet-canny-avatar\n  description: [Text2Video-Zero](https://arxiv.org/abs/2303.13439) is a zero-shot text to video generator. It can perform `zero-shot text-to-video generation`, `Video Instruct Pix2Pix` (instruction-guided video editing), \n`text and pose conditional video generation`, `text and canny-edge conditional video generation`, and \n`text, canny-edge and dreambooth conditional video generation`. For more information about this work, \nplease have a look at our [paper](https://arxiv.org/abs/2303.13439) and our demo: [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/PAIR/Text2Video-Zero)\nOur [code](https://github.com/Picsart-AI-Research/Text2Video-Zero) works with any StableDiffusion base model.\n\nThis model provides [DreamBooth](https://arxiv.org/abs/2208.12242) weights for the `Avatar style` to be used with edge guidance (using [ControlNet](https://arxiv.org/abs/2302.05543)) in text2video zero.\n created at: 2023-03-27\n- model_name: Wan-AI/Wan2.1-T2V-1.3B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-01T08:39:07+00:00\n- model_name: bullerwins/Wan2.2-T2V-A14B-GGUF\n  description: you need to download both a high-noise model and a low-noise model. high noise is used for the first steps and the low-noise for the details. place them on comfyui/models/unet you will need from  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T16:29:01+00:00\n- model_name: Wan-AI/Wan2.2-T2V-A14B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T09:04:28+00:00\n- model_name: genmo/mochi-1-preview\n  description: a state of the art video generation model by . mochi 1 preview is an open state-of-the-art video generation model with high-fidelity motion and strong prompt adherence in preliminary evaluation. this model dramatically closes the gap between closed and open video generation systems. we‚Äôre releasing the model under a permissive apache 2.0 license. try this model for free on .\n created at: 2024-10-22T07:36:03+00:00\n- model_name: Wan-AI/Wan2.1-T2V-14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-02-25T07:26:34+00:00\n- model_name: Wan-AI/Wan2.1-T2V-14B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-01T08:39:52+00:00\n- model_name: QuantStack/Wan2.1_T2V_14B_LightX2V_StepCfgDistill_VACE-GGUF\n  description: this is a gguf conversion of an addon of and scopes. the process involved extracting vace scopes and injecting into the target models, using scripts provided by . all quantized versions were created from the fp16 model using the conversion scripts provided by city96, available at the github repository.\n created at: 2025-06-16T15:13:22+00:00\n- model_name: QuantStack/Wan2.1_14B_VACE-GGUF\n  description: - based on the this is a direct gguf conversion of all quants are created from the fp32 base file, though i only uploaded the q8 0 and less, if you want the f16 or bf16 one i would upload it per request.\n created at: 2025-05-16T14:55:27+00:00\n- model_name: Wan-AI/Wan2.2-TI2V-5B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T09:03:29+00:00\n- model_name: Skywork/SkyReels-V2-DF-14B-720P\n  description: skyreels v2: infinite-length film generative model üìë technical report ¬∑ üëã playground ¬∑ üí¨ discord ¬∑ ü§ó hugging face ¬∑ ü§ñ modelscope ¬∑ üåê github\n created at: 2025-04-18T13:34:52+00:00\n- model_name: Wan-AI/Wan2.2-TI2V-5B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-18T09:35:44+00:00\n- model_name: QuantStack/Wan2.1_T2V_14B_FusionX_VACE-GGUF\n  description: this is a gguf conversion of . all quantized versions were created from the base fp16 model using the conversion scripts provided by city96, available at the github repository. the model files can be used in with the custom node. place the required models in the following folders:\n created at: 2025-06-08T02:59:38+00:00\n- model_name: FastVideo/FastWan2.2-TI2V-5B-FullAttn-Diffusers\n  description: fastvideo team&emsp; hf paper vsa arxiv paper vsa github  we're excited to introduce the fastwan2.2 series a new line of models finetuned with our novel sparse-distill strategy. this approach jointly integrates dmd and vsa in a single training process, combining the benefits of both distillation to shorten diffusion steps and sparse attention to reduce attention computations, enabling even faster video generation. fastwan2.2-ti2v-5b-full-diffusers is built upon wan-ai/wan2.2-ti2v-5b-diffusers. it supports efficient 3-step inference and produces high-quality videos at 121√ó704√ó1280 resolution. for training, we used simulated forward for the generator model, making the process data-free. the current fastwan2.2-ti2v-5b-full-diffusers model is trained using only dmd .\n created at: 2025-08-02T04:49:50+00:00\n- model_name: Wan-AI/Wan2.2-T2V-A14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-24T15:07:15+00:00\n- model_name: magespace/Wan2.2-I2V-A14B-Lightning-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp üîó lightx2v distilled & optimized wan2.2 for fast, high-quality 480p / 720p image-to-video generation   wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-08-14T20:06:59+00:00\n- model_name: Wan-AI/Wan2.1-T2V-1.3B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-02-25T11:13:48+00:00\n- model_name: QuantStack/Wan2.1_T2V_14B_FusionX-GGUF\n  description: this is a gguf conversion of by . all quantized versions were created from the base t2v fp16 model using the conversion scripts provided by city96, available at the github repository. the model files can be used in with the custom node. place the required models in the following folders:\n created at: 2025-06-07T01:47:02+00:00\n- model_name: DFloat11/Wan2.2-T2V-A14B-DF11\n  description: this is a dfloat11 losslessly compressed version of the original `wan-ai/wan2.2-t2v-a14b` model. it reduces model size by 32% compared to the original bfloat16 model, while maintaining bit-identical outputs and supporting efficient gpu inference . üî•üî•üî• thanks to dfloat11 compression, `wan-ai/wan2.2-t2v-a14b` can now generate a 5-second 720p video on a single 24gb gpu, while maintaining full model quality. üî•üî•üî• model model size peak gpu memory 5-second 720p generation generation time a100 gpu\n created at: 2025-08-01T20:24:13+00:00\n- model_name: nvidia/Cosmos-1.0-Diffusion-7B-Text2World\n  description: cosmos world foundation models : a family of highly performant pre-trained world foundation models purpose-built for generating physics-aware videos and world states for physical ai development. the cosmos diffusion models are a collection of diffusion based world foundation models that generate dynamic, high quality videos from text, image, or video inputs. it can serve as the building block for various applications or research that are related to world generation. the models are ready for commercial use under nvidia open model license agreement.  cosmos world foundation models : a family of highly performant pre-trained world foundation models purpose-built for generating physics-aware videos and world states for physical ai development. the cosmos diffusion models are a collection of diffusion based world foundation models that generate dynamic, high quality videos from text, image, or video inputs. it can serve as the building block for various applications or research that are related to world generation. the models are ready for commercial use under nvidia open model license agreement.  model developer : nvidia\n created at: 2025-01-07T04:52:52+00:00\n- model_name: DFloat11/Wan2.2-T2V-A14B-2-DF11\n  description: this is a dfloat11 losslessly compressed version of the original `wan-ai/wan2.2-t2v-a14b` model. it reduces model size by 32% compared to the original bfloat16 model, while maintaining bit-identical outputs and supporting efficient gpu inference . üî•üî•üî• thanks to dfloat11 compression, `wan-ai/wan2.2-t2v-a14b` can now generate a 5-second 720p video on a single 24gb gpu, while maintaining full model quality. üî•üî•üî• model model size peak gpu memory 5-second 720p generation generation time a100 gpu\n created at: 2025-08-01T20:25:49+00:00\n\nExamples:\n- example user query: Our company aims to build a product that generates videos from text descriptions. We need to create a video of \"Spiderman surfing.\"\n  model_name: damo-vilab/text-to-video-ms-1.7b\n- example user query: Our customer, a children's toy company, wants to create an animated video advertisement based on their story concept: \"A brave teddy bear saving a cat from a tall tree\".\n  model_name: nitrosocke/mo-di-diffusion\n- example user query: As a film producer, I need a tool that could create a short video sequence based on a given script to visualize a specific scene.\n  model_name: chavinlo/TempoFunk\n- example user query: Our user research team wants to know whether we can create video for advertisement by just describing the advertisement.\n  model_name: ImRma/Brucelee\n- example user query: We want to create a short video clip that showcases a superhero-themed pizza commercial. The commercial should involve Spider-Man delivering pizzas to customers in a city setting. The video should be 5 seconds long and 256x256px dimensions.\n  model_name: nitrosocke/redshift-diffusion\n- example user query: I am writing a script for an educational video on biology. Can you generate a video using the text of my script?\n  model_name: duncan93/video\n\nUser prompt:\nUsing the provided this depth video, let's generate a video with the focus on 'a couple walking hand in hand on the beach'.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <controllable_video_generation>\n\nCandidate Models:\n- model_name: camenduru/text2-video-zero\n  description: This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.\n created at: 2023-03-29\n- model_name: chavinlo/TempoFunk\n  description: A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text.\n created at: 2023-02-07\n- model_name: damo-vilab/text-to-video-ms-1.7b\n  description: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. The overall model parameters are about 1.7 billion. Currently, it only supports English input.\n created at: 2023-03-22\n- model_name: nitrosocke/redshift-diffusion\n  description: Tune-A-Video - Redshift is a text-to-video generation model based on the nitrosocke/redshift-diffusion model. It generates videos based on textual prompts, such as 'a man is skiing' or '(redshift style) spider man is skiing'.\n created at: 2022-11-06\n- model_name: nitrosocke/mo-di-diffusion\n  description: Tune-A-Video is a text-to-video generation model based on the Hugging Face framework. The model generates videos based on textual prompts in a modern Disney style.\n created at: 2022-10-27\n- model_name: ImRma/Brucelee\n  description: A Hugging Face model for converting Persian and English text into video.\n created at: 2023-04-23\n- model_name: damo-vilab/text-to-video-ms-1.7b-legacy\n  description: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.\n created at: 2023-03-22\n- model_name: duncan93/video\n  description: A text-to-video model trained on OpenAssistant/oasst1 dataset.\n created at: 2023-05-02\n- model_name: cerspense/zeroscope_v2_576w\n  description: zeroscope_v2_576w uses 7.9gb of vram when rendering 30 frames at 576x320\n\n### Using it with the 1111 text2video extension\n\n1. Download files in the zs2_576w folder.\n2. Replace the respective files in the 'stable-diffusion-webui\\models\\ModelScope\\t2v' directory.\n\n### Upscaling recommendations\n\nFor upscaling, it's recommended to use [zeroscope_v2_XL](https://huggingface.co/cerspense/zeroscope_v2_XL) via vid2vid in the 1111 extension. It works best at 1024x576 with a denoise strength between 0.66 and 0.85. Remember to use the same prompt that was used to generate the original clip. <br />\n\n### Usage in üß® Diffusers\n\nLet's first install the libraries required:\n\n```bash\n$ pip install diffusers transformers accelerate torch\n```\n\nNow, generate a video:\n\n```py\nimport torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_video\n\npipe = DiffusionPipeline.from_pretrained(\"cerspense/zeroscope_v2_576w\", torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n\nprompt = \"Darth Vader is surfing on waves\"\nvideo_frames = pipe(prompt, num_inference_steps=40, height=320, width=576, num_frames=24).frames\nvideo_path = export_to_video(video_frames)\n```\n\nHere are some results:\n\n<table>\n    <tr>\n        Darth vader is surfing on waves.\n        <br>\n        <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/darthvader_cerpense.gif\"\n            alt=\"Darth vader surfing in waves.\"\n            style=\"width: 576;\" />\n        </center></td>\n    </tr>\n</table>\n\n### Known issues\n\nLower resolutions or fewer frames could lead to suboptimal output. <br />\n\nThanks to [camenduru](https://github.com/camenduru), [kabachuha](https://github.com/kabachuha), [ExponentialML](https://github.com/ExponentialML), [dotsimulate](https://www.instagram.com/dotsimulate/), [VANYA](https://twitter.com/veryVANYA), [polyware](https://twitter.com/polyware_ai), [tin2tin](https://github.com/tin2tin)<br />\n created at: 2023-06-21\n- model_name: Searchium-ai/clip4clip-webvid150k\n  description: A CLIP4Clip video-text retrieval model trained on a subset of the WebVid dataset. \nThe model and training method are described in the paper [\"Clip4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval\"](https://arxiv.org/pdf/2104.08860.pdf) by Lou et el, and implemented in the accompanying [GitHub repository](https://github.com/ArrowLuo/CLIP4Clip).\n\nThe training process utilized the [WebVid Dataset](https://m-bain.github.io/webvid-dataset/), a comprehensive collection of short videos with corresponding textual descriptions sourced from the web. \nFor training purposes, a subset consisting of the first 150,000 video-text pairs from the dataset were used.\n\nThis HF model is based on the [clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32) architecture, with weights trained by Daphna Idelson at [Searchium](https://www.searchium.ai).\n\n\n# How to use\n### Extracting Text Embeddings:\n\n```python\nimport numpy as np\nimport torch\nfrom transformers import CLIPTokenizer, CLIPTextModelWithProjection\n\n\nsearch_sentence = \"a basketball player performing a slam dunk\"\n\nmodel = CLIPTextModelWithProjection.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\ntokenizer = CLIPTokenizer.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\n\ninputs = tokenizer(text=search_sentence , return_tensors=\"pt\")\noutputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n\n# Normalize embeddings for retrieval:\nfinal_output = outputs[0] / outputs[0].norm(dim=-1, keepdim=True)\nfinal_output = final_output.cpu().detach().numpy()\nprint(\"final output: \", final_output)\n```\n\n### Extracting Video Embeddings:\n\nAn additional notebook [\"GSI_VideoRetrieval_VideoEmbedding.ipynb\"](https://huggingface.co/Searchium-ai/clip4clip-webvid150k/blob/main/Notebooks/GSI_VideoRetrieval_VideoEmbedding.ipynb), provides instructions for extracting video embeddings and includes the necessary tools for preprocessing videos.\n created at: 2023-04-17\n- model_name: PAIR/text2video-zero-controlnet-canny-avatar\n  description: [Text2Video-Zero](https://arxiv.org/abs/2303.13439) is a zero-shot text to video generator. It can perform `zero-shot text-to-video generation`, `Video Instruct Pix2Pix` (instruction-guided video editing), \n`text and pose conditional video generation`, `text and canny-edge conditional video generation`, and \n`text, canny-edge and dreambooth conditional video generation`. For more information about this work, \nplease have a look at our [paper](https://arxiv.org/abs/2303.13439) and our demo: [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/PAIR/Text2Video-Zero)\nOur [code](https://github.com/Picsart-AI-Research/Text2Video-Zero) works with any StableDiffusion base model.\n\nThis model provides [DreamBooth](https://arxiv.org/abs/2208.12242) weights for the `Avatar style` to be used with edge guidance (using [ControlNet](https://arxiv.org/abs/2302.05543)) in text2video zero.\n created at: 2023-03-27\n- model_name: Wan-AI/Wan2.1-T2V-1.3B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-01T08:39:07+00:00\n- model_name: bullerwins/Wan2.2-T2V-A14B-GGUF\n  description: you need to download both a high-noise model and a low-noise model. high noise is used for the first steps and the low-noise for the details. place them on comfyui/models/unet you will need from  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T16:29:01+00:00\n- model_name: Wan-AI/Wan2.2-T2V-A14B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T09:04:28+00:00\n- model_name: genmo/mochi-1-preview\n  description: a state of the art video generation model by . mochi 1 preview is an open state-of-the-art video generation model with high-fidelity motion and strong prompt adherence in preliminary evaluation. this model dramatically closes the gap between closed and open video generation systems. we‚Äôre releasing the model under a permissive apache 2.0 license. try this model for free on .\n created at: 2024-10-22T07:36:03+00:00\n- model_name: Wan-AI/Wan2.1-T2V-14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-02-25T07:26:34+00:00\n- model_name: Wan-AI/Wan2.1-T2V-14B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-01T08:39:52+00:00\n- model_name: QuantStack/Wan2.1_T2V_14B_LightX2V_StepCfgDistill_VACE-GGUF\n  description: this is a gguf conversion of an addon of and scopes. the process involved extracting vace scopes and injecting into the target models, using scripts provided by . all quantized versions were created from the fp16 model using the conversion scripts provided by city96, available at the github repository.\n created at: 2025-06-16T15:13:22+00:00\n- model_name: QuantStack/Wan2.1_14B_VACE-GGUF\n  description: - based on the this is a direct gguf conversion of all quants are created from the fp32 base file, though i only uploaded the q8 0 and less, if you want the f16 or bf16 one i would upload it per request.\n created at: 2025-05-16T14:55:27+00:00\n- model_name: Wan-AI/Wan2.2-TI2V-5B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T09:03:29+00:00\n- model_name: Skywork/SkyReels-V2-DF-14B-720P\n  description: skyreels v2: infinite-length film generative model üìë technical report ¬∑ üëã playground ¬∑ üí¨ discord ¬∑ ü§ó hugging face ¬∑ ü§ñ modelscope ¬∑ üåê github\n created at: 2025-04-18T13:34:52+00:00\n- model_name: Wan-AI/Wan2.2-TI2V-5B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-18T09:35:44+00:00\n- model_name: QuantStack/Wan2.1_T2V_14B_FusionX_VACE-GGUF\n  description: this is a gguf conversion of . all quantized versions were created from the base fp16 model using the conversion scripts provided by city96, available at the github repository. the model files can be used in with the custom node. place the required models in the following folders:\n created at: 2025-06-08T02:59:38+00:00\n- model_name: FastVideo/FastWan2.2-TI2V-5B-FullAttn-Diffusers\n  description: fastvideo team&emsp; hf paper vsa arxiv paper vsa github  we're excited to introduce the fastwan2.2 series a new line of models finetuned with our novel sparse-distill strategy. this approach jointly integrates dmd and vsa in a single training process, combining the benefits of both distillation to shorten diffusion steps and sparse attention to reduce attention computations, enabling even faster video generation. fastwan2.2-ti2v-5b-full-diffusers is built upon wan-ai/wan2.2-ti2v-5b-diffusers. it supports efficient 3-step inference and produces high-quality videos at 121√ó704√ó1280 resolution. for training, we used simulated forward for the generator model, making the process data-free. the current fastwan2.2-ti2v-5b-full-diffusers model is trained using only dmd .\n created at: 2025-08-02T04:49:50+00:00\n- model_name: Wan-AI/Wan2.2-T2V-A14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-24T15:07:15+00:00\n- model_name: magespace/Wan2.2-I2V-A14B-Lightning-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp üîó lightx2v distilled & optimized wan2.2 for fast, high-quality 480p / 720p image-to-video generation   wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-08-14T20:06:59+00:00\n- model_name: Wan-AI/Wan2.1-T2V-1.3B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-02-25T11:13:48+00:00\n- model_name: QuantStack/Wan2.1_T2V_14B_FusionX-GGUF\n  description: this is a gguf conversion of by . all quantized versions were created from the base t2v fp16 model using the conversion scripts provided by city96, available at the github repository. the model files can be used in with the custom node. place the required models in the following folders:\n created at: 2025-06-07T01:47:02+00:00\n- model_name: DFloat11/Wan2.2-T2V-A14B-DF11\n  description: this is a dfloat11 losslessly compressed version of the original `wan-ai/wan2.2-t2v-a14b` model. it reduces model size by 32% compared to the original bfloat16 model, while maintaining bit-identical outputs and supporting efficient gpu inference . üî•üî•üî• thanks to dfloat11 compression, `wan-ai/wan2.2-t2v-a14b` can now generate a 5-second 720p video on a single 24gb gpu, while maintaining full model quality. üî•üî•üî• model model size peak gpu memory 5-second 720p generation generation time a100 gpu\n created at: 2025-08-01T20:24:13+00:00\n- model_name: nvidia/Cosmos-1.0-Diffusion-7B-Text2World\n  description: cosmos world foundation models : a family of highly performant pre-trained world foundation models purpose-built for generating physics-aware videos and world states for physical ai development. the cosmos diffusion models are a collection of diffusion based world foundation models that generate dynamic, high quality videos from text, image, or video inputs. it can serve as the building block for various applications or research that are related to world generation. the models are ready for commercial use under nvidia open model license agreement.  cosmos world foundation models : a family of highly performant pre-trained world foundation models purpose-built for generating physics-aware videos and world states for physical ai development. the cosmos diffusion models are a collection of diffusion based world foundation models that generate dynamic, high quality videos from text, image, or video inputs. it can serve as the building block for various applications or research that are related to world generation. the models are ready for commercial use under nvidia open model license agreement.  model developer : nvidia\n created at: 2025-01-07T04:52:52+00:00\n- model_name: DFloat11/Wan2.2-T2V-A14B-2-DF11\n  description: this is a dfloat11 losslessly compressed version of the original `wan-ai/wan2.2-t2v-a14b` model. it reduces model size by 32% compared to the original bfloat16 model, while maintaining bit-identical outputs and supporting efficient gpu inference . üî•üî•üî• thanks to dfloat11 compression, `wan-ai/wan2.2-t2v-a14b` can now generate a 5-second 720p video on a single 24gb gpu, while maintaining full model quality. üî•üî•üî• model model size peak gpu memory 5-second 720p generation generation time a100 gpu\n created at: 2025-08-01T20:25:49+00:00\n\nExamples:\n- example user query: Our company aims to build a product that generates videos from text descriptions. We need to create a video of \"Spiderman surfing.\"\n  model_name: damo-vilab/text-to-video-ms-1.7b\n- example user query: Our customer, a children's toy company, wants to create an animated video advertisement based on their story concept: \"A brave teddy bear saving a cat from a tall tree\".\n  model_name: nitrosocke/mo-di-diffusion\n- example user query: As a film producer, I need a tool that could create a short video sequence based on a given script to visualize a specific scene.\n  model_name: chavinlo/TempoFunk\n- example user query: Our user research team wants to know whether we can create video for advertisement by just describing the advertisement.\n  model_name: ImRma/Brucelee\n- example user query: We want to create a short video clip that showcases a superhero-themed pizza commercial. The commercial should involve Spider-Man delivering pizzas to customers in a city setting. The video should be 5 seconds long and 256x256px dimensions.\n  model_name: nitrosocke/redshift-diffusion\n- example user query: I am writing a script for an educational video on biology. Can you generate a video using the text of my script?\n  model_name: duncan93/video\n\nUser prompt:\nLet‚Äôs depict a film that reflects a description 'a resting fish' using the segmentation demo.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <controllable_video_generation>\n\nCandidate Models:\n- model_name: camenduru/text2-video-zero\n  description: This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.\n created at: 2023-03-29\n- model_name: chavinlo/TempoFunk\n  description: A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text.\n created at: 2023-02-07\n- model_name: damo-vilab/text-to-video-ms-1.7b\n  description: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. The overall model parameters are about 1.7 billion. Currently, it only supports English input.\n created at: 2023-03-22\n- model_name: nitrosocke/redshift-diffusion\n  description: Tune-A-Video - Redshift is a text-to-video generation model based on the nitrosocke/redshift-diffusion model. It generates videos based on textual prompts, such as 'a man is skiing' or '(redshift style) spider man is skiing'.\n created at: 2022-11-06\n- model_name: nitrosocke/mo-di-diffusion\n  description: Tune-A-Video is a text-to-video generation model based on the Hugging Face framework. The model generates videos based on textual prompts in a modern Disney style.\n created at: 2022-10-27\n- model_name: ImRma/Brucelee\n  description: A Hugging Face model for converting Persian and English text into video.\n created at: 2023-04-23\n- model_name: damo-vilab/text-to-video-ms-1.7b-legacy\n  description: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.\n created at: 2023-03-22\n- model_name: duncan93/video\n  description: A text-to-video model trained on OpenAssistant/oasst1 dataset.\n created at: 2023-05-02\n- model_name: cerspense/zeroscope_v2_576w\n  description: zeroscope_v2_576w uses 7.9gb of vram when rendering 30 frames at 576x320\n\n### Using it with the 1111 text2video extension\n\n1. Download files in the zs2_576w folder.\n2. Replace the respective files in the 'stable-diffusion-webui\\models\\ModelScope\\t2v' directory.\n\n### Upscaling recommendations\n\nFor upscaling, it's recommended to use [zeroscope_v2_XL](https://huggingface.co/cerspense/zeroscope_v2_XL) via vid2vid in the 1111 extension. It works best at 1024x576 with a denoise strength between 0.66 and 0.85. Remember to use the same prompt that was used to generate the original clip. <br />\n\n### Usage in üß® Diffusers\n\nLet's first install the libraries required:\n\n```bash\n$ pip install diffusers transformers accelerate torch\n```\n\nNow, generate a video:\n\n```py\nimport torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_video\n\npipe = DiffusionPipeline.from_pretrained(\"cerspense/zeroscope_v2_576w\", torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n\nprompt = \"Darth Vader is surfing on waves\"\nvideo_frames = pipe(prompt, num_inference_steps=40, height=320, width=576, num_frames=24).frames\nvideo_path = export_to_video(video_frames)\n```\n\nHere are some results:\n\n<table>\n    <tr>\n        Darth vader is surfing on waves.\n        <br>\n        <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/darthvader_cerpense.gif\"\n            alt=\"Darth vader surfing in waves.\"\n            style=\"width: 576;\" />\n        </center></td>\n    </tr>\n</table>\n\n### Known issues\n\nLower resolutions or fewer frames could lead to suboptimal output. <br />\n\nThanks to [camenduru](https://github.com/camenduru), [kabachuha](https://github.com/kabachuha), [ExponentialML](https://github.com/ExponentialML), [dotsimulate](https://www.instagram.com/dotsimulate/), [VANYA](https://twitter.com/veryVANYA), [polyware](https://twitter.com/polyware_ai), [tin2tin](https://github.com/tin2tin)<br />\n created at: 2023-06-21\n- model_name: Searchium-ai/clip4clip-webvid150k\n  description: A CLIP4Clip video-text retrieval model trained on a subset of the WebVid dataset. \nThe model and training method are described in the paper [\"Clip4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval\"](https://arxiv.org/pdf/2104.08860.pdf) by Lou et el, and implemented in the accompanying [GitHub repository](https://github.com/ArrowLuo/CLIP4Clip).\n\nThe training process utilized the [WebVid Dataset](https://m-bain.github.io/webvid-dataset/), a comprehensive collection of short videos with corresponding textual descriptions sourced from the web. \nFor training purposes, a subset consisting of the first 150,000 video-text pairs from the dataset were used.\n\nThis HF model is based on the [clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32) architecture, with weights trained by Daphna Idelson at [Searchium](https://www.searchium.ai).\n\n\n# How to use\n### Extracting Text Embeddings:\n\n```python\nimport numpy as np\nimport torch\nfrom transformers import CLIPTokenizer, CLIPTextModelWithProjection\n\n\nsearch_sentence = \"a basketball player performing a slam dunk\"\n\nmodel = CLIPTextModelWithProjection.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\ntokenizer = CLIPTokenizer.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\n\ninputs = tokenizer(text=search_sentence , return_tensors=\"pt\")\noutputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n\n# Normalize embeddings for retrieval:\nfinal_output = outputs[0] / outputs[0].norm(dim=-1, keepdim=True)\nfinal_output = final_output.cpu().detach().numpy()\nprint(\"final output: \", final_output)\n```\n\n### Extracting Video Embeddings:\n\nAn additional notebook [\"GSI_VideoRetrieval_VideoEmbedding.ipynb\"](https://huggingface.co/Searchium-ai/clip4clip-webvid150k/blob/main/Notebooks/GSI_VideoRetrieval_VideoEmbedding.ipynb), provides instructions for extracting video embeddings and includes the necessary tools for preprocessing videos.\n created at: 2023-04-17\n- model_name: PAIR/text2video-zero-controlnet-canny-avatar\n  description: [Text2Video-Zero](https://arxiv.org/abs/2303.13439) is a zero-shot text to video generator. It can perform `zero-shot text-to-video generation`, `Video Instruct Pix2Pix` (instruction-guided video editing), \n`text and pose conditional video generation`, `text and canny-edge conditional video generation`, and \n`text, canny-edge and dreambooth conditional video generation`. For more information about this work, \nplease have a look at our [paper](https://arxiv.org/abs/2303.13439) and our demo: [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/PAIR/Text2Video-Zero)\nOur [code](https://github.com/Picsart-AI-Research/Text2Video-Zero) works with any StableDiffusion base model.\n\nThis model provides [DreamBooth](https://arxiv.org/abs/2208.12242) weights for the `Avatar style` to be used with edge guidance (using [ControlNet](https://arxiv.org/abs/2302.05543)) in text2video zero.\n created at: 2023-03-27\n- model_name: Wan-AI/Wan2.1-T2V-1.3B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-01T08:39:07+00:00\n- model_name: bullerwins/Wan2.2-T2V-A14B-GGUF\n  description: you need to download both a high-noise model and a low-noise model. high noise is used for the first steps and the low-noise for the details. place them on comfyui/models/unet you will need from  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T16:29:01+00:00\n- model_name: Wan-AI/Wan2.2-T2V-A14B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T09:04:28+00:00\n- model_name: genmo/mochi-1-preview\n  description: a state of the art video generation model by . mochi 1 preview is an open state-of-the-art video generation model with high-fidelity motion and strong prompt adherence in preliminary evaluation. this model dramatically closes the gap between closed and open video generation systems. we‚Äôre releasing the model under a permissive apache 2.0 license. try this model for free on .\n created at: 2024-10-22T07:36:03+00:00\n- model_name: Wan-AI/Wan2.1-T2V-14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-02-25T07:26:34+00:00\n- model_name: Wan-AI/Wan2.1-T2V-14B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-01T08:39:52+00:00\n- model_name: QuantStack/Wan2.1_T2V_14B_LightX2V_StepCfgDistill_VACE-GGUF\n  description: this is a gguf conversion of an addon of and scopes. the process involved extracting vace scopes and injecting into the target models, using scripts provided by . all quantized versions were created from the fp16 model using the conversion scripts provided by city96, available at the github repository.\n created at: 2025-06-16T15:13:22+00:00\n- model_name: QuantStack/Wan2.1_14B_VACE-GGUF\n  description: - based on the this is a direct gguf conversion of all quants are created from the fp32 base file, though i only uploaded the q8 0 and less, if you want the f16 or bf16 one i would upload it per request.\n created at: 2025-05-16T14:55:27+00:00\n- model_name: Wan-AI/Wan2.2-TI2V-5B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T09:03:29+00:00\n- model_name: Skywork/SkyReels-V2-DF-14B-720P\n  description: skyreels v2: infinite-length film generative model üìë technical report ¬∑ üëã playground ¬∑ üí¨ discord ¬∑ ü§ó hugging face ¬∑ ü§ñ modelscope ¬∑ üåê github\n created at: 2025-04-18T13:34:52+00:00\n- model_name: Wan-AI/Wan2.2-TI2V-5B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-18T09:35:44+00:00\n- model_name: QuantStack/Wan2.1_T2V_14B_FusionX_VACE-GGUF\n  description: this is a gguf conversion of . all quantized versions were created from the base fp16 model using the conversion scripts provided by city96, available at the github repository. the model files can be used in with the custom node. place the required models in the following folders:\n created at: 2025-06-08T02:59:38+00:00\n- model_name: FastVideo/FastWan2.2-TI2V-5B-FullAttn-Diffusers\n  description: fastvideo team&emsp; hf paper vsa arxiv paper vsa github  we're excited to introduce the fastwan2.2 series a new line of models finetuned with our novel sparse-distill strategy. this approach jointly integrates dmd and vsa in a single training process, combining the benefits of both distillation to shorten diffusion steps and sparse attention to reduce attention computations, enabling even faster video generation. fastwan2.2-ti2v-5b-full-diffusers is built upon wan-ai/wan2.2-ti2v-5b-diffusers. it supports efficient 3-step inference and produces high-quality videos at 121√ó704√ó1280 resolution. for training, we used simulated forward for the generator model, making the process data-free. the current fastwan2.2-ti2v-5b-full-diffusers model is trained using only dmd .\n created at: 2025-08-02T04:49:50+00:00\n- model_name: Wan-AI/Wan2.2-T2V-A14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-24T15:07:15+00:00\n- model_name: magespace/Wan2.2-I2V-A14B-Lightning-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp üîó lightx2v distilled & optimized wan2.2 for fast, high-quality 480p / 720p image-to-video generation   wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-08-14T20:06:59+00:00\n- model_name: Wan-AI/Wan2.1-T2V-1.3B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-02-25T11:13:48+00:00\n- model_name: QuantStack/Wan2.1_T2V_14B_FusionX-GGUF\n  description: this is a gguf conversion of by . all quantized versions were created from the base t2v fp16 model using the conversion scripts provided by city96, available at the github repository. the model files can be used in with the custom node. place the required models in the following folders:\n created at: 2025-06-07T01:47:02+00:00\n- model_name: DFloat11/Wan2.2-T2V-A14B-DF11\n  description: this is a dfloat11 losslessly compressed version of the original `wan-ai/wan2.2-t2v-a14b` model. it reduces model size by 32% compared to the original bfloat16 model, while maintaining bit-identical outputs and supporting efficient gpu inference . üî•üî•üî• thanks to dfloat11 compression, `wan-ai/wan2.2-t2v-a14b` can now generate a 5-second 720p video on a single 24gb gpu, while maintaining full model quality. üî•üî•üî• model model size peak gpu memory 5-second 720p generation generation time a100 gpu\n created at: 2025-08-01T20:24:13+00:00\n- model_name: nvidia/Cosmos-1.0-Diffusion-7B-Text2World\n  description: cosmos world foundation models : a family of highly performant pre-trained world foundation models purpose-built for generating physics-aware videos and world states for physical ai development. the cosmos diffusion models are a collection of diffusion based world foundation models that generate dynamic, high quality videos from text, image, or video inputs. it can serve as the building block for various applications or research that are related to world generation. the models are ready for commercial use under nvidia open model license agreement.  cosmos world foundation models : a family of highly performant pre-trained world foundation models purpose-built for generating physics-aware videos and world states for physical ai development. the cosmos diffusion models are a collection of diffusion based world foundation models that generate dynamic, high quality videos from text, image, or video inputs. it can serve as the building block for various applications or research that are related to world generation. the models are ready for commercial use under nvidia open model license agreement.  model developer : nvidia\n created at: 2025-01-07T04:52:52+00:00\n- model_name: DFloat11/Wan2.2-T2V-A14B-2-DF11\n  description: this is a dfloat11 losslessly compressed version of the original `wan-ai/wan2.2-t2v-a14b` model. it reduces model size by 32% compared to the original bfloat16 model, while maintaining bit-identical outputs and supporting efficient gpu inference . üî•üî•üî• thanks to dfloat11 compression, `wan-ai/wan2.2-t2v-a14b` can now generate a 5-second 720p video on a single 24gb gpu, while maintaining full model quality. üî•üî•üî• model model size peak gpu memory 5-second 720p generation generation time a100 gpu\n created at: 2025-08-01T20:25:49+00:00\n\nExamples:\n- example user query: Our company aims to build a product that generates videos from text descriptions. We need to create a video of \"Spiderman surfing.\"\n  model_name: damo-vilab/text-to-video-ms-1.7b\n- example user query: Our customer, a children's toy company, wants to create an animated video advertisement based on their story concept: \"A brave teddy bear saving a cat from a tall tree\".\n  model_name: nitrosocke/mo-di-diffusion\n- example user query: As a film producer, I need a tool that could create a short video sequence based on a given script to visualize a specific scene.\n  model_name: chavinlo/TempoFunk\n- example user query: Our user research team wants to know whether we can create video for advertisement by just describing the advertisement.\n  model_name: ImRma/Brucelee\n- example user query: We want to create a short video clip that showcases a superhero-themed pizza commercial. The commercial should involve Spider-Man delivering pizzas to customers in a city setting. The video should be 5 seconds long and 256x256px dimensions.\n  model_name: nitrosocke/redshift-diffusion\n- example user query: I am writing a script for an educational video on biology. Can you generate a video using the text of my script?\n  model_name: duncan93/video\n\nUser prompt:\nWould you mind reveal a captivating demo utilizing the pose recording with 'a snowboarder descending a snowy mountain slope'?\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <controllable_video_generation>\n\nCandidate Models:\n- model_name: camenduru/text2-video-zero\n  description: This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.\n created at: 2023-03-29\n- model_name: chavinlo/TempoFunk\n  description: A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text.\n created at: 2023-02-07\n- model_name: damo-vilab/text-to-video-ms-1.7b\n  description: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. The overall model parameters are about 1.7 billion. Currently, it only supports English input.\n created at: 2023-03-22\n- model_name: nitrosocke/redshift-diffusion\n  description: Tune-A-Video - Redshift is a text-to-video generation model based on the nitrosocke/redshift-diffusion model. It generates videos based on textual prompts, such as 'a man is skiing' or '(redshift style) spider man is skiing'.\n created at: 2022-11-06\n- model_name: nitrosocke/mo-di-diffusion\n  description: Tune-A-Video is a text-to-video generation model based on the Hugging Face framework. The model generates videos based on textual prompts in a modern Disney style.\n created at: 2022-10-27\n- model_name: ImRma/Brucelee\n  description: A Hugging Face model for converting Persian and English text into video.\n created at: 2023-04-23\n- model_name: damo-vilab/text-to-video-ms-1.7b-legacy\n  description: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.\n created at: 2023-03-22\n- model_name: duncan93/video\n  description: A text-to-video model trained on OpenAssistant/oasst1 dataset.\n created at: 2023-05-02\n- model_name: cerspense/zeroscope_v2_576w\n  description: zeroscope_v2_576w uses 7.9gb of vram when rendering 30 frames at 576x320\n\n### Using it with the 1111 text2video extension\n\n1. Download files in the zs2_576w folder.\n2. Replace the respective files in the 'stable-diffusion-webui\\models\\ModelScope\\t2v' directory.\n\n### Upscaling recommendations\n\nFor upscaling, it's recommended to use [zeroscope_v2_XL](https://huggingface.co/cerspense/zeroscope_v2_XL) via vid2vid in the 1111 extension. It works best at 1024x576 with a denoise strength between 0.66 and 0.85. Remember to use the same prompt that was used to generate the original clip. <br />\n\n### Usage in üß® Diffusers\n\nLet's first install the libraries required:\n\n```bash\n$ pip install diffusers transformers accelerate torch\n```\n\nNow, generate a video:\n\n```py\nimport torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_video\n\npipe = DiffusionPipeline.from_pretrained(\"cerspense/zeroscope_v2_576w\", torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n\nprompt = \"Darth Vader is surfing on waves\"\nvideo_frames = pipe(prompt, num_inference_steps=40, height=320, width=576, num_frames=24).frames\nvideo_path = export_to_video(video_frames)\n```\n\nHere are some results:\n\n<table>\n    <tr>\n        Darth vader is surfing on waves.\n        <br>\n        <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/darthvader_cerpense.gif\"\n            alt=\"Darth vader surfing in waves.\"\n            style=\"width: 576;\" />\n        </center></td>\n    </tr>\n</table>\n\n### Known issues\n\nLower resolutions or fewer frames could lead to suboptimal output. <br />\n\nThanks to [camenduru](https://github.com/camenduru), [kabachuha](https://github.com/kabachuha), [ExponentialML](https://github.com/ExponentialML), [dotsimulate](https://www.instagram.com/dotsimulate/), [VANYA](https://twitter.com/veryVANYA), [polyware](https://twitter.com/polyware_ai), [tin2tin](https://github.com/tin2tin)<br />\n created at: 2023-06-21\n- model_name: Searchium-ai/clip4clip-webvid150k\n  description: A CLIP4Clip video-text retrieval model trained on a subset of the WebVid dataset. \nThe model and training method are described in the paper [\"Clip4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval\"](https://arxiv.org/pdf/2104.08860.pdf) by Lou et el, and implemented in the accompanying [GitHub repository](https://github.com/ArrowLuo/CLIP4Clip).\n\nThe training process utilized the [WebVid Dataset](https://m-bain.github.io/webvid-dataset/), a comprehensive collection of short videos with corresponding textual descriptions sourced from the web. \nFor training purposes, a subset consisting of the first 150,000 video-text pairs from the dataset were used.\n\nThis HF model is based on the [clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32) architecture, with weights trained by Daphna Idelson at [Searchium](https://www.searchium.ai).\n\n\n# How to use\n### Extracting Text Embeddings:\n\n```python\nimport numpy as np\nimport torch\nfrom transformers import CLIPTokenizer, CLIPTextModelWithProjection\n\n\nsearch_sentence = \"a basketball player performing a slam dunk\"\n\nmodel = CLIPTextModelWithProjection.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\ntokenizer = CLIPTokenizer.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\n\ninputs = tokenizer(text=search_sentence , return_tensors=\"pt\")\noutputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n\n# Normalize embeddings for retrieval:\nfinal_output = outputs[0] / outputs[0].norm(dim=-1, keepdim=True)\nfinal_output = final_output.cpu().detach().numpy()\nprint(\"final output: \", final_output)\n```\n\n### Extracting Video Embeddings:\n\nAn additional notebook [\"GSI_VideoRetrieval_VideoEmbedding.ipynb\"](https://huggingface.co/Searchium-ai/clip4clip-webvid150k/blob/main/Notebooks/GSI_VideoRetrieval_VideoEmbedding.ipynb), provides instructions for extracting video embeddings and includes the necessary tools for preprocessing videos.\n created at: 2023-04-17\n- model_name: PAIR/text2video-zero-controlnet-canny-avatar\n  description: [Text2Video-Zero](https://arxiv.org/abs/2303.13439) is a zero-shot text to video generator. It can perform `zero-shot text-to-video generation`, `Video Instruct Pix2Pix` (instruction-guided video editing), \n`text and pose conditional video generation`, `text and canny-edge conditional video generation`, and \n`text, canny-edge and dreambooth conditional video generation`. For more information about this work, \nplease have a look at our [paper](https://arxiv.org/abs/2303.13439) and our demo: [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/PAIR/Text2Video-Zero)\nOur [code](https://github.com/Picsart-AI-Research/Text2Video-Zero) works with any StableDiffusion base model.\n\nThis model provides [DreamBooth](https://arxiv.org/abs/2208.12242) weights for the `Avatar style` to be used with edge guidance (using [ControlNet](https://arxiv.org/abs/2302.05543)) in text2video zero.\n created at: 2023-03-27\n- model_name: Wan-AI/Wan2.1-T2V-1.3B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-01T08:39:07+00:00\n- model_name: bullerwins/Wan2.2-T2V-A14B-GGUF\n  description: you need to download both a high-noise model and a low-noise model. high noise is used for the first steps and the low-noise for the details. place them on comfyui/models/unet you will need from  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T16:29:01+00:00\n- model_name: Wan-AI/Wan2.2-T2V-A14B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T09:04:28+00:00\n- model_name: genmo/mochi-1-preview\n  description: a state of the art video generation model by . mochi 1 preview is an open state-of-the-art video generation model with high-fidelity motion and strong prompt adherence in preliminary evaluation. this model dramatically closes the gap between closed and open video generation systems. we‚Äôre releasing the model under a permissive apache 2.0 license. try this model for free on .\n created at: 2024-10-22T07:36:03+00:00\n- model_name: Wan-AI/Wan2.1-T2V-14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-02-25T07:26:34+00:00\n- model_name: Wan-AI/Wan2.1-T2V-14B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-01T08:39:52+00:00\n- model_name: QuantStack/Wan2.1_T2V_14B_LightX2V_StepCfgDistill_VACE-GGUF\n  description: this is a gguf conversion of an addon of and scopes. the process involved extracting vace scopes and injecting into the target models, using scripts provided by . all quantized versions were created from the fp16 model using the conversion scripts provided by city96, available at the github repository.\n created at: 2025-06-16T15:13:22+00:00\n- model_name: QuantStack/Wan2.1_14B_VACE-GGUF\n  description: - based on the this is a direct gguf conversion of all quants are created from the fp32 base file, though i only uploaded the q8 0 and less, if you want the f16 or bf16 one i would upload it per request.\n created at: 2025-05-16T14:55:27+00:00\n- model_name: Wan-AI/Wan2.2-TI2V-5B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T09:03:29+00:00\n- model_name: Skywork/SkyReels-V2-DF-14B-720P\n  description: skyreels v2: infinite-length film generative model üìë technical report ¬∑ üëã playground ¬∑ üí¨ discord ¬∑ ü§ó hugging face ¬∑ ü§ñ modelscope ¬∑ üåê github\n created at: 2025-04-18T13:34:52+00:00\n- model_name: Wan-AI/Wan2.2-TI2V-5B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-18T09:35:44+00:00\n- model_name: QuantStack/Wan2.1_T2V_14B_FusionX_VACE-GGUF\n  description: this is a gguf conversion of . all quantized versions were created from the base fp16 model using the conversion scripts provided by city96, available at the github repository. the model files can be used in with the custom node. place the required models in the following folders:\n created at: 2025-06-08T02:59:38+00:00\n- model_name: FastVideo/FastWan2.2-TI2V-5B-FullAttn-Diffusers\n  description: fastvideo team&emsp; hf paper vsa arxiv paper vsa github  we're excited to introduce the fastwan2.2 series a new line of models finetuned with our novel sparse-distill strategy. this approach jointly integrates dmd and vsa in a single training process, combining the benefits of both distillation to shorten diffusion steps and sparse attention to reduce attention computations, enabling even faster video generation. fastwan2.2-ti2v-5b-full-diffusers is built upon wan-ai/wan2.2-ti2v-5b-diffusers. it supports efficient 3-step inference and produces high-quality videos at 121√ó704√ó1280 resolution. for training, we used simulated forward for the generator model, making the process data-free. the current fastwan2.2-ti2v-5b-full-diffusers model is trained using only dmd .\n created at: 2025-08-02T04:49:50+00:00\n- model_name: Wan-AI/Wan2.2-T2V-A14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-24T15:07:15+00:00\n- model_name: magespace/Wan2.2-I2V-A14B-Lightning-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp üîó lightx2v distilled & optimized wan2.2 for fast, high-quality 480p / 720p image-to-video generation   wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-08-14T20:06:59+00:00\n- model_name: Wan-AI/Wan2.1-T2V-1.3B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-02-25T11:13:48+00:00\n- model_name: QuantStack/Wan2.1_T2V_14B_FusionX-GGUF\n  description: this is a gguf conversion of by . all quantized versions were created from the base t2v fp16 model using the conversion scripts provided by city96, available at the github repository. the model files can be used in with the custom node. place the required models in the following folders:\n created at: 2025-06-07T01:47:02+00:00\n- model_name: DFloat11/Wan2.2-T2V-A14B-DF11\n  description: this is a dfloat11 losslessly compressed version of the original `wan-ai/wan2.2-t2v-a14b` model. it reduces model size by 32% compared to the original bfloat16 model, while maintaining bit-identical outputs and supporting efficient gpu inference . üî•üî•üî• thanks to dfloat11 compression, `wan-ai/wan2.2-t2v-a14b` can now generate a 5-second 720p video on a single 24gb gpu, while maintaining full model quality. üî•üî•üî• model model size peak gpu memory 5-second 720p generation generation time a100 gpu\n created at: 2025-08-01T20:24:13+00:00\n- model_name: nvidia/Cosmos-1.0-Diffusion-7B-Text2World\n  description: cosmos world foundation models : a family of highly performant pre-trained world foundation models purpose-built for generating physics-aware videos and world states for physical ai development. the cosmos diffusion models are a collection of diffusion based world foundation models that generate dynamic, high quality videos from text, image, or video inputs. it can serve as the building block for various applications or research that are related to world generation. the models are ready for commercial use under nvidia open model license agreement.  cosmos world foundation models : a family of highly performant pre-trained world foundation models purpose-built for generating physics-aware videos and world states for physical ai development. the cosmos diffusion models are a collection of diffusion based world foundation models that generate dynamic, high quality videos from text, image, or video inputs. it can serve as the building block for various applications or research that are related to world generation. the models are ready for commercial use under nvidia open model license agreement.  model developer : nvidia\n created at: 2025-01-07T04:52:52+00:00\n- model_name: DFloat11/Wan2.2-T2V-A14B-2-DF11\n  description: this is a dfloat11 losslessly compressed version of the original `wan-ai/wan2.2-t2v-a14b` model. it reduces model size by 32% compared to the original bfloat16 model, while maintaining bit-identical outputs and supporting efficient gpu inference . üî•üî•üî• thanks to dfloat11 compression, `wan-ai/wan2.2-t2v-a14b` can now generate a 5-second 720p video on a single 24gb gpu, while maintaining full model quality. üî•üî•üî• model model size peak gpu memory 5-second 720p generation generation time a100 gpu\n created at: 2025-08-01T20:25:49+00:00\n\nExamples:\n- example user query: Our company aims to build a product that generates videos from text descriptions. We need to create a video of \"Spiderman surfing.\"\n  model_name: damo-vilab/text-to-video-ms-1.7b\n- example user query: Our customer, a children's toy company, wants to create an animated video advertisement based on their story concept: \"A brave teddy bear saving a cat from a tall tree\".\n  model_name: nitrosocke/mo-di-diffusion\n- example user query: As a film producer, I need a tool that could create a short video sequence based on a given script to visualize a specific scene.\n  model_name: chavinlo/TempoFunk\n- example user query: Our user research team wants to know whether we can create video for advertisement by just describing the advertisement.\n  model_name: ImRma/Brucelee\n- example user query: We want to create a short video clip that showcases a superhero-themed pizza commercial. The commercial should involve Spider-Man delivering pizzas to customers in a city setting. The video should be 5 seconds long and 256x256px dimensions.\n  model_name: nitrosocke/redshift-diffusion\n- example user query: I am writing a script for an educational video on biology. Can you generate a video using the text of my script?\n  model_name: duncan93/video\n\nUser prompt:\nFrom the normal animation and a guideline 'a luminous orchard bathed in light', create a episode.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <controllable_video_generation>\n\nCandidate Models:\n- model_name: camenduru/text2-video-zero\n  description: This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.\n created at: 2023-03-29\n- model_name: chavinlo/TempoFunk\n  description: A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text.\n created at: 2023-02-07\n- model_name: damo-vilab/text-to-video-ms-1.7b\n  description: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. The overall model parameters are about 1.7 billion. Currently, it only supports English input.\n created at: 2023-03-22\n- model_name: nitrosocke/redshift-diffusion\n  description: Tune-A-Video - Redshift is a text-to-video generation model based on the nitrosocke/redshift-diffusion model. It generates videos based on textual prompts, such as 'a man is skiing' or '(redshift style) spider man is skiing'.\n created at: 2022-11-06\n- model_name: nitrosocke/mo-di-diffusion\n  description: Tune-A-Video is a text-to-video generation model based on the Hugging Face framework. The model generates videos based on textual prompts in a modern Disney style.\n created at: 2022-10-27\n- model_name: ImRma/Brucelee\n  description: A Hugging Face model for converting Persian and English text into video.\n created at: 2023-04-23\n- model_name: damo-vilab/text-to-video-ms-1.7b-legacy\n  description: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.\n created at: 2023-03-22\n- model_name: duncan93/video\n  description: A text-to-video model trained on OpenAssistant/oasst1 dataset.\n created at: 2023-05-02\n- model_name: cerspense/zeroscope_v2_576w\n  description: zeroscope_v2_576w uses 7.9gb of vram when rendering 30 frames at 576x320\n\n### Using it with the 1111 text2video extension\n\n1. Download files in the zs2_576w folder.\n2. Replace the respective files in the 'stable-diffusion-webui\\models\\ModelScope\\t2v' directory.\n\n### Upscaling recommendations\n\nFor upscaling, it's recommended to use [zeroscope_v2_XL](https://huggingface.co/cerspense/zeroscope_v2_XL) via vid2vid in the 1111 extension. It works best at 1024x576 with a denoise strength between 0.66 and 0.85. Remember to use the same prompt that was used to generate the original clip. <br />\n\n### Usage in üß® Diffusers\n\nLet's first install the libraries required:\n\n```bash\n$ pip install diffusers transformers accelerate torch\n```\n\nNow, generate a video:\n\n```py\nimport torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_video\n\npipe = DiffusionPipeline.from_pretrained(\"cerspense/zeroscope_v2_576w\", torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n\nprompt = \"Darth Vader is surfing on waves\"\nvideo_frames = pipe(prompt, num_inference_steps=40, height=320, width=576, num_frames=24).frames\nvideo_path = export_to_video(video_frames)\n```\n\nHere are some results:\n\n<table>\n    <tr>\n        Darth vader is surfing on waves.\n        <br>\n        <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/darthvader_cerpense.gif\"\n            alt=\"Darth vader surfing in waves.\"\n            style=\"width: 576;\" />\n        </center></td>\n    </tr>\n</table>\n\n### Known issues\n\nLower resolutions or fewer frames could lead to suboptimal output. <br />\n\nThanks to [camenduru](https://github.com/camenduru), [kabachuha](https://github.com/kabachuha), [ExponentialML](https://github.com/ExponentialML), [dotsimulate](https://www.instagram.com/dotsimulate/), [VANYA](https://twitter.com/veryVANYA), [polyware](https://twitter.com/polyware_ai), [tin2tin](https://github.com/tin2tin)<br />\n created at: 2023-06-21\n- model_name: Searchium-ai/clip4clip-webvid150k\n  description: A CLIP4Clip video-text retrieval model trained on a subset of the WebVid dataset. \nThe model and training method are described in the paper [\"Clip4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval\"](https://arxiv.org/pdf/2104.08860.pdf) by Lou et el, and implemented in the accompanying [GitHub repository](https://github.com/ArrowLuo/CLIP4Clip).\n\nThe training process utilized the [WebVid Dataset](https://m-bain.github.io/webvid-dataset/), a comprehensive collection of short videos with corresponding textual descriptions sourced from the web. \nFor training purposes, a subset consisting of the first 150,000 video-text pairs from the dataset were used.\n\nThis HF model is based on the [clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32) architecture, with weights trained by Daphna Idelson at [Searchium](https://www.searchium.ai).\n\n\n# How to use\n### Extracting Text Embeddings:\n\n```python\nimport numpy as np\nimport torch\nfrom transformers import CLIPTokenizer, CLIPTextModelWithProjection\n\n\nsearch_sentence = \"a basketball player performing a slam dunk\"\n\nmodel = CLIPTextModelWithProjection.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\ntokenizer = CLIPTokenizer.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\n\ninputs = tokenizer(text=search_sentence , return_tensors=\"pt\")\noutputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n\n# Normalize embeddings for retrieval:\nfinal_output = outputs[0] / outputs[0].norm(dim=-1, keepdim=True)\nfinal_output = final_output.cpu().detach().numpy()\nprint(\"final output: \", final_output)\n```\n\n### Extracting Video Embeddings:\n\nAn additional notebook [\"GSI_VideoRetrieval_VideoEmbedding.ipynb\"](https://huggingface.co/Searchium-ai/clip4clip-webvid150k/blob/main/Notebooks/GSI_VideoRetrieval_VideoEmbedding.ipynb), provides instructions for extracting video embeddings and includes the necessary tools for preprocessing videos.\n created at: 2023-04-17\n- model_name: PAIR/text2video-zero-controlnet-canny-avatar\n  description: [Text2Video-Zero](https://arxiv.org/abs/2303.13439) is a zero-shot text to video generator. It can perform `zero-shot text-to-video generation`, `Video Instruct Pix2Pix` (instruction-guided video editing), \n`text and pose conditional video generation`, `text and canny-edge conditional video generation`, and \n`text, canny-edge and dreambooth conditional video generation`. For more information about this work, \nplease have a look at our [paper](https://arxiv.org/abs/2303.13439) and our demo: [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/PAIR/Text2Video-Zero)\nOur [code](https://github.com/Picsart-AI-Research/Text2Video-Zero) works with any StableDiffusion base model.\n\nThis model provides [DreamBooth](https://arxiv.org/abs/2208.12242) weights for the `Avatar style` to be used with edge guidance (using [ControlNet](https://arxiv.org/abs/2302.05543)) in text2video zero.\n created at: 2023-03-27\n- model_name: Wan-AI/Wan2.1-T2V-1.3B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-01T08:39:07+00:00\n- model_name: bullerwins/Wan2.2-T2V-A14B-GGUF\n  description: you need to download both a high-noise model and a low-noise model. high noise is used for the first steps and the low-noise for the details. place them on comfyui/models/unet you will need from  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T16:29:01+00:00\n- model_name: Wan-AI/Wan2.2-T2V-A14B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T09:04:28+00:00\n- model_name: genmo/mochi-1-preview\n  description: a state of the art video generation model by . mochi 1 preview is an open state-of-the-art video generation model with high-fidelity motion and strong prompt adherence in preliminary evaluation. this model dramatically closes the gap between closed and open video generation systems. we‚Äôre releasing the model under a permissive apache 2.0 license. try this model for free on .\n created at: 2024-10-22T07:36:03+00:00\n- model_name: Wan-AI/Wan2.1-T2V-14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-02-25T07:26:34+00:00\n- model_name: Wan-AI/Wan2.1-T2V-14B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-01T08:39:52+00:00\n- model_name: QuantStack/Wan2.1_T2V_14B_LightX2V_StepCfgDistill_VACE-GGUF\n  description: this is a gguf conversion of an addon of and scopes. the process involved extracting vace scopes and injecting into the target models, using scripts provided by . all quantized versions were created from the fp16 model using the conversion scripts provided by city96, available at the github repository.\n created at: 2025-06-16T15:13:22+00:00\n- model_name: QuantStack/Wan2.1_14B_VACE-GGUF\n  description: - based on the this is a direct gguf conversion of all quants are created from the fp32 base file, though i only uploaded the q8 0 and less, if you want the f16 or bf16 one i would upload it per request.\n created at: 2025-05-16T14:55:27+00:00\n- model_name: Wan-AI/Wan2.2-TI2V-5B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T09:03:29+00:00\n- model_name: Skywork/SkyReels-V2-DF-14B-720P\n  description: skyreels v2: infinite-length film generative model üìë technical report ¬∑ üëã playground ¬∑ üí¨ discord ¬∑ ü§ó hugging face ¬∑ ü§ñ modelscope ¬∑ üåê github\n created at: 2025-04-18T13:34:52+00:00\n- model_name: Wan-AI/Wan2.2-TI2V-5B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-18T09:35:44+00:00\n- model_name: QuantStack/Wan2.1_T2V_14B_FusionX_VACE-GGUF\n  description: this is a gguf conversion of . all quantized versions were created from the base fp16 model using the conversion scripts provided by city96, available at the github repository. the model files can be used in with the custom node. place the required models in the following folders:\n created at: 2025-06-08T02:59:38+00:00\n- model_name: FastVideo/FastWan2.2-TI2V-5B-FullAttn-Diffusers\n  description: fastvideo team&emsp; hf paper vsa arxiv paper vsa github  we're excited to introduce the fastwan2.2 series a new line of models finetuned with our novel sparse-distill strategy. this approach jointly integrates dmd and vsa in a single training process, combining the benefits of both distillation to shorten diffusion steps and sparse attention to reduce attention computations, enabling even faster video generation. fastwan2.2-ti2v-5b-full-diffusers is built upon wan-ai/wan2.2-ti2v-5b-diffusers. it supports efficient 3-step inference and produces high-quality videos at 121√ó704√ó1280 resolution. for training, we used simulated forward for the generator model, making the process data-free. the current fastwan2.2-ti2v-5b-full-diffusers model is trained using only dmd .\n created at: 2025-08-02T04:49:50+00:00\n- model_name: Wan-AI/Wan2.2-T2V-A14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-24T15:07:15+00:00\n- model_name: magespace/Wan2.2-I2V-A14B-Lightning-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp üîó lightx2v distilled & optimized wan2.2 for fast, high-quality 480p / 720p image-to-video generation   wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-08-14T20:06:59+00:00\n- model_name: Wan-AI/Wan2.1-T2V-1.3B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-02-25T11:13:48+00:00\n- model_name: QuantStack/Wan2.1_T2V_14B_FusionX-GGUF\n  description: this is a gguf conversion of by . all quantized versions were created from the base t2v fp16 model using the conversion scripts provided by city96, available at the github repository. the model files can be used in with the custom node. place the required models in the following folders:\n created at: 2025-06-07T01:47:02+00:00\n- model_name: DFloat11/Wan2.2-T2V-A14B-DF11\n  description: this is a dfloat11 losslessly compressed version of the original `wan-ai/wan2.2-t2v-a14b` model. it reduces model size by 32% compared to the original bfloat16 model, while maintaining bit-identical outputs and supporting efficient gpu inference . üî•üî•üî• thanks to dfloat11 compression, `wan-ai/wan2.2-t2v-a14b` can now generate a 5-second 720p video on a single 24gb gpu, while maintaining full model quality. üî•üî•üî• model model size peak gpu memory 5-second 720p generation generation time a100 gpu\n created at: 2025-08-01T20:24:13+00:00\n- model_name: nvidia/Cosmos-1.0-Diffusion-7B-Text2World\n  description: cosmos world foundation models : a family of highly performant pre-trained world foundation models purpose-built for generating physics-aware videos and world states for physical ai development. the cosmos diffusion models are a collection of diffusion based world foundation models that generate dynamic, high quality videos from text, image, or video inputs. it can serve as the building block for various applications or research that are related to world generation. the models are ready for commercial use under nvidia open model license agreement.  cosmos world foundation models : a family of highly performant pre-trained world foundation models purpose-built for generating physics-aware videos and world states for physical ai development. the cosmos diffusion models are a collection of diffusion based world foundation models that generate dynamic, high quality videos from text, image, or video inputs. it can serve as the building block for various applications or research that are related to world generation. the models are ready for commercial use under nvidia open model license agreement.  model developer : nvidia\n created at: 2025-01-07T04:52:52+00:00\n- model_name: DFloat11/Wan2.2-T2V-A14B-2-DF11\n  description: this is a dfloat11 losslessly compressed version of the original `wan-ai/wan2.2-t2v-a14b` model. it reduces model size by 32% compared to the original bfloat16 model, while maintaining bit-identical outputs and supporting efficient gpu inference . üî•üî•üî• thanks to dfloat11 compression, `wan-ai/wan2.2-t2v-a14b` can now generate a 5-second 720p video on a single 24gb gpu, while maintaining full model quality. üî•üî•üî• model model size peak gpu memory 5-second 720p generation generation time a100 gpu\n created at: 2025-08-01T20:25:49+00:00\n\nExamples:\n- example user query: Our company aims to build a product that generates videos from text descriptions. We need to create a video of \"Spiderman surfing.\"\n  model_name: damo-vilab/text-to-video-ms-1.7b\n- example user query: Our customer, a children's toy company, wants to create an animated video advertisement based on their story concept: \"A brave teddy bear saving a cat from a tall tree\".\n  model_name: nitrosocke/mo-di-diffusion\n- example user query: As a film producer, I need a tool that could create a short video sequence based on a given script to visualize a specific scene.\n  model_name: chavinlo/TempoFunk\n- example user query: Our user research team wants to know whether we can create video for advertisement by just describing the advertisement.\n  model_name: ImRma/Brucelee\n- example user query: We want to create a short video clip that showcases a superhero-themed pizza commercial. The commercial should involve Spider-Man delivering pizzas to customers in a city setting. The video should be 5 seconds long and 256x256px dimensions.\n  model_name: nitrosocke/redshift-diffusion\n- example user query: I am writing a script for an educational video on biology. Can you generate a video using the text of my script?\n  model_name: duncan93/video\n\nUser prompt:\nGenerating a video based on the provided canny video and a narrative portraying 'a chef preparing an elaborate dish in a gourmet kitchen'.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n"
  ],
  "<image_canny>": [
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_canny>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: nunchaku-tech/nunchaku-flux.1-canny-dev\n  description: this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:48:02+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-canny-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:07:43+00:00\n- model_name: fotographerai/zenctrl_tools\n  description: zenctrl we are making an agent that can automate the whole personalized visual content creation process. it will need to perform multiple types of tasks, including designing tasks and training a model for it's own use. this agent will handle the data, the models and ensure the quality of the outputs.\n created at: 2025-03-25T21:40:00+00:00\n- model_name: mit-han-lab/svdq-int4-flux.1-canny-dev\n  description: this repository has been deprecated and will be hidden in december 2025. please use the flux.1 dev model is licensed by black forest labs inc. under the flux.1 dev non-commercial license. copyright black forest labs inc. in no event shall black forest labs inc. be liable for any claim, damages or other liability, whether in an action of contract, tort or otherwise, arising from, out of or in connection with use of this model.\n created at: 2025-02-04T04:41:47+00:00\n\nExamples:\n- example user query: We need artistic images by enhancing the existing collection. We want to create a new dataset with stylized images based on Canny edge detection.\n  model_name: lllyasviel/sd-controlnet-canny\n- example user query: We are a company that prints custom artwork on T-shirts. We need to use AI to generate an image of \"a green unicorn in a magical forest\" based on Canny edges.\n  model_name: lllyasviel/control_v11p_sd15_canny\n- example user query: Our art department needs a tool that uses the Canny edge detection algorithm combined with Stable Diffusion to transform images.\n  model_name: lllyasviel/sd-controlnet-canny\n- example user query: Create a colored image of a blue paradise bird in the jungle using Canny edge detection.\n  model_name: lllyasviel/control_v11p_sd15_canny\n- example user query: Use AI to convert a text description into an image based on the Canny edge detection.\n  model_name: lllyasviel/control_v11p_sd15_canny\n- example user query: We are asked to generate an image by applying Canny edge detection to an existing image.\n  model_name: lllyasviel/sd-controlnet-canny\n\nUser prompt:\nCan you provide a canny edge output from the supplied visual.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_canny>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: nunchaku-tech/nunchaku-flux.1-canny-dev\n  description: this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:48:02+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-canny-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:07:43+00:00\n- model_name: fotographerai/zenctrl_tools\n  description: zenctrl we are making an agent that can automate the whole personalized visual content creation process. it will need to perform multiple types of tasks, including designing tasks and training a model for it's own use. this agent will handle the data, the models and ensure the quality of the outputs.\n created at: 2025-03-25T21:40:00+00:00\n- model_name: mit-han-lab/svdq-int4-flux.1-canny-dev\n  description: this repository has been deprecated and will be hidden in december 2025. please use the flux.1 dev model is licensed by black forest labs inc. under the flux.1 dev non-commercial license. copyright black forest labs inc. in no event shall black forest labs inc. be liable for any claim, damages or other liability, whether in an action of contract, tort or otherwise, arising from, out of or in connection with use of this model.\n created at: 2025-02-04T04:41:47+00:00\n\nExamples:\n- example user query: We need artistic images by enhancing the existing collection. We want to create a new dataset with stylized images based on Canny edge detection.\n  model_name: lllyasviel/sd-controlnet-canny\n- example user query: We are a company that prints custom artwork on T-shirts. We need to use AI to generate an image of \"a green unicorn in a magical forest\" based on Canny edges.\n  model_name: lllyasviel/control_v11p_sd15_canny\n- example user query: Our art department needs a tool that uses the Canny edge detection algorithm combined with Stable Diffusion to transform images.\n  model_name: lllyasviel/sd-controlnet-canny\n- example user query: Create a colored image of a blue paradise bird in the jungle using Canny edge detection.\n  model_name: lllyasviel/control_v11p_sd15_canny\n- example user query: Use AI to convert a text description into an image based on the Canny edge detection.\n  model_name: lllyasviel/control_v11p_sd15_canny\n- example user query: We are asked to generate an image by applying Canny edge detection to an existing image.\n  model_name: lllyasviel/sd-controlnet-canny\n\nUser prompt:\nI would appreciate rendering a canny edge version to map the image's surfaces.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_canny>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: nunchaku-tech/nunchaku-flux.1-canny-dev\n  description: this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:48:02+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-canny-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:07:43+00:00\n- model_name: fotographerai/zenctrl_tools\n  description: zenctrl we are making an agent that can automate the whole personalized visual content creation process. it will need to perform multiple types of tasks, including designing tasks and training a model for it's own use. this agent will handle the data, the models and ensure the quality of the outputs.\n created at: 2025-03-25T21:40:00+00:00\n- model_name: mit-han-lab/svdq-int4-flux.1-canny-dev\n  description: this repository has been deprecated and will be hidden in december 2025. please use the flux.1 dev model is licensed by black forest labs inc. under the flux.1 dev non-commercial license. copyright black forest labs inc. in no event shall black forest labs inc. be liable for any claim, damages or other liability, whether in an action of contract, tort or otherwise, arising from, out of or in connection with use of this model.\n created at: 2025-02-04T04:41:47+00:00\n\nExamples:\n- example user query: We need artistic images by enhancing the existing collection. We want to create a new dataset with stylized images based on Canny edge detection.\n  model_name: lllyasviel/sd-controlnet-canny\n- example user query: We are a company that prints custom artwork on T-shirts. We need to use AI to generate an image of \"a green unicorn in a magical forest\" based on Canny edges.\n  model_name: lllyasviel/control_v11p_sd15_canny\n- example user query: Our art department needs a tool that uses the Canny edge detection algorithm combined with Stable Diffusion to transform images.\n  model_name: lllyasviel/sd-controlnet-canny\n- example user query: Create a colored image of a blue paradise bird in the jungle using Canny edge detection.\n  model_name: lllyasviel/control_v11p_sd15_canny\n- example user query: Use AI to convert a text description into an image based on the Canny edge detection.\n  model_name: lllyasviel/control_v11p_sd15_canny\n- example user query: We are asked to generate an image by applying Canny edge detection to an existing image.\n  model_name: lllyasviel/sd-controlnet-canny\n\nUser prompt:\nLet‚Äôs produce a canny outline to enhance the visual quality.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_canny>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: nunchaku-tech/nunchaku-flux.1-canny-dev\n  description: this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:48:02+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-canny-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:07:43+00:00\n- model_name: fotographerai/zenctrl_tools\n  description: zenctrl we are making an agent that can automate the whole personalized visual content creation process. it will need to perform multiple types of tasks, including designing tasks and training a model for it's own use. this agent will handle the data, the models and ensure the quality of the outputs.\n created at: 2025-03-25T21:40:00+00:00\n- model_name: mit-han-lab/svdq-int4-flux.1-canny-dev\n  description: this repository has been deprecated and will be hidden in december 2025. please use the flux.1 dev model is licensed by black forest labs inc. under the flux.1 dev non-commercial license. copyright black forest labs inc. in no event shall black forest labs inc. be liable for any claim, damages or other liability, whether in an action of contract, tort or otherwise, arising from, out of or in connection with use of this model.\n created at: 2025-02-04T04:41:47+00:00\n\nExamples:\n- example user query: We need artistic images by enhancing the existing collection. We want to create a new dataset with stylized images based on Canny edge detection.\n  model_name: lllyasviel/sd-controlnet-canny\n- example user query: We are a company that prints custom artwork on T-shirts. We need to use AI to generate an image of \"a green unicorn in a magical forest\" based on Canny edges.\n  model_name: lllyasviel/control_v11p_sd15_canny\n- example user query: Our art department needs a tool that uses the Canny edge detection algorithm combined with Stable Diffusion to transform images.\n  model_name: lllyasviel/sd-controlnet-canny\n- example user query: Create a colored image of a blue paradise bird in the jungle using Canny edge detection.\n  model_name: lllyasviel/control_v11p_sd15_canny\n- example user query: Use AI to convert a text description into an image based on the Canny edge detection.\n  model_name: lllyasviel/control_v11p_sd15_canny\n- example user query: We are asked to generate an image by applying Canny edge detection to an existing image.\n  model_name: lllyasviel/sd-controlnet-canny\n\nUser prompt:\nLet‚Äôs output a canny edge map to visualize the pose structure.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_canny>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: nunchaku-tech/nunchaku-flux.1-canny-dev\n  description: this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:48:02+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-canny-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:07:43+00:00\n- model_name: fotographerai/zenctrl_tools\n  description: zenctrl we are making an agent that can automate the whole personalized visual content creation process. it will need to perform multiple types of tasks, including designing tasks and training a model for it's own use. this agent will handle the data, the models and ensure the quality of the outputs.\n created at: 2025-03-25T21:40:00+00:00\n- model_name: mit-han-lab/svdq-int4-flux.1-canny-dev\n  description: this repository has been deprecated and will be hidden in december 2025. please use the flux.1 dev model is licensed by black forest labs inc. under the flux.1 dev non-commercial license. copyright black forest labs inc. in no event shall black forest labs inc. be liable for any claim, damages or other liability, whether in an action of contract, tort or otherwise, arising from, out of or in connection with use of this model.\n created at: 2025-02-04T04:41:47+00:00\n\nExamples:\n- example user query: We need artistic images by enhancing the existing collection. We want to create a new dataset with stylized images based on Canny edge detection.\n  model_name: lllyasviel/sd-controlnet-canny\n- example user query: We are a company that prints custom artwork on T-shirts. We need to use AI to generate an image of \"a green unicorn in a magical forest\" based on Canny edges.\n  model_name: lllyasviel/control_v11p_sd15_canny\n- example user query: Our art department needs a tool that uses the Canny edge detection algorithm combined with Stable Diffusion to transform images.\n  model_name: lllyasviel/sd-controlnet-canny\n- example user query: Create a colored image of a blue paradise bird in the jungle using Canny edge detection.\n  model_name: lllyasviel/control_v11p_sd15_canny\n- example user query: Use AI to convert a text description into an image based on the Canny edge detection.\n  model_name: lllyasviel/control_v11p_sd15_canny\n- example user query: We are asked to generate an image by applying Canny edge detection to an existing image.\n  model_name: lllyasviel/sd-controlnet-canny\n\nUser prompt:\nI‚Äôm asking you to make a canny detection for surface shading effects.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_canny>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: nunchaku-tech/nunchaku-flux.1-canny-dev\n  description: this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:48:02+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-canny-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:07:43+00:00\n- model_name: fotographerai/zenctrl_tools\n  description: zenctrl we are making an agent that can automate the whole personalized visual content creation process. it will need to perform multiple types of tasks, including designing tasks and training a model for it's own use. this agent will handle the data, the models and ensure the quality of the outputs.\n created at: 2025-03-25T21:40:00+00:00\n- model_name: mit-han-lab/svdq-int4-flux.1-canny-dev\n  description: this repository has been deprecated and will be hidden in december 2025. please use the flux.1 dev model is licensed by black forest labs inc. under the flux.1 dev non-commercial license. copyright black forest labs inc. in no event shall black forest labs inc. be liable for any claim, damages or other liability, whether in an action of contract, tort or otherwise, arising from, out of or in connection with use of this model.\n created at: 2025-02-04T04:41:47+00:00\n\nExamples:\n- example user query: We need artistic images by enhancing the existing collection. We want to create a new dataset with stylized images based on Canny edge detection.\n  model_name: lllyasviel/sd-controlnet-canny\n- example user query: We are a company that prints custom artwork on T-shirts. We need to use AI to generate an image of \"a green unicorn in a magical forest\" based on Canny edges.\n  model_name: lllyasviel/control_v11p_sd15_canny\n- example user query: Our art department needs a tool that uses the Canny edge detection algorithm combined with Stable Diffusion to transform images.\n  model_name: lllyasviel/sd-controlnet-canny\n- example user query: Create a colored image of a blue paradise bird in the jungle using Canny edge detection.\n  model_name: lllyasviel/control_v11p_sd15_canny\n- example user query: Use AI to convert a text description into an image based on the Canny edge detection.\n  model_name: lllyasviel/control_v11p_sd15_canny\n- example user query: We are asked to generate an image by applying Canny edge detection to an existing image.\n  model_name: lllyasviel/sd-controlnet-canny\n\nUser prompt:\nI want you to create a canny edge image based on this snapshot.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_canny>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: nunchaku-tech/nunchaku-flux.1-canny-dev\n  description: this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:48:02+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-canny-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:07:43+00:00\n- model_name: fotographerai/zenctrl_tools\n  description: zenctrl we are making an agent that can automate the whole personalized visual content creation process. it will need to perform multiple types of tasks, including designing tasks and training a model for it's own use. this agent will handle the data, the models and ensure the quality of the outputs.\n created at: 2025-03-25T21:40:00+00:00\n- model_name: mit-han-lab/svdq-int4-flux.1-canny-dev\n  description: this repository has been deprecated and will be hidden in december 2025. please use the flux.1 dev model is licensed by black forest labs inc. under the flux.1 dev non-commercial license. copyright black forest labs inc. in no event shall black forest labs inc. be liable for any claim, damages or other liability, whether in an action of contract, tort or otherwise, arising from, out of or in connection with use of this model.\n created at: 2025-02-04T04:41:47+00:00\n\nExamples:\n- example user query: We need artistic images by enhancing the existing collection. We want to create a new dataset with stylized images based on Canny edge detection.\n  model_name: lllyasviel/sd-controlnet-canny\n- example user query: We are a company that prints custom artwork on T-shirts. We need to use AI to generate an image of \"a green unicorn in a magical forest\" based on Canny edges.\n  model_name: lllyasviel/control_v11p_sd15_canny\n- example user query: Our art department needs a tool that uses the Canny edge detection algorithm combined with Stable Diffusion to transform images.\n  model_name: lllyasviel/sd-controlnet-canny\n- example user query: Create a colored image of a blue paradise bird in the jungle using Canny edge detection.\n  model_name: lllyasviel/control_v11p_sd15_canny\n- example user query: Use AI to convert a text description into an image based on the Canny edge detection.\n  model_name: lllyasviel/control_v11p_sd15_canny\n- example user query: We are asked to generate an image by applying Canny edge detection to an existing image.\n  model_name: lllyasviel/sd-controlnet-canny\n\nUser prompt:\nI would appreciate producing a canny edges file to visualize surface normals.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_canny>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: nunchaku-tech/nunchaku-flux.1-canny-dev\n  description: this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:48:02+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-canny-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:07:43+00:00\n- model_name: fotographerai/zenctrl_tools\n  description: zenctrl we are making an agent that can automate the whole personalized visual content creation process. it will need to perform multiple types of tasks, including designing tasks and training a model for it's own use. this agent will handle the data, the models and ensure the quality of the outputs.\n created at: 2025-03-25T21:40:00+00:00\n- model_name: mit-han-lab/svdq-int4-flux.1-canny-dev\n  description: this repository has been deprecated and will be hidden in december 2025. please use the flux.1 dev model is licensed by black forest labs inc. under the flux.1 dev non-commercial license. copyright black forest labs inc. in no event shall black forest labs inc. be liable for any claim, damages or other liability, whether in an action of contract, tort or otherwise, arising from, out of or in connection with use of this model.\n created at: 2025-02-04T04:41:47+00:00\n\nExamples:\n- example user query: We need artistic images by enhancing the existing collection. We want to create a new dataset with stylized images based on Canny edge detection.\n  model_name: lllyasviel/sd-controlnet-canny\n- example user query: We are a company that prints custom artwork on T-shirts. We need to use AI to generate an image of \"a green unicorn in a magical forest\" based on Canny edges.\n  model_name: lllyasviel/control_v11p_sd15_canny\n- example user query: Our art department needs a tool that uses the Canny edge detection algorithm combined with Stable Diffusion to transform images.\n  model_name: lllyasviel/sd-controlnet-canny\n- example user query: Create a colored image of a blue paradise bird in the jungle using Canny edge detection.\n  model_name: lllyasviel/control_v11p_sd15_canny\n- example user query: Use AI to convert a text description into an image based on the Canny edge detection.\n  model_name: lllyasviel/control_v11p_sd15_canny\n- example user query: We are asked to generate an image by applying Canny edge detection to an existing image.\n  model_name: lllyasviel/sd-controlnet-canny\n\nUser prompt:\nCould you kindly output a canny result to detect pose and movement.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_canny>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: nunchaku-tech/nunchaku-flux.1-canny-dev\n  description: this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:48:02+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-canny-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:07:43+00:00\n- model_name: fotographerai/zenctrl_tools\n  description: zenctrl we are making an agent that can automate the whole personalized visual content creation process. it will need to perform multiple types of tasks, including designing tasks and training a model for it's own use. this agent will handle the data, the models and ensure the quality of the outputs.\n created at: 2025-03-25T21:40:00+00:00\n- model_name: mit-han-lab/svdq-int4-flux.1-canny-dev\n  description: this repository has been deprecated and will be hidden in december 2025. please use the flux.1 dev model is licensed by black forest labs inc. under the flux.1 dev non-commercial license. copyright black forest labs inc. in no event shall black forest labs inc. be liable for any claim, damages or other liability, whether in an action of contract, tort or otherwise, arising from, out of or in connection with use of this model.\n created at: 2025-02-04T04:41:47+00:00\n\nExamples:\n- example user query: We need artistic images by enhancing the existing collection. We want to create a new dataset with stylized images based on Canny edge detection.\n  model_name: lllyasviel/sd-controlnet-canny\n- example user query: We are a company that prints custom artwork on T-shirts. We need to use AI to generate an image of \"a green unicorn in a magical forest\" based on Canny edges.\n  model_name: lllyasviel/control_v11p_sd15_canny\n- example user query: Our art department needs a tool that uses the Canny edge detection algorithm combined with Stable Diffusion to transform images.\n  model_name: lllyasviel/sd-controlnet-canny\n- example user query: Create a colored image of a blue paradise bird in the jungle using Canny edge detection.\n  model_name: lllyasviel/control_v11p_sd15_canny\n- example user query: Use AI to convert a text description into an image based on the Canny edge detection.\n  model_name: lllyasviel/control_v11p_sd15_canny\n- example user query: We are asked to generate an image by applying Canny edge detection to an existing image.\n  model_name: lllyasviel/sd-controlnet-canny\n\nUser prompt:\nPlease output a canny result for the input image.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_canny>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: nunchaku-tech/nunchaku-flux.1-canny-dev\n  description: this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:48:02+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-canny-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of generating an image based on a text description while following the structure of a given input image. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:07:43+00:00\n- model_name: fotographerai/zenctrl_tools\n  description: zenctrl we are making an agent that can automate the whole personalized visual content creation process. it will need to perform multiple types of tasks, including designing tasks and training a model for it's own use. this agent will handle the data, the models and ensure the quality of the outputs.\n created at: 2025-03-25T21:40:00+00:00\n- model_name: mit-han-lab/svdq-int4-flux.1-canny-dev\n  description: this repository has been deprecated and will be hidden in december 2025. please use the flux.1 dev model is licensed by black forest labs inc. under the flux.1 dev non-commercial license. copyright black forest labs inc. in no event shall black forest labs inc. be liable for any claim, damages or other liability, whether in an action of contract, tort or otherwise, arising from, out of or in connection with use of this model.\n created at: 2025-02-04T04:41:47+00:00\n\nExamples:\n- example user query: We need artistic images by enhancing the existing collection. We want to create a new dataset with stylized images based on Canny edge detection.\n  model_name: lllyasviel/sd-controlnet-canny\n- example user query: We are a company that prints custom artwork on T-shirts. We need to use AI to generate an image of \"a green unicorn in a magical forest\" based on Canny edges.\n  model_name: lllyasviel/control_v11p_sd15_canny\n- example user query: Our art department needs a tool that uses the Canny edge detection algorithm combined with Stable Diffusion to transform images.\n  model_name: lllyasviel/sd-controlnet-canny\n- example user query: Create a colored image of a blue paradise bird in the jungle using Canny edge detection.\n  model_name: lllyasviel/control_v11p_sd15_canny\n- example user query: Use AI to convert a text description into an image based on the Canny edge detection.\n  model_name: lllyasviel/control_v11p_sd15_canny\n- example user query: We are asked to generate an image by applying Canny edge detection to an existing image.\n  model_name: lllyasviel/sd-controlnet-canny\n\nUser prompt:\nI recommend you to show the canny edges result for the given image.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n"
  ],
  "<image_deblur>": [
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_deblur>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: AlekseyCalvin/Flux_Kontext_Dev_fp8_scaled_diffusers\n  description: 'flux.1 kontext dev' is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. it was developed by . for more info, read bfl's and !. 1. change existing images based on an edit instruction. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-27T00:38:23+00:00\n\nExamples:\n- example user query: Our graphic design company wants to restore damaged images. We need to use an AI-based solution for it.\n  model_name: lllyasviel/control_v11p_sd15_inpaint\n- example user query: As a photographer who often takes images in shaky conditions, I want my images to be deblurred automatically.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: Our customer often receives images in a blurred format, we need a solution to generate the clear form of the image.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: A CEO is taking a walk in a rainy weather. He is holding an umbrella. We need a clear image of his face. We have low-quality image. What can we do to achive this?\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: We need to enhance the clarity of a low-quality image by deblurring it, and after that, process the image to be used in a gallery show.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: We need to restore an old and damaged photo. The damage includes stains and scratches, so we need to refill the missing areas.\n  model_name: lllyasviel/control_v11p_sd15_inpaint\n\nUser prompt:\nCombat haziness in the layered elements of the shot.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_deblur>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: AlekseyCalvin/Flux_Kontext_Dev_fp8_scaled_diffusers\n  description: 'flux.1 kontext dev' is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. it was developed by . for more info, read bfl's and !. 1. change existing images based on an edit instruction. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-27T00:38:23+00:00\n\nExamples:\n- example user query: Our graphic design company wants to restore damaged images. We need to use an AI-based solution for it.\n  model_name: lllyasviel/control_v11p_sd15_inpaint\n- example user query: As a photographer who often takes images in shaky conditions, I want my images to be deblurred automatically.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: Our customer often receives images in a blurred format, we need a solution to generate the clear form of the image.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: A CEO is taking a walk in a rainy weather. He is holding an umbrella. We need a clear image of his face. We have low-quality image. What can we do to achive this?\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: We need to enhance the clarity of a low-quality image by deblurring it, and after that, process the image to be used in a gallery show.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: We need to restore an old and damaged photo. The damage includes stains and scratches, so we need to refill the missing areas.\n  model_name: lllyasviel/control_v11p_sd15_inpaint\n\nUser prompt:\nI surmise you can manage haziness in the bokeh background.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_deblur>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: AlekseyCalvin/Flux_Kontext_Dev_fp8_scaled_diffusers\n  description: 'flux.1 kontext dev' is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. it was developed by . for more info, read bfl's and !. 1. change existing images based on an edit instruction. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-27T00:38:23+00:00\n\nExamples:\n- example user query: Our graphic design company wants to restore damaged images. We need to use an AI-based solution for it.\n  model_name: lllyasviel/control_v11p_sd15_inpaint\n- example user query: As a photographer who often takes images in shaky conditions, I want my images to be deblurred automatically.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: Our customer often receives images in a blurred format, we need a solution to generate the clear form of the image.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: A CEO is taking a walk in a rainy weather. He is holding an umbrella. We need a clear image of his face. We have low-quality image. What can we do to achive this?\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: We need to enhance the clarity of a low-quality image by deblurring it, and after that, process the image to be used in a gallery show.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: We need to restore an old and damaged photo. The damage includes stains and scratches, so we need to refill the missing areas.\n  model_name: lllyasviel/control_v11p_sd15_inpaint\n\nUser prompt:\nI implore you to clean smudges from the smooth gradients of the depiction.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_deblur>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: AlekseyCalvin/Flux_Kontext_Dev_fp8_scaled_diffusers\n  description: 'flux.1 kontext dev' is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. it was developed by . for more info, read bfl's and !. 1. change existing images based on an edit instruction. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-27T00:38:23+00:00\n\nExamples:\n- example user query: Our graphic design company wants to restore damaged images. We need to use an AI-based solution for it.\n  model_name: lllyasviel/control_v11p_sd15_inpaint\n- example user query: As a photographer who often takes images in shaky conditions, I want my images to be deblurred automatically.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: Our customer often receives images in a blurred format, we need a solution to generate the clear form of the image.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: A CEO is taking a walk in a rainy weather. He is holding an umbrella. We need a clear image of his face. We have low-quality image. What can we do to achive this?\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: We need to enhance the clarity of a low-quality image by deblurring it, and after that, process the image to be used in a gallery show.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: We need to restore an old and damaged photo. The damage includes stains and scratches, so we need to refill the missing areas.\n  model_name: lllyasviel/control_v11p_sd15_inpaint\n\nUser prompt:\nRemove blur from the full visual.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_deblur>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: AlekseyCalvin/Flux_Kontext_Dev_fp8_scaled_diffusers\n  description: 'flux.1 kontext dev' is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. it was developed by . for more info, read bfl's and !. 1. change existing images based on an edit instruction. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-27T00:38:23+00:00\n\nExamples:\n- example user query: Our graphic design company wants to restore damaged images. We need to use an AI-based solution for it.\n  model_name: lllyasviel/control_v11p_sd15_inpaint\n- example user query: As a photographer who often takes images in shaky conditions, I want my images to be deblurred automatically.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: Our customer often receives images in a blurred format, we need a solution to generate the clear form of the image.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: A CEO is taking a walk in a rainy weather. He is holding an umbrella. We need a clear image of his face. We have low-quality image. What can we do to achive this?\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: We need to enhance the clarity of a low-quality image by deblurring it, and after that, process the image to be used in a gallery show.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: We need to restore an old and damaged photo. The damage includes stains and scratches, so we need to refill the missing areas.\n  model_name: lllyasviel/control_v11p_sd15_inpaint\n\nUser prompt:\nAddress blurriness on the asymmetrical elements of the visual.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_deblur>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: AlekseyCalvin/Flux_Kontext_Dev_fp8_scaled_diffusers\n  description: 'flux.1 kontext dev' is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. it was developed by . for more info, read bfl's and !. 1. change existing images based on an edit instruction. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-27T00:38:23+00:00\n\nExamples:\n- example user query: Our graphic design company wants to restore damaged images. We need to use an AI-based solution for it.\n  model_name: lllyasviel/control_v11p_sd15_inpaint\n- example user query: As a photographer who often takes images in shaky conditions, I want my images to be deblurred automatically.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: Our customer often receives images in a blurred format, we need a solution to generate the clear form of the image.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: A CEO is taking a walk in a rainy weather. He is holding an umbrella. We need a clear image of his face. We have low-quality image. What can we do to achive this?\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: We need to enhance the clarity of a low-quality image by deblurring it, and after that, process the image to be used in a gallery show.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: We need to restore an old and damaged photo. The damage includes stains and scratches, so we need to refill the missing areas.\n  model_name: lllyasviel/control_v11p_sd15_inpaint\n\nUser prompt:\nMinimize blur in the subtle cues of the portrayal.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_deblur>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: AlekseyCalvin/Flux_Kontext_Dev_fp8_scaled_diffusers\n  description: 'flux.1 kontext dev' is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. it was developed by . for more info, read bfl's and !. 1. change existing images based on an edit instruction. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-27T00:38:23+00:00\n\nExamples:\n- example user query: Our graphic design company wants to restore damaged images. We need to use an AI-based solution for it.\n  model_name: lllyasviel/control_v11p_sd15_inpaint\n- example user query: As a photographer who often takes images in shaky conditions, I want my images to be deblurred automatically.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: Our customer often receives images in a blurred format, we need a solution to generate the clear form of the image.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: A CEO is taking a walk in a rainy weather. He is holding an umbrella. We need a clear image of his face. We have low-quality image. What can we do to achive this?\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: We need to enhance the clarity of a low-quality image by deblurring it, and after that, process the image to be used in a gallery show.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: We need to restore an old and damaged photo. The damage includes stains and scratches, so we need to refill the missing areas.\n  model_name: lllyasviel/control_v11p_sd15_inpaint\n\nUser prompt:\nDeal with blurriness in the complex textures of the view.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_deblur>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: AlekseyCalvin/Flux_Kontext_Dev_fp8_scaled_diffusers\n  description: 'flux.1 kontext dev' is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. it was developed by . for more info, read bfl's and !. 1. change existing images based on an edit instruction. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-27T00:38:23+00:00\n\nExamples:\n- example user query: Our graphic design company wants to restore damaged images. We need to use an AI-based solution for it.\n  model_name: lllyasviel/control_v11p_sd15_inpaint\n- example user query: As a photographer who often takes images in shaky conditions, I want my images to be deblurred automatically.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: Our customer often receives images in a blurred format, we need a solution to generate the clear form of the image.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: A CEO is taking a walk in a rainy weather. He is holding an umbrella. We need a clear image of his face. We have low-quality image. What can we do to achive this?\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: We need to enhance the clarity of a low-quality image by deblurring it, and after that, process the image to be used in a gallery show.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: We need to restore an old and damaged photo. The damage includes stains and scratches, so we need to refill the missing areas.\n  model_name: lllyasviel/control_v11p_sd15_inpaint\n\nUser prompt:\nI would like clear haziness from the contrasting colors of the frame.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_deblur>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: AlekseyCalvin/Flux_Kontext_Dev_fp8_scaled_diffusers\n  description: 'flux.1 kontext dev' is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. it was developed by . for more info, read bfl's and !. 1. change existing images based on an edit instruction. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-27T00:38:23+00:00\n\nExamples:\n- example user query: Our graphic design company wants to restore damaged images. We need to use an AI-based solution for it.\n  model_name: lllyasviel/control_v11p_sd15_inpaint\n- example user query: As a photographer who often takes images in shaky conditions, I want my images to be deblurred automatically.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: Our customer often receives images in a blurred format, we need a solution to generate the clear form of the image.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: A CEO is taking a walk in a rainy weather. He is holding an umbrella. We need a clear image of his face. We have low-quality image. What can we do to achive this?\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: We need to enhance the clarity of a low-quality image by deblurring it, and after that, process the image to be used in a gallery show.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: We need to restore an old and damaged photo. The damage includes stains and scratches, so we need to refill the missing areas.\n  model_name: lllyasviel/control_v11p_sd15_inpaint\n\nUser prompt:\nOvercome haziness in the sharp edges of the shot.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_deblur>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: AlekseyCalvin/Flux_Kontext_Dev_fp8_scaled_diffusers\n  description: 'flux.1 kontext dev' is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. it was developed by . for more info, read bfl's and !. 1. change existing images based on an edit instruction. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-27T00:38:23+00:00\n\nExamples:\n- example user query: Our graphic design company wants to restore damaged images. We need to use an AI-based solution for it.\n  model_name: lllyasviel/control_v11p_sd15_inpaint\n- example user query: As a photographer who often takes images in shaky conditions, I want my images to be deblurred automatically.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: Our customer often receives images in a blurred format, we need a solution to generate the clear form of the image.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: A CEO is taking a walk in a rainy weather. He is holding an umbrella. We need a clear image of his face. We have low-quality image. What can we do to achive this?\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: We need to enhance the clarity of a low-quality image by deblurring it, and after that, process the image to be used in a gallery show.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: We need to restore an old and damaged photo. The damage includes stains and scratches, so we need to refill the missing areas.\n  model_name: lllyasviel/control_v11p_sd15_inpaint\n\nUser prompt:\nDeal with blur in the detailed shadows of the portrayal.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n"
  ],
  "<image_denoise>": [
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_denoise>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: prs-eth/marigold-normals-v1-1\n  description: marigold normals v1-1 model card this is a model card for the `marigold-normals-v1-1` model for monocular normals estimation from a single image. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-10-21T19:16:05+00:00\n- model_name: alimama-creative/FLUX.1-dev-Controlnet-Inpainting-Beta\n  description: this repository hosts an improved inpainting controlnet checkpoint for the model, developed by the alimamacreative team. - 2024.10.16 we release a 8-step flux.1 distilled lora , which is compatible with our inpainting controlnet. our latest inpainting model brings significant improvements compared to the previous version:\n created at: 2024-10-08T10:53:43+00:00\n- model_name: jasperai/Flux.1-dev-Controlnet-Upscaler\n  description: this is controlnet for low resolution images developed by jasper research team. this model can be used directly with the `diffusers` library this model was trained with a synthetic complex data degradation scheme taking as input a real-life image and artificially degrading it by combining several degradations such as amongst other image noising gaussian, poisson, image blurring and jpeg compression in a similar spirit as 1\n created at: 2024-09-23T13:23:33+00:00\n- model_name: prs-eth/marigold-iid-lighting-v1-1\n  description: marigold intrinsic image decomposition iid lighting v1-1 model card this is a model card for the `marigold-iid-lighting-v1-1` model for single-image intrinsic image decomposition iid. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-12-16T12:04:04+00:00\n- model_name: prs-eth/marigold-iid-appearance-v1-1\n  description: marigold intrinsic image decomposition iid appearance v1-1 model card this is a model card for the `marigold-iid-appearance-v1-1` model for single-image intrinsic image decomposition iid. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-10-21T19:40:41+00:00\n- model_name: reverentelusarca/kontext-turnaround-sheet-lora-v1\n  description: recommended prompt:create turnaround sheet of this exact character, 5 full-body poses on pure white background: front view, 3/4 left, left profile, back view, right profile, 3/4 right evenly spaced in a clean horizontal row, consistent lighting and style, no background noise, no shadows, same proportions and detailing across all views feel free to tweak the prompt, but make sure you have \"create turnaround sheet of this character\" in your updated prompt. - it won't work well on real human photos\n created at: 2025-07-07T10:01:22+00:00\n- model_name: gpustack/stable-diffusion-xl-refiner-1.0-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: diffusion-based text-to-image generative model - license:  - model description: this is a model that can be used to generate and modify images based on text prompts. it is a that uses two fixed, pretrained text encoders and . - resources for more information: check out our and the .\n created at: 2024-11-08T08:17:23+00:00\n- model_name: DFloat11/Qwen-Image-Edit-DF11\n  description: this is a dfloat11 losslessly compressed version of the original `qwen/qwen-image-edit` model. it reduces model size by 32% compared to the original bfloat16 model, while maintaining bit-identical outputs and supporting efficient gpu inference . üî•üî•üî• thanks to dfloat11 compression, qwen-image-edit can now run on a single 32gb gpu , or on a single 24gb gpu with cpu offloading , while maintaining full model quality. üî•üî•üî• model model size peak gpu memory generation time a100 gpu\n created at: 2025-08-19T08:20:45+00:00\n\nExamples:\n- example user query: We need to improve the quality of a cartoon image by removing noise from it.\n  model_name: lllyasviel/sd-controlnet-canny\n\nUser prompt:\nThe snapshot suffers from fuzziness; could you diminish it?\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_denoise>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: prs-eth/marigold-normals-v1-1\n  description: marigold normals v1-1 model card this is a model card for the `marigold-normals-v1-1` model for monocular normals estimation from a single image. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-10-21T19:16:05+00:00\n- model_name: alimama-creative/FLUX.1-dev-Controlnet-Inpainting-Beta\n  description: this repository hosts an improved inpainting controlnet checkpoint for the model, developed by the alimamacreative team. - 2024.10.16 we release a 8-step flux.1 distilled lora , which is compatible with our inpainting controlnet. our latest inpainting model brings significant improvements compared to the previous version:\n created at: 2024-10-08T10:53:43+00:00\n- model_name: jasperai/Flux.1-dev-Controlnet-Upscaler\n  description: this is controlnet for low resolution images developed by jasper research team. this model can be used directly with the `diffusers` library this model was trained with a synthetic complex data degradation scheme taking as input a real-life image and artificially degrading it by combining several degradations such as amongst other image noising gaussian, poisson, image blurring and jpeg compression in a similar spirit as 1\n created at: 2024-09-23T13:23:33+00:00\n- model_name: prs-eth/marigold-iid-lighting-v1-1\n  description: marigold intrinsic image decomposition iid lighting v1-1 model card this is a model card for the `marigold-iid-lighting-v1-1` model for single-image intrinsic image decomposition iid. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-12-16T12:04:04+00:00\n- model_name: prs-eth/marigold-iid-appearance-v1-1\n  description: marigold intrinsic image decomposition iid appearance v1-1 model card this is a model card for the `marigold-iid-appearance-v1-1` model for single-image intrinsic image decomposition iid. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-10-21T19:40:41+00:00\n- model_name: reverentelusarca/kontext-turnaround-sheet-lora-v1\n  description: recommended prompt:create turnaround sheet of this exact character, 5 full-body poses on pure white background: front view, 3/4 left, left profile, back view, right profile, 3/4 right evenly spaced in a clean horizontal row, consistent lighting and style, no background noise, no shadows, same proportions and detailing across all views feel free to tweak the prompt, but make sure you have \"create turnaround sheet of this character\" in your updated prompt. - it won't work well on real human photos\n created at: 2025-07-07T10:01:22+00:00\n- model_name: gpustack/stable-diffusion-xl-refiner-1.0-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: diffusion-based text-to-image generative model - license:  - model description: this is a model that can be used to generate and modify images based on text prompts. it is a that uses two fixed, pretrained text encoders and . - resources for more information: check out our and the .\n created at: 2024-11-08T08:17:23+00:00\n- model_name: DFloat11/Qwen-Image-Edit-DF11\n  description: this is a dfloat11 losslessly compressed version of the original `qwen/qwen-image-edit` model. it reduces model size by 32% compared to the original bfloat16 model, while maintaining bit-identical outputs and supporting efficient gpu inference . üî•üî•üî• thanks to dfloat11 compression, qwen-image-edit can now run on a single 32gb gpu , or on a single 24gb gpu with cpu offloading , while maintaining full model quality. üî•üî•üî• model model size peak gpu memory generation time a100 gpu\n created at: 2025-08-19T08:20:45+00:00\n\nExamples:\n- example user query: We need to improve the quality of a cartoon image by removing noise from it.\n  model_name: lllyasviel/sd-controlnet-canny\n\nUser prompt:\nPlease diminish the visual clutter so the picture is clearer.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_denoise>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: prs-eth/marigold-normals-v1-1\n  description: marigold normals v1-1 model card this is a model card for the `marigold-normals-v1-1` model for monocular normals estimation from a single image. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-10-21T19:16:05+00:00\n- model_name: alimama-creative/FLUX.1-dev-Controlnet-Inpainting-Beta\n  description: this repository hosts an improved inpainting controlnet checkpoint for the model, developed by the alimamacreative team. - 2024.10.16 we release a 8-step flux.1 distilled lora , which is compatible with our inpainting controlnet. our latest inpainting model brings significant improvements compared to the previous version:\n created at: 2024-10-08T10:53:43+00:00\n- model_name: jasperai/Flux.1-dev-Controlnet-Upscaler\n  description: this is controlnet for low resolution images developed by jasper research team. this model can be used directly with the `diffusers` library this model was trained with a synthetic complex data degradation scheme taking as input a real-life image and artificially degrading it by combining several degradations such as amongst other image noising gaussian, poisson, image blurring and jpeg compression in a similar spirit as 1\n created at: 2024-09-23T13:23:33+00:00\n- model_name: prs-eth/marigold-iid-lighting-v1-1\n  description: marigold intrinsic image decomposition iid lighting v1-1 model card this is a model card for the `marigold-iid-lighting-v1-1` model for single-image intrinsic image decomposition iid. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-12-16T12:04:04+00:00\n- model_name: prs-eth/marigold-iid-appearance-v1-1\n  description: marigold intrinsic image decomposition iid appearance v1-1 model card this is a model card for the `marigold-iid-appearance-v1-1` model for single-image intrinsic image decomposition iid. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-10-21T19:40:41+00:00\n- model_name: reverentelusarca/kontext-turnaround-sheet-lora-v1\n  description: recommended prompt:create turnaround sheet of this exact character, 5 full-body poses on pure white background: front view, 3/4 left, left profile, back view, right profile, 3/4 right evenly spaced in a clean horizontal row, consistent lighting and style, no background noise, no shadows, same proportions and detailing across all views feel free to tweak the prompt, but make sure you have \"create turnaround sheet of this character\" in your updated prompt. - it won't work well on real human photos\n created at: 2025-07-07T10:01:22+00:00\n- model_name: gpustack/stable-diffusion-xl-refiner-1.0-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: diffusion-based text-to-image generative model - license:  - model description: this is a model that can be used to generate and modify images based on text prompts. it is a that uses two fixed, pretrained text encoders and . - resources for more information: check out our and the .\n created at: 2024-11-08T08:17:23+00:00\n- model_name: DFloat11/Qwen-Image-Edit-DF11\n  description: this is a dfloat11 losslessly compressed version of the original `qwen/qwen-image-edit` model. it reduces model size by 32% compared to the original bfloat16 model, while maintaining bit-identical outputs and supporting efficient gpu inference . üî•üî•üî• thanks to dfloat11 compression, qwen-image-edit can now run on a single 32gb gpu , or on a single 24gb gpu with cpu offloading , while maintaining full model quality. üî•üî•üî• model model size peak gpu memory generation time a100 gpu\n created at: 2025-08-19T08:20:45+00:00\n\nExamples:\n- example user query: We need to improve the quality of a cartoon image by removing noise from it.\n  model_name: lllyasviel/sd-controlnet-canny\n\nUser prompt:\nCan you cleanse the visual clutter in the photograph?\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_denoise>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: prs-eth/marigold-normals-v1-1\n  description: marigold normals v1-1 model card this is a model card for the `marigold-normals-v1-1` model for monocular normals estimation from a single image. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-10-21T19:16:05+00:00\n- model_name: alimama-creative/FLUX.1-dev-Controlnet-Inpainting-Beta\n  description: this repository hosts an improved inpainting controlnet checkpoint for the model, developed by the alimamacreative team. - 2024.10.16 we release a 8-step flux.1 distilled lora , which is compatible with our inpainting controlnet. our latest inpainting model brings significant improvements compared to the previous version:\n created at: 2024-10-08T10:53:43+00:00\n- model_name: jasperai/Flux.1-dev-Controlnet-Upscaler\n  description: this is controlnet for low resolution images developed by jasper research team. this model can be used directly with the `diffusers` library this model was trained with a synthetic complex data degradation scheme taking as input a real-life image and artificially degrading it by combining several degradations such as amongst other image noising gaussian, poisson, image blurring and jpeg compression in a similar spirit as 1\n created at: 2024-09-23T13:23:33+00:00\n- model_name: prs-eth/marigold-iid-lighting-v1-1\n  description: marigold intrinsic image decomposition iid lighting v1-1 model card this is a model card for the `marigold-iid-lighting-v1-1` model for single-image intrinsic image decomposition iid. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-12-16T12:04:04+00:00\n- model_name: prs-eth/marigold-iid-appearance-v1-1\n  description: marigold intrinsic image decomposition iid appearance v1-1 model card this is a model card for the `marigold-iid-appearance-v1-1` model for single-image intrinsic image decomposition iid. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-10-21T19:40:41+00:00\n- model_name: reverentelusarca/kontext-turnaround-sheet-lora-v1\n  description: recommended prompt:create turnaround sheet of this exact character, 5 full-body poses on pure white background: front view, 3/4 left, left profile, back view, right profile, 3/4 right evenly spaced in a clean horizontal row, consistent lighting and style, no background noise, no shadows, same proportions and detailing across all views feel free to tweak the prompt, but make sure you have \"create turnaround sheet of this character\" in your updated prompt. - it won't work well on real human photos\n created at: 2025-07-07T10:01:22+00:00\n- model_name: gpustack/stable-diffusion-xl-refiner-1.0-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: diffusion-based text-to-image generative model - license:  - model description: this is a model that can be used to generate and modify images based on text prompts. it is a that uses two fixed, pretrained text encoders and . - resources for more information: check out our and the .\n created at: 2024-11-08T08:17:23+00:00\n- model_name: DFloat11/Qwen-Image-Edit-DF11\n  description: this is a dfloat11 losslessly compressed version of the original `qwen/qwen-image-edit` model. it reduces model size by 32% compared to the original bfloat16 model, while maintaining bit-identical outputs and supporting efficient gpu inference . üî•üî•üî• thanks to dfloat11 compression, qwen-image-edit can now run on a single 32gb gpu , or on a single 24gb gpu with cpu offloading , while maintaining full model quality. üî•üî•üî• model model size peak gpu memory generation time a100 gpu\n created at: 2025-08-19T08:20:45+00:00\n\nExamples:\n- example user query: We need to improve the quality of a cartoon image by removing noise from it.\n  model_name: lllyasviel/sd-controlnet-canny\n\nUser prompt:\nPlease improve the distortion so the photograph is clearer.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_denoise>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: prs-eth/marigold-normals-v1-1\n  description: marigold normals v1-1 model card this is a model card for the `marigold-normals-v1-1` model for monocular normals estimation from a single image. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-10-21T19:16:05+00:00\n- model_name: alimama-creative/FLUX.1-dev-Controlnet-Inpainting-Beta\n  description: this repository hosts an improved inpainting controlnet checkpoint for the model, developed by the alimamacreative team. - 2024.10.16 we release a 8-step flux.1 distilled lora , which is compatible with our inpainting controlnet. our latest inpainting model brings significant improvements compared to the previous version:\n created at: 2024-10-08T10:53:43+00:00\n- model_name: jasperai/Flux.1-dev-Controlnet-Upscaler\n  description: this is controlnet for low resolution images developed by jasper research team. this model can be used directly with the `diffusers` library this model was trained with a synthetic complex data degradation scheme taking as input a real-life image and artificially degrading it by combining several degradations such as amongst other image noising gaussian, poisson, image blurring and jpeg compression in a similar spirit as 1\n created at: 2024-09-23T13:23:33+00:00\n- model_name: prs-eth/marigold-iid-lighting-v1-1\n  description: marigold intrinsic image decomposition iid lighting v1-1 model card this is a model card for the `marigold-iid-lighting-v1-1` model for single-image intrinsic image decomposition iid. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-12-16T12:04:04+00:00\n- model_name: prs-eth/marigold-iid-appearance-v1-1\n  description: marigold intrinsic image decomposition iid appearance v1-1 model card this is a model card for the `marigold-iid-appearance-v1-1` model for single-image intrinsic image decomposition iid. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-10-21T19:40:41+00:00\n- model_name: reverentelusarca/kontext-turnaround-sheet-lora-v1\n  description: recommended prompt:create turnaround sheet of this exact character, 5 full-body poses on pure white background: front view, 3/4 left, left profile, back view, right profile, 3/4 right evenly spaced in a clean horizontal row, consistent lighting and style, no background noise, no shadows, same proportions and detailing across all views feel free to tweak the prompt, but make sure you have \"create turnaround sheet of this character\" in your updated prompt. - it won't work well on real human photos\n created at: 2025-07-07T10:01:22+00:00\n- model_name: gpustack/stable-diffusion-xl-refiner-1.0-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: diffusion-based text-to-image generative model - license:  - model description: this is a model that can be used to generate and modify images based on text prompts. it is a that uses two fixed, pretrained text encoders and . - resources for more information: check out our and the .\n created at: 2024-11-08T08:17:23+00:00\n- model_name: DFloat11/Qwen-Image-Edit-DF11\n  description: this is a dfloat11 losslessly compressed version of the original `qwen/qwen-image-edit` model. it reduces model size by 32% compared to the original bfloat16 model, while maintaining bit-identical outputs and supporting efficient gpu inference . üî•üî•üî• thanks to dfloat11 compression, qwen-image-edit can now run on a single 32gb gpu , or on a single 24gb gpu with cpu offloading , while maintaining full model quality. üî•üî•üî• model model size peak gpu memory generation time a100 gpu\n created at: 2025-08-19T08:20:45+00:00\n\nExamples:\n- example user query: We need to improve the quality of a cartoon image by removing noise from it.\n  model_name: lllyasviel/sd-controlnet-canny\n\nUser prompt:\nthe image appears noisy; clean up the visual clutter for clarity.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_denoise>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: prs-eth/marigold-normals-v1-1\n  description: marigold normals v1-1 model card this is a model card for the `marigold-normals-v1-1` model for monocular normals estimation from a single image. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-10-21T19:16:05+00:00\n- model_name: alimama-creative/FLUX.1-dev-Controlnet-Inpainting-Beta\n  description: this repository hosts an improved inpainting controlnet checkpoint for the model, developed by the alimamacreative team. - 2024.10.16 we release a 8-step flux.1 distilled lora , which is compatible with our inpainting controlnet. our latest inpainting model brings significant improvements compared to the previous version:\n created at: 2024-10-08T10:53:43+00:00\n- model_name: jasperai/Flux.1-dev-Controlnet-Upscaler\n  description: this is controlnet for low resolution images developed by jasper research team. this model can be used directly with the `diffusers` library this model was trained with a synthetic complex data degradation scheme taking as input a real-life image and artificially degrading it by combining several degradations such as amongst other image noising gaussian, poisson, image blurring and jpeg compression in a similar spirit as 1\n created at: 2024-09-23T13:23:33+00:00\n- model_name: prs-eth/marigold-iid-lighting-v1-1\n  description: marigold intrinsic image decomposition iid lighting v1-1 model card this is a model card for the `marigold-iid-lighting-v1-1` model for single-image intrinsic image decomposition iid. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-12-16T12:04:04+00:00\n- model_name: prs-eth/marigold-iid-appearance-v1-1\n  description: marigold intrinsic image decomposition iid appearance v1-1 model card this is a model card for the `marigold-iid-appearance-v1-1` model for single-image intrinsic image decomposition iid. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-10-21T19:40:41+00:00\n- model_name: reverentelusarca/kontext-turnaround-sheet-lora-v1\n  description: recommended prompt:create turnaround sheet of this exact character, 5 full-body poses on pure white background: front view, 3/4 left, left profile, back view, right profile, 3/4 right evenly spaced in a clean horizontal row, consistent lighting and style, no background noise, no shadows, same proportions and detailing across all views feel free to tweak the prompt, but make sure you have \"create turnaround sheet of this character\" in your updated prompt. - it won't work well on real human photos\n created at: 2025-07-07T10:01:22+00:00\n- model_name: gpustack/stable-diffusion-xl-refiner-1.0-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: diffusion-based text-to-image generative model - license:  - model description: this is a model that can be used to generate and modify images based on text prompts. it is a that uses two fixed, pretrained text encoders and . - resources for more information: check out our and the .\n created at: 2024-11-08T08:17:23+00:00\n- model_name: DFloat11/Qwen-Image-Edit-DF11\n  description: this is a dfloat11 losslessly compressed version of the original `qwen/qwen-image-edit` model. it reduces model size by 32% compared to the original bfloat16 model, while maintaining bit-identical outputs and supporting efficient gpu inference . üî•üî•üî• thanks to dfloat11 compression, qwen-image-edit can now run on a single 32gb gpu , or on a single 24gb gpu with cpu offloading , while maintaining full model quality. üî•üî•üî• model model size peak gpu memory generation time a100 gpu\n created at: 2025-08-19T08:20:45+00:00\n\nExamples:\n- example user query: We need to improve the quality of a cartoon image by removing noise from it.\n  model_name: lllyasviel/sd-controlnet-canny\n\nUser prompt:\nHow can I correct the noise in the image?\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_denoise>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: prs-eth/marigold-normals-v1-1\n  description: marigold normals v1-1 model card this is a model card for the `marigold-normals-v1-1` model for monocular normals estimation from a single image. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-10-21T19:16:05+00:00\n- model_name: alimama-creative/FLUX.1-dev-Controlnet-Inpainting-Beta\n  description: this repository hosts an improved inpainting controlnet checkpoint for the model, developed by the alimamacreative team. - 2024.10.16 we release a 8-step flux.1 distilled lora , which is compatible with our inpainting controlnet. our latest inpainting model brings significant improvements compared to the previous version:\n created at: 2024-10-08T10:53:43+00:00\n- model_name: jasperai/Flux.1-dev-Controlnet-Upscaler\n  description: this is controlnet for low resolution images developed by jasper research team. this model can be used directly with the `diffusers` library this model was trained with a synthetic complex data degradation scheme taking as input a real-life image and artificially degrading it by combining several degradations such as amongst other image noising gaussian, poisson, image blurring and jpeg compression in a similar spirit as 1\n created at: 2024-09-23T13:23:33+00:00\n- model_name: prs-eth/marigold-iid-lighting-v1-1\n  description: marigold intrinsic image decomposition iid lighting v1-1 model card this is a model card for the `marigold-iid-lighting-v1-1` model for single-image intrinsic image decomposition iid. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-12-16T12:04:04+00:00\n- model_name: prs-eth/marigold-iid-appearance-v1-1\n  description: marigold intrinsic image decomposition iid appearance v1-1 model card this is a model card for the `marigold-iid-appearance-v1-1` model for single-image intrinsic image decomposition iid. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-10-21T19:40:41+00:00\n- model_name: reverentelusarca/kontext-turnaround-sheet-lora-v1\n  description: recommended prompt:create turnaround sheet of this exact character, 5 full-body poses on pure white background: front view, 3/4 left, left profile, back view, right profile, 3/4 right evenly spaced in a clean horizontal row, consistent lighting and style, no background noise, no shadows, same proportions and detailing across all views feel free to tweak the prompt, but make sure you have \"create turnaround sheet of this character\" in your updated prompt. - it won't work well on real human photos\n created at: 2025-07-07T10:01:22+00:00\n- model_name: gpustack/stable-diffusion-xl-refiner-1.0-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: diffusion-based text-to-image generative model - license:  - model description: this is a model that can be used to generate and modify images based on text prompts. it is a that uses two fixed, pretrained text encoders and . - resources for more information: check out our and the .\n created at: 2024-11-08T08:17:23+00:00\n- model_name: DFloat11/Qwen-Image-Edit-DF11\n  description: this is a dfloat11 losslessly compressed version of the original `qwen/qwen-image-edit` model. it reduces model size by 32% compared to the original bfloat16 model, while maintaining bit-identical outputs and supporting efficient gpu inference . üî•üî•üî• thanks to dfloat11 compression, qwen-image-edit can now run on a single 32gb gpu , or on a single 24gb gpu with cpu offloading , while maintaining full model quality. üî•üî•üî• model model size peak gpu memory generation time a100 gpu\n created at: 2025-08-19T08:20:45+00:00\n\nExamples:\n- example user query: We need to improve the quality of a cartoon image by removing noise from it.\n  model_name: lllyasviel/sd-controlnet-canny\n\nUser prompt:\nThe noise in the picture makes it unclear; please correct it.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_denoise>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: prs-eth/marigold-normals-v1-1\n  description: marigold normals v1-1 model card this is a model card for the `marigold-normals-v1-1` model for monocular normals estimation from a single image. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-10-21T19:16:05+00:00\n- model_name: alimama-creative/FLUX.1-dev-Controlnet-Inpainting-Beta\n  description: this repository hosts an improved inpainting controlnet checkpoint for the model, developed by the alimamacreative team. - 2024.10.16 we release a 8-step flux.1 distilled lora , which is compatible with our inpainting controlnet. our latest inpainting model brings significant improvements compared to the previous version:\n created at: 2024-10-08T10:53:43+00:00\n- model_name: jasperai/Flux.1-dev-Controlnet-Upscaler\n  description: this is controlnet for low resolution images developed by jasper research team. this model can be used directly with the `diffusers` library this model was trained with a synthetic complex data degradation scheme taking as input a real-life image and artificially degrading it by combining several degradations such as amongst other image noising gaussian, poisson, image blurring and jpeg compression in a similar spirit as 1\n created at: 2024-09-23T13:23:33+00:00\n- model_name: prs-eth/marigold-iid-lighting-v1-1\n  description: marigold intrinsic image decomposition iid lighting v1-1 model card this is a model card for the `marigold-iid-lighting-v1-1` model for single-image intrinsic image decomposition iid. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-12-16T12:04:04+00:00\n- model_name: prs-eth/marigold-iid-appearance-v1-1\n  description: marigold intrinsic image decomposition iid appearance v1-1 model card this is a model card for the `marigold-iid-appearance-v1-1` model for single-image intrinsic image decomposition iid. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-10-21T19:40:41+00:00\n- model_name: reverentelusarca/kontext-turnaround-sheet-lora-v1\n  description: recommended prompt:create turnaround sheet of this exact character, 5 full-body poses on pure white background: front view, 3/4 left, left profile, back view, right profile, 3/4 right evenly spaced in a clean horizontal row, consistent lighting and style, no background noise, no shadows, same proportions and detailing across all views feel free to tweak the prompt, but make sure you have \"create turnaround sheet of this character\" in your updated prompt. - it won't work well on real human photos\n created at: 2025-07-07T10:01:22+00:00\n- model_name: gpustack/stable-diffusion-xl-refiner-1.0-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: diffusion-based text-to-image generative model - license:  - model description: this is a model that can be used to generate and modify images based on text prompts. it is a that uses two fixed, pretrained text encoders and . - resources for more information: check out our and the .\n created at: 2024-11-08T08:17:23+00:00\n- model_name: DFloat11/Qwen-Image-Edit-DF11\n  description: this is a dfloat11 losslessly compressed version of the original `qwen/qwen-image-edit` model. it reduces model size by 32% compared to the original bfloat16 model, while maintaining bit-identical outputs and supporting efficient gpu inference . üî•üî•üî• thanks to dfloat11 compression, qwen-image-edit can now run on a single 32gb gpu , or on a single 24gb gpu with cpu offloading , while maintaining full model quality. üî•üî•üî• model model size peak gpu memory generation time a100 gpu\n created at: 2025-08-19T08:20:45+00:00\n\nExamples:\n- example user query: We need to improve the quality of a cartoon image by removing noise from it.\n  model_name: lllyasviel/sd-controlnet-canny\n\nUser prompt:\nPlease refine the distortion affecting the image.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_denoise>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: prs-eth/marigold-normals-v1-1\n  description: marigold normals v1-1 model card this is a model card for the `marigold-normals-v1-1` model for monocular normals estimation from a single image. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-10-21T19:16:05+00:00\n- model_name: alimama-creative/FLUX.1-dev-Controlnet-Inpainting-Beta\n  description: this repository hosts an improved inpainting controlnet checkpoint for the model, developed by the alimamacreative team. - 2024.10.16 we release a 8-step flux.1 distilled lora , which is compatible with our inpainting controlnet. our latest inpainting model brings significant improvements compared to the previous version:\n created at: 2024-10-08T10:53:43+00:00\n- model_name: jasperai/Flux.1-dev-Controlnet-Upscaler\n  description: this is controlnet for low resolution images developed by jasper research team. this model can be used directly with the `diffusers` library this model was trained with a synthetic complex data degradation scheme taking as input a real-life image and artificially degrading it by combining several degradations such as amongst other image noising gaussian, poisson, image blurring and jpeg compression in a similar spirit as 1\n created at: 2024-09-23T13:23:33+00:00\n- model_name: prs-eth/marigold-iid-lighting-v1-1\n  description: marigold intrinsic image decomposition iid lighting v1-1 model card this is a model card for the `marigold-iid-lighting-v1-1` model for single-image intrinsic image decomposition iid. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-12-16T12:04:04+00:00\n- model_name: prs-eth/marigold-iid-appearance-v1-1\n  description: marigold intrinsic image decomposition iid appearance v1-1 model card this is a model card for the `marigold-iid-appearance-v1-1` model for single-image intrinsic image decomposition iid. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-10-21T19:40:41+00:00\n- model_name: reverentelusarca/kontext-turnaround-sheet-lora-v1\n  description: recommended prompt:create turnaround sheet of this exact character, 5 full-body poses on pure white background: front view, 3/4 left, left profile, back view, right profile, 3/4 right evenly spaced in a clean horizontal row, consistent lighting and style, no background noise, no shadows, same proportions and detailing across all views feel free to tweak the prompt, but make sure you have \"create turnaround sheet of this character\" in your updated prompt. - it won't work well on real human photos\n created at: 2025-07-07T10:01:22+00:00\n- model_name: gpustack/stable-diffusion-xl-refiner-1.0-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: diffusion-based text-to-image generative model - license:  - model description: this is a model that can be used to generate and modify images based on text prompts. it is a that uses two fixed, pretrained text encoders and . - resources for more information: check out our and the .\n created at: 2024-11-08T08:17:23+00:00\n- model_name: DFloat11/Qwen-Image-Edit-DF11\n  description: this is a dfloat11 losslessly compressed version of the original `qwen/qwen-image-edit` model. it reduces model size by 32% compared to the original bfloat16 model, while maintaining bit-identical outputs and supporting efficient gpu inference . üî•üî•üî• thanks to dfloat11 compression, qwen-image-edit can now run on a single 32gb gpu , or on a single 24gb gpu with cpu offloading , while maintaining full model quality. üî•üî•üî• model model size peak gpu memory generation time a100 gpu\n created at: 2025-08-19T08:20:45+00:00\n\nExamples:\n- example user query: We need to improve the quality of a cartoon image by removing noise from it.\n  model_name: lllyasviel/sd-controlnet-canny\n\nUser prompt:\nthis snapshot appears noisy; diminish the static for clarity.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_denoise>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: prs-eth/marigold-normals-v1-1\n  description: marigold normals v1-1 model card this is a model card for the `marigold-normals-v1-1` model for monocular normals estimation from a single image. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-10-21T19:16:05+00:00\n- model_name: alimama-creative/FLUX.1-dev-Controlnet-Inpainting-Beta\n  description: this repository hosts an improved inpainting controlnet checkpoint for the model, developed by the alimamacreative team. - 2024.10.16 we release a 8-step flux.1 distilled lora , which is compatible with our inpainting controlnet. our latest inpainting model brings significant improvements compared to the previous version:\n created at: 2024-10-08T10:53:43+00:00\n- model_name: jasperai/Flux.1-dev-Controlnet-Upscaler\n  description: this is controlnet for low resolution images developed by jasper research team. this model can be used directly with the `diffusers` library this model was trained with a synthetic complex data degradation scheme taking as input a real-life image and artificially degrading it by combining several degradations such as amongst other image noising gaussian, poisson, image blurring and jpeg compression in a similar spirit as 1\n created at: 2024-09-23T13:23:33+00:00\n- model_name: prs-eth/marigold-iid-lighting-v1-1\n  description: marigold intrinsic image decomposition iid lighting v1-1 model card this is a model card for the `marigold-iid-lighting-v1-1` model for single-image intrinsic image decomposition iid. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-12-16T12:04:04+00:00\n- model_name: prs-eth/marigold-iid-appearance-v1-1\n  description: marigold intrinsic image decomposition iid appearance v1-1 model card this is a model card for the `marigold-iid-appearance-v1-1` model for single-image intrinsic image decomposition iid. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-10-21T19:40:41+00:00\n- model_name: reverentelusarca/kontext-turnaround-sheet-lora-v1\n  description: recommended prompt:create turnaround sheet of this exact character, 5 full-body poses on pure white background: front view, 3/4 left, left profile, back view, right profile, 3/4 right evenly spaced in a clean horizontal row, consistent lighting and style, no background noise, no shadows, same proportions and detailing across all views feel free to tweak the prompt, but make sure you have \"create turnaround sheet of this character\" in your updated prompt. - it won't work well on real human photos\n created at: 2025-07-07T10:01:22+00:00\n- model_name: gpustack/stable-diffusion-xl-refiner-1.0-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: diffusion-based text-to-image generative model - license:  - model description: this is a model that can be used to generate and modify images based on text prompts. it is a that uses two fixed, pretrained text encoders and . - resources for more information: check out our and the .\n created at: 2024-11-08T08:17:23+00:00\n- model_name: DFloat11/Qwen-Image-Edit-DF11\n  description: this is a dfloat11 losslessly compressed version of the original `qwen/qwen-image-edit` model. it reduces model size by 32% compared to the original bfloat16 model, while maintaining bit-identical outputs and supporting efficient gpu inference . üî•üî•üî• thanks to dfloat11 compression, qwen-image-edit can now run on a single 32gb gpu , or on a single 24gb gpu with cpu offloading , while maintaining full model quality. üî•üî•üî• model model size peak gpu memory generation time a100 gpu\n created at: 2025-08-19T08:20:45+00:00\n\nExamples:\n- example user query: We need to improve the quality of a cartoon image by removing noise from it.\n  model_name: lllyasviel/sd-controlnet-canny\n\nUser prompt:\nthe picture appears noisy; improve the artifacts for clarity.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n"
  ],
  "<image_depth>": [
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_depth>\n\nCandidate Models:\n- model_name: hf-tiny-model-private/tiny-random-DPTForDepthEstimation\n  description: A tiny random DPT model for depth estimation using Hugging Face Transformers library.\n created at: 2023-03-29\n- model_name: nielsr/dpt-large-redesign\n  description: A depth estimation model based on the DPT architecture.\n created at: 2022-03-04\n- model_name: vinvino02/glpn-nyu\n  description: Global-Local Path Networks (GLPN) model trained on NYUv2 for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: Intel/dpt-hybrid-midas\n  description: Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021) and first released in this repository. DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation. This repository hosts the hybrid version of the model as stated in the paper. DPT-Hybrid diverges from DPT by using ViT-hybrid as a backbone and taking some activations from the backbone.\n created at: 2022-12-06\n- model_name: vinvino02/glpn-kitti\n  description: Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: hf-tiny-model-private/tiny-random-GLPNForDepthEstimation\n  description: A tiny random GLPN model for depth estimation using the Hugging Face Transformers library.\n created at: 2023-03-29\n- model_name: Intel/dpt-large\n  description: Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021). DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation.\n created at: 2022-03-02\n- model_name: apple/DepthPro-hf\n  description: this is the transformers version of depthpro, a foundation model for zero-shot metric monocular depth estimation, designed to generate high-resolution depth maps with remarkable sharpness and fine-grained details. for the checkpoint compatible with the original codebase, please check . - -\n created at: 2024-11-27T13:52:11+00:00\n- model_name: prs-eth/marigold-depth-v1-1\n  description: marigold depth v1-1 model card this is a model card for the `marigold-depth-v1-1` model for monocular depth estimation from a single image. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-12-12T15:50:26+00:00\n- model_name: apple/DepthPro\n  description: we present a foundation model for zero-shot metric monocular depth estimation. our model, depth pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. the predictions are metric, with absolute scale, without relying on the availability of metadata such as camera intrinsics. and the model is fast, producing a 2.25-megapixel depth map in 0.3 seconds on a standard gpu. these characteristics are enabled by a number of technical contributions, including an efficient multi-scale vision transformer for dense prediction, a training protocol that combines real and synthetic datasets to achieve high metric accuracy alongside fine boundary tracing, dedicated evaluation metrics for boundary accuracy in estimated depth maps, and state-of-the-art focal length estimation from a single image. depth pro was introduced in , by aleksei bochkovskii, ama√´l delaunoy, hugo germain, marcel santos, yichao zhou, stephan r. richter, and vladlen koltun . the checkpoint in this repository is a reference implementation, which has been re-trained. its performance is close to the model reported in the paper but does not match it exactly.\n created at: 2024-10-03T14:45:37+00:00\n- model_name: xingyang1/Distill-Any-Depth-Large-hf\n  description: we present distill-any-depth, a new sota monocular depth estimation model trained with our proposed knowledge distillation algorithms. it was introduced in the paper . this model checkpoint is compatible with the transformers library. . we present distill-any-depth, a new sota monocular depth estimation model trained with our proposed knowledge distillation algorithms. it was introduced in the paper . this model checkpoint is compatible with the transformers library. .\n created at: 2025-03-09T07:44:19+00:00\n- model_name: depth-anything/prompt-depth-anything-vits-hf\n  description: prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. - a scalable data pipeline is introduced to train the method. - prompt depth anything benefits downstream applications, including 3d reconstruction and generalized robotic grasping.\n created at: 2024-12-23T05:53:53+00:00\n- model_name: xingyang1/Distill-Any-Depth-Small-hf\n  description: we present distill-any-depth, a new sota monocular depth estimation model trained with our proposed knowledge distillation algorithms. it was introduced in the paper . this model checkpoint is compatible with the transformers library. . we present distill-any-depth, a new sota monocular depth estimation model trained with our proposed knowledge distillation algorithms. it was introduced in the paper . this model checkpoint is compatible with the transformers library. .\n created at: 2025-03-09T06:16:15+00:00\n- model_name: jingheya/lotus-depth-d-v2-0-disparity\n  description: - this model belongs to the family of official lotus models. - compared to the , this model is trained in disparity space inverse depth, achieving better performance and more stable video depth estimation. developed by:\n created at: 2024-11-13T04:08:57+00:00\n- model_name: jingheya/lotus-depth-g-v2-1-disparity\n  description: - this model belongs to the family of official lotus models. - compared to the , this model was trained with random flip augmentation and achieves better results. developed by:\n created at: 2025-01-16T12:15:46+00:00\n- model_name: depth-anything/prompt-depth-anything-vitl-hf\n  description: prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. - a scalable data pipeline is introduced to train the method. - prompt depth anything benefits downstream applications, including 3d reconstruction and generalized robotic grasping.\n created at: 2024-12-23T13:35:16+00:00\n- model_name: prs-eth/marigold-depth-hr-v1-1\n  description: high-resolution marigold depth v1-1 model card this is a model card for the `marigold-depth-hr-v1-1` model for monocular depth estimation from a single image. the model is fine-tuned from the `marigold-depth-v1-1` as\n created at: 2025-01-15T08:01:15+00:00\n- model_name: facebook/sapiens-depth-1b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T21:04:13+00:00\n- model_name: qualcomm/Depth-Anything-V2\n  description: depth anything is designed for estimating depth at each point in an image. this model is an implementation of depth-anything-v2 found . this repository provides scripts to run depth-anything-v2 on qualcomm¬Æ devices.\n created at: 2024-12-12T21:21:46+00:00\n- model_name: jingheya/lotus-depth-g-v2-0-disparity\n  description: - this model belongs to the family of official lotus models. - compared to the , this model is trained in disparity space inverse depth, achieving better performance and more stable video depth estimation. developed by:\n created at: 2024-11-13T04:18:55+00:00\n- model_name: depth-anything/prompt-depth-anything-vits-transparent-hf\n  description: prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. - a scalable data pipeline is introduced to train the method. - prompt depth anything benefits downstream applications, including 3d reconstruction and generalized robotic grasping.\n created at: 2024-12-23T13:35:01+00:00\n- model_name: facebook/sapiens-depth-0.3b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.3b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T21:07:22+00:00\n- model_name: qualcomm/Depth-Anything\n  description: depth anything is designed for estimating depth at each point in an image. this model is an implementation of depth-anything found . this repository provides scripts to run depth-anything on qualcomm¬Æ devices.\n created at: 2024-12-12T22:54:10+00:00\n- model_name: facebook/sapiens-depth-2b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-2b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T21:00:37+00:00\n- model_name: facebook/sapiens-depth-0.6b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.6b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T21:06:11+00:00\n- model_name: KeighBee/coreml-DepthPro\n  description: depthpro is a monocular depth estimation model. this means that it is trained to predict depth on a single image. variant size mb  :\n created at: 2024-11-27T17:11:57+00:00\n- model_name: lc700x/dpt-hybrid-midas-hf\n  description: dense prediction transformer dpt model trained on 1.4 million images for monocular depth estimation. it was introduced in the paper by ranftl et al. 2021 and first released in . dpt uses the vision transformer vit as backbone and adds a neck + head on top for monocular depth estimation.\n created at: 2025-08-20T01:56:41+00:00\n\nExamples:\n- example user query: Our company wants to build a drone system for terrain mapping. We need to estimate the depth of the terrain.\n  model_name: Intel/dpt-hybrid-midas\n- example user query: I would like to obtain depth estimation for given pictures to use in an augmented reality game.\n  model_name: vinvino02/glpn-kitti\n- example user query: We are designing an application to predict the depth of objects in images. Implement a depth estimation model.\n  model_name: hf-tiny-model-private/tiny-random-GLPNForDepthEstimation\n- example user query: I want to calculate the depth information about the objects in an image from a specific URL and then save the output depth map as an image.\n  model_name: Intel/dpt-large\n- example user query: Our latest project involves computer vision tasks, and we need to build a model for depth estimation. We are particularly interested in a large-scale and accurate DPT architecture.\n  model_name: nielsr/dpt-large-redesign\n- example user query: The company is building an autonomous driving car. We need a solution to estimate depth in order to perceive the surroundings of the car.\n  model_name: hf-tiny-model-private/tiny-random-DPTForDepthEstimation\n\nUser prompt:\nI‚Äôm asking you to get a depth image for extracting pose data.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_depth>\n\nCandidate Models:\n- model_name: hf-tiny-model-private/tiny-random-DPTForDepthEstimation\n  description: A tiny random DPT model for depth estimation using Hugging Face Transformers library.\n created at: 2023-03-29\n- model_name: nielsr/dpt-large-redesign\n  description: A depth estimation model based on the DPT architecture.\n created at: 2022-03-04\n- model_name: vinvino02/glpn-nyu\n  description: Global-Local Path Networks (GLPN) model trained on NYUv2 for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: Intel/dpt-hybrid-midas\n  description: Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021) and first released in this repository. DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation. This repository hosts the hybrid version of the model as stated in the paper. DPT-Hybrid diverges from DPT by using ViT-hybrid as a backbone and taking some activations from the backbone.\n created at: 2022-12-06\n- model_name: vinvino02/glpn-kitti\n  description: Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: hf-tiny-model-private/tiny-random-GLPNForDepthEstimation\n  description: A tiny random GLPN model for depth estimation using the Hugging Face Transformers library.\n created at: 2023-03-29\n- model_name: Intel/dpt-large\n  description: Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021). DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation.\n created at: 2022-03-02\n- model_name: apple/DepthPro-hf\n  description: this is the transformers version of depthpro, a foundation model for zero-shot metric monocular depth estimation, designed to generate high-resolution depth maps with remarkable sharpness and fine-grained details. for the checkpoint compatible with the original codebase, please check . - -\n created at: 2024-11-27T13:52:11+00:00\n- model_name: prs-eth/marigold-depth-v1-1\n  description: marigold depth v1-1 model card this is a model card for the `marigold-depth-v1-1` model for monocular depth estimation from a single image. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-12-12T15:50:26+00:00\n- model_name: apple/DepthPro\n  description: we present a foundation model for zero-shot metric monocular depth estimation. our model, depth pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. the predictions are metric, with absolute scale, without relying on the availability of metadata such as camera intrinsics. and the model is fast, producing a 2.25-megapixel depth map in 0.3 seconds on a standard gpu. these characteristics are enabled by a number of technical contributions, including an efficient multi-scale vision transformer for dense prediction, a training protocol that combines real and synthetic datasets to achieve high metric accuracy alongside fine boundary tracing, dedicated evaluation metrics for boundary accuracy in estimated depth maps, and state-of-the-art focal length estimation from a single image. depth pro was introduced in , by aleksei bochkovskii, ama√´l delaunoy, hugo germain, marcel santos, yichao zhou, stephan r. richter, and vladlen koltun . the checkpoint in this repository is a reference implementation, which has been re-trained. its performance is close to the model reported in the paper but does not match it exactly.\n created at: 2024-10-03T14:45:37+00:00\n- model_name: xingyang1/Distill-Any-Depth-Large-hf\n  description: we present distill-any-depth, a new sota monocular depth estimation model trained with our proposed knowledge distillation algorithms. it was introduced in the paper . this model checkpoint is compatible with the transformers library. . we present distill-any-depth, a new sota monocular depth estimation model trained with our proposed knowledge distillation algorithms. it was introduced in the paper . this model checkpoint is compatible with the transformers library. .\n created at: 2025-03-09T07:44:19+00:00\n- model_name: depth-anything/prompt-depth-anything-vits-hf\n  description: prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. - a scalable data pipeline is introduced to train the method. - prompt depth anything benefits downstream applications, including 3d reconstruction and generalized robotic grasping.\n created at: 2024-12-23T05:53:53+00:00\n- model_name: xingyang1/Distill-Any-Depth-Small-hf\n  description: we present distill-any-depth, a new sota monocular depth estimation model trained with our proposed knowledge distillation algorithms. it was introduced in the paper . this model checkpoint is compatible with the transformers library. . we present distill-any-depth, a new sota monocular depth estimation model trained with our proposed knowledge distillation algorithms. it was introduced in the paper . this model checkpoint is compatible with the transformers library. .\n created at: 2025-03-09T06:16:15+00:00\n- model_name: jingheya/lotus-depth-d-v2-0-disparity\n  description: - this model belongs to the family of official lotus models. - compared to the , this model is trained in disparity space inverse depth, achieving better performance and more stable video depth estimation. developed by:\n created at: 2024-11-13T04:08:57+00:00\n- model_name: jingheya/lotus-depth-g-v2-1-disparity\n  description: - this model belongs to the family of official lotus models. - compared to the , this model was trained with random flip augmentation and achieves better results. developed by:\n created at: 2025-01-16T12:15:46+00:00\n- model_name: depth-anything/prompt-depth-anything-vitl-hf\n  description: prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. - a scalable data pipeline is introduced to train the method. - prompt depth anything benefits downstream applications, including 3d reconstruction and generalized robotic grasping.\n created at: 2024-12-23T13:35:16+00:00\n- model_name: prs-eth/marigold-depth-hr-v1-1\n  description: high-resolution marigold depth v1-1 model card this is a model card for the `marigold-depth-hr-v1-1` model for monocular depth estimation from a single image. the model is fine-tuned from the `marigold-depth-v1-1` as\n created at: 2025-01-15T08:01:15+00:00\n- model_name: facebook/sapiens-depth-1b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T21:04:13+00:00\n- model_name: qualcomm/Depth-Anything-V2\n  description: depth anything is designed for estimating depth at each point in an image. this model is an implementation of depth-anything-v2 found . this repository provides scripts to run depth-anything-v2 on qualcomm¬Æ devices.\n created at: 2024-12-12T21:21:46+00:00\n- model_name: jingheya/lotus-depth-g-v2-0-disparity\n  description: - this model belongs to the family of official lotus models. - compared to the , this model is trained in disparity space inverse depth, achieving better performance and more stable video depth estimation. developed by:\n created at: 2024-11-13T04:18:55+00:00\n- model_name: depth-anything/prompt-depth-anything-vits-transparent-hf\n  description: prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. - a scalable data pipeline is introduced to train the method. - prompt depth anything benefits downstream applications, including 3d reconstruction and generalized robotic grasping.\n created at: 2024-12-23T13:35:01+00:00\n- model_name: facebook/sapiens-depth-0.3b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.3b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T21:07:22+00:00\n- model_name: qualcomm/Depth-Anything\n  description: depth anything is designed for estimating depth at each point in an image. this model is an implementation of depth-anything found . this repository provides scripts to run depth-anything on qualcomm¬Æ devices.\n created at: 2024-12-12T22:54:10+00:00\n- model_name: facebook/sapiens-depth-2b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-2b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T21:00:37+00:00\n- model_name: facebook/sapiens-depth-0.6b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.6b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T21:06:11+00:00\n- model_name: KeighBee/coreml-DepthPro\n  description: depthpro is a monocular depth estimation model. this means that it is trained to predict depth on a single image. variant size mb  :\n created at: 2024-11-27T17:11:57+00:00\n- model_name: lc700x/dpt-hybrid-midas-hf\n  description: dense prediction transformer dpt model trained on 1.4 million images for monocular depth estimation. it was introduced in the paper by ranftl et al. 2021 and first released in . dpt uses the vision transformer vit as backbone and adds a neck + head on top for monocular depth estimation.\n created at: 2025-08-20T01:56:41+00:00\n\nExamples:\n- example user query: Our company wants to build a drone system for terrain mapping. We need to estimate the depth of the terrain.\n  model_name: Intel/dpt-hybrid-midas\n- example user query: I would like to obtain depth estimation for given pictures to use in an augmented reality game.\n  model_name: vinvino02/glpn-kitti\n- example user query: We are designing an application to predict the depth of objects in images. Implement a depth estimation model.\n  model_name: hf-tiny-model-private/tiny-random-GLPNForDepthEstimation\n- example user query: I want to calculate the depth information about the objects in an image from a specific URL and then save the output depth map as an image.\n  model_name: Intel/dpt-large\n- example user query: Our latest project involves computer vision tasks, and we need to build a model for depth estimation. We are particularly interested in a large-scale and accurate DPT architecture.\n  model_name: nielsr/dpt-large-redesign\n- example user query: The company is building an autonomous driving car. We need a solution to estimate depth in order to perceive the surroundings of the car.\n  model_name: hf-tiny-model-private/tiny-random-DPTForDepthEstimation\n\nUser prompt:\nMake sure to generate depth data for a 3D reconstruction process.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_depth>\n\nCandidate Models:\n- model_name: hf-tiny-model-private/tiny-random-DPTForDepthEstimation\n  description: A tiny random DPT model for depth estimation using Hugging Face Transformers library.\n created at: 2023-03-29\n- model_name: nielsr/dpt-large-redesign\n  description: A depth estimation model based on the DPT architecture.\n created at: 2022-03-04\n- model_name: vinvino02/glpn-nyu\n  description: Global-Local Path Networks (GLPN) model trained on NYUv2 for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: Intel/dpt-hybrid-midas\n  description: Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021) and first released in this repository. DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation. This repository hosts the hybrid version of the model as stated in the paper. DPT-Hybrid diverges from DPT by using ViT-hybrid as a backbone and taking some activations from the backbone.\n created at: 2022-12-06\n- model_name: vinvino02/glpn-kitti\n  description: Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: hf-tiny-model-private/tiny-random-GLPNForDepthEstimation\n  description: A tiny random GLPN model for depth estimation using the Hugging Face Transformers library.\n created at: 2023-03-29\n- model_name: Intel/dpt-large\n  description: Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021). DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation.\n created at: 2022-03-02\n- model_name: apple/DepthPro-hf\n  description: this is the transformers version of depthpro, a foundation model for zero-shot metric monocular depth estimation, designed to generate high-resolution depth maps with remarkable sharpness and fine-grained details. for the checkpoint compatible with the original codebase, please check . - -\n created at: 2024-11-27T13:52:11+00:00\n- model_name: prs-eth/marigold-depth-v1-1\n  description: marigold depth v1-1 model card this is a model card for the `marigold-depth-v1-1` model for monocular depth estimation from a single image. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-12-12T15:50:26+00:00\n- model_name: apple/DepthPro\n  description: we present a foundation model for zero-shot metric monocular depth estimation. our model, depth pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. the predictions are metric, with absolute scale, without relying on the availability of metadata such as camera intrinsics. and the model is fast, producing a 2.25-megapixel depth map in 0.3 seconds on a standard gpu. these characteristics are enabled by a number of technical contributions, including an efficient multi-scale vision transformer for dense prediction, a training protocol that combines real and synthetic datasets to achieve high metric accuracy alongside fine boundary tracing, dedicated evaluation metrics for boundary accuracy in estimated depth maps, and state-of-the-art focal length estimation from a single image. depth pro was introduced in , by aleksei bochkovskii, ama√´l delaunoy, hugo germain, marcel santos, yichao zhou, stephan r. richter, and vladlen koltun . the checkpoint in this repository is a reference implementation, which has been re-trained. its performance is close to the model reported in the paper but does not match it exactly.\n created at: 2024-10-03T14:45:37+00:00\n- model_name: xingyang1/Distill-Any-Depth-Large-hf\n  description: we present distill-any-depth, a new sota monocular depth estimation model trained with our proposed knowledge distillation algorithms. it was introduced in the paper . this model checkpoint is compatible with the transformers library. . we present distill-any-depth, a new sota monocular depth estimation model trained with our proposed knowledge distillation algorithms. it was introduced in the paper . this model checkpoint is compatible with the transformers library. .\n created at: 2025-03-09T07:44:19+00:00\n- model_name: depth-anything/prompt-depth-anything-vits-hf\n  description: prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. - a scalable data pipeline is introduced to train the method. - prompt depth anything benefits downstream applications, including 3d reconstruction and generalized robotic grasping.\n created at: 2024-12-23T05:53:53+00:00\n- model_name: xingyang1/Distill-Any-Depth-Small-hf\n  description: we present distill-any-depth, a new sota monocular depth estimation model trained with our proposed knowledge distillation algorithms. it was introduced in the paper . this model checkpoint is compatible with the transformers library. . we present distill-any-depth, a new sota monocular depth estimation model trained with our proposed knowledge distillation algorithms. it was introduced in the paper . this model checkpoint is compatible with the transformers library. .\n created at: 2025-03-09T06:16:15+00:00\n- model_name: jingheya/lotus-depth-d-v2-0-disparity\n  description: - this model belongs to the family of official lotus models. - compared to the , this model is trained in disparity space inverse depth, achieving better performance and more stable video depth estimation. developed by:\n created at: 2024-11-13T04:08:57+00:00\n- model_name: jingheya/lotus-depth-g-v2-1-disparity\n  description: - this model belongs to the family of official lotus models. - compared to the , this model was trained with random flip augmentation and achieves better results. developed by:\n created at: 2025-01-16T12:15:46+00:00\n- model_name: depth-anything/prompt-depth-anything-vitl-hf\n  description: prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. - a scalable data pipeline is introduced to train the method. - prompt depth anything benefits downstream applications, including 3d reconstruction and generalized robotic grasping.\n created at: 2024-12-23T13:35:16+00:00\n- model_name: prs-eth/marigold-depth-hr-v1-1\n  description: high-resolution marigold depth v1-1 model card this is a model card for the `marigold-depth-hr-v1-1` model for monocular depth estimation from a single image. the model is fine-tuned from the `marigold-depth-v1-1` as\n created at: 2025-01-15T08:01:15+00:00\n- model_name: facebook/sapiens-depth-1b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T21:04:13+00:00\n- model_name: qualcomm/Depth-Anything-V2\n  description: depth anything is designed for estimating depth at each point in an image. this model is an implementation of depth-anything-v2 found . this repository provides scripts to run depth-anything-v2 on qualcomm¬Æ devices.\n created at: 2024-12-12T21:21:46+00:00\n- model_name: jingheya/lotus-depth-g-v2-0-disparity\n  description: - this model belongs to the family of official lotus models. - compared to the , this model is trained in disparity space inverse depth, achieving better performance and more stable video depth estimation. developed by:\n created at: 2024-11-13T04:18:55+00:00\n- model_name: depth-anything/prompt-depth-anything-vits-transparent-hf\n  description: prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. - a scalable data pipeline is introduced to train the method. - prompt depth anything benefits downstream applications, including 3d reconstruction and generalized robotic grasping.\n created at: 2024-12-23T13:35:01+00:00\n- model_name: facebook/sapiens-depth-0.3b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.3b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T21:07:22+00:00\n- model_name: qualcomm/Depth-Anything\n  description: depth anything is designed for estimating depth at each point in an image. this model is an implementation of depth-anything found . this repository provides scripts to run depth-anything on qualcomm¬Æ devices.\n created at: 2024-12-12T22:54:10+00:00\n- model_name: facebook/sapiens-depth-2b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-2b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T21:00:37+00:00\n- model_name: facebook/sapiens-depth-0.6b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.6b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T21:06:11+00:00\n- model_name: KeighBee/coreml-DepthPro\n  description: depthpro is a monocular depth estimation model. this means that it is trained to predict depth on a single image. variant size mb  :\n created at: 2024-11-27T17:11:57+00:00\n- model_name: lc700x/dpt-hybrid-midas-hf\n  description: dense prediction transformer dpt model trained on 1.4 million images for monocular depth estimation. it was introduced in the paper by ranftl et al. 2021 and first released in . dpt uses the vision transformer vit as backbone and adds a neck + head on top for monocular depth estimation.\n created at: 2025-08-20T01:56:41+00:00\n\nExamples:\n- example user query: Our company wants to build a drone system for terrain mapping. We need to estimate the depth of the terrain.\n  model_name: Intel/dpt-hybrid-midas\n- example user query: I would like to obtain depth estimation for given pictures to use in an augmented reality game.\n  model_name: vinvino02/glpn-kitti\n- example user query: We are designing an application to predict the depth of objects in images. Implement a depth estimation model.\n  model_name: hf-tiny-model-private/tiny-random-GLPNForDepthEstimation\n- example user query: I want to calculate the depth information about the objects in an image from a specific URL and then save the output depth map as an image.\n  model_name: Intel/dpt-large\n- example user query: Our latest project involves computer vision tasks, and we need to build a model for depth estimation. We are particularly interested in a large-scale and accurate DPT architecture.\n  model_name: nielsr/dpt-large-redesign\n- example user query: The company is building an autonomous driving car. We need a solution to estimate depth in order to perceive the surroundings of the car.\n  model_name: hf-tiny-model-private/tiny-random-DPTForDepthEstimation\n\nUser prompt:\nMake sure to create a depth rendering with the image provided.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_depth>\n\nCandidate Models:\n- model_name: hf-tiny-model-private/tiny-random-DPTForDepthEstimation\n  description: A tiny random DPT model for depth estimation using Hugging Face Transformers library.\n created at: 2023-03-29\n- model_name: nielsr/dpt-large-redesign\n  description: A depth estimation model based on the DPT architecture.\n created at: 2022-03-04\n- model_name: vinvino02/glpn-nyu\n  description: Global-Local Path Networks (GLPN) model trained on NYUv2 for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: Intel/dpt-hybrid-midas\n  description: Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021) and first released in this repository. DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation. This repository hosts the hybrid version of the model as stated in the paper. DPT-Hybrid diverges from DPT by using ViT-hybrid as a backbone and taking some activations from the backbone.\n created at: 2022-12-06\n- model_name: vinvino02/glpn-kitti\n  description: Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: hf-tiny-model-private/tiny-random-GLPNForDepthEstimation\n  description: A tiny random GLPN model for depth estimation using the Hugging Face Transformers library.\n created at: 2023-03-29\n- model_name: Intel/dpt-large\n  description: Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021). DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation.\n created at: 2022-03-02\n- model_name: apple/DepthPro-hf\n  description: this is the transformers version of depthpro, a foundation model for zero-shot metric monocular depth estimation, designed to generate high-resolution depth maps with remarkable sharpness and fine-grained details. for the checkpoint compatible with the original codebase, please check . - -\n created at: 2024-11-27T13:52:11+00:00\n- model_name: prs-eth/marigold-depth-v1-1\n  description: marigold depth v1-1 model card this is a model card for the `marigold-depth-v1-1` model for monocular depth estimation from a single image. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-12-12T15:50:26+00:00\n- model_name: apple/DepthPro\n  description: we present a foundation model for zero-shot metric monocular depth estimation. our model, depth pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. the predictions are metric, with absolute scale, without relying on the availability of metadata such as camera intrinsics. and the model is fast, producing a 2.25-megapixel depth map in 0.3 seconds on a standard gpu. these characteristics are enabled by a number of technical contributions, including an efficient multi-scale vision transformer for dense prediction, a training protocol that combines real and synthetic datasets to achieve high metric accuracy alongside fine boundary tracing, dedicated evaluation metrics for boundary accuracy in estimated depth maps, and state-of-the-art focal length estimation from a single image. depth pro was introduced in , by aleksei bochkovskii, ama√´l delaunoy, hugo germain, marcel santos, yichao zhou, stephan r. richter, and vladlen koltun . the checkpoint in this repository is a reference implementation, which has been re-trained. its performance is close to the model reported in the paper but does not match it exactly.\n created at: 2024-10-03T14:45:37+00:00\n- model_name: xingyang1/Distill-Any-Depth-Large-hf\n  description: we present distill-any-depth, a new sota monocular depth estimation model trained with our proposed knowledge distillation algorithms. it was introduced in the paper . this model checkpoint is compatible with the transformers library. . we present distill-any-depth, a new sota monocular depth estimation model trained with our proposed knowledge distillation algorithms. it was introduced in the paper . this model checkpoint is compatible with the transformers library. .\n created at: 2025-03-09T07:44:19+00:00\n- model_name: depth-anything/prompt-depth-anything-vits-hf\n  description: prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. - a scalable data pipeline is introduced to train the method. - prompt depth anything benefits downstream applications, including 3d reconstruction and generalized robotic grasping.\n created at: 2024-12-23T05:53:53+00:00\n- model_name: xingyang1/Distill-Any-Depth-Small-hf\n  description: we present distill-any-depth, a new sota monocular depth estimation model trained with our proposed knowledge distillation algorithms. it was introduced in the paper . this model checkpoint is compatible with the transformers library. . we present distill-any-depth, a new sota monocular depth estimation model trained with our proposed knowledge distillation algorithms. it was introduced in the paper . this model checkpoint is compatible with the transformers library. .\n created at: 2025-03-09T06:16:15+00:00\n- model_name: jingheya/lotus-depth-d-v2-0-disparity\n  description: - this model belongs to the family of official lotus models. - compared to the , this model is trained in disparity space inverse depth, achieving better performance and more stable video depth estimation. developed by:\n created at: 2024-11-13T04:08:57+00:00\n- model_name: jingheya/lotus-depth-g-v2-1-disparity\n  description: - this model belongs to the family of official lotus models. - compared to the , this model was trained with random flip augmentation and achieves better results. developed by:\n created at: 2025-01-16T12:15:46+00:00\n- model_name: depth-anything/prompt-depth-anything-vitl-hf\n  description: prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. - a scalable data pipeline is introduced to train the method. - prompt depth anything benefits downstream applications, including 3d reconstruction and generalized robotic grasping.\n created at: 2024-12-23T13:35:16+00:00\n- model_name: prs-eth/marigold-depth-hr-v1-1\n  description: high-resolution marigold depth v1-1 model card this is a model card for the `marigold-depth-hr-v1-1` model for monocular depth estimation from a single image. the model is fine-tuned from the `marigold-depth-v1-1` as\n created at: 2025-01-15T08:01:15+00:00\n- model_name: facebook/sapiens-depth-1b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T21:04:13+00:00\n- model_name: qualcomm/Depth-Anything-V2\n  description: depth anything is designed for estimating depth at each point in an image. this model is an implementation of depth-anything-v2 found . this repository provides scripts to run depth-anything-v2 on qualcomm¬Æ devices.\n created at: 2024-12-12T21:21:46+00:00\n- model_name: jingheya/lotus-depth-g-v2-0-disparity\n  description: - this model belongs to the family of official lotus models. - compared to the , this model is trained in disparity space inverse depth, achieving better performance and more stable video depth estimation. developed by:\n created at: 2024-11-13T04:18:55+00:00\n- model_name: depth-anything/prompt-depth-anything-vits-transparent-hf\n  description: prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. - a scalable data pipeline is introduced to train the method. - prompt depth anything benefits downstream applications, including 3d reconstruction and generalized robotic grasping.\n created at: 2024-12-23T13:35:01+00:00\n- model_name: facebook/sapiens-depth-0.3b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.3b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T21:07:22+00:00\n- model_name: qualcomm/Depth-Anything\n  description: depth anything is designed for estimating depth at each point in an image. this model is an implementation of depth-anything found . this repository provides scripts to run depth-anything on qualcomm¬Æ devices.\n created at: 2024-12-12T22:54:10+00:00\n- model_name: facebook/sapiens-depth-2b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-2b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T21:00:37+00:00\n- model_name: facebook/sapiens-depth-0.6b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.6b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T21:06:11+00:00\n- model_name: KeighBee/coreml-DepthPro\n  description: depthpro is a monocular depth estimation model. this means that it is trained to predict depth on a single image. variant size mb  :\n created at: 2024-11-27T17:11:57+00:00\n- model_name: lc700x/dpt-hybrid-midas-hf\n  description: dense prediction transformer dpt model trained on 1.4 million images for monocular depth estimation. it was introduced in the paper by ranftl et al. 2021 and first released in . dpt uses the vision transformer vit as backbone and adds a neck + head on top for monocular depth estimation.\n created at: 2025-08-20T01:56:41+00:00\n\nExamples:\n- example user query: Our company wants to build a drone system for terrain mapping. We need to estimate the depth of the terrain.\n  model_name: Intel/dpt-hybrid-midas\n- example user query: I would like to obtain depth estimation for given pictures to use in an augmented reality game.\n  model_name: vinvino02/glpn-kitti\n- example user query: We are designing an application to predict the depth of objects in images. Implement a depth estimation model.\n  model_name: hf-tiny-model-private/tiny-random-GLPNForDepthEstimation\n- example user query: I want to calculate the depth information about the objects in an image from a specific URL and then save the output depth map as an image.\n  model_name: Intel/dpt-large\n- example user query: Our latest project involves computer vision tasks, and we need to build a model for depth estimation. We are particularly interested in a large-scale and accurate DPT architecture.\n  model_name: nielsr/dpt-large-redesign\n- example user query: The company is building an autonomous driving car. We need a solution to estimate depth in order to perceive the surroundings of the car.\n  model_name: hf-tiny-model-private/tiny-random-DPTForDepthEstimation\n\nUser prompt:\nDo you mind get a depth image to visualize the pose structure.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_depth>\n\nCandidate Models:\n- model_name: hf-tiny-model-private/tiny-random-DPTForDepthEstimation\n  description: A tiny random DPT model for depth estimation using Hugging Face Transformers library.\n created at: 2023-03-29\n- model_name: nielsr/dpt-large-redesign\n  description: A depth estimation model based on the DPT architecture.\n created at: 2022-03-04\n- model_name: vinvino02/glpn-nyu\n  description: Global-Local Path Networks (GLPN) model trained on NYUv2 for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: Intel/dpt-hybrid-midas\n  description: Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021) and first released in this repository. DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation. This repository hosts the hybrid version of the model as stated in the paper. DPT-Hybrid diverges from DPT by using ViT-hybrid as a backbone and taking some activations from the backbone.\n created at: 2022-12-06\n- model_name: vinvino02/glpn-kitti\n  description: Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: hf-tiny-model-private/tiny-random-GLPNForDepthEstimation\n  description: A tiny random GLPN model for depth estimation using the Hugging Face Transformers library.\n created at: 2023-03-29\n- model_name: Intel/dpt-large\n  description: Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021). DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation.\n created at: 2022-03-02\n- model_name: apple/DepthPro-hf\n  description: this is the transformers version of depthpro, a foundation model for zero-shot metric monocular depth estimation, designed to generate high-resolution depth maps with remarkable sharpness and fine-grained details. for the checkpoint compatible with the original codebase, please check . - -\n created at: 2024-11-27T13:52:11+00:00\n- model_name: prs-eth/marigold-depth-v1-1\n  description: marigold depth v1-1 model card this is a model card for the `marigold-depth-v1-1` model for monocular depth estimation from a single image. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-12-12T15:50:26+00:00\n- model_name: apple/DepthPro\n  description: we present a foundation model for zero-shot metric monocular depth estimation. our model, depth pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. the predictions are metric, with absolute scale, without relying on the availability of metadata such as camera intrinsics. and the model is fast, producing a 2.25-megapixel depth map in 0.3 seconds on a standard gpu. these characteristics are enabled by a number of technical contributions, including an efficient multi-scale vision transformer for dense prediction, a training protocol that combines real and synthetic datasets to achieve high metric accuracy alongside fine boundary tracing, dedicated evaluation metrics for boundary accuracy in estimated depth maps, and state-of-the-art focal length estimation from a single image. depth pro was introduced in , by aleksei bochkovskii, ama√´l delaunoy, hugo germain, marcel santos, yichao zhou, stephan r. richter, and vladlen koltun . the checkpoint in this repository is a reference implementation, which has been re-trained. its performance is close to the model reported in the paper but does not match it exactly.\n created at: 2024-10-03T14:45:37+00:00\n- model_name: xingyang1/Distill-Any-Depth-Large-hf\n  description: we present distill-any-depth, a new sota monocular depth estimation model trained with our proposed knowledge distillation algorithms. it was introduced in the paper . this model checkpoint is compatible with the transformers library. . we present distill-any-depth, a new sota monocular depth estimation model trained with our proposed knowledge distillation algorithms. it was introduced in the paper . this model checkpoint is compatible with the transformers library. .\n created at: 2025-03-09T07:44:19+00:00\n- model_name: depth-anything/prompt-depth-anything-vits-hf\n  description: prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. - a scalable data pipeline is introduced to train the method. - prompt depth anything benefits downstream applications, including 3d reconstruction and generalized robotic grasping.\n created at: 2024-12-23T05:53:53+00:00\n- model_name: xingyang1/Distill-Any-Depth-Small-hf\n  description: we present distill-any-depth, a new sota monocular depth estimation model trained with our proposed knowledge distillation algorithms. it was introduced in the paper . this model checkpoint is compatible with the transformers library. . we present distill-any-depth, a new sota monocular depth estimation model trained with our proposed knowledge distillation algorithms. it was introduced in the paper . this model checkpoint is compatible with the transformers library. .\n created at: 2025-03-09T06:16:15+00:00\n- model_name: jingheya/lotus-depth-d-v2-0-disparity\n  description: - this model belongs to the family of official lotus models. - compared to the , this model is trained in disparity space inverse depth, achieving better performance and more stable video depth estimation. developed by:\n created at: 2024-11-13T04:08:57+00:00\n- model_name: jingheya/lotus-depth-g-v2-1-disparity\n  description: - this model belongs to the family of official lotus models. - compared to the , this model was trained with random flip augmentation and achieves better results. developed by:\n created at: 2025-01-16T12:15:46+00:00\n- model_name: depth-anything/prompt-depth-anything-vitl-hf\n  description: prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. - a scalable data pipeline is introduced to train the method. - prompt depth anything benefits downstream applications, including 3d reconstruction and generalized robotic grasping.\n created at: 2024-12-23T13:35:16+00:00\n- model_name: prs-eth/marigold-depth-hr-v1-1\n  description: high-resolution marigold depth v1-1 model card this is a model card for the `marigold-depth-hr-v1-1` model for monocular depth estimation from a single image. the model is fine-tuned from the `marigold-depth-v1-1` as\n created at: 2025-01-15T08:01:15+00:00\n- model_name: facebook/sapiens-depth-1b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T21:04:13+00:00\n- model_name: qualcomm/Depth-Anything-V2\n  description: depth anything is designed for estimating depth at each point in an image. this model is an implementation of depth-anything-v2 found . this repository provides scripts to run depth-anything-v2 on qualcomm¬Æ devices.\n created at: 2024-12-12T21:21:46+00:00\n- model_name: jingheya/lotus-depth-g-v2-0-disparity\n  description: - this model belongs to the family of official lotus models. - compared to the , this model is trained in disparity space inverse depth, achieving better performance and more stable video depth estimation. developed by:\n created at: 2024-11-13T04:18:55+00:00\n- model_name: depth-anything/prompt-depth-anything-vits-transparent-hf\n  description: prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. - a scalable data pipeline is introduced to train the method. - prompt depth anything benefits downstream applications, including 3d reconstruction and generalized robotic grasping.\n created at: 2024-12-23T13:35:01+00:00\n- model_name: facebook/sapiens-depth-0.3b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.3b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T21:07:22+00:00\n- model_name: qualcomm/Depth-Anything\n  description: depth anything is designed for estimating depth at each point in an image. this model is an implementation of depth-anything found . this repository provides scripts to run depth-anything on qualcomm¬Æ devices.\n created at: 2024-12-12T22:54:10+00:00\n- model_name: facebook/sapiens-depth-2b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-2b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T21:00:37+00:00\n- model_name: facebook/sapiens-depth-0.6b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.6b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T21:06:11+00:00\n- model_name: KeighBee/coreml-DepthPro\n  description: depthpro is a monocular depth estimation model. this means that it is trained to predict depth on a single image. variant size mb  :\n created at: 2024-11-27T17:11:57+00:00\n- model_name: lc700x/dpt-hybrid-midas-hf\n  description: dense prediction transformer dpt model trained on 1.4 million images for monocular depth estimation. it was introduced in the paper by ranftl et al. 2021 and first released in . dpt uses the vision transformer vit as backbone and adds a neck + head on top for monocular depth estimation.\n created at: 2025-08-20T01:56:41+00:00\n\nExamples:\n- example user query: Our company wants to build a drone system for terrain mapping. We need to estimate the depth of the terrain.\n  model_name: Intel/dpt-hybrid-midas\n- example user query: I would like to obtain depth estimation for given pictures to use in an augmented reality game.\n  model_name: vinvino02/glpn-kitti\n- example user query: We are designing an application to predict the depth of objects in images. Implement a depth estimation model.\n  model_name: hf-tiny-model-private/tiny-random-GLPNForDepthEstimation\n- example user query: I want to calculate the depth information about the objects in an image from a specific URL and then save the output depth map as an image.\n  model_name: Intel/dpt-large\n- example user query: Our latest project involves computer vision tasks, and we need to build a model for depth estimation. We are particularly interested in a large-scale and accurate DPT architecture.\n  model_name: nielsr/dpt-large-redesign\n- example user query: The company is building an autonomous driving car. We need a solution to estimate depth in order to perceive the surroundings of the car.\n  model_name: hf-tiny-model-private/tiny-random-DPTForDepthEstimation\n\nUser prompt:\nI‚Äôd appreciate it if you could create a depth map to show the object's pose.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_depth>\n\nCandidate Models:\n- model_name: hf-tiny-model-private/tiny-random-DPTForDepthEstimation\n  description: A tiny random DPT model for depth estimation using Hugging Face Transformers library.\n created at: 2023-03-29\n- model_name: nielsr/dpt-large-redesign\n  description: A depth estimation model based on the DPT architecture.\n created at: 2022-03-04\n- model_name: vinvino02/glpn-nyu\n  description: Global-Local Path Networks (GLPN) model trained on NYUv2 for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: Intel/dpt-hybrid-midas\n  description: Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021) and first released in this repository. DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation. This repository hosts the hybrid version of the model as stated in the paper. DPT-Hybrid diverges from DPT by using ViT-hybrid as a backbone and taking some activations from the backbone.\n created at: 2022-12-06\n- model_name: vinvino02/glpn-kitti\n  description: Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: hf-tiny-model-private/tiny-random-GLPNForDepthEstimation\n  description: A tiny random GLPN model for depth estimation using the Hugging Face Transformers library.\n created at: 2023-03-29\n- model_name: Intel/dpt-large\n  description: Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021). DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation.\n created at: 2022-03-02\n- model_name: apple/DepthPro-hf\n  description: this is the transformers version of depthpro, a foundation model for zero-shot metric monocular depth estimation, designed to generate high-resolution depth maps with remarkable sharpness and fine-grained details. for the checkpoint compatible with the original codebase, please check . - -\n created at: 2024-11-27T13:52:11+00:00\n- model_name: prs-eth/marigold-depth-v1-1\n  description: marigold depth v1-1 model card this is a model card for the `marigold-depth-v1-1` model for monocular depth estimation from a single image. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-12-12T15:50:26+00:00\n- model_name: apple/DepthPro\n  description: we present a foundation model for zero-shot metric monocular depth estimation. our model, depth pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. the predictions are metric, with absolute scale, without relying on the availability of metadata such as camera intrinsics. and the model is fast, producing a 2.25-megapixel depth map in 0.3 seconds on a standard gpu. these characteristics are enabled by a number of technical contributions, including an efficient multi-scale vision transformer for dense prediction, a training protocol that combines real and synthetic datasets to achieve high metric accuracy alongside fine boundary tracing, dedicated evaluation metrics for boundary accuracy in estimated depth maps, and state-of-the-art focal length estimation from a single image. depth pro was introduced in , by aleksei bochkovskii, ama√´l delaunoy, hugo germain, marcel santos, yichao zhou, stephan r. richter, and vladlen koltun . the checkpoint in this repository is a reference implementation, which has been re-trained. its performance is close to the model reported in the paper but does not match it exactly.\n created at: 2024-10-03T14:45:37+00:00\n- model_name: xingyang1/Distill-Any-Depth-Large-hf\n  description: we present distill-any-depth, a new sota monocular depth estimation model trained with our proposed knowledge distillation algorithms. it was introduced in the paper . this model checkpoint is compatible with the transformers library. . we present distill-any-depth, a new sota monocular depth estimation model trained with our proposed knowledge distillation algorithms. it was introduced in the paper . this model checkpoint is compatible with the transformers library. .\n created at: 2025-03-09T07:44:19+00:00\n- model_name: depth-anything/prompt-depth-anything-vits-hf\n  description: prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. - a scalable data pipeline is introduced to train the method. - prompt depth anything benefits downstream applications, including 3d reconstruction and generalized robotic grasping.\n created at: 2024-12-23T05:53:53+00:00\n- model_name: xingyang1/Distill-Any-Depth-Small-hf\n  description: we present distill-any-depth, a new sota monocular depth estimation model trained with our proposed knowledge distillation algorithms. it was introduced in the paper . this model checkpoint is compatible with the transformers library. . we present distill-any-depth, a new sota monocular depth estimation model trained with our proposed knowledge distillation algorithms. it was introduced in the paper . this model checkpoint is compatible with the transformers library. .\n created at: 2025-03-09T06:16:15+00:00\n- model_name: jingheya/lotus-depth-d-v2-0-disparity\n  description: - this model belongs to the family of official lotus models. - compared to the , this model is trained in disparity space inverse depth, achieving better performance and more stable video depth estimation. developed by:\n created at: 2024-11-13T04:08:57+00:00\n- model_name: jingheya/lotus-depth-g-v2-1-disparity\n  description: - this model belongs to the family of official lotus models. - compared to the , this model was trained with random flip augmentation and achieves better results. developed by:\n created at: 2025-01-16T12:15:46+00:00\n- model_name: depth-anything/prompt-depth-anything-vitl-hf\n  description: prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. - a scalable data pipeline is introduced to train the method. - prompt depth anything benefits downstream applications, including 3d reconstruction and generalized robotic grasping.\n created at: 2024-12-23T13:35:16+00:00\n- model_name: prs-eth/marigold-depth-hr-v1-1\n  description: high-resolution marigold depth v1-1 model card this is a model card for the `marigold-depth-hr-v1-1` model for monocular depth estimation from a single image. the model is fine-tuned from the `marigold-depth-v1-1` as\n created at: 2025-01-15T08:01:15+00:00\n- model_name: facebook/sapiens-depth-1b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T21:04:13+00:00\n- model_name: qualcomm/Depth-Anything-V2\n  description: depth anything is designed for estimating depth at each point in an image. this model is an implementation of depth-anything-v2 found . this repository provides scripts to run depth-anything-v2 on qualcomm¬Æ devices.\n created at: 2024-12-12T21:21:46+00:00\n- model_name: jingheya/lotus-depth-g-v2-0-disparity\n  description: - this model belongs to the family of official lotus models. - compared to the , this model is trained in disparity space inverse depth, achieving better performance and more stable video depth estimation. developed by:\n created at: 2024-11-13T04:18:55+00:00\n- model_name: depth-anything/prompt-depth-anything-vits-transparent-hf\n  description: prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. - a scalable data pipeline is introduced to train the method. - prompt depth anything benefits downstream applications, including 3d reconstruction and generalized robotic grasping.\n created at: 2024-12-23T13:35:01+00:00\n- model_name: facebook/sapiens-depth-0.3b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.3b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T21:07:22+00:00\n- model_name: qualcomm/Depth-Anything\n  description: depth anything is designed for estimating depth at each point in an image. this model is an implementation of depth-anything found . this repository provides scripts to run depth-anything on qualcomm¬Æ devices.\n created at: 2024-12-12T22:54:10+00:00\n- model_name: facebook/sapiens-depth-2b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-2b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T21:00:37+00:00\n- model_name: facebook/sapiens-depth-0.6b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.6b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T21:06:11+00:00\n- model_name: KeighBee/coreml-DepthPro\n  description: depthpro is a monocular depth estimation model. this means that it is trained to predict depth on a single image. variant size mb  :\n created at: 2024-11-27T17:11:57+00:00\n- model_name: lc700x/dpt-hybrid-midas-hf\n  description: dense prediction transformer dpt model trained on 1.4 million images for monocular depth estimation. it was introduced in the paper by ranftl et al. 2021 and first released in . dpt uses the vision transformer vit as backbone and adds a neck + head on top for monocular depth estimation.\n created at: 2025-08-20T01:56:41+00:00\n\nExamples:\n- example user query: Our company wants to build a drone system for terrain mapping. We need to estimate the depth of the terrain.\n  model_name: Intel/dpt-hybrid-midas\n- example user query: I would like to obtain depth estimation for given pictures to use in an augmented reality game.\n  model_name: vinvino02/glpn-kitti\n- example user query: We are designing an application to predict the depth of objects in images. Implement a depth estimation model.\n  model_name: hf-tiny-model-private/tiny-random-GLPNForDepthEstimation\n- example user query: I want to calculate the depth information about the objects in an image from a specific URL and then save the output depth map as an image.\n  model_name: Intel/dpt-large\n- example user query: Our latest project involves computer vision tasks, and we need to build a model for depth estimation. We are particularly interested in a large-scale and accurate DPT architecture.\n  model_name: nielsr/dpt-large-redesign\n- example user query: The company is building an autonomous driving car. We need a solution to estimate depth in order to perceive the surroundings of the car.\n  model_name: hf-tiny-model-private/tiny-random-DPTForDepthEstimation\n\nUser prompt:\nMake sure to make a depth layer from this image file.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_depth>\n\nCandidate Models:\n- model_name: hf-tiny-model-private/tiny-random-DPTForDepthEstimation\n  description: A tiny random DPT model for depth estimation using Hugging Face Transformers library.\n created at: 2023-03-29\n- model_name: nielsr/dpt-large-redesign\n  description: A depth estimation model based on the DPT architecture.\n created at: 2022-03-04\n- model_name: vinvino02/glpn-nyu\n  description: Global-Local Path Networks (GLPN) model trained on NYUv2 for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: Intel/dpt-hybrid-midas\n  description: Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021) and first released in this repository. DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation. This repository hosts the hybrid version of the model as stated in the paper. DPT-Hybrid diverges from DPT by using ViT-hybrid as a backbone and taking some activations from the backbone.\n created at: 2022-12-06\n- model_name: vinvino02/glpn-kitti\n  description: Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: hf-tiny-model-private/tiny-random-GLPNForDepthEstimation\n  description: A tiny random GLPN model for depth estimation using the Hugging Face Transformers library.\n created at: 2023-03-29\n- model_name: Intel/dpt-large\n  description: Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021). DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation.\n created at: 2022-03-02\n- model_name: apple/DepthPro-hf\n  description: this is the transformers version of depthpro, a foundation model for zero-shot metric monocular depth estimation, designed to generate high-resolution depth maps with remarkable sharpness and fine-grained details. for the checkpoint compatible with the original codebase, please check . - -\n created at: 2024-11-27T13:52:11+00:00\n- model_name: prs-eth/marigold-depth-v1-1\n  description: marigold depth v1-1 model card this is a model card for the `marigold-depth-v1-1` model for monocular depth estimation from a single image. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-12-12T15:50:26+00:00\n- model_name: apple/DepthPro\n  description: we present a foundation model for zero-shot metric monocular depth estimation. our model, depth pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. the predictions are metric, with absolute scale, without relying on the availability of metadata such as camera intrinsics. and the model is fast, producing a 2.25-megapixel depth map in 0.3 seconds on a standard gpu. these characteristics are enabled by a number of technical contributions, including an efficient multi-scale vision transformer for dense prediction, a training protocol that combines real and synthetic datasets to achieve high metric accuracy alongside fine boundary tracing, dedicated evaluation metrics for boundary accuracy in estimated depth maps, and state-of-the-art focal length estimation from a single image. depth pro was introduced in , by aleksei bochkovskii, ama√´l delaunoy, hugo germain, marcel santos, yichao zhou, stephan r. richter, and vladlen koltun . the checkpoint in this repository is a reference implementation, which has been re-trained. its performance is close to the model reported in the paper but does not match it exactly.\n created at: 2024-10-03T14:45:37+00:00\n- model_name: xingyang1/Distill-Any-Depth-Large-hf\n  description: we present distill-any-depth, a new sota monocular depth estimation model trained with our proposed knowledge distillation algorithms. it was introduced in the paper . this model checkpoint is compatible with the transformers library. . we present distill-any-depth, a new sota monocular depth estimation model trained with our proposed knowledge distillation algorithms. it was introduced in the paper . this model checkpoint is compatible with the transformers library. .\n created at: 2025-03-09T07:44:19+00:00\n- model_name: depth-anything/prompt-depth-anything-vits-hf\n  description: prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. - a scalable data pipeline is introduced to train the method. - prompt depth anything benefits downstream applications, including 3d reconstruction and generalized robotic grasping.\n created at: 2024-12-23T05:53:53+00:00\n- model_name: xingyang1/Distill-Any-Depth-Small-hf\n  description: we present distill-any-depth, a new sota monocular depth estimation model trained with our proposed knowledge distillation algorithms. it was introduced in the paper . this model checkpoint is compatible with the transformers library. . we present distill-any-depth, a new sota monocular depth estimation model trained with our proposed knowledge distillation algorithms. it was introduced in the paper . this model checkpoint is compatible with the transformers library. .\n created at: 2025-03-09T06:16:15+00:00\n- model_name: jingheya/lotus-depth-d-v2-0-disparity\n  description: - this model belongs to the family of official lotus models. - compared to the , this model is trained in disparity space inverse depth, achieving better performance and more stable video depth estimation. developed by:\n created at: 2024-11-13T04:08:57+00:00\n- model_name: jingheya/lotus-depth-g-v2-1-disparity\n  description: - this model belongs to the family of official lotus models. - compared to the , this model was trained with random flip augmentation and achieves better results. developed by:\n created at: 2025-01-16T12:15:46+00:00\n- model_name: depth-anything/prompt-depth-anything-vitl-hf\n  description: prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. - a scalable data pipeline is introduced to train the method. - prompt depth anything benefits downstream applications, including 3d reconstruction and generalized robotic grasping.\n created at: 2024-12-23T13:35:16+00:00\n- model_name: prs-eth/marigold-depth-hr-v1-1\n  description: high-resolution marigold depth v1-1 model card this is a model card for the `marigold-depth-hr-v1-1` model for monocular depth estimation from a single image. the model is fine-tuned from the `marigold-depth-v1-1` as\n created at: 2025-01-15T08:01:15+00:00\n- model_name: facebook/sapiens-depth-1b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T21:04:13+00:00\n- model_name: qualcomm/Depth-Anything-V2\n  description: depth anything is designed for estimating depth at each point in an image. this model is an implementation of depth-anything-v2 found . this repository provides scripts to run depth-anything-v2 on qualcomm¬Æ devices.\n created at: 2024-12-12T21:21:46+00:00\n- model_name: jingheya/lotus-depth-g-v2-0-disparity\n  description: - this model belongs to the family of official lotus models. - compared to the , this model is trained in disparity space inverse depth, achieving better performance and more stable video depth estimation. developed by:\n created at: 2024-11-13T04:18:55+00:00\n- model_name: depth-anything/prompt-depth-anything-vits-transparent-hf\n  description: prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. - a scalable data pipeline is introduced to train the method. - prompt depth anything benefits downstream applications, including 3d reconstruction and generalized robotic grasping.\n created at: 2024-12-23T13:35:01+00:00\n- model_name: facebook/sapiens-depth-0.3b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.3b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T21:07:22+00:00\n- model_name: qualcomm/Depth-Anything\n  description: depth anything is designed for estimating depth at each point in an image. this model is an implementation of depth-anything found . this repository provides scripts to run depth-anything on qualcomm¬Æ devices.\n created at: 2024-12-12T22:54:10+00:00\n- model_name: facebook/sapiens-depth-2b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-2b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T21:00:37+00:00\n- model_name: facebook/sapiens-depth-0.6b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.6b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T21:06:11+00:00\n- model_name: KeighBee/coreml-DepthPro\n  description: depthpro is a monocular depth estimation model. this means that it is trained to predict depth on a single image. variant size mb  :\n created at: 2024-11-27T17:11:57+00:00\n- model_name: lc700x/dpt-hybrid-midas-hf\n  description: dense prediction transformer dpt model trained on 1.4 million images for monocular depth estimation. it was introduced in the paper by ranftl et al. 2021 and first released in . dpt uses the vision transformer vit as backbone and adds a neck + head on top for monocular depth estimation.\n created at: 2025-08-20T01:56:41+00:00\n\nExamples:\n- example user query: Our company wants to build a drone system for terrain mapping. We need to estimate the depth of the terrain.\n  model_name: Intel/dpt-hybrid-midas\n- example user query: I would like to obtain depth estimation for given pictures to use in an augmented reality game.\n  model_name: vinvino02/glpn-kitti\n- example user query: We are designing an application to predict the depth of objects in images. Implement a depth estimation model.\n  model_name: hf-tiny-model-private/tiny-random-GLPNForDepthEstimation\n- example user query: I want to calculate the depth information about the objects in an image from a specific URL and then save the output depth map as an image.\n  model_name: Intel/dpt-large\n- example user query: Our latest project involves computer vision tasks, and we need to build a model for depth estimation. We are particularly interested in a large-scale and accurate DPT architecture.\n  model_name: nielsr/dpt-large-redesign\n- example user query: The company is building an autonomous driving car. We need a solution to estimate depth in order to perceive the surroundings of the car.\n  model_name: hf-tiny-model-private/tiny-random-DPTForDepthEstimation\n\nUser prompt:\nMake sure to render a depth map using the given picture.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_depth>\n\nCandidate Models:\n- model_name: hf-tiny-model-private/tiny-random-DPTForDepthEstimation\n  description: A tiny random DPT model for depth estimation using Hugging Face Transformers library.\n created at: 2023-03-29\n- model_name: nielsr/dpt-large-redesign\n  description: A depth estimation model based on the DPT architecture.\n created at: 2022-03-04\n- model_name: vinvino02/glpn-nyu\n  description: Global-Local Path Networks (GLPN) model trained on NYUv2 for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: Intel/dpt-hybrid-midas\n  description: Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021) and first released in this repository. DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation. This repository hosts the hybrid version of the model as stated in the paper. DPT-Hybrid diverges from DPT by using ViT-hybrid as a backbone and taking some activations from the backbone.\n created at: 2022-12-06\n- model_name: vinvino02/glpn-kitti\n  description: Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: hf-tiny-model-private/tiny-random-GLPNForDepthEstimation\n  description: A tiny random GLPN model for depth estimation using the Hugging Face Transformers library.\n created at: 2023-03-29\n- model_name: Intel/dpt-large\n  description: Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021). DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation.\n created at: 2022-03-02\n- model_name: apple/DepthPro-hf\n  description: this is the transformers version of depthpro, a foundation model for zero-shot metric monocular depth estimation, designed to generate high-resolution depth maps with remarkable sharpness and fine-grained details. for the checkpoint compatible with the original codebase, please check . - -\n created at: 2024-11-27T13:52:11+00:00\n- model_name: prs-eth/marigold-depth-v1-1\n  description: marigold depth v1-1 model card this is a model card for the `marigold-depth-v1-1` model for monocular depth estimation from a single image. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-12-12T15:50:26+00:00\n- model_name: apple/DepthPro\n  description: we present a foundation model for zero-shot metric monocular depth estimation. our model, depth pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. the predictions are metric, with absolute scale, without relying on the availability of metadata such as camera intrinsics. and the model is fast, producing a 2.25-megapixel depth map in 0.3 seconds on a standard gpu. these characteristics are enabled by a number of technical contributions, including an efficient multi-scale vision transformer for dense prediction, a training protocol that combines real and synthetic datasets to achieve high metric accuracy alongside fine boundary tracing, dedicated evaluation metrics for boundary accuracy in estimated depth maps, and state-of-the-art focal length estimation from a single image. depth pro was introduced in , by aleksei bochkovskii, ama√´l delaunoy, hugo germain, marcel santos, yichao zhou, stephan r. richter, and vladlen koltun . the checkpoint in this repository is a reference implementation, which has been re-trained. its performance is close to the model reported in the paper but does not match it exactly.\n created at: 2024-10-03T14:45:37+00:00\n- model_name: xingyang1/Distill-Any-Depth-Large-hf\n  description: we present distill-any-depth, a new sota monocular depth estimation model trained with our proposed knowledge distillation algorithms. it was introduced in the paper . this model checkpoint is compatible with the transformers library. . we present distill-any-depth, a new sota monocular depth estimation model trained with our proposed knowledge distillation algorithms. it was introduced in the paper . this model checkpoint is compatible with the transformers library. .\n created at: 2025-03-09T07:44:19+00:00\n- model_name: depth-anything/prompt-depth-anything-vits-hf\n  description: prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. - a scalable data pipeline is introduced to train the method. - prompt depth anything benefits downstream applications, including 3d reconstruction and generalized robotic grasping.\n created at: 2024-12-23T05:53:53+00:00\n- model_name: xingyang1/Distill-Any-Depth-Small-hf\n  description: we present distill-any-depth, a new sota monocular depth estimation model trained with our proposed knowledge distillation algorithms. it was introduced in the paper . this model checkpoint is compatible with the transformers library. . we present distill-any-depth, a new sota monocular depth estimation model trained with our proposed knowledge distillation algorithms. it was introduced in the paper . this model checkpoint is compatible with the transformers library. .\n created at: 2025-03-09T06:16:15+00:00\n- model_name: jingheya/lotus-depth-d-v2-0-disparity\n  description: - this model belongs to the family of official lotus models. - compared to the , this model is trained in disparity space inverse depth, achieving better performance and more stable video depth estimation. developed by:\n created at: 2024-11-13T04:08:57+00:00\n- model_name: jingheya/lotus-depth-g-v2-1-disparity\n  description: - this model belongs to the family of official lotus models. - compared to the , this model was trained with random flip augmentation and achieves better results. developed by:\n created at: 2025-01-16T12:15:46+00:00\n- model_name: depth-anything/prompt-depth-anything-vitl-hf\n  description: prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. - a scalable data pipeline is introduced to train the method. - prompt depth anything benefits downstream applications, including 3d reconstruction and generalized robotic grasping.\n created at: 2024-12-23T13:35:16+00:00\n- model_name: prs-eth/marigold-depth-hr-v1-1\n  description: high-resolution marigold depth v1-1 model card this is a model card for the `marigold-depth-hr-v1-1` model for monocular depth estimation from a single image. the model is fine-tuned from the `marigold-depth-v1-1` as\n created at: 2025-01-15T08:01:15+00:00\n- model_name: facebook/sapiens-depth-1b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T21:04:13+00:00\n- model_name: qualcomm/Depth-Anything-V2\n  description: depth anything is designed for estimating depth at each point in an image. this model is an implementation of depth-anything-v2 found . this repository provides scripts to run depth-anything-v2 on qualcomm¬Æ devices.\n created at: 2024-12-12T21:21:46+00:00\n- model_name: jingheya/lotus-depth-g-v2-0-disparity\n  description: - this model belongs to the family of official lotus models. - compared to the , this model is trained in disparity space inverse depth, achieving better performance and more stable video depth estimation. developed by:\n created at: 2024-11-13T04:18:55+00:00\n- model_name: depth-anything/prompt-depth-anything-vits-transparent-hf\n  description: prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. - a scalable data pipeline is introduced to train the method. - prompt depth anything benefits downstream applications, including 3d reconstruction and generalized robotic grasping.\n created at: 2024-12-23T13:35:01+00:00\n- model_name: facebook/sapiens-depth-0.3b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.3b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T21:07:22+00:00\n- model_name: qualcomm/Depth-Anything\n  description: depth anything is designed for estimating depth at each point in an image. this model is an implementation of depth-anything found . this repository provides scripts to run depth-anything on qualcomm¬Æ devices.\n created at: 2024-12-12T22:54:10+00:00\n- model_name: facebook/sapiens-depth-2b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-2b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T21:00:37+00:00\n- model_name: facebook/sapiens-depth-0.6b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.6b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T21:06:11+00:00\n- model_name: KeighBee/coreml-DepthPro\n  description: depthpro is a monocular depth estimation model. this means that it is trained to predict depth on a single image. variant size mb  :\n created at: 2024-11-27T17:11:57+00:00\n- model_name: lc700x/dpt-hybrid-midas-hf\n  description: dense prediction transformer dpt model trained on 1.4 million images for monocular depth estimation. it was introduced in the paper by ranftl et al. 2021 and first released in . dpt uses the vision transformer vit as backbone and adds a neck + head on top for monocular depth estimation.\n created at: 2025-08-20T01:56:41+00:00\n\nExamples:\n- example user query: Our company wants to build a drone system for terrain mapping. We need to estimate the depth of the terrain.\n  model_name: Intel/dpt-hybrid-midas\n- example user query: I would like to obtain depth estimation for given pictures to use in an augmented reality game.\n  model_name: vinvino02/glpn-kitti\n- example user query: We are designing an application to predict the depth of objects in images. Implement a depth estimation model.\n  model_name: hf-tiny-model-private/tiny-random-GLPNForDepthEstimation\n- example user query: I want to calculate the depth information about the objects in an image from a specific URL and then save the output depth map as an image.\n  model_name: Intel/dpt-large\n- example user query: Our latest project involves computer vision tasks, and we need to build a model for depth estimation. We are particularly interested in a large-scale and accurate DPT architecture.\n  model_name: nielsr/dpt-large-redesign\n- example user query: The company is building an autonomous driving car. We need a solution to estimate depth in order to perceive the surroundings of the car.\n  model_name: hf-tiny-model-private/tiny-random-DPTForDepthEstimation\n\nUser prompt:\nI would appreciate outputting a depth version to highlight the essential shapes.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_depth>\n\nCandidate Models:\n- model_name: hf-tiny-model-private/tiny-random-DPTForDepthEstimation\n  description: A tiny random DPT model for depth estimation using Hugging Face Transformers library.\n created at: 2023-03-29\n- model_name: nielsr/dpt-large-redesign\n  description: A depth estimation model based on the DPT architecture.\n created at: 2022-03-04\n- model_name: vinvino02/glpn-nyu\n  description: Global-Local Path Networks (GLPN) model trained on NYUv2 for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: Intel/dpt-hybrid-midas\n  description: Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021) and first released in this repository. DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation. This repository hosts the hybrid version of the model as stated in the paper. DPT-Hybrid diverges from DPT by using ViT-hybrid as a backbone and taking some activations from the backbone.\n created at: 2022-12-06\n- model_name: vinvino02/glpn-kitti\n  description: Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: hf-tiny-model-private/tiny-random-GLPNForDepthEstimation\n  description: A tiny random GLPN model for depth estimation using the Hugging Face Transformers library.\n created at: 2023-03-29\n- model_name: Intel/dpt-large\n  description: Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021). DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation.\n created at: 2022-03-02\n- model_name: apple/DepthPro-hf\n  description: this is the transformers version of depthpro, a foundation model for zero-shot metric monocular depth estimation, designed to generate high-resolution depth maps with remarkable sharpness and fine-grained details. for the checkpoint compatible with the original codebase, please check . - -\n created at: 2024-11-27T13:52:11+00:00\n- model_name: prs-eth/marigold-depth-v1-1\n  description: marigold depth v1-1 model card this is a model card for the `marigold-depth-v1-1` model for monocular depth estimation from a single image. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-12-12T15:50:26+00:00\n- model_name: apple/DepthPro\n  description: we present a foundation model for zero-shot metric monocular depth estimation. our model, depth pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. the predictions are metric, with absolute scale, without relying on the availability of metadata such as camera intrinsics. and the model is fast, producing a 2.25-megapixel depth map in 0.3 seconds on a standard gpu. these characteristics are enabled by a number of technical contributions, including an efficient multi-scale vision transformer for dense prediction, a training protocol that combines real and synthetic datasets to achieve high metric accuracy alongside fine boundary tracing, dedicated evaluation metrics for boundary accuracy in estimated depth maps, and state-of-the-art focal length estimation from a single image. depth pro was introduced in , by aleksei bochkovskii, ama√´l delaunoy, hugo germain, marcel santos, yichao zhou, stephan r. richter, and vladlen koltun . the checkpoint in this repository is a reference implementation, which has been re-trained. its performance is close to the model reported in the paper but does not match it exactly.\n created at: 2024-10-03T14:45:37+00:00\n- model_name: xingyang1/Distill-Any-Depth-Large-hf\n  description: we present distill-any-depth, a new sota monocular depth estimation model trained with our proposed knowledge distillation algorithms. it was introduced in the paper . this model checkpoint is compatible with the transformers library. . we present distill-any-depth, a new sota monocular depth estimation model trained with our proposed knowledge distillation algorithms. it was introduced in the paper . this model checkpoint is compatible with the transformers library. .\n created at: 2025-03-09T07:44:19+00:00\n- model_name: depth-anything/prompt-depth-anything-vits-hf\n  description: prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. - a scalable data pipeline is introduced to train the method. - prompt depth anything benefits downstream applications, including 3d reconstruction and generalized robotic grasping.\n created at: 2024-12-23T05:53:53+00:00\n- model_name: xingyang1/Distill-Any-Depth-Small-hf\n  description: we present distill-any-depth, a new sota monocular depth estimation model trained with our proposed knowledge distillation algorithms. it was introduced in the paper . this model checkpoint is compatible with the transformers library. . we present distill-any-depth, a new sota monocular depth estimation model trained with our proposed knowledge distillation algorithms. it was introduced in the paper . this model checkpoint is compatible with the transformers library. .\n created at: 2025-03-09T06:16:15+00:00\n- model_name: jingheya/lotus-depth-d-v2-0-disparity\n  description: - this model belongs to the family of official lotus models. - compared to the , this model is trained in disparity space inverse depth, achieving better performance and more stable video depth estimation. developed by:\n created at: 2024-11-13T04:08:57+00:00\n- model_name: jingheya/lotus-depth-g-v2-1-disparity\n  description: - this model belongs to the family of official lotus models. - compared to the , this model was trained with random flip augmentation and achieves better results. developed by:\n created at: 2025-01-16T12:15:46+00:00\n- model_name: depth-anything/prompt-depth-anything-vitl-hf\n  description: prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. - a scalable data pipeline is introduced to train the method. - prompt depth anything benefits downstream applications, including 3d reconstruction and generalized robotic grasping.\n created at: 2024-12-23T13:35:16+00:00\n- model_name: prs-eth/marigold-depth-hr-v1-1\n  description: high-resolution marigold depth v1-1 model card this is a model card for the `marigold-depth-hr-v1-1` model for monocular depth estimation from a single image. the model is fine-tuned from the `marigold-depth-v1-1` as\n created at: 2025-01-15T08:01:15+00:00\n- model_name: facebook/sapiens-depth-1b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T21:04:13+00:00\n- model_name: qualcomm/Depth-Anything-V2\n  description: depth anything is designed for estimating depth at each point in an image. this model is an implementation of depth-anything-v2 found . this repository provides scripts to run depth-anything-v2 on qualcomm¬Æ devices.\n created at: 2024-12-12T21:21:46+00:00\n- model_name: jingheya/lotus-depth-g-v2-0-disparity\n  description: - this model belongs to the family of official lotus models. - compared to the , this model is trained in disparity space inverse depth, achieving better performance and more stable video depth estimation. developed by:\n created at: 2024-11-13T04:18:55+00:00\n- model_name: depth-anything/prompt-depth-anything-vits-transparent-hf\n  description: prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. - a scalable data pipeline is introduced to train the method. - prompt depth anything benefits downstream applications, including 3d reconstruction and generalized robotic grasping.\n created at: 2024-12-23T13:35:01+00:00\n- model_name: facebook/sapiens-depth-0.3b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.3b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T21:07:22+00:00\n- model_name: qualcomm/Depth-Anything\n  description: depth anything is designed for estimating depth at each point in an image. this model is an implementation of depth-anything found . this repository provides scripts to run depth-anything on qualcomm¬Æ devices.\n created at: 2024-12-12T22:54:10+00:00\n- model_name: facebook/sapiens-depth-2b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-2b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T21:00:37+00:00\n- model_name: facebook/sapiens-depth-0.6b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.6b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T21:06:11+00:00\n- model_name: KeighBee/coreml-DepthPro\n  description: depthpro is a monocular depth estimation model. this means that it is trained to predict depth on a single image. variant size mb  :\n created at: 2024-11-27T17:11:57+00:00\n- model_name: lc700x/dpt-hybrid-midas-hf\n  description: dense prediction transformer dpt model trained on 1.4 million images for monocular depth estimation. it was introduced in the paper by ranftl et al. 2021 and first released in . dpt uses the vision transformer vit as backbone and adds a neck + head on top for monocular depth estimation.\n created at: 2025-08-20T01:56:41+00:00\n\nExamples:\n- example user query: Our company wants to build a drone system for terrain mapping. We need to estimate the depth of the terrain.\n  model_name: Intel/dpt-hybrid-midas\n- example user query: I would like to obtain depth estimation for given pictures to use in an augmented reality game.\n  model_name: vinvino02/glpn-kitti\n- example user query: We are designing an application to predict the depth of objects in images. Implement a depth estimation model.\n  model_name: hf-tiny-model-private/tiny-random-GLPNForDepthEstimation\n- example user query: I want to calculate the depth information about the objects in an image from a specific URL and then save the output depth map as an image.\n  model_name: Intel/dpt-large\n- example user query: Our latest project involves computer vision tasks, and we need to build a model for depth estimation. We are particularly interested in a large-scale and accurate DPT architecture.\n  model_name: nielsr/dpt-large-redesign\n- example user query: The company is building an autonomous driving car. We need a solution to estimate depth in order to perceive the surroundings of the car.\n  model_name: hf-tiny-model-private/tiny-random-DPTForDepthEstimation\n\nUser prompt:\nWould you kindly generate a depth map to map the image's surfaces.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_depth>\n\nCandidate Models:\n- model_name: hf-tiny-model-private/tiny-random-DPTForDepthEstimation\n  description: A tiny random DPT model for depth estimation using Hugging Face Transformers library.\n created at: 2023-03-29\n- model_name: nielsr/dpt-large-redesign\n  description: A depth estimation model based on the DPT architecture.\n created at: 2022-03-04\n- model_name: vinvino02/glpn-nyu\n  description: Global-Local Path Networks (GLPN) model trained on NYUv2 for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: Intel/dpt-hybrid-midas\n  description: Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021) and first released in this repository. DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation. This repository hosts the hybrid version of the model as stated in the paper. DPT-Hybrid diverges from DPT by using ViT-hybrid as a backbone and taking some activations from the backbone.\n created at: 2022-12-06\n- model_name: vinvino02/glpn-kitti\n  description: Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: hf-tiny-model-private/tiny-random-GLPNForDepthEstimation\n  description: A tiny random GLPN model for depth estimation using the Hugging Face Transformers library.\n created at: 2023-03-29\n- model_name: Intel/dpt-large\n  description: Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021). DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation.\n created at: 2022-03-02\n- model_name: apple/DepthPro-hf\n  description: this is the transformers version of depthpro, a foundation model for zero-shot metric monocular depth estimation, designed to generate high-resolution depth maps with remarkable sharpness and fine-grained details. for the checkpoint compatible with the original codebase, please check . - -\n created at: 2024-11-27T13:52:11+00:00\n- model_name: prs-eth/marigold-depth-v1-1\n  description: marigold depth v1-1 model card this is a model card for the `marigold-depth-v1-1` model for monocular depth estimation from a single image. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-12-12T15:50:26+00:00\n- model_name: apple/DepthPro\n  description: we present a foundation model for zero-shot metric monocular depth estimation. our model, depth pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. the predictions are metric, with absolute scale, without relying on the availability of metadata such as camera intrinsics. and the model is fast, producing a 2.25-megapixel depth map in 0.3 seconds on a standard gpu. these characteristics are enabled by a number of technical contributions, including an efficient multi-scale vision transformer for dense prediction, a training protocol that combines real and synthetic datasets to achieve high metric accuracy alongside fine boundary tracing, dedicated evaluation metrics for boundary accuracy in estimated depth maps, and state-of-the-art focal length estimation from a single image. depth pro was introduced in , by aleksei bochkovskii, ama√´l delaunoy, hugo germain, marcel santos, yichao zhou, stephan r. richter, and vladlen koltun . the checkpoint in this repository is a reference implementation, which has been re-trained. its performance is close to the model reported in the paper but does not match it exactly.\n created at: 2024-10-03T14:45:37+00:00\n- model_name: xingyang1/Distill-Any-Depth-Large-hf\n  description: we present distill-any-depth, a new sota monocular depth estimation model trained with our proposed knowledge distillation algorithms. it was introduced in the paper . this model checkpoint is compatible with the transformers library. . we present distill-any-depth, a new sota monocular depth estimation model trained with our proposed knowledge distillation algorithms. it was introduced in the paper . this model checkpoint is compatible with the transformers library. .\n created at: 2025-03-09T07:44:19+00:00\n- model_name: depth-anything/prompt-depth-anything-vits-hf\n  description: prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. - a scalable data pipeline is introduced to train the method. - prompt depth anything benefits downstream applications, including 3d reconstruction and generalized robotic grasping.\n created at: 2024-12-23T05:53:53+00:00\n- model_name: xingyang1/Distill-Any-Depth-Small-hf\n  description: we present distill-any-depth, a new sota monocular depth estimation model trained with our proposed knowledge distillation algorithms. it was introduced in the paper . this model checkpoint is compatible with the transformers library. . we present distill-any-depth, a new sota monocular depth estimation model trained with our proposed knowledge distillation algorithms. it was introduced in the paper . this model checkpoint is compatible with the transformers library. .\n created at: 2025-03-09T06:16:15+00:00\n- model_name: jingheya/lotus-depth-d-v2-0-disparity\n  description: - this model belongs to the family of official lotus models. - compared to the , this model is trained in disparity space inverse depth, achieving better performance and more stable video depth estimation. developed by:\n created at: 2024-11-13T04:08:57+00:00\n- model_name: jingheya/lotus-depth-g-v2-1-disparity\n  description: - this model belongs to the family of official lotus models. - compared to the , this model was trained with random flip augmentation and achieves better results. developed by:\n created at: 2025-01-16T12:15:46+00:00\n- model_name: depth-anything/prompt-depth-anything-vitl-hf\n  description: prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. - a scalable data pipeline is introduced to train the method. - prompt depth anything benefits downstream applications, including 3d reconstruction and generalized robotic grasping.\n created at: 2024-12-23T13:35:16+00:00\n- model_name: prs-eth/marigold-depth-hr-v1-1\n  description: high-resolution marigold depth v1-1 model card this is a model card for the `marigold-depth-hr-v1-1` model for monocular depth estimation from a single image. the model is fine-tuned from the `marigold-depth-v1-1` as\n created at: 2025-01-15T08:01:15+00:00\n- model_name: facebook/sapiens-depth-1b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T21:04:13+00:00\n- model_name: qualcomm/Depth-Anything-V2\n  description: depth anything is designed for estimating depth at each point in an image. this model is an implementation of depth-anything-v2 found . this repository provides scripts to run depth-anything-v2 on qualcomm¬Æ devices.\n created at: 2024-12-12T21:21:46+00:00\n- model_name: jingheya/lotus-depth-g-v2-0-disparity\n  description: - this model belongs to the family of official lotus models. - compared to the , this model is trained in disparity space inverse depth, achieving better performance and more stable video depth estimation. developed by:\n created at: 2024-11-13T04:18:55+00:00\n- model_name: depth-anything/prompt-depth-anything-vits-transparent-hf\n  description: prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. prompt depth anything is a high-resolution and accurate metric depth estimation method, with the following highlights: - using prompting to unleash the power of depth foundation models, inspired by success of prompting in vlm and llm foundation models. - the widely available iphone lidar is taken as the prompt, guiding the model to produce up to 4k resolution accurate metric depth. - a scalable data pipeline is introduced to train the method. - prompt depth anything benefits downstream applications, including 3d reconstruction and generalized robotic grasping.\n created at: 2024-12-23T13:35:01+00:00\n- model_name: facebook/sapiens-depth-0.3b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.3b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T21:07:22+00:00\n- model_name: qualcomm/Depth-Anything\n  description: depth anything is designed for estimating depth at each point in an image. this model is an implementation of depth-anything found . this repository provides scripts to run depth-anything on qualcomm¬Æ devices.\n created at: 2024-12-12T22:54:10+00:00\n- model_name: facebook/sapiens-depth-2b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-2b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T21:00:37+00:00\n- model_name: facebook/sapiens-depth-0.6b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.6b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T21:06:11+00:00\n- model_name: KeighBee/coreml-DepthPro\n  description: depthpro is a monocular depth estimation model. this means that it is trained to predict depth on a single image. variant size mb  :\n created at: 2024-11-27T17:11:57+00:00\n- model_name: lc700x/dpt-hybrid-midas-hf\n  description: dense prediction transformer dpt model trained on 1.4 million images for monocular depth estimation. it was introduced in the paper by ranftl et al. 2021 and first released in . dpt uses the vision transformer vit as backbone and adds a neck + head on top for monocular depth estimation.\n created at: 2025-08-20T01:56:41+00:00\n\nExamples:\n- example user query: Our company wants to build a drone system for terrain mapping. We need to estimate the depth of the terrain.\n  model_name: Intel/dpt-hybrid-midas\n- example user query: I would like to obtain depth estimation for given pictures to use in an augmented reality game.\n  model_name: vinvino02/glpn-kitti\n- example user query: We are designing an application to predict the depth of objects in images. Implement a depth estimation model.\n  model_name: hf-tiny-model-private/tiny-random-GLPNForDepthEstimation\n- example user query: I want to calculate the depth information about the objects in an image from a specific URL and then save the output depth map as an image.\n  model_name: Intel/dpt-large\n- example user query: Our latest project involves computer vision tasks, and we need to build a model for depth estimation. We are particularly interested in a large-scale and accurate DPT architecture.\n  model_name: nielsr/dpt-large-redesign\n- example user query: The company is building an autonomous driving car. We need a solution to estimate depth in order to perceive the surroundings of the car.\n  model_name: hf-tiny-model-private/tiny-random-DPTForDepthEstimation\n\nUser prompt:\nDo you mind producing a depth image for this visual.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n"
  ],
  "<image_derain>": [
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_derain>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n\nExamples:\n- example user query: A CEO is taking a walk in a rainy weather. He is holding an umbrella. We need a clear image of his face. We have low-quality image. What can we do to achive this?\n  model_name: google/maxim-s3-deblurring-gopro\n\nUser prompt:\nThe snapshot has too much shower. Can you improve it?\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_derain>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n\nExamples:\n- example user query: A CEO is taking a walk in a rainy weather. He is holding an umbrella. We need a clear image of his face. We have low-quality image. What can we do to achive this?\n  model_name: google/maxim-s3-deblurring-gopro\n\nUser prompt:\nErase the showers off the view to make it clearer.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_derain>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n\nExamples:\n- example user query: A CEO is taking a walk in a rainy weather. He is holding an umbrella. We need a clear image of his face. We have low-quality image. What can we do to achive this?\n  model_name: google/maxim-s3-deblurring-gopro\n\nUser prompt:\nSprinkle is affecting the photo. Could you improve it?\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_derain>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n\nExamples:\n- example user query: A CEO is taking a walk in a rainy weather. He is holding an umbrella. We need a clear image of his face. We have low-quality image. What can we do to achive this?\n  model_name: google/maxim-s3-deblurring-gopro\n\nUser prompt:\nDownpour is making the frame look unclear‚Äîcan you reduce it?\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_derain>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n\nExamples:\n- example user query: A CEO is taking a walk in a rainy weather. He is holding an umbrella. We need a clear image of his face. We have low-quality image. What can we do to achive this?\n  model_name: google/maxim-s3-deblurring-gopro\n\nUser prompt:\nI need to filter the shower from this photo.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_derain>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n\nExamples:\n- example user query: A CEO is taking a walk in a rainy weather. He is holding an umbrella. We need a clear image of his face. We have low-quality image. What can we do to achive this?\n  model_name: google/maxim-s3-deblurring-gopro\n\nUser prompt:\nThe view has too much rainy. Can you clean it?\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_derain>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n\nExamples:\n- example user query: A CEO is taking a walk in a rainy weather. He is holding an umbrella. We need a clear image of his face. We have low-quality image. What can we do to achive this?\n  model_name: google/maxim-s3-deblurring-gopro\n\nUser prompt:\nThe snapshot has too much raindrops. Can you fix it?\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_derain>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n\nExamples:\n- example user query: A CEO is taking a walk in a rainy weather. He is holding an umbrella. We need a clear image of his face. We have low-quality image. What can we do to achive this?\n  model_name: google/maxim-s3-deblurring-gopro\n\nUser prompt:\nIs there a way to diminish the wetness from this visual?\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_derain>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n\nExamples:\n- example user query: A CEO is taking a walk in a rainy weather. He is holding an umbrella. We need a clear image of his face. We have low-quality image. What can we do to achive this?\n  model_name: google/maxim-s3-deblurring-gopro\n\nUser prompt:\nCan you clear the drizzle from this scene?\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_derain>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n\nExamples:\n- example user query: A CEO is taking a walk in a rainy weather. He is holding an umbrella. We need a clear image of his face. We have low-quality image. What can we do to achive this?\n  model_name: google/maxim-s3-deblurring-gopro\n\nUser prompt:\nThe bicycle is obscured by downpour, please make it clearer.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n"
  ],
  "<image_det>": [
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_det>\n\nCandidate Models:\n- model_name: microsoft/table-transformer-structure-recognition\n  description: Table Transformer (DETR) model trained on PubTables1M for detecting the structure (like rows, columns) in tables.\n created at: 2022-10-14\n- model_name: microsoft/table-transformer-detection\n  description: Table Transformer (DETR) model trained on PubTables1M for detecting tables in documents. Introduced in the paper PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents by Smock et al.\n created at: 2022-10-14\n- model_name: facebook/detr-resnet-50\n  description: DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: hustvl/yolos-tiny\n  description: YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN). The model is trained using a bipartite matching loss: one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a no object as class and no bounding box as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.\n created at: 2022-04-26\n- model_name: TahaDouaji/detr-doc-table-detection\n  description: detr-doc-table-detection is a model trained to detect both Bordered and Borderless tables in documents, based on facebook/detr-resnet-50.\n created at: 2022-03-11\n- model_name: facebook/detr-resnet-101\n  description: DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: keremberke/yolov8m-table-extraction\n  description: A YOLOv8 model for table extraction in images, capable of detecting both bordered and borderless tables. Trained using the keremberke/table-extraction dataset.\n created at: 2023-01-29\n- model_name: SenseTime/deformable-detr\n  description: Deformable DETR model with ResNet-50 backbone trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper Deformable DETR: Deformable Transformers for End-to-End Object Detection by Zhu et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: facebook/detr-resnet-101-dc5\n  description: DETR (End-to-End Object Detection) model with ResNet-101 backbone (dilated C5 stage). The model is trained on COCO 2017 object detection dataset and achieves an average precision (AP) of 44.9 on the COCO 2017 validation set.\n created at: 2022-03-02\n- model_name: hustvl/yolos-small\n  description: YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Fang et al. and first released in this repository. YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN).\n created at: 2022-04-26\n- model_name: keremberke/yolov8m-hard-hat-detection\n  description: A YOLOv8 model for detecting hard hats in images. The model can distinguish between 'Hardhat' and 'NO-Hardhat' classes. It can be used to ensure safety compliance in construction sites or other industrial environments where hard hats are required.\n created at: 2023-01-29\n- model_name: keremberke/yolov5m-license-plate\n  description: A YOLOv5 model for license plate detection trained on a custom dataset. The model can detect license plates in images with high accuracy.\n created at: 2023-01-01\n- model_name: keremberke/yolov8s-table-extraction\n  description: A YOLOv8 model for table extraction in documents, capable of detecting bordered and borderless tables. Trained on the table-extraction dataset, the model achieves a mAP@0.5 of 0.984 on the validation set.\n created at: 2023-01-29\n- model_name: keremberke/yolov8m-csgo-player-detection\n  description: An object detection model trained to detect Counter-Strike: Global Offensive (CS:GO) players. The model is based on the YOLOv8 architecture and can identify 'ct', 'cthead', 't', and 'thead' labels.\n created at: 2023-01-29\n- model_name: keremberke/yolov8m-nlf-head-detection\n  description: A YOLOv8 model trained for head detection in American football. The model is capable of detecting helmets, blurred helmets, difficult helmets, partial helmets, and sideline helmets.\n created at: 2023-01-29\n- model_name: keremberke/yolov8m-forklift-detection\n  description: A YOLOv8 model for detecting forklifts and persons in images.\n created at: 2023-01-22\n- model_name: google/owlvit-large-patch14\n  description: OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. It uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. OWL-ViT is trained on publicly available image-caption data and fine-tuned on publicly available object detection datasets such as COCO and OpenImages.\n created at: 2022-07-05\n- model_name: keremberke/yolov8m-plane-detection\n  description: A YOLOv8 model for plane detection trained on the keremberke/plane-detection dataset. The model is capable of detecting planes in images with high accuracy.\n created at: 2023-01-29\n- model_name: keremberke/yolov8s-csgo-player-detection\n  description: A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players. Supports the labels ['ct', 'cthead', 't', 'thead'].\n created at: 2023-01-29\n- model_name: fcakyon/yolov5s-v7.0\n  description: Yolov5s-v7.0 is an object detection model trained on the COCO dataset. It can detect objects in images and return their bounding boxes, scores, and categories.\n created at: 2022-12-13\n- model_name: keremberke/yolov8s-hard-hat-detection\n  description: An object detection model trained to detect hard hats and no-hard hats in images. The model is based on YOLOv8 architecture and can be used for safety applications.\n created at: 2023-01-29\n- model_name: keremberke/yolov8m-blood-cell-detection\n  description: A YOLOv8 model for blood cell detection, including Platelets, RBC, and WBC. Trained on the blood-cell-object-detection dataset.\n created at: 2023-01-29\n- model_name: keremberke/yolov8n-table-extraction\n  description: An object detection model for extracting tables from documents. Supports two label types: 'bordered' and 'borderless'.\n created at: 2023-01-29\n- model_name: keremberke/yolov8n-csgo-player-detection\n  description: A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players with supported labels: ['ct', 'cthead', 't', 'thead'].\n created at: 2023-01-29\n- model_name: keremberke/yolov5s-license-plate\n  description: A YOLOv5 based license plate detection model trained on a custom dataset.\n created at: 2023-01-01\n- model_name: keremberke/yolov8n-blood-cell-detection\n  description: This model detects blood cells in images, specifically Platelets, RBC, and WBC. It is based on the YOLOv8 architecture and trained on the blood-cell-object-detection dataset.\n created at: 2023-01-29\n- model_name: google/owlvit-base-patch32\n  description: OWL-ViT is a zero-shot text-conditioned object detection model that uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. The model can be used to query an image with one or multiple text queries.\n created at: 2022-07-05\n- model_name: keremberke/yolov8m-valorant-detection\n  description: A YOLOv8 model for object detection in Valorant game, trained on a custom dataset. It detects dropped spike, enemy, planted spike, and teammate objects.\n created at: 2023-01-28\n- model_name: google/owlvit-base-patch16\n  description: OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. OWL-ViT uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features.\n created at: 2022-07-05\n- model_name: google/openimages_v4/ssd/mobilenet_v2\n  description: A pre-trained image object detection model based on the SSD architecture with MobileNetV2 as the backbone. The model is capable of detecting objects in images and returning their class labels and bounding box coordinates.\n created at: 2020-10-06\n- model_name: google/faster_rcnn/openimages_v4/inception_resnet_v2\n  description: Detects objects in images using the Faster R-CNN model with Inception-ResNet V2 backbone trained on the OpenImages V4 dataset.\n created at: 2020-10-06\n- model_name: tensorflow/ssd_mobilenet_v2\n  description: This API provides a pre-trained object detection model using TensorFlow Hub. The model is based on the SSD MobileNet V2 architecture and is trained on the COCO dataset. It can be used to detect multiple objects in an image and returns their class, bounding box coordinates, and confidence scores.\n created at: 2020-10-06\n- model_name: tensorflow/efficientdet/lite0/detection\n  description: A collection of TensorFlow Hub models for detecting objects in images using the EfficientDet architecture. These models can be used for various tasks such as object detection, instance segmentation, and more.\n created at: 2020-10-06\n- model_name: tensorflow/efficientdet/lite2/detection\n  description: This API allows for object detection in images using TensorFlow Hub and the EfficientDet model. It can be used to load a pre-trained model and detect objects in a given image.\n created at: 2020-10-06\n- model_name: tensorflow/centernet/resnet50v1_fpn_512x512\n  description: Detect objects in images using TensorFlow Hub and a pre-trained CenterNet model with ResNet50V1 FPN backbone\n created at: 2020-10-06\n- model_name: tensorflow/efficientdet/d0\n  description: Load a pre-trained model from TensorFlow Hub for object detection in images.\n created at: 2020-10-06\n- model_name: tensorflow/centernet/hourglass_512x512\n  description: A pre-trained TensorFlow Hub model for detecting objects in images using the CenterNet Hourglass architecture\n created at: 2020-10-06\n- model_name: tensorflow/faster_rcnn/resnet50_v1_640x640\n  description: A pre-trained Faster R-CNN model with ResNet-50 backbone for object detection in images\n created at: 2020-10-06\n- model_name: tensorflow/ssd_mobilenet_v2/fpnlite_640x640\n  description: A pre-trained object detection model based on the SSD Mobilenet V2 architecture, capable of detecting multiple objects in an image. The model is trained on the COCO dataset and achieves a mean Average Precision (mAP) of 0.32.\n created at: 2020-10-06\n- model_name: tensorflow/centernet/hourglass_1024x1024\n  description: A pre-trained TensorFlow Hub model for detecting objects in images using the CenterNet architecture with an Hourglass backbone.\n created at: 2020-10-06\n- model_name: hustvl/yolop/yolop\n  description: YOLOP is an efficient multi-task network that can jointly handle three crucial tasks in autonomous driving: object detection, drivable area segmentation and lane detection. And it is also the first to reach real-time on embedded devices while maintaining state-of-the-art level performance on the BDD100K dataset.\n created at: 2021-08-25\n- model_name: datvuthanh/hybridnets/hybridnets\n  description: HybridNets is an end2end perception network for multi-tasks. Our work focused on traffic object detection, drivable area segmentation and lane detection. HybridNets can run real-time on embedded systems, and obtains SOTA Object Detection, Lane Detection on BDD100K Dataset.\n created at: 2021-12-23\n- model_name: ultralytics/yolov5/yolov5s\n  description: YOLOv5 is a family of compound-scaled object detection models trained on the COCO dataset, and includes simple functionality for Test Time Augmentation (TTA), model ensembling, hyperparameter evolution, and export to ONNX, CoreML and TFLite.\n created at: 2020-05-18\n- model_name: NVIDIA/DeepLearningExamples/nvidia_ssd\n  description: The SSD (Single Shot MultiBox Detector) model is an object detection model based on the paper 'SSD: Single Shot MultiBox Detector'. It uses a deep neural network for detecting objects in images. This implementation replaces the obsolete VGG model backbone with the more modern ResNet-50 model. The SSD model is trained on the COCO dataset and can be used to detect objects in images with high accuracy and efficiency.\n created at: 2018-05-02\n- model_name: valentinafeve/yolos-fashionpedia\n  description: None\n created at: 2022-11-17\n- model_name: microsoft/conditional-detr-resnet-50\n  description: Conditional DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper [Conditional DETR for Fast Training Convergence](https://arxiv.org/abs/2108.06152) by Meng et al. and first released in [this repository](https://github.com/Atten4Vis/ConditionalDETR).\n created at: 2022-09-09\n- model_name: Yifeng-Liu/rt-detr-finetuned-for-satellite-image-roofs-detection\n  description: roof detection for remote sensing task. - model type: object detection for remote sensing task. - license: mit - model type: object detection for remote sensing task. - license: mit\n created at: 2024-09-03T08:30:42+00:00\n- model_name: devxyasir/florence-finetuned-license-plate-detection\n  description: this model is a fine-tuned version of microsoft's florence-2 large `florence2-base-ft` adapted for automatic number plate detection and recognition . it processes vehicle images to localize number plates with bounding boxes and applies ocr to extract the license plate text, enabling high-accuracy license plate reading. the model leverages transformer-based vision-language architectures and is trained on a custom dataset of vehicle images with annotated license plates.\n created at: 2025-01-27T05:49:01+00:00\n- model_name: SimulaMet-HOST/HockeyAI\n  description: üîó this model is trained on the hockeyai dataset. - üìä access the dataset used for training here: - üöÄ try the model in action with our interactive hugging face space:\n created at: 2025-01-09T09:23:25+00:00\n- model_name: jameslahm/yoloe\n  description: official pytorch implementation of yoloe . comparison of performance, training cost, and inference efficiency between yoloe ours and yolo-worldv2 in terms of open text prompts. .\\\n created at: 2025-03-10T07:54:02+00:00\n- model_name: PekingU/rtdetr_v2_r18vd\n  description: the rt-detrv2 model was proposed in by wenyu lv, yian zhao, qinyao chang, kui huang, guanzhong wang, yi liu. rt-detrv2 refines rt-detr by introducing selective multi-scale feature extraction, a discrete sampling operator for broader deployment compatibility, and improved training strategies like dynamic data augmentation and scale-adaptive hyperparameters. these changes enhance flexibility and practicality while maintaining real-time performance. this model was contributed by with the help of and\n created at: 2025-01-31T17:10:44+00:00\n- model_name: ustc-community/dfine-xlarge-coco\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-02-06T15:36:03+00:00\n- model_name: ustc-community/dfine-xlarge-obj365\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-03-28T12:55:40+00:00\n- model_name: PekingU/rtdetr_v2_r101vd\n  description: the rt-detrv2 model was proposed in by wenyu lv, yian zhao, qinyao chang, kui huang, guanzhong wang, yi liu. rt-detrv2 refines rt-detr by introducing selective multi-scale feature extraction, a discrete sampling operator for broader deployment compatibility, and improved training strategies like dynamic data augmentation and scale-adaptive hyperparameters. these changes enhance flexibility and practicality while maintaining real-time performance. this model was contributed by with the help of and\n created at: 2025-01-31T18:14:53+00:00\n- model_name: PekingU/rtdetr_v2_r50vd\n  description: the rt-detrv2 model was proposed in by wenyu lv, yian zhao, qinyao chang, kui huang, guanzhong wang, yi liu. rt-detrv2 refines rt-detr by introducing selective multi-scale feature extraction, a discrete sampling operator for broader deployment compatibility, and improved training strategies like dynamic data augmentation and scale-adaptive hyperparameters. these changes enhance flexibility and practicality while maintaining real-time performance. this model was contributed by with the help of and\n created at: 2025-01-31T18:14:46+00:00\n- model_name: ustc-community/dfine-large-coco\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-02-22T12:04:35+00:00\n- model_name: ustc-community/dfine-nano-coco\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-03-28T12:48:09+00:00\n- model_name: apkonsta/table-transformer-detection-ifrs\n  description: this repository contains a fine-tuned version of the table transformer model, specifically adapted for detecting tables in ifrs international financial reporting standards pdfs. the model is based on the table transformer architecture, which is designed to extract tables from unstructured documents such as pdfs and images. base model: microsoft/table-transformer-detection library: transformers\n created at: 2024-11-24T18:13:18+00:00\n- model_name: mdefrance/yolos-tiny-signature-detection\n  description: yolos model finetuned to detect handwritten signatures in document images using dataset. original yolos was introduced in the paper by fang et al. and first released in . yolos is a vision transformer vit trained using the detr loss. despite its simplicity, a base-sized yolos model is able to achieve 42 ap on coco validation 2017 similar to detr and more complex frameworks such as faster r-cnn. yolos is a vision transformer vit trained using the detr loss. despite its simplicity, a base-sized yolos model is able to achieve 42 ap on coco validation 2017 similar to detr and more complex frameworks such as faster r-cnn. - finetuned by:  - repository:  - model type:  - license: apache 2.0 license - finetuned from model\n created at: 2025-05-06T14:15:22+00:00\n- model_name: ustc-community/dfine-small-coco\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-02-11T14:30:13+00:00\n- model_name: Adit-jain/soccana\n  description: this repository hosts a yolov8-based object detection model trained on a curated and segmented dataset derived from soccernet and other public football datasets. the model is designed to detect key entities in a football game players , referees , and the ball with high accuracy, even in challenging scenes.  - architecture : yolov8n + sahi sliced aided hyper inference\n created at: 2025-07-26T10:43:57+00:00\n- model_name: HuggingPanda/docling-layout\n  description: this is the docling model for layout detection , designed to facilitate easy importing and usage like any other hugging face model. this model is part of the , which provides document layout analysis tools. here's how you can load and use the model:\n created at: 2025-03-13T11:39:16+00:00\n- model_name: yifeihu/Florence-2-DocLayNet-Fixed\n  description: we finetuned the florence-2-large-ft hf model using the doclaynet-v1.1 dataset. to prevent the model from generating hallucinated class names, we re-mapped all class names to single tokens: original class names new class names\n created at: 2024-10-29T17:23:16+00:00\n- model_name: ustc-community/dfine-large-obj365\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-03-28T13:00:47+00:00\n- model_name: Ultralytics/YOLOv5\n  description: yolov5 üöÄ is the world's most loved vision ai, representing ultralytics open-source research into future vision ai methods, incorporating lessons learned and best practices evolved over thousands of hours of research and development. we hope that the resources here will help you get the most out of yolov5. please browse the yolov5 docs for details, raise an issue on github for support, and join our discord community for questions and discussions! to request an enterprise license please complete the form at . yolo11 üöÄ new\n created at: 2024-11-14T09:33:32+00:00\n- model_name: tech4humans/conditional-detr-50-signature-detector\n  description: this repository presents a conditional-detr model with resnet-50 backbone, fine-tuned to detect handwritten signatures in document images. this model achieved the highest map@0.5 93.65% among all tested architectures in our comprehensive evaluation. resource links / badges details\n created at: 2025-06-18T03:39:12+00:00\n\nExamples:\n- example user query: I would like to know if there are any cats or dogs in the photos taken in my friend's backyard.\n  model_name: tensorflow/ssd_mobilenet_v2\n- example user query: I recently installed a new security camera. I need to identify the objects in the recorded video.\n  model_name: hustvl/yolos-tiny\n- example user query: I want to monitor if any car accidents happen on the street by detecting the position of vehicles and pedestrians from CCTV footage.\n  model_name: google/faster_rcnn/openimages_v4/inception_resnet_v2\n- example user query: I want to create an app which detects the license plates of any parked vehicle in my city.\n  model_name: keremberke/yolov5m-license-plate\n- example user query: Your advertising company is looking for a solution that will help them detect objects in images as part of their marketing campaigns.\n  model_name: fcakyon/yolov5s-v7.0\n- example user query: I own a pathology lab and I want to scan blood cell images using a computer vision model. How do I detect blood cells in images using an AI model?\n  model_name: keremberke/yolov8m-blood-cell-detection\n\nUser prompt:\nIdentify all things in the picture.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_det>\n\nCandidate Models:\n- model_name: microsoft/table-transformer-structure-recognition\n  description: Table Transformer (DETR) model trained on PubTables1M for detecting the structure (like rows, columns) in tables.\n created at: 2022-10-14\n- model_name: microsoft/table-transformer-detection\n  description: Table Transformer (DETR) model trained on PubTables1M for detecting tables in documents. Introduced in the paper PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents by Smock et al.\n created at: 2022-10-14\n- model_name: facebook/detr-resnet-50\n  description: DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: hustvl/yolos-tiny\n  description: YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN). The model is trained using a bipartite matching loss: one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a no object as class and no bounding box as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.\n created at: 2022-04-26\n- model_name: TahaDouaji/detr-doc-table-detection\n  description: detr-doc-table-detection is a model trained to detect both Bordered and Borderless tables in documents, based on facebook/detr-resnet-50.\n created at: 2022-03-11\n- model_name: facebook/detr-resnet-101\n  description: DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: keremberke/yolov8m-table-extraction\n  description: A YOLOv8 model for table extraction in images, capable of detecting both bordered and borderless tables. Trained using the keremberke/table-extraction dataset.\n created at: 2023-01-29\n- model_name: SenseTime/deformable-detr\n  description: Deformable DETR model with ResNet-50 backbone trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper Deformable DETR: Deformable Transformers for End-to-End Object Detection by Zhu et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: facebook/detr-resnet-101-dc5\n  description: DETR (End-to-End Object Detection) model with ResNet-101 backbone (dilated C5 stage). The model is trained on COCO 2017 object detection dataset and achieves an average precision (AP) of 44.9 on the COCO 2017 validation set.\n created at: 2022-03-02\n- model_name: hustvl/yolos-small\n  description: YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Fang et al. and first released in this repository. YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN).\n created at: 2022-04-26\n- model_name: keremberke/yolov8m-hard-hat-detection\n  description: A YOLOv8 model for detecting hard hats in images. The model can distinguish between 'Hardhat' and 'NO-Hardhat' classes. It can be used to ensure safety compliance in construction sites or other industrial environments where hard hats are required.\n created at: 2023-01-29\n- model_name: keremberke/yolov5m-license-plate\n  description: A YOLOv5 model for license plate detection trained on a custom dataset. The model can detect license plates in images with high accuracy.\n created at: 2023-01-01\n- model_name: keremberke/yolov8s-table-extraction\n  description: A YOLOv8 model for table extraction in documents, capable of detecting bordered and borderless tables. Trained on the table-extraction dataset, the model achieves a mAP@0.5 of 0.984 on the validation set.\n created at: 2023-01-29\n- model_name: keremberke/yolov8m-csgo-player-detection\n  description: An object detection model trained to detect Counter-Strike: Global Offensive (CS:GO) players. The model is based on the YOLOv8 architecture and can identify 'ct', 'cthead', 't', and 'thead' labels.\n created at: 2023-01-29\n- model_name: keremberke/yolov8m-nlf-head-detection\n  description: A YOLOv8 model trained for head detection in American football. The model is capable of detecting helmets, blurred helmets, difficult helmets, partial helmets, and sideline helmets.\n created at: 2023-01-29\n- model_name: keremberke/yolov8m-forklift-detection\n  description: A YOLOv8 model for detecting forklifts and persons in images.\n created at: 2023-01-22\n- model_name: google/owlvit-large-patch14\n  description: OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. It uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. OWL-ViT is trained on publicly available image-caption data and fine-tuned on publicly available object detection datasets such as COCO and OpenImages.\n created at: 2022-07-05\n- model_name: keremberke/yolov8m-plane-detection\n  description: A YOLOv8 model for plane detection trained on the keremberke/plane-detection dataset. The model is capable of detecting planes in images with high accuracy.\n created at: 2023-01-29\n- model_name: keremberke/yolov8s-csgo-player-detection\n  description: A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players. Supports the labels ['ct', 'cthead', 't', 'thead'].\n created at: 2023-01-29\n- model_name: fcakyon/yolov5s-v7.0\n  description: Yolov5s-v7.0 is an object detection model trained on the COCO dataset. It can detect objects in images and return their bounding boxes, scores, and categories.\n created at: 2022-12-13\n- model_name: keremberke/yolov8s-hard-hat-detection\n  description: An object detection model trained to detect hard hats and no-hard hats in images. The model is based on YOLOv8 architecture and can be used for safety applications.\n created at: 2023-01-29\n- model_name: keremberke/yolov8m-blood-cell-detection\n  description: A YOLOv8 model for blood cell detection, including Platelets, RBC, and WBC. Trained on the blood-cell-object-detection dataset.\n created at: 2023-01-29\n- model_name: keremberke/yolov8n-table-extraction\n  description: An object detection model for extracting tables from documents. Supports two label types: 'bordered' and 'borderless'.\n created at: 2023-01-29\n- model_name: keremberke/yolov8n-csgo-player-detection\n  description: A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players with supported labels: ['ct', 'cthead', 't', 'thead'].\n created at: 2023-01-29\n- model_name: keremberke/yolov5s-license-plate\n  description: A YOLOv5 based license plate detection model trained on a custom dataset.\n created at: 2023-01-01\n- model_name: keremberke/yolov8n-blood-cell-detection\n  description: This model detects blood cells in images, specifically Platelets, RBC, and WBC. It is based on the YOLOv8 architecture and trained on the blood-cell-object-detection dataset.\n created at: 2023-01-29\n- model_name: google/owlvit-base-patch32\n  description: OWL-ViT is a zero-shot text-conditioned object detection model that uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. The model can be used to query an image with one or multiple text queries.\n created at: 2022-07-05\n- model_name: keremberke/yolov8m-valorant-detection\n  description: A YOLOv8 model for object detection in Valorant game, trained on a custom dataset. It detects dropped spike, enemy, planted spike, and teammate objects.\n created at: 2023-01-28\n- model_name: google/owlvit-base-patch16\n  description: OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. OWL-ViT uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features.\n created at: 2022-07-05\n- model_name: google/openimages_v4/ssd/mobilenet_v2\n  description: A pre-trained image object detection model based on the SSD architecture with MobileNetV2 as the backbone. The model is capable of detecting objects in images and returning their class labels and bounding box coordinates.\n created at: 2020-10-06\n- model_name: google/faster_rcnn/openimages_v4/inception_resnet_v2\n  description: Detects objects in images using the Faster R-CNN model with Inception-ResNet V2 backbone trained on the OpenImages V4 dataset.\n created at: 2020-10-06\n- model_name: tensorflow/ssd_mobilenet_v2\n  description: This API provides a pre-trained object detection model using TensorFlow Hub. The model is based on the SSD MobileNet V2 architecture and is trained on the COCO dataset. It can be used to detect multiple objects in an image and returns their class, bounding box coordinates, and confidence scores.\n created at: 2020-10-06\n- model_name: tensorflow/efficientdet/lite0/detection\n  description: A collection of TensorFlow Hub models for detecting objects in images using the EfficientDet architecture. These models can be used for various tasks such as object detection, instance segmentation, and more.\n created at: 2020-10-06\n- model_name: tensorflow/efficientdet/lite2/detection\n  description: This API allows for object detection in images using TensorFlow Hub and the EfficientDet model. It can be used to load a pre-trained model and detect objects in a given image.\n created at: 2020-10-06\n- model_name: tensorflow/centernet/resnet50v1_fpn_512x512\n  description: Detect objects in images using TensorFlow Hub and a pre-trained CenterNet model with ResNet50V1 FPN backbone\n created at: 2020-10-06\n- model_name: tensorflow/efficientdet/d0\n  description: Load a pre-trained model from TensorFlow Hub for object detection in images.\n created at: 2020-10-06\n- model_name: tensorflow/centernet/hourglass_512x512\n  description: A pre-trained TensorFlow Hub model for detecting objects in images using the CenterNet Hourglass architecture\n created at: 2020-10-06\n- model_name: tensorflow/faster_rcnn/resnet50_v1_640x640\n  description: A pre-trained Faster R-CNN model with ResNet-50 backbone for object detection in images\n created at: 2020-10-06\n- model_name: tensorflow/ssd_mobilenet_v2/fpnlite_640x640\n  description: A pre-trained object detection model based on the SSD Mobilenet V2 architecture, capable of detecting multiple objects in an image. The model is trained on the COCO dataset and achieves a mean Average Precision (mAP) of 0.32.\n created at: 2020-10-06\n- model_name: tensorflow/centernet/hourglass_1024x1024\n  description: A pre-trained TensorFlow Hub model for detecting objects in images using the CenterNet architecture with an Hourglass backbone.\n created at: 2020-10-06\n- model_name: hustvl/yolop/yolop\n  description: YOLOP is an efficient multi-task network that can jointly handle three crucial tasks in autonomous driving: object detection, drivable area segmentation and lane detection. And it is also the first to reach real-time on embedded devices while maintaining state-of-the-art level performance on the BDD100K dataset.\n created at: 2021-08-25\n- model_name: datvuthanh/hybridnets/hybridnets\n  description: HybridNets is an end2end perception network for multi-tasks. Our work focused on traffic object detection, drivable area segmentation and lane detection. HybridNets can run real-time on embedded systems, and obtains SOTA Object Detection, Lane Detection on BDD100K Dataset.\n created at: 2021-12-23\n- model_name: ultralytics/yolov5/yolov5s\n  description: YOLOv5 is a family of compound-scaled object detection models trained on the COCO dataset, and includes simple functionality for Test Time Augmentation (TTA), model ensembling, hyperparameter evolution, and export to ONNX, CoreML and TFLite.\n created at: 2020-05-18\n- model_name: NVIDIA/DeepLearningExamples/nvidia_ssd\n  description: The SSD (Single Shot MultiBox Detector) model is an object detection model based on the paper 'SSD: Single Shot MultiBox Detector'. It uses a deep neural network for detecting objects in images. This implementation replaces the obsolete VGG model backbone with the more modern ResNet-50 model. The SSD model is trained on the COCO dataset and can be used to detect objects in images with high accuracy and efficiency.\n created at: 2018-05-02\n- model_name: valentinafeve/yolos-fashionpedia\n  description: None\n created at: 2022-11-17\n- model_name: microsoft/conditional-detr-resnet-50\n  description: Conditional DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper [Conditional DETR for Fast Training Convergence](https://arxiv.org/abs/2108.06152) by Meng et al. and first released in [this repository](https://github.com/Atten4Vis/ConditionalDETR).\n created at: 2022-09-09\n- model_name: Yifeng-Liu/rt-detr-finetuned-for-satellite-image-roofs-detection\n  description: roof detection for remote sensing task. - model type: object detection for remote sensing task. - license: mit - model type: object detection for remote sensing task. - license: mit\n created at: 2024-09-03T08:30:42+00:00\n- model_name: devxyasir/florence-finetuned-license-plate-detection\n  description: this model is a fine-tuned version of microsoft's florence-2 large `florence2-base-ft` adapted for automatic number plate detection and recognition . it processes vehicle images to localize number plates with bounding boxes and applies ocr to extract the license plate text, enabling high-accuracy license plate reading. the model leverages transformer-based vision-language architectures and is trained on a custom dataset of vehicle images with annotated license plates.\n created at: 2025-01-27T05:49:01+00:00\n- model_name: SimulaMet-HOST/HockeyAI\n  description: üîó this model is trained on the hockeyai dataset. - üìä access the dataset used for training here: - üöÄ try the model in action with our interactive hugging face space:\n created at: 2025-01-09T09:23:25+00:00\n- model_name: jameslahm/yoloe\n  description: official pytorch implementation of yoloe . comparison of performance, training cost, and inference efficiency between yoloe ours and yolo-worldv2 in terms of open text prompts. .\\\n created at: 2025-03-10T07:54:02+00:00\n- model_name: PekingU/rtdetr_v2_r18vd\n  description: the rt-detrv2 model was proposed in by wenyu lv, yian zhao, qinyao chang, kui huang, guanzhong wang, yi liu. rt-detrv2 refines rt-detr by introducing selective multi-scale feature extraction, a discrete sampling operator for broader deployment compatibility, and improved training strategies like dynamic data augmentation and scale-adaptive hyperparameters. these changes enhance flexibility and practicality while maintaining real-time performance. this model was contributed by with the help of and\n created at: 2025-01-31T17:10:44+00:00\n- model_name: ustc-community/dfine-xlarge-coco\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-02-06T15:36:03+00:00\n- model_name: ustc-community/dfine-xlarge-obj365\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-03-28T12:55:40+00:00\n- model_name: PekingU/rtdetr_v2_r101vd\n  description: the rt-detrv2 model was proposed in by wenyu lv, yian zhao, qinyao chang, kui huang, guanzhong wang, yi liu. rt-detrv2 refines rt-detr by introducing selective multi-scale feature extraction, a discrete sampling operator for broader deployment compatibility, and improved training strategies like dynamic data augmentation and scale-adaptive hyperparameters. these changes enhance flexibility and practicality while maintaining real-time performance. this model was contributed by with the help of and\n created at: 2025-01-31T18:14:53+00:00\n- model_name: PekingU/rtdetr_v2_r50vd\n  description: the rt-detrv2 model was proposed in by wenyu lv, yian zhao, qinyao chang, kui huang, guanzhong wang, yi liu. rt-detrv2 refines rt-detr by introducing selective multi-scale feature extraction, a discrete sampling operator for broader deployment compatibility, and improved training strategies like dynamic data augmentation and scale-adaptive hyperparameters. these changes enhance flexibility and practicality while maintaining real-time performance. this model was contributed by with the help of and\n created at: 2025-01-31T18:14:46+00:00\n- model_name: ustc-community/dfine-large-coco\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-02-22T12:04:35+00:00\n- model_name: ustc-community/dfine-nano-coco\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-03-28T12:48:09+00:00\n- model_name: apkonsta/table-transformer-detection-ifrs\n  description: this repository contains a fine-tuned version of the table transformer model, specifically adapted for detecting tables in ifrs international financial reporting standards pdfs. the model is based on the table transformer architecture, which is designed to extract tables from unstructured documents such as pdfs and images. base model: microsoft/table-transformer-detection library: transformers\n created at: 2024-11-24T18:13:18+00:00\n- model_name: mdefrance/yolos-tiny-signature-detection\n  description: yolos model finetuned to detect handwritten signatures in document images using dataset. original yolos was introduced in the paper by fang et al. and first released in . yolos is a vision transformer vit trained using the detr loss. despite its simplicity, a base-sized yolos model is able to achieve 42 ap on coco validation 2017 similar to detr and more complex frameworks such as faster r-cnn. yolos is a vision transformer vit trained using the detr loss. despite its simplicity, a base-sized yolos model is able to achieve 42 ap on coco validation 2017 similar to detr and more complex frameworks such as faster r-cnn. - finetuned by:  - repository:  - model type:  - license: apache 2.0 license - finetuned from model\n created at: 2025-05-06T14:15:22+00:00\n- model_name: ustc-community/dfine-small-coco\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-02-11T14:30:13+00:00\n- model_name: Adit-jain/soccana\n  description: this repository hosts a yolov8-based object detection model trained on a curated and segmented dataset derived from soccernet and other public football datasets. the model is designed to detect key entities in a football game players , referees , and the ball with high accuracy, even in challenging scenes.  - architecture : yolov8n + sahi sliced aided hyper inference\n created at: 2025-07-26T10:43:57+00:00\n- model_name: HuggingPanda/docling-layout\n  description: this is the docling model for layout detection , designed to facilitate easy importing and usage like any other hugging face model. this model is part of the , which provides document layout analysis tools. here's how you can load and use the model:\n created at: 2025-03-13T11:39:16+00:00\n- model_name: yifeihu/Florence-2-DocLayNet-Fixed\n  description: we finetuned the florence-2-large-ft hf model using the doclaynet-v1.1 dataset. to prevent the model from generating hallucinated class names, we re-mapped all class names to single tokens: original class names new class names\n created at: 2024-10-29T17:23:16+00:00\n- model_name: ustc-community/dfine-large-obj365\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-03-28T13:00:47+00:00\n- model_name: Ultralytics/YOLOv5\n  description: yolov5 üöÄ is the world's most loved vision ai, representing ultralytics open-source research into future vision ai methods, incorporating lessons learned and best practices evolved over thousands of hours of research and development. we hope that the resources here will help you get the most out of yolov5. please browse the yolov5 docs for details, raise an issue on github for support, and join our discord community for questions and discussions! to request an enterprise license please complete the form at . yolo11 üöÄ new\n created at: 2024-11-14T09:33:32+00:00\n- model_name: tech4humans/conditional-detr-50-signature-detector\n  description: this repository presents a conditional-detr model with resnet-50 backbone, fine-tuned to detect handwritten signatures in document images. this model achieved the highest map@0.5 93.65% among all tested architectures in our comprehensive evaluation. resource links / badges details\n created at: 2025-06-18T03:39:12+00:00\n\nExamples:\n- example user query: I would like to know if there are any cats or dogs in the photos taken in my friend's backyard.\n  model_name: tensorflow/ssd_mobilenet_v2\n- example user query: I recently installed a new security camera. I need to identify the objects in the recorded video.\n  model_name: hustvl/yolos-tiny\n- example user query: I want to monitor if any car accidents happen on the street by detecting the position of vehicles and pedestrians from CCTV footage.\n  model_name: google/faster_rcnn/openimages_v4/inception_resnet_v2\n- example user query: I want to create an app which detects the license plates of any parked vehicle in my city.\n  model_name: keremberke/yolov5m-license-plate\n- example user query: Your advertising company is looking for a solution that will help them detect objects in images as part of their marketing campaigns.\n  model_name: fcakyon/yolov5s-v7.0\n- example user query: I own a pathology lab and I want to scan blood cell images using a computer vision model. How do I detect blood cells in images using an AI model?\n  model_name: keremberke/yolov8m-blood-cell-detection\n\nUser prompt:\nReveal all elements in the portrayal.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_det>\n\nCandidate Models:\n- model_name: microsoft/table-transformer-structure-recognition\n  description: Table Transformer (DETR) model trained on PubTables1M for detecting the structure (like rows, columns) in tables.\n created at: 2022-10-14\n- model_name: microsoft/table-transformer-detection\n  description: Table Transformer (DETR) model trained on PubTables1M for detecting tables in documents. Introduced in the paper PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents by Smock et al.\n created at: 2022-10-14\n- model_name: facebook/detr-resnet-50\n  description: DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: hustvl/yolos-tiny\n  description: YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN). The model is trained using a bipartite matching loss: one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a no object as class and no bounding box as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.\n created at: 2022-04-26\n- model_name: TahaDouaji/detr-doc-table-detection\n  description: detr-doc-table-detection is a model trained to detect both Bordered and Borderless tables in documents, based on facebook/detr-resnet-50.\n created at: 2022-03-11\n- model_name: facebook/detr-resnet-101\n  description: DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: keremberke/yolov8m-table-extraction\n  description: A YOLOv8 model for table extraction in images, capable of detecting both bordered and borderless tables. Trained using the keremberke/table-extraction dataset.\n created at: 2023-01-29\n- model_name: SenseTime/deformable-detr\n  description: Deformable DETR model with ResNet-50 backbone trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper Deformable DETR: Deformable Transformers for End-to-End Object Detection by Zhu et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: facebook/detr-resnet-101-dc5\n  description: DETR (End-to-End Object Detection) model with ResNet-101 backbone (dilated C5 stage). The model is trained on COCO 2017 object detection dataset and achieves an average precision (AP) of 44.9 on the COCO 2017 validation set.\n created at: 2022-03-02\n- model_name: hustvl/yolos-small\n  description: YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Fang et al. and first released in this repository. YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN).\n created at: 2022-04-26\n- model_name: keremberke/yolov8m-hard-hat-detection\n  description: A YOLOv8 model for detecting hard hats in images. The model can distinguish between 'Hardhat' and 'NO-Hardhat' classes. It can be used to ensure safety compliance in construction sites or other industrial environments where hard hats are required.\n created at: 2023-01-29\n- model_name: keremberke/yolov5m-license-plate\n  description: A YOLOv5 model for license plate detection trained on a custom dataset. The model can detect license plates in images with high accuracy.\n created at: 2023-01-01\n- model_name: keremberke/yolov8s-table-extraction\n  description: A YOLOv8 model for table extraction in documents, capable of detecting bordered and borderless tables. Trained on the table-extraction dataset, the model achieves a mAP@0.5 of 0.984 on the validation set.\n created at: 2023-01-29\n- model_name: keremberke/yolov8m-csgo-player-detection\n  description: An object detection model trained to detect Counter-Strike: Global Offensive (CS:GO) players. The model is based on the YOLOv8 architecture and can identify 'ct', 'cthead', 't', and 'thead' labels.\n created at: 2023-01-29\n- model_name: keremberke/yolov8m-nlf-head-detection\n  description: A YOLOv8 model trained for head detection in American football. The model is capable of detecting helmets, blurred helmets, difficult helmets, partial helmets, and sideline helmets.\n created at: 2023-01-29\n- model_name: keremberke/yolov8m-forklift-detection\n  description: A YOLOv8 model for detecting forklifts and persons in images.\n created at: 2023-01-22\n- model_name: google/owlvit-large-patch14\n  description: OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. It uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. OWL-ViT is trained on publicly available image-caption data and fine-tuned on publicly available object detection datasets such as COCO and OpenImages.\n created at: 2022-07-05\n- model_name: keremberke/yolov8m-plane-detection\n  description: A YOLOv8 model for plane detection trained on the keremberke/plane-detection dataset. The model is capable of detecting planes in images with high accuracy.\n created at: 2023-01-29\n- model_name: keremberke/yolov8s-csgo-player-detection\n  description: A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players. Supports the labels ['ct', 'cthead', 't', 'thead'].\n created at: 2023-01-29\n- model_name: fcakyon/yolov5s-v7.0\n  description: Yolov5s-v7.0 is an object detection model trained on the COCO dataset. It can detect objects in images and return their bounding boxes, scores, and categories.\n created at: 2022-12-13\n- model_name: keremberke/yolov8s-hard-hat-detection\n  description: An object detection model trained to detect hard hats and no-hard hats in images. The model is based on YOLOv8 architecture and can be used for safety applications.\n created at: 2023-01-29\n- model_name: keremberke/yolov8m-blood-cell-detection\n  description: A YOLOv8 model for blood cell detection, including Platelets, RBC, and WBC. Trained on the blood-cell-object-detection dataset.\n created at: 2023-01-29\n- model_name: keremberke/yolov8n-table-extraction\n  description: An object detection model for extracting tables from documents. Supports two label types: 'bordered' and 'borderless'.\n created at: 2023-01-29\n- model_name: keremberke/yolov8n-csgo-player-detection\n  description: A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players with supported labels: ['ct', 'cthead', 't', 'thead'].\n created at: 2023-01-29\n- model_name: keremberke/yolov5s-license-plate\n  description: A YOLOv5 based license plate detection model trained on a custom dataset.\n created at: 2023-01-01\n- model_name: keremberke/yolov8n-blood-cell-detection\n  description: This model detects blood cells in images, specifically Platelets, RBC, and WBC. It is based on the YOLOv8 architecture and trained on the blood-cell-object-detection dataset.\n created at: 2023-01-29\n- model_name: google/owlvit-base-patch32\n  description: OWL-ViT is a zero-shot text-conditioned object detection model that uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. The model can be used to query an image with one or multiple text queries.\n created at: 2022-07-05\n- model_name: keremberke/yolov8m-valorant-detection\n  description: A YOLOv8 model for object detection in Valorant game, trained on a custom dataset. It detects dropped spike, enemy, planted spike, and teammate objects.\n created at: 2023-01-28\n- model_name: google/owlvit-base-patch16\n  description: OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. OWL-ViT uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features.\n created at: 2022-07-05\n- model_name: google/openimages_v4/ssd/mobilenet_v2\n  description: A pre-trained image object detection model based on the SSD architecture with MobileNetV2 as the backbone. The model is capable of detecting objects in images and returning their class labels and bounding box coordinates.\n created at: 2020-10-06\n- model_name: google/faster_rcnn/openimages_v4/inception_resnet_v2\n  description: Detects objects in images using the Faster R-CNN model with Inception-ResNet V2 backbone trained on the OpenImages V4 dataset.\n created at: 2020-10-06\n- model_name: tensorflow/ssd_mobilenet_v2\n  description: This API provides a pre-trained object detection model using TensorFlow Hub. The model is based on the SSD MobileNet V2 architecture and is trained on the COCO dataset. It can be used to detect multiple objects in an image and returns their class, bounding box coordinates, and confidence scores.\n created at: 2020-10-06\n- model_name: tensorflow/efficientdet/lite0/detection\n  description: A collection of TensorFlow Hub models for detecting objects in images using the EfficientDet architecture. These models can be used for various tasks such as object detection, instance segmentation, and more.\n created at: 2020-10-06\n- model_name: tensorflow/efficientdet/lite2/detection\n  description: This API allows for object detection in images using TensorFlow Hub and the EfficientDet model. It can be used to load a pre-trained model and detect objects in a given image.\n created at: 2020-10-06\n- model_name: tensorflow/centernet/resnet50v1_fpn_512x512\n  description: Detect objects in images using TensorFlow Hub and a pre-trained CenterNet model with ResNet50V1 FPN backbone\n created at: 2020-10-06\n- model_name: tensorflow/efficientdet/d0\n  description: Load a pre-trained model from TensorFlow Hub for object detection in images.\n created at: 2020-10-06\n- model_name: tensorflow/centernet/hourglass_512x512\n  description: A pre-trained TensorFlow Hub model for detecting objects in images using the CenterNet Hourglass architecture\n created at: 2020-10-06\n- model_name: tensorflow/faster_rcnn/resnet50_v1_640x640\n  description: A pre-trained Faster R-CNN model with ResNet-50 backbone for object detection in images\n created at: 2020-10-06\n- model_name: tensorflow/ssd_mobilenet_v2/fpnlite_640x640\n  description: A pre-trained object detection model based on the SSD Mobilenet V2 architecture, capable of detecting multiple objects in an image. The model is trained on the COCO dataset and achieves a mean Average Precision (mAP) of 0.32.\n created at: 2020-10-06\n- model_name: tensorflow/centernet/hourglass_1024x1024\n  description: A pre-trained TensorFlow Hub model for detecting objects in images using the CenterNet architecture with an Hourglass backbone.\n created at: 2020-10-06\n- model_name: hustvl/yolop/yolop\n  description: YOLOP is an efficient multi-task network that can jointly handle three crucial tasks in autonomous driving: object detection, drivable area segmentation and lane detection. And it is also the first to reach real-time on embedded devices while maintaining state-of-the-art level performance on the BDD100K dataset.\n created at: 2021-08-25\n- model_name: datvuthanh/hybridnets/hybridnets\n  description: HybridNets is an end2end perception network for multi-tasks. Our work focused on traffic object detection, drivable area segmentation and lane detection. HybridNets can run real-time on embedded systems, and obtains SOTA Object Detection, Lane Detection on BDD100K Dataset.\n created at: 2021-12-23\n- model_name: ultralytics/yolov5/yolov5s\n  description: YOLOv5 is a family of compound-scaled object detection models trained on the COCO dataset, and includes simple functionality for Test Time Augmentation (TTA), model ensembling, hyperparameter evolution, and export to ONNX, CoreML and TFLite.\n created at: 2020-05-18\n- model_name: NVIDIA/DeepLearningExamples/nvidia_ssd\n  description: The SSD (Single Shot MultiBox Detector) model is an object detection model based on the paper 'SSD: Single Shot MultiBox Detector'. It uses a deep neural network for detecting objects in images. This implementation replaces the obsolete VGG model backbone with the more modern ResNet-50 model. The SSD model is trained on the COCO dataset and can be used to detect objects in images with high accuracy and efficiency.\n created at: 2018-05-02\n- model_name: valentinafeve/yolos-fashionpedia\n  description: None\n created at: 2022-11-17\n- model_name: microsoft/conditional-detr-resnet-50\n  description: Conditional DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper [Conditional DETR for Fast Training Convergence](https://arxiv.org/abs/2108.06152) by Meng et al. and first released in [this repository](https://github.com/Atten4Vis/ConditionalDETR).\n created at: 2022-09-09\n- model_name: Yifeng-Liu/rt-detr-finetuned-for-satellite-image-roofs-detection\n  description: roof detection for remote sensing task. - model type: object detection for remote sensing task. - license: mit - model type: object detection for remote sensing task. - license: mit\n created at: 2024-09-03T08:30:42+00:00\n- model_name: devxyasir/florence-finetuned-license-plate-detection\n  description: this model is a fine-tuned version of microsoft's florence-2 large `florence2-base-ft` adapted for automatic number plate detection and recognition . it processes vehicle images to localize number plates with bounding boxes and applies ocr to extract the license plate text, enabling high-accuracy license plate reading. the model leverages transformer-based vision-language architectures and is trained on a custom dataset of vehicle images with annotated license plates.\n created at: 2025-01-27T05:49:01+00:00\n- model_name: SimulaMet-HOST/HockeyAI\n  description: üîó this model is trained on the hockeyai dataset. - üìä access the dataset used for training here: - üöÄ try the model in action with our interactive hugging face space:\n created at: 2025-01-09T09:23:25+00:00\n- model_name: jameslahm/yoloe\n  description: official pytorch implementation of yoloe . comparison of performance, training cost, and inference efficiency between yoloe ours and yolo-worldv2 in terms of open text prompts. .\\\n created at: 2025-03-10T07:54:02+00:00\n- model_name: PekingU/rtdetr_v2_r18vd\n  description: the rt-detrv2 model was proposed in by wenyu lv, yian zhao, qinyao chang, kui huang, guanzhong wang, yi liu. rt-detrv2 refines rt-detr by introducing selective multi-scale feature extraction, a discrete sampling operator for broader deployment compatibility, and improved training strategies like dynamic data augmentation and scale-adaptive hyperparameters. these changes enhance flexibility and practicality while maintaining real-time performance. this model was contributed by with the help of and\n created at: 2025-01-31T17:10:44+00:00\n- model_name: ustc-community/dfine-xlarge-coco\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-02-06T15:36:03+00:00\n- model_name: ustc-community/dfine-xlarge-obj365\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-03-28T12:55:40+00:00\n- model_name: PekingU/rtdetr_v2_r101vd\n  description: the rt-detrv2 model was proposed in by wenyu lv, yian zhao, qinyao chang, kui huang, guanzhong wang, yi liu. rt-detrv2 refines rt-detr by introducing selective multi-scale feature extraction, a discrete sampling operator for broader deployment compatibility, and improved training strategies like dynamic data augmentation and scale-adaptive hyperparameters. these changes enhance flexibility and practicality while maintaining real-time performance. this model was contributed by with the help of and\n created at: 2025-01-31T18:14:53+00:00\n- model_name: PekingU/rtdetr_v2_r50vd\n  description: the rt-detrv2 model was proposed in by wenyu lv, yian zhao, qinyao chang, kui huang, guanzhong wang, yi liu. rt-detrv2 refines rt-detr by introducing selective multi-scale feature extraction, a discrete sampling operator for broader deployment compatibility, and improved training strategies like dynamic data augmentation and scale-adaptive hyperparameters. these changes enhance flexibility and practicality while maintaining real-time performance. this model was contributed by with the help of and\n created at: 2025-01-31T18:14:46+00:00\n- model_name: ustc-community/dfine-large-coco\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-02-22T12:04:35+00:00\n- model_name: ustc-community/dfine-nano-coco\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-03-28T12:48:09+00:00\n- model_name: apkonsta/table-transformer-detection-ifrs\n  description: this repository contains a fine-tuned version of the table transformer model, specifically adapted for detecting tables in ifrs international financial reporting standards pdfs. the model is based on the table transformer architecture, which is designed to extract tables from unstructured documents such as pdfs and images. base model: microsoft/table-transformer-detection library: transformers\n created at: 2024-11-24T18:13:18+00:00\n- model_name: mdefrance/yolos-tiny-signature-detection\n  description: yolos model finetuned to detect handwritten signatures in document images using dataset. original yolos was introduced in the paper by fang et al. and first released in . yolos is a vision transformer vit trained using the detr loss. despite its simplicity, a base-sized yolos model is able to achieve 42 ap on coco validation 2017 similar to detr and more complex frameworks such as faster r-cnn. yolos is a vision transformer vit trained using the detr loss. despite its simplicity, a base-sized yolos model is able to achieve 42 ap on coco validation 2017 similar to detr and more complex frameworks such as faster r-cnn. - finetuned by:  - repository:  - model type:  - license: apache 2.0 license - finetuned from model\n created at: 2025-05-06T14:15:22+00:00\n- model_name: ustc-community/dfine-small-coco\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-02-11T14:30:13+00:00\n- model_name: Adit-jain/soccana\n  description: this repository hosts a yolov8-based object detection model trained on a curated and segmented dataset derived from soccernet and other public football datasets. the model is designed to detect key entities in a football game players , referees , and the ball with high accuracy, even in challenging scenes.  - architecture : yolov8n + sahi sliced aided hyper inference\n created at: 2025-07-26T10:43:57+00:00\n- model_name: HuggingPanda/docling-layout\n  description: this is the docling model for layout detection , designed to facilitate easy importing and usage like any other hugging face model. this model is part of the , which provides document layout analysis tools. here's how you can load and use the model:\n created at: 2025-03-13T11:39:16+00:00\n- model_name: yifeihu/Florence-2-DocLayNet-Fixed\n  description: we finetuned the florence-2-large-ft hf model using the doclaynet-v1.1 dataset. to prevent the model from generating hallucinated class names, we re-mapped all class names to single tokens: original class names new class names\n created at: 2024-10-29T17:23:16+00:00\n- model_name: ustc-community/dfine-large-obj365\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-03-28T13:00:47+00:00\n- model_name: Ultralytics/YOLOv5\n  description: yolov5 üöÄ is the world's most loved vision ai, representing ultralytics open-source research into future vision ai methods, incorporating lessons learned and best practices evolved over thousands of hours of research and development. we hope that the resources here will help you get the most out of yolov5. please browse the yolov5 docs for details, raise an issue on github for support, and join our discord community for questions and discussions! to request an enterprise license please complete the form at . yolo11 üöÄ new\n created at: 2024-11-14T09:33:32+00:00\n- model_name: tech4humans/conditional-detr-50-signature-detector\n  description: this repository presents a conditional-detr model with resnet-50 backbone, fine-tuned to detect handwritten signatures in document images. this model achieved the highest map@0.5 93.65% among all tested architectures in our comprehensive evaluation. resource links / badges details\n created at: 2025-06-18T03:39:12+00:00\n\nExamples:\n- example user query: I would like to know if there are any cats or dogs in the photos taken in my friend's backyard.\n  model_name: tensorflow/ssd_mobilenet_v2\n- example user query: I recently installed a new security camera. I need to identify the objects in the recorded video.\n  model_name: hustvl/yolos-tiny\n- example user query: I want to monitor if any car accidents happen on the street by detecting the position of vehicles and pedestrians from CCTV footage.\n  model_name: google/faster_rcnn/openimages_v4/inception_resnet_v2\n- example user query: I want to create an app which detects the license plates of any parked vehicle in my city.\n  model_name: keremberke/yolov5m-license-plate\n- example user query: Your advertising company is looking for a solution that will help them detect objects in images as part of their marketing campaigns.\n  model_name: fcakyon/yolov5s-v7.0\n- example user query: I own a pathology lab and I want to scan blood cell images using a computer vision model. How do I detect blood cells in images using an AI model?\n  model_name: keremberke/yolov8m-blood-cell-detection\n\nUser prompt:\nSurvey every item within this photograph.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_det>\n\nCandidate Models:\n- model_name: microsoft/table-transformer-structure-recognition\n  description: Table Transformer (DETR) model trained on PubTables1M for detecting the structure (like rows, columns) in tables.\n created at: 2022-10-14\n- model_name: microsoft/table-transformer-detection\n  description: Table Transformer (DETR) model trained on PubTables1M for detecting tables in documents. Introduced in the paper PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents by Smock et al.\n created at: 2022-10-14\n- model_name: facebook/detr-resnet-50\n  description: DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: hustvl/yolos-tiny\n  description: YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN). The model is trained using a bipartite matching loss: one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a no object as class and no bounding box as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.\n created at: 2022-04-26\n- model_name: TahaDouaji/detr-doc-table-detection\n  description: detr-doc-table-detection is a model trained to detect both Bordered and Borderless tables in documents, based on facebook/detr-resnet-50.\n created at: 2022-03-11\n- model_name: facebook/detr-resnet-101\n  description: DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: keremberke/yolov8m-table-extraction\n  description: A YOLOv8 model for table extraction in images, capable of detecting both bordered and borderless tables. Trained using the keremberke/table-extraction dataset.\n created at: 2023-01-29\n- model_name: SenseTime/deformable-detr\n  description: Deformable DETR model with ResNet-50 backbone trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper Deformable DETR: Deformable Transformers for End-to-End Object Detection by Zhu et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: facebook/detr-resnet-101-dc5\n  description: DETR (End-to-End Object Detection) model with ResNet-101 backbone (dilated C5 stage). The model is trained on COCO 2017 object detection dataset and achieves an average precision (AP) of 44.9 on the COCO 2017 validation set.\n created at: 2022-03-02\n- model_name: hustvl/yolos-small\n  description: YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Fang et al. and first released in this repository. YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN).\n created at: 2022-04-26\n- model_name: keremberke/yolov8m-hard-hat-detection\n  description: A YOLOv8 model for detecting hard hats in images. The model can distinguish between 'Hardhat' and 'NO-Hardhat' classes. It can be used to ensure safety compliance in construction sites or other industrial environments where hard hats are required.\n created at: 2023-01-29\n- model_name: keremberke/yolov5m-license-plate\n  description: A YOLOv5 model for license plate detection trained on a custom dataset. The model can detect license plates in images with high accuracy.\n created at: 2023-01-01\n- model_name: keremberke/yolov8s-table-extraction\n  description: A YOLOv8 model for table extraction in documents, capable of detecting bordered and borderless tables. Trained on the table-extraction dataset, the model achieves a mAP@0.5 of 0.984 on the validation set.\n created at: 2023-01-29\n- model_name: keremberke/yolov8m-csgo-player-detection\n  description: An object detection model trained to detect Counter-Strike: Global Offensive (CS:GO) players. The model is based on the YOLOv8 architecture and can identify 'ct', 'cthead', 't', and 'thead' labels.\n created at: 2023-01-29\n- model_name: keremberke/yolov8m-nlf-head-detection\n  description: A YOLOv8 model trained for head detection in American football. The model is capable of detecting helmets, blurred helmets, difficult helmets, partial helmets, and sideline helmets.\n created at: 2023-01-29\n- model_name: keremberke/yolov8m-forklift-detection\n  description: A YOLOv8 model for detecting forklifts and persons in images.\n created at: 2023-01-22\n- model_name: google/owlvit-large-patch14\n  description: OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. It uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. OWL-ViT is trained on publicly available image-caption data and fine-tuned on publicly available object detection datasets such as COCO and OpenImages.\n created at: 2022-07-05\n- model_name: keremberke/yolov8m-plane-detection\n  description: A YOLOv8 model for plane detection trained on the keremberke/plane-detection dataset. The model is capable of detecting planes in images with high accuracy.\n created at: 2023-01-29\n- model_name: keremberke/yolov8s-csgo-player-detection\n  description: A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players. Supports the labels ['ct', 'cthead', 't', 'thead'].\n created at: 2023-01-29\n- model_name: fcakyon/yolov5s-v7.0\n  description: Yolov5s-v7.0 is an object detection model trained on the COCO dataset. It can detect objects in images and return their bounding boxes, scores, and categories.\n created at: 2022-12-13\n- model_name: keremberke/yolov8s-hard-hat-detection\n  description: An object detection model trained to detect hard hats and no-hard hats in images. The model is based on YOLOv8 architecture and can be used for safety applications.\n created at: 2023-01-29\n- model_name: keremberke/yolov8m-blood-cell-detection\n  description: A YOLOv8 model for blood cell detection, including Platelets, RBC, and WBC. Trained on the blood-cell-object-detection dataset.\n created at: 2023-01-29\n- model_name: keremberke/yolov8n-table-extraction\n  description: An object detection model for extracting tables from documents. Supports two label types: 'bordered' and 'borderless'.\n created at: 2023-01-29\n- model_name: keremberke/yolov8n-csgo-player-detection\n  description: A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players with supported labels: ['ct', 'cthead', 't', 'thead'].\n created at: 2023-01-29\n- model_name: keremberke/yolov5s-license-plate\n  description: A YOLOv5 based license plate detection model trained on a custom dataset.\n created at: 2023-01-01\n- model_name: keremberke/yolov8n-blood-cell-detection\n  description: This model detects blood cells in images, specifically Platelets, RBC, and WBC. It is based on the YOLOv8 architecture and trained on the blood-cell-object-detection dataset.\n created at: 2023-01-29\n- model_name: google/owlvit-base-patch32\n  description: OWL-ViT is a zero-shot text-conditioned object detection model that uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. The model can be used to query an image with one or multiple text queries.\n created at: 2022-07-05\n- model_name: keremberke/yolov8m-valorant-detection\n  description: A YOLOv8 model for object detection in Valorant game, trained on a custom dataset. It detects dropped spike, enemy, planted spike, and teammate objects.\n created at: 2023-01-28\n- model_name: google/owlvit-base-patch16\n  description: OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. OWL-ViT uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features.\n created at: 2022-07-05\n- model_name: google/openimages_v4/ssd/mobilenet_v2\n  description: A pre-trained image object detection model based on the SSD architecture with MobileNetV2 as the backbone. The model is capable of detecting objects in images and returning their class labels and bounding box coordinates.\n created at: 2020-10-06\n- model_name: google/faster_rcnn/openimages_v4/inception_resnet_v2\n  description: Detects objects in images using the Faster R-CNN model with Inception-ResNet V2 backbone trained on the OpenImages V4 dataset.\n created at: 2020-10-06\n- model_name: tensorflow/ssd_mobilenet_v2\n  description: This API provides a pre-trained object detection model using TensorFlow Hub. The model is based on the SSD MobileNet V2 architecture and is trained on the COCO dataset. It can be used to detect multiple objects in an image and returns their class, bounding box coordinates, and confidence scores.\n created at: 2020-10-06\n- model_name: tensorflow/efficientdet/lite0/detection\n  description: A collection of TensorFlow Hub models for detecting objects in images using the EfficientDet architecture. These models can be used for various tasks such as object detection, instance segmentation, and more.\n created at: 2020-10-06\n- model_name: tensorflow/efficientdet/lite2/detection\n  description: This API allows for object detection in images using TensorFlow Hub and the EfficientDet model. It can be used to load a pre-trained model and detect objects in a given image.\n created at: 2020-10-06\n- model_name: tensorflow/centernet/resnet50v1_fpn_512x512\n  description: Detect objects in images using TensorFlow Hub and a pre-trained CenterNet model with ResNet50V1 FPN backbone\n created at: 2020-10-06\n- model_name: tensorflow/efficientdet/d0\n  description: Load a pre-trained model from TensorFlow Hub for object detection in images.\n created at: 2020-10-06\n- model_name: tensorflow/centernet/hourglass_512x512\n  description: A pre-trained TensorFlow Hub model for detecting objects in images using the CenterNet Hourglass architecture\n created at: 2020-10-06\n- model_name: tensorflow/faster_rcnn/resnet50_v1_640x640\n  description: A pre-trained Faster R-CNN model with ResNet-50 backbone for object detection in images\n created at: 2020-10-06\n- model_name: tensorflow/ssd_mobilenet_v2/fpnlite_640x640\n  description: A pre-trained object detection model based on the SSD Mobilenet V2 architecture, capable of detecting multiple objects in an image. The model is trained on the COCO dataset and achieves a mean Average Precision (mAP) of 0.32.\n created at: 2020-10-06\n- model_name: tensorflow/centernet/hourglass_1024x1024\n  description: A pre-trained TensorFlow Hub model for detecting objects in images using the CenterNet architecture with an Hourglass backbone.\n created at: 2020-10-06\n- model_name: hustvl/yolop/yolop\n  description: YOLOP is an efficient multi-task network that can jointly handle three crucial tasks in autonomous driving: object detection, drivable area segmentation and lane detection. And it is also the first to reach real-time on embedded devices while maintaining state-of-the-art level performance on the BDD100K dataset.\n created at: 2021-08-25\n- model_name: datvuthanh/hybridnets/hybridnets\n  description: HybridNets is an end2end perception network for multi-tasks. Our work focused on traffic object detection, drivable area segmentation and lane detection. HybridNets can run real-time on embedded systems, and obtains SOTA Object Detection, Lane Detection on BDD100K Dataset.\n created at: 2021-12-23\n- model_name: ultralytics/yolov5/yolov5s\n  description: YOLOv5 is a family of compound-scaled object detection models trained on the COCO dataset, and includes simple functionality for Test Time Augmentation (TTA), model ensembling, hyperparameter evolution, and export to ONNX, CoreML and TFLite.\n created at: 2020-05-18\n- model_name: NVIDIA/DeepLearningExamples/nvidia_ssd\n  description: The SSD (Single Shot MultiBox Detector) model is an object detection model based on the paper 'SSD: Single Shot MultiBox Detector'. It uses a deep neural network for detecting objects in images. This implementation replaces the obsolete VGG model backbone with the more modern ResNet-50 model. The SSD model is trained on the COCO dataset and can be used to detect objects in images with high accuracy and efficiency.\n created at: 2018-05-02\n- model_name: valentinafeve/yolos-fashionpedia\n  description: None\n created at: 2022-11-17\n- model_name: microsoft/conditional-detr-resnet-50\n  description: Conditional DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper [Conditional DETR for Fast Training Convergence](https://arxiv.org/abs/2108.06152) by Meng et al. and first released in [this repository](https://github.com/Atten4Vis/ConditionalDETR).\n created at: 2022-09-09\n- model_name: Yifeng-Liu/rt-detr-finetuned-for-satellite-image-roofs-detection\n  description: roof detection for remote sensing task. - model type: object detection for remote sensing task. - license: mit - model type: object detection for remote sensing task. - license: mit\n created at: 2024-09-03T08:30:42+00:00\n- model_name: devxyasir/florence-finetuned-license-plate-detection\n  description: this model is a fine-tuned version of microsoft's florence-2 large `florence2-base-ft` adapted for automatic number plate detection and recognition . it processes vehicle images to localize number plates with bounding boxes and applies ocr to extract the license plate text, enabling high-accuracy license plate reading. the model leverages transformer-based vision-language architectures and is trained on a custom dataset of vehicle images with annotated license plates.\n created at: 2025-01-27T05:49:01+00:00\n- model_name: SimulaMet-HOST/HockeyAI\n  description: üîó this model is trained on the hockeyai dataset. - üìä access the dataset used for training here: - üöÄ try the model in action with our interactive hugging face space:\n created at: 2025-01-09T09:23:25+00:00\n- model_name: jameslahm/yoloe\n  description: official pytorch implementation of yoloe . comparison of performance, training cost, and inference efficiency between yoloe ours and yolo-worldv2 in terms of open text prompts. .\\\n created at: 2025-03-10T07:54:02+00:00\n- model_name: PekingU/rtdetr_v2_r18vd\n  description: the rt-detrv2 model was proposed in by wenyu lv, yian zhao, qinyao chang, kui huang, guanzhong wang, yi liu. rt-detrv2 refines rt-detr by introducing selective multi-scale feature extraction, a discrete sampling operator for broader deployment compatibility, and improved training strategies like dynamic data augmentation and scale-adaptive hyperparameters. these changes enhance flexibility and practicality while maintaining real-time performance. this model was contributed by with the help of and\n created at: 2025-01-31T17:10:44+00:00\n- model_name: ustc-community/dfine-xlarge-coco\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-02-06T15:36:03+00:00\n- model_name: ustc-community/dfine-xlarge-obj365\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-03-28T12:55:40+00:00\n- model_name: PekingU/rtdetr_v2_r101vd\n  description: the rt-detrv2 model was proposed in by wenyu lv, yian zhao, qinyao chang, kui huang, guanzhong wang, yi liu. rt-detrv2 refines rt-detr by introducing selective multi-scale feature extraction, a discrete sampling operator for broader deployment compatibility, and improved training strategies like dynamic data augmentation and scale-adaptive hyperparameters. these changes enhance flexibility and practicality while maintaining real-time performance. this model was contributed by with the help of and\n created at: 2025-01-31T18:14:53+00:00\n- model_name: PekingU/rtdetr_v2_r50vd\n  description: the rt-detrv2 model was proposed in by wenyu lv, yian zhao, qinyao chang, kui huang, guanzhong wang, yi liu. rt-detrv2 refines rt-detr by introducing selective multi-scale feature extraction, a discrete sampling operator for broader deployment compatibility, and improved training strategies like dynamic data augmentation and scale-adaptive hyperparameters. these changes enhance flexibility and practicality while maintaining real-time performance. this model was contributed by with the help of and\n created at: 2025-01-31T18:14:46+00:00\n- model_name: ustc-community/dfine-large-coco\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-02-22T12:04:35+00:00\n- model_name: ustc-community/dfine-nano-coco\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-03-28T12:48:09+00:00\n- model_name: apkonsta/table-transformer-detection-ifrs\n  description: this repository contains a fine-tuned version of the table transformer model, specifically adapted for detecting tables in ifrs international financial reporting standards pdfs. the model is based on the table transformer architecture, which is designed to extract tables from unstructured documents such as pdfs and images. base model: microsoft/table-transformer-detection library: transformers\n created at: 2024-11-24T18:13:18+00:00\n- model_name: mdefrance/yolos-tiny-signature-detection\n  description: yolos model finetuned to detect handwritten signatures in document images using dataset. original yolos was introduced in the paper by fang et al. and first released in . yolos is a vision transformer vit trained using the detr loss. despite its simplicity, a base-sized yolos model is able to achieve 42 ap on coco validation 2017 similar to detr and more complex frameworks such as faster r-cnn. yolos is a vision transformer vit trained using the detr loss. despite its simplicity, a base-sized yolos model is able to achieve 42 ap on coco validation 2017 similar to detr and more complex frameworks such as faster r-cnn. - finetuned by:  - repository:  - model type:  - license: apache 2.0 license - finetuned from model\n created at: 2025-05-06T14:15:22+00:00\n- model_name: ustc-community/dfine-small-coco\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-02-11T14:30:13+00:00\n- model_name: Adit-jain/soccana\n  description: this repository hosts a yolov8-based object detection model trained on a curated and segmented dataset derived from soccernet and other public football datasets. the model is designed to detect key entities in a football game players , referees , and the ball with high accuracy, even in challenging scenes.  - architecture : yolov8n + sahi sliced aided hyper inference\n created at: 2025-07-26T10:43:57+00:00\n- model_name: HuggingPanda/docling-layout\n  description: this is the docling model for layout detection , designed to facilitate easy importing and usage like any other hugging face model. this model is part of the , which provides document layout analysis tools. here's how you can load and use the model:\n created at: 2025-03-13T11:39:16+00:00\n- model_name: yifeihu/Florence-2-DocLayNet-Fixed\n  description: we finetuned the florence-2-large-ft hf model using the doclaynet-v1.1 dataset. to prevent the model from generating hallucinated class names, we re-mapped all class names to single tokens: original class names new class names\n created at: 2024-10-29T17:23:16+00:00\n- model_name: ustc-community/dfine-large-obj365\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-03-28T13:00:47+00:00\n- model_name: Ultralytics/YOLOv5\n  description: yolov5 üöÄ is the world's most loved vision ai, representing ultralytics open-source research into future vision ai methods, incorporating lessons learned and best practices evolved over thousands of hours of research and development. we hope that the resources here will help you get the most out of yolov5. please browse the yolov5 docs for details, raise an issue on github for support, and join our discord community for questions and discussions! to request an enterprise license please complete the form at . yolo11 üöÄ new\n created at: 2024-11-14T09:33:32+00:00\n- model_name: tech4humans/conditional-detr-50-signature-detector\n  description: this repository presents a conditional-detr model with resnet-50 backbone, fine-tuned to detect handwritten signatures in document images. this model achieved the highest map@0.5 93.65% among all tested architectures in our comprehensive evaluation. resource links / badges details\n created at: 2025-06-18T03:39:12+00:00\n\nExamples:\n- example user query: I would like to know if there are any cats or dogs in the photos taken in my friend's backyard.\n  model_name: tensorflow/ssd_mobilenet_v2\n- example user query: I recently installed a new security camera. I need to identify the objects in the recorded video.\n  model_name: hustvl/yolos-tiny\n- example user query: I want to monitor if any car accidents happen on the street by detecting the position of vehicles and pedestrians from CCTV footage.\n  model_name: google/faster_rcnn/openimages_v4/inception_resnet_v2\n- example user query: I want to create an app which detects the license plates of any parked vehicle in my city.\n  model_name: keremberke/yolov5m-license-plate\n- example user query: Your advertising company is looking for a solution that will help them detect objects in images as part of their marketing campaigns.\n  model_name: fcakyon/yolov5s-v7.0\n- example user query: I own a pathology lab and I want to scan blood cell images using a computer vision model. How do I detect blood cells in images using an AI model?\n  model_name: keremberke/yolov8m-blood-cell-detection\n\nUser prompt:\nReveal every part in the photo.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_det>\n\nCandidate Models:\n- model_name: microsoft/table-transformer-structure-recognition\n  description: Table Transformer (DETR) model trained on PubTables1M for detecting the structure (like rows, columns) in tables.\n created at: 2022-10-14\n- model_name: microsoft/table-transformer-detection\n  description: Table Transformer (DETR) model trained on PubTables1M for detecting tables in documents. Introduced in the paper PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents by Smock et al.\n created at: 2022-10-14\n- model_name: facebook/detr-resnet-50\n  description: DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: hustvl/yolos-tiny\n  description: YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN). The model is trained using a bipartite matching loss: one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a no object as class and no bounding box as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.\n created at: 2022-04-26\n- model_name: TahaDouaji/detr-doc-table-detection\n  description: detr-doc-table-detection is a model trained to detect both Bordered and Borderless tables in documents, based on facebook/detr-resnet-50.\n created at: 2022-03-11\n- model_name: facebook/detr-resnet-101\n  description: DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: keremberke/yolov8m-table-extraction\n  description: A YOLOv8 model for table extraction in images, capable of detecting both bordered and borderless tables. Trained using the keremberke/table-extraction dataset.\n created at: 2023-01-29\n- model_name: SenseTime/deformable-detr\n  description: Deformable DETR model with ResNet-50 backbone trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper Deformable DETR: Deformable Transformers for End-to-End Object Detection by Zhu et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: facebook/detr-resnet-101-dc5\n  description: DETR (End-to-End Object Detection) model with ResNet-101 backbone (dilated C5 stage). The model is trained on COCO 2017 object detection dataset and achieves an average precision (AP) of 44.9 on the COCO 2017 validation set.\n created at: 2022-03-02\n- model_name: hustvl/yolos-small\n  description: YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Fang et al. and first released in this repository. YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN).\n created at: 2022-04-26\n- model_name: keremberke/yolov8m-hard-hat-detection\n  description: A YOLOv8 model for detecting hard hats in images. The model can distinguish between 'Hardhat' and 'NO-Hardhat' classes. It can be used to ensure safety compliance in construction sites or other industrial environments where hard hats are required.\n created at: 2023-01-29\n- model_name: keremberke/yolov5m-license-plate\n  description: A YOLOv5 model for license plate detection trained on a custom dataset. The model can detect license plates in images with high accuracy.\n created at: 2023-01-01\n- model_name: keremberke/yolov8s-table-extraction\n  description: A YOLOv8 model for table extraction in documents, capable of detecting bordered and borderless tables. Trained on the table-extraction dataset, the model achieves a mAP@0.5 of 0.984 on the validation set.\n created at: 2023-01-29\n- model_name: keremberke/yolov8m-csgo-player-detection\n  description: An object detection model trained to detect Counter-Strike: Global Offensive (CS:GO) players. The model is based on the YOLOv8 architecture and can identify 'ct', 'cthead', 't', and 'thead' labels.\n created at: 2023-01-29\n- model_name: keremberke/yolov8m-nlf-head-detection\n  description: A YOLOv8 model trained for head detection in American football. The model is capable of detecting helmets, blurred helmets, difficult helmets, partial helmets, and sideline helmets.\n created at: 2023-01-29\n- model_name: keremberke/yolov8m-forklift-detection\n  description: A YOLOv8 model for detecting forklifts and persons in images.\n created at: 2023-01-22\n- model_name: google/owlvit-large-patch14\n  description: OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. It uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. OWL-ViT is trained on publicly available image-caption data and fine-tuned on publicly available object detection datasets such as COCO and OpenImages.\n created at: 2022-07-05\n- model_name: keremberke/yolov8m-plane-detection\n  description: A YOLOv8 model for plane detection trained on the keremberke/plane-detection dataset. The model is capable of detecting planes in images with high accuracy.\n created at: 2023-01-29\n- model_name: keremberke/yolov8s-csgo-player-detection\n  description: A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players. Supports the labels ['ct', 'cthead', 't', 'thead'].\n created at: 2023-01-29\n- model_name: fcakyon/yolov5s-v7.0\n  description: Yolov5s-v7.0 is an object detection model trained on the COCO dataset. It can detect objects in images and return their bounding boxes, scores, and categories.\n created at: 2022-12-13\n- model_name: keremberke/yolov8s-hard-hat-detection\n  description: An object detection model trained to detect hard hats and no-hard hats in images. The model is based on YOLOv8 architecture and can be used for safety applications.\n created at: 2023-01-29\n- model_name: keremberke/yolov8m-blood-cell-detection\n  description: A YOLOv8 model for blood cell detection, including Platelets, RBC, and WBC. Trained on the blood-cell-object-detection dataset.\n created at: 2023-01-29\n- model_name: keremberke/yolov8n-table-extraction\n  description: An object detection model for extracting tables from documents. Supports two label types: 'bordered' and 'borderless'.\n created at: 2023-01-29\n- model_name: keremberke/yolov8n-csgo-player-detection\n  description: A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players with supported labels: ['ct', 'cthead', 't', 'thead'].\n created at: 2023-01-29\n- model_name: keremberke/yolov5s-license-plate\n  description: A YOLOv5 based license plate detection model trained on a custom dataset.\n created at: 2023-01-01\n- model_name: keremberke/yolov8n-blood-cell-detection\n  description: This model detects blood cells in images, specifically Platelets, RBC, and WBC. It is based on the YOLOv8 architecture and trained on the blood-cell-object-detection dataset.\n created at: 2023-01-29\n- model_name: google/owlvit-base-patch32\n  description: OWL-ViT is a zero-shot text-conditioned object detection model that uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. The model can be used to query an image with one or multiple text queries.\n created at: 2022-07-05\n- model_name: keremberke/yolov8m-valorant-detection\n  description: A YOLOv8 model for object detection in Valorant game, trained on a custom dataset. It detects dropped spike, enemy, planted spike, and teammate objects.\n created at: 2023-01-28\n- model_name: google/owlvit-base-patch16\n  description: OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. OWL-ViT uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features.\n created at: 2022-07-05\n- model_name: google/openimages_v4/ssd/mobilenet_v2\n  description: A pre-trained image object detection model based on the SSD architecture with MobileNetV2 as the backbone. The model is capable of detecting objects in images and returning their class labels and bounding box coordinates.\n created at: 2020-10-06\n- model_name: google/faster_rcnn/openimages_v4/inception_resnet_v2\n  description: Detects objects in images using the Faster R-CNN model with Inception-ResNet V2 backbone trained on the OpenImages V4 dataset.\n created at: 2020-10-06\n- model_name: tensorflow/ssd_mobilenet_v2\n  description: This API provides a pre-trained object detection model using TensorFlow Hub. The model is based on the SSD MobileNet V2 architecture and is trained on the COCO dataset. It can be used to detect multiple objects in an image and returns their class, bounding box coordinates, and confidence scores.\n created at: 2020-10-06\n- model_name: tensorflow/efficientdet/lite0/detection\n  description: A collection of TensorFlow Hub models for detecting objects in images using the EfficientDet architecture. These models can be used for various tasks such as object detection, instance segmentation, and more.\n created at: 2020-10-06\n- model_name: tensorflow/efficientdet/lite2/detection\n  description: This API allows for object detection in images using TensorFlow Hub and the EfficientDet model. It can be used to load a pre-trained model and detect objects in a given image.\n created at: 2020-10-06\n- model_name: tensorflow/centernet/resnet50v1_fpn_512x512\n  description: Detect objects in images using TensorFlow Hub and a pre-trained CenterNet model with ResNet50V1 FPN backbone\n created at: 2020-10-06\n- model_name: tensorflow/efficientdet/d0\n  description: Load a pre-trained model from TensorFlow Hub for object detection in images.\n created at: 2020-10-06\n- model_name: tensorflow/centernet/hourglass_512x512\n  description: A pre-trained TensorFlow Hub model for detecting objects in images using the CenterNet Hourglass architecture\n created at: 2020-10-06\n- model_name: tensorflow/faster_rcnn/resnet50_v1_640x640\n  description: A pre-trained Faster R-CNN model with ResNet-50 backbone for object detection in images\n created at: 2020-10-06\n- model_name: tensorflow/ssd_mobilenet_v2/fpnlite_640x640\n  description: A pre-trained object detection model based on the SSD Mobilenet V2 architecture, capable of detecting multiple objects in an image. The model is trained on the COCO dataset and achieves a mean Average Precision (mAP) of 0.32.\n created at: 2020-10-06\n- model_name: tensorflow/centernet/hourglass_1024x1024\n  description: A pre-trained TensorFlow Hub model for detecting objects in images using the CenterNet architecture with an Hourglass backbone.\n created at: 2020-10-06\n- model_name: hustvl/yolop/yolop\n  description: YOLOP is an efficient multi-task network that can jointly handle three crucial tasks in autonomous driving: object detection, drivable area segmentation and lane detection. And it is also the first to reach real-time on embedded devices while maintaining state-of-the-art level performance on the BDD100K dataset.\n created at: 2021-08-25\n- model_name: datvuthanh/hybridnets/hybridnets\n  description: HybridNets is an end2end perception network for multi-tasks. Our work focused on traffic object detection, drivable area segmentation and lane detection. HybridNets can run real-time on embedded systems, and obtains SOTA Object Detection, Lane Detection on BDD100K Dataset.\n created at: 2021-12-23\n- model_name: ultralytics/yolov5/yolov5s\n  description: YOLOv5 is a family of compound-scaled object detection models trained on the COCO dataset, and includes simple functionality for Test Time Augmentation (TTA), model ensembling, hyperparameter evolution, and export to ONNX, CoreML and TFLite.\n created at: 2020-05-18\n- model_name: NVIDIA/DeepLearningExamples/nvidia_ssd\n  description: The SSD (Single Shot MultiBox Detector) model is an object detection model based on the paper 'SSD: Single Shot MultiBox Detector'. It uses a deep neural network for detecting objects in images. This implementation replaces the obsolete VGG model backbone with the more modern ResNet-50 model. The SSD model is trained on the COCO dataset and can be used to detect objects in images with high accuracy and efficiency.\n created at: 2018-05-02\n- model_name: valentinafeve/yolos-fashionpedia\n  description: None\n created at: 2022-11-17\n- model_name: microsoft/conditional-detr-resnet-50\n  description: Conditional DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper [Conditional DETR for Fast Training Convergence](https://arxiv.org/abs/2108.06152) by Meng et al. and first released in [this repository](https://github.com/Atten4Vis/ConditionalDETR).\n created at: 2022-09-09\n- model_name: Yifeng-Liu/rt-detr-finetuned-for-satellite-image-roofs-detection\n  description: roof detection for remote sensing task. - model type: object detection for remote sensing task. - license: mit - model type: object detection for remote sensing task. - license: mit\n created at: 2024-09-03T08:30:42+00:00\n- model_name: devxyasir/florence-finetuned-license-plate-detection\n  description: this model is a fine-tuned version of microsoft's florence-2 large `florence2-base-ft` adapted for automatic number plate detection and recognition . it processes vehicle images to localize number plates with bounding boxes and applies ocr to extract the license plate text, enabling high-accuracy license plate reading. the model leverages transformer-based vision-language architectures and is trained on a custom dataset of vehicle images with annotated license plates.\n created at: 2025-01-27T05:49:01+00:00\n- model_name: SimulaMet-HOST/HockeyAI\n  description: üîó this model is trained on the hockeyai dataset. - üìä access the dataset used for training here: - üöÄ try the model in action with our interactive hugging face space:\n created at: 2025-01-09T09:23:25+00:00\n- model_name: jameslahm/yoloe\n  description: official pytorch implementation of yoloe . comparison of performance, training cost, and inference efficiency between yoloe ours and yolo-worldv2 in terms of open text prompts. .\\\n created at: 2025-03-10T07:54:02+00:00\n- model_name: PekingU/rtdetr_v2_r18vd\n  description: the rt-detrv2 model was proposed in by wenyu lv, yian zhao, qinyao chang, kui huang, guanzhong wang, yi liu. rt-detrv2 refines rt-detr by introducing selective multi-scale feature extraction, a discrete sampling operator for broader deployment compatibility, and improved training strategies like dynamic data augmentation and scale-adaptive hyperparameters. these changes enhance flexibility and practicality while maintaining real-time performance. this model was contributed by with the help of and\n created at: 2025-01-31T17:10:44+00:00\n- model_name: ustc-community/dfine-xlarge-coco\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-02-06T15:36:03+00:00\n- model_name: ustc-community/dfine-xlarge-obj365\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-03-28T12:55:40+00:00\n- model_name: PekingU/rtdetr_v2_r101vd\n  description: the rt-detrv2 model was proposed in by wenyu lv, yian zhao, qinyao chang, kui huang, guanzhong wang, yi liu. rt-detrv2 refines rt-detr by introducing selective multi-scale feature extraction, a discrete sampling operator for broader deployment compatibility, and improved training strategies like dynamic data augmentation and scale-adaptive hyperparameters. these changes enhance flexibility and practicality while maintaining real-time performance. this model was contributed by with the help of and\n created at: 2025-01-31T18:14:53+00:00\n- model_name: PekingU/rtdetr_v2_r50vd\n  description: the rt-detrv2 model was proposed in by wenyu lv, yian zhao, qinyao chang, kui huang, guanzhong wang, yi liu. rt-detrv2 refines rt-detr by introducing selective multi-scale feature extraction, a discrete sampling operator for broader deployment compatibility, and improved training strategies like dynamic data augmentation and scale-adaptive hyperparameters. these changes enhance flexibility and practicality while maintaining real-time performance. this model was contributed by with the help of and\n created at: 2025-01-31T18:14:46+00:00\n- model_name: ustc-community/dfine-large-coco\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-02-22T12:04:35+00:00\n- model_name: ustc-community/dfine-nano-coco\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-03-28T12:48:09+00:00\n- model_name: apkonsta/table-transformer-detection-ifrs\n  description: this repository contains a fine-tuned version of the table transformer model, specifically adapted for detecting tables in ifrs international financial reporting standards pdfs. the model is based on the table transformer architecture, which is designed to extract tables from unstructured documents such as pdfs and images. base model: microsoft/table-transformer-detection library: transformers\n created at: 2024-11-24T18:13:18+00:00\n- model_name: mdefrance/yolos-tiny-signature-detection\n  description: yolos model finetuned to detect handwritten signatures in document images using dataset. original yolos was introduced in the paper by fang et al. and first released in . yolos is a vision transformer vit trained using the detr loss. despite its simplicity, a base-sized yolos model is able to achieve 42 ap on coco validation 2017 similar to detr and more complex frameworks such as faster r-cnn. yolos is a vision transformer vit trained using the detr loss. despite its simplicity, a base-sized yolos model is able to achieve 42 ap on coco validation 2017 similar to detr and more complex frameworks such as faster r-cnn. - finetuned by:  - repository:  - model type:  - license: apache 2.0 license - finetuned from model\n created at: 2025-05-06T14:15:22+00:00\n- model_name: ustc-community/dfine-small-coco\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-02-11T14:30:13+00:00\n- model_name: Adit-jain/soccana\n  description: this repository hosts a yolov8-based object detection model trained on a curated and segmented dataset derived from soccernet and other public football datasets. the model is designed to detect key entities in a football game players , referees , and the ball with high accuracy, even in challenging scenes.  - architecture : yolov8n + sahi sliced aided hyper inference\n created at: 2025-07-26T10:43:57+00:00\n- model_name: HuggingPanda/docling-layout\n  description: this is the docling model for layout detection , designed to facilitate easy importing and usage like any other hugging face model. this model is part of the , which provides document layout analysis tools. here's how you can load and use the model:\n created at: 2025-03-13T11:39:16+00:00\n- model_name: yifeihu/Florence-2-DocLayNet-Fixed\n  description: we finetuned the florence-2-large-ft hf model using the doclaynet-v1.1 dataset. to prevent the model from generating hallucinated class names, we re-mapped all class names to single tokens: original class names new class names\n created at: 2024-10-29T17:23:16+00:00\n- model_name: ustc-community/dfine-large-obj365\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-03-28T13:00:47+00:00\n- model_name: Ultralytics/YOLOv5\n  description: yolov5 üöÄ is the world's most loved vision ai, representing ultralytics open-source research into future vision ai methods, incorporating lessons learned and best practices evolved over thousands of hours of research and development. we hope that the resources here will help you get the most out of yolov5. please browse the yolov5 docs for details, raise an issue on github for support, and join our discord community for questions and discussions! to request an enterprise license please complete the form at . yolo11 üöÄ new\n created at: 2024-11-14T09:33:32+00:00\n- model_name: tech4humans/conditional-detr-50-signature-detector\n  description: this repository presents a conditional-detr model with resnet-50 backbone, fine-tuned to detect handwritten signatures in document images. this model achieved the highest map@0.5 93.65% among all tested architectures in our comprehensive evaluation. resource links / badges details\n created at: 2025-06-18T03:39:12+00:00\n\nExamples:\n- example user query: I would like to know if there are any cats or dogs in the photos taken in my friend's backyard.\n  model_name: tensorflow/ssd_mobilenet_v2\n- example user query: I recently installed a new security camera. I need to identify the objects in the recorded video.\n  model_name: hustvl/yolos-tiny\n- example user query: I want to monitor if any car accidents happen on the street by detecting the position of vehicles and pedestrians from CCTV footage.\n  model_name: google/faster_rcnn/openimages_v4/inception_resnet_v2\n- example user query: I want to create an app which detects the license plates of any parked vehicle in my city.\n  model_name: keremberke/yolov5m-license-plate\n- example user query: Your advertising company is looking for a solution that will help them detect objects in images as part of their marketing campaigns.\n  model_name: fcakyon/yolov5s-v7.0\n- example user query: I own a pathology lab and I want to scan blood cell images using a computer vision model. How do I detect blood cells in images using an AI model?\n  model_name: keremberke/yolov8m-blood-cell-detection\n\nUser prompt:\nI desire you to reveal all elements within this illustration.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_det>\n\nCandidate Models:\n- model_name: microsoft/table-transformer-structure-recognition\n  description: Table Transformer (DETR) model trained on PubTables1M for detecting the structure (like rows, columns) in tables.\n created at: 2022-10-14\n- model_name: microsoft/table-transformer-detection\n  description: Table Transformer (DETR) model trained on PubTables1M for detecting tables in documents. Introduced in the paper PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents by Smock et al.\n created at: 2022-10-14\n- model_name: facebook/detr-resnet-50\n  description: DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: hustvl/yolos-tiny\n  description: YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN). The model is trained using a bipartite matching loss: one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a no object as class and no bounding box as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.\n created at: 2022-04-26\n- model_name: TahaDouaji/detr-doc-table-detection\n  description: detr-doc-table-detection is a model trained to detect both Bordered and Borderless tables in documents, based on facebook/detr-resnet-50.\n created at: 2022-03-11\n- model_name: facebook/detr-resnet-101\n  description: DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: keremberke/yolov8m-table-extraction\n  description: A YOLOv8 model for table extraction in images, capable of detecting both bordered and borderless tables. Trained using the keremberke/table-extraction dataset.\n created at: 2023-01-29\n- model_name: SenseTime/deformable-detr\n  description: Deformable DETR model with ResNet-50 backbone trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper Deformable DETR: Deformable Transformers for End-to-End Object Detection by Zhu et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: facebook/detr-resnet-101-dc5\n  description: DETR (End-to-End Object Detection) model with ResNet-101 backbone (dilated C5 stage). The model is trained on COCO 2017 object detection dataset and achieves an average precision (AP) of 44.9 on the COCO 2017 validation set.\n created at: 2022-03-02\n- model_name: hustvl/yolos-small\n  description: YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Fang et al. and first released in this repository. YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN).\n created at: 2022-04-26\n- model_name: keremberke/yolov8m-hard-hat-detection\n  description: A YOLOv8 model for detecting hard hats in images. The model can distinguish between 'Hardhat' and 'NO-Hardhat' classes. It can be used to ensure safety compliance in construction sites or other industrial environments where hard hats are required.\n created at: 2023-01-29\n- model_name: keremberke/yolov5m-license-plate\n  description: A YOLOv5 model for license plate detection trained on a custom dataset. The model can detect license plates in images with high accuracy.\n created at: 2023-01-01\n- model_name: keremberke/yolov8s-table-extraction\n  description: A YOLOv8 model for table extraction in documents, capable of detecting bordered and borderless tables. Trained on the table-extraction dataset, the model achieves a mAP@0.5 of 0.984 on the validation set.\n created at: 2023-01-29\n- model_name: keremberke/yolov8m-csgo-player-detection\n  description: An object detection model trained to detect Counter-Strike: Global Offensive (CS:GO) players. The model is based on the YOLOv8 architecture and can identify 'ct', 'cthead', 't', and 'thead' labels.\n created at: 2023-01-29\n- model_name: keremberke/yolov8m-nlf-head-detection\n  description: A YOLOv8 model trained for head detection in American football. The model is capable of detecting helmets, blurred helmets, difficult helmets, partial helmets, and sideline helmets.\n created at: 2023-01-29\n- model_name: keremberke/yolov8m-forklift-detection\n  description: A YOLOv8 model for detecting forklifts and persons in images.\n created at: 2023-01-22\n- model_name: google/owlvit-large-patch14\n  description: OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. It uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. OWL-ViT is trained on publicly available image-caption data and fine-tuned on publicly available object detection datasets such as COCO and OpenImages.\n created at: 2022-07-05\n- model_name: keremberke/yolov8m-plane-detection\n  description: A YOLOv8 model for plane detection trained on the keremberke/plane-detection dataset. The model is capable of detecting planes in images with high accuracy.\n created at: 2023-01-29\n- model_name: keremberke/yolov8s-csgo-player-detection\n  description: A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players. Supports the labels ['ct', 'cthead', 't', 'thead'].\n created at: 2023-01-29\n- model_name: fcakyon/yolov5s-v7.0\n  description: Yolov5s-v7.0 is an object detection model trained on the COCO dataset. It can detect objects in images and return their bounding boxes, scores, and categories.\n created at: 2022-12-13\n- model_name: keremberke/yolov8s-hard-hat-detection\n  description: An object detection model trained to detect hard hats and no-hard hats in images. The model is based on YOLOv8 architecture and can be used for safety applications.\n created at: 2023-01-29\n- model_name: keremberke/yolov8m-blood-cell-detection\n  description: A YOLOv8 model for blood cell detection, including Platelets, RBC, and WBC. Trained on the blood-cell-object-detection dataset.\n created at: 2023-01-29\n- model_name: keremberke/yolov8n-table-extraction\n  description: An object detection model for extracting tables from documents. Supports two label types: 'bordered' and 'borderless'.\n created at: 2023-01-29\n- model_name: keremberke/yolov8n-csgo-player-detection\n  description: A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players with supported labels: ['ct', 'cthead', 't', 'thead'].\n created at: 2023-01-29\n- model_name: keremberke/yolov5s-license-plate\n  description: A YOLOv5 based license plate detection model trained on a custom dataset.\n created at: 2023-01-01\n- model_name: keremberke/yolov8n-blood-cell-detection\n  description: This model detects blood cells in images, specifically Platelets, RBC, and WBC. It is based on the YOLOv8 architecture and trained on the blood-cell-object-detection dataset.\n created at: 2023-01-29\n- model_name: google/owlvit-base-patch32\n  description: OWL-ViT is a zero-shot text-conditioned object detection model that uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. The model can be used to query an image with one or multiple text queries.\n created at: 2022-07-05\n- model_name: keremberke/yolov8m-valorant-detection\n  description: A YOLOv8 model for object detection in Valorant game, trained on a custom dataset. It detects dropped spike, enemy, planted spike, and teammate objects.\n created at: 2023-01-28\n- model_name: google/owlvit-base-patch16\n  description: OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. OWL-ViT uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features.\n created at: 2022-07-05\n- model_name: google/openimages_v4/ssd/mobilenet_v2\n  description: A pre-trained image object detection model based on the SSD architecture with MobileNetV2 as the backbone. The model is capable of detecting objects in images and returning their class labels and bounding box coordinates.\n created at: 2020-10-06\n- model_name: google/faster_rcnn/openimages_v4/inception_resnet_v2\n  description: Detects objects in images using the Faster R-CNN model with Inception-ResNet V2 backbone trained on the OpenImages V4 dataset.\n created at: 2020-10-06\n- model_name: tensorflow/ssd_mobilenet_v2\n  description: This API provides a pre-trained object detection model using TensorFlow Hub. The model is based on the SSD MobileNet V2 architecture and is trained on the COCO dataset. It can be used to detect multiple objects in an image and returns their class, bounding box coordinates, and confidence scores.\n created at: 2020-10-06\n- model_name: tensorflow/efficientdet/lite0/detection\n  description: A collection of TensorFlow Hub models for detecting objects in images using the EfficientDet architecture. These models can be used for various tasks such as object detection, instance segmentation, and more.\n created at: 2020-10-06\n- model_name: tensorflow/efficientdet/lite2/detection\n  description: This API allows for object detection in images using TensorFlow Hub and the EfficientDet model. It can be used to load a pre-trained model and detect objects in a given image.\n created at: 2020-10-06\n- model_name: tensorflow/centernet/resnet50v1_fpn_512x512\n  description: Detect objects in images using TensorFlow Hub and a pre-trained CenterNet model with ResNet50V1 FPN backbone\n created at: 2020-10-06\n- model_name: tensorflow/efficientdet/d0\n  description: Load a pre-trained model from TensorFlow Hub for object detection in images.\n created at: 2020-10-06\n- model_name: tensorflow/centernet/hourglass_512x512\n  description: A pre-trained TensorFlow Hub model for detecting objects in images using the CenterNet Hourglass architecture\n created at: 2020-10-06\n- model_name: tensorflow/faster_rcnn/resnet50_v1_640x640\n  description: A pre-trained Faster R-CNN model with ResNet-50 backbone for object detection in images\n created at: 2020-10-06\n- model_name: tensorflow/ssd_mobilenet_v2/fpnlite_640x640\n  description: A pre-trained object detection model based on the SSD Mobilenet V2 architecture, capable of detecting multiple objects in an image. The model is trained on the COCO dataset and achieves a mean Average Precision (mAP) of 0.32.\n created at: 2020-10-06\n- model_name: tensorflow/centernet/hourglass_1024x1024\n  description: A pre-trained TensorFlow Hub model for detecting objects in images using the CenterNet architecture with an Hourglass backbone.\n created at: 2020-10-06\n- model_name: hustvl/yolop/yolop\n  description: YOLOP is an efficient multi-task network that can jointly handle three crucial tasks in autonomous driving: object detection, drivable area segmentation and lane detection. And it is also the first to reach real-time on embedded devices while maintaining state-of-the-art level performance on the BDD100K dataset.\n created at: 2021-08-25\n- model_name: datvuthanh/hybridnets/hybridnets\n  description: HybridNets is an end2end perception network for multi-tasks. Our work focused on traffic object detection, drivable area segmentation and lane detection. HybridNets can run real-time on embedded systems, and obtains SOTA Object Detection, Lane Detection on BDD100K Dataset.\n created at: 2021-12-23\n- model_name: ultralytics/yolov5/yolov5s\n  description: YOLOv5 is a family of compound-scaled object detection models trained on the COCO dataset, and includes simple functionality for Test Time Augmentation (TTA), model ensembling, hyperparameter evolution, and export to ONNX, CoreML and TFLite.\n created at: 2020-05-18\n- model_name: NVIDIA/DeepLearningExamples/nvidia_ssd\n  description: The SSD (Single Shot MultiBox Detector) model is an object detection model based on the paper 'SSD: Single Shot MultiBox Detector'. It uses a deep neural network for detecting objects in images. This implementation replaces the obsolete VGG model backbone with the more modern ResNet-50 model. The SSD model is trained on the COCO dataset and can be used to detect objects in images with high accuracy and efficiency.\n created at: 2018-05-02\n- model_name: valentinafeve/yolos-fashionpedia\n  description: None\n created at: 2022-11-17\n- model_name: microsoft/conditional-detr-resnet-50\n  description: Conditional DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper [Conditional DETR for Fast Training Convergence](https://arxiv.org/abs/2108.06152) by Meng et al. and first released in [this repository](https://github.com/Atten4Vis/ConditionalDETR).\n created at: 2022-09-09\n- model_name: Yifeng-Liu/rt-detr-finetuned-for-satellite-image-roofs-detection\n  description: roof detection for remote sensing task. - model type: object detection for remote sensing task. - license: mit - model type: object detection for remote sensing task. - license: mit\n created at: 2024-09-03T08:30:42+00:00\n- model_name: devxyasir/florence-finetuned-license-plate-detection\n  description: this model is a fine-tuned version of microsoft's florence-2 large `florence2-base-ft` adapted for automatic number plate detection and recognition . it processes vehicle images to localize number plates with bounding boxes and applies ocr to extract the license plate text, enabling high-accuracy license plate reading. the model leverages transformer-based vision-language architectures and is trained on a custom dataset of vehicle images with annotated license plates.\n created at: 2025-01-27T05:49:01+00:00\n- model_name: SimulaMet-HOST/HockeyAI\n  description: üîó this model is trained on the hockeyai dataset. - üìä access the dataset used for training here: - üöÄ try the model in action with our interactive hugging face space:\n created at: 2025-01-09T09:23:25+00:00\n- model_name: jameslahm/yoloe\n  description: official pytorch implementation of yoloe . comparison of performance, training cost, and inference efficiency between yoloe ours and yolo-worldv2 in terms of open text prompts. .\\\n created at: 2025-03-10T07:54:02+00:00\n- model_name: PekingU/rtdetr_v2_r18vd\n  description: the rt-detrv2 model was proposed in by wenyu lv, yian zhao, qinyao chang, kui huang, guanzhong wang, yi liu. rt-detrv2 refines rt-detr by introducing selective multi-scale feature extraction, a discrete sampling operator for broader deployment compatibility, and improved training strategies like dynamic data augmentation and scale-adaptive hyperparameters. these changes enhance flexibility and practicality while maintaining real-time performance. this model was contributed by with the help of and\n created at: 2025-01-31T17:10:44+00:00\n- model_name: ustc-community/dfine-xlarge-coco\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-02-06T15:36:03+00:00\n- model_name: ustc-community/dfine-xlarge-obj365\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-03-28T12:55:40+00:00\n- model_name: PekingU/rtdetr_v2_r101vd\n  description: the rt-detrv2 model was proposed in by wenyu lv, yian zhao, qinyao chang, kui huang, guanzhong wang, yi liu. rt-detrv2 refines rt-detr by introducing selective multi-scale feature extraction, a discrete sampling operator for broader deployment compatibility, and improved training strategies like dynamic data augmentation and scale-adaptive hyperparameters. these changes enhance flexibility and practicality while maintaining real-time performance. this model was contributed by with the help of and\n created at: 2025-01-31T18:14:53+00:00\n- model_name: PekingU/rtdetr_v2_r50vd\n  description: the rt-detrv2 model was proposed in by wenyu lv, yian zhao, qinyao chang, kui huang, guanzhong wang, yi liu. rt-detrv2 refines rt-detr by introducing selective multi-scale feature extraction, a discrete sampling operator for broader deployment compatibility, and improved training strategies like dynamic data augmentation and scale-adaptive hyperparameters. these changes enhance flexibility and practicality while maintaining real-time performance. this model was contributed by with the help of and\n created at: 2025-01-31T18:14:46+00:00\n- model_name: ustc-community/dfine-large-coco\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-02-22T12:04:35+00:00\n- model_name: ustc-community/dfine-nano-coco\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-03-28T12:48:09+00:00\n- model_name: apkonsta/table-transformer-detection-ifrs\n  description: this repository contains a fine-tuned version of the table transformer model, specifically adapted for detecting tables in ifrs international financial reporting standards pdfs. the model is based on the table transformer architecture, which is designed to extract tables from unstructured documents such as pdfs and images. base model: microsoft/table-transformer-detection library: transformers\n created at: 2024-11-24T18:13:18+00:00\n- model_name: mdefrance/yolos-tiny-signature-detection\n  description: yolos model finetuned to detect handwritten signatures in document images using dataset. original yolos was introduced in the paper by fang et al. and first released in . yolos is a vision transformer vit trained using the detr loss. despite its simplicity, a base-sized yolos model is able to achieve 42 ap on coco validation 2017 similar to detr and more complex frameworks such as faster r-cnn. yolos is a vision transformer vit trained using the detr loss. despite its simplicity, a base-sized yolos model is able to achieve 42 ap on coco validation 2017 similar to detr and more complex frameworks such as faster r-cnn. - finetuned by:  - repository:  - model type:  - license: apache 2.0 license - finetuned from model\n created at: 2025-05-06T14:15:22+00:00\n- model_name: ustc-community/dfine-small-coco\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-02-11T14:30:13+00:00\n- model_name: Adit-jain/soccana\n  description: this repository hosts a yolov8-based object detection model trained on a curated and segmented dataset derived from soccernet and other public football datasets. the model is designed to detect key entities in a football game players , referees , and the ball with high accuracy, even in challenging scenes.  - architecture : yolov8n + sahi sliced aided hyper inference\n created at: 2025-07-26T10:43:57+00:00\n- model_name: HuggingPanda/docling-layout\n  description: this is the docling model for layout detection , designed to facilitate easy importing and usage like any other hugging face model. this model is part of the , which provides document layout analysis tools. here's how you can load and use the model:\n created at: 2025-03-13T11:39:16+00:00\n- model_name: yifeihu/Florence-2-DocLayNet-Fixed\n  description: we finetuned the florence-2-large-ft hf model using the doclaynet-v1.1 dataset. to prevent the model from generating hallucinated class names, we re-mapped all class names to single tokens: original class names new class names\n created at: 2024-10-29T17:23:16+00:00\n- model_name: ustc-community/dfine-large-obj365\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-03-28T13:00:47+00:00\n- model_name: Ultralytics/YOLOv5\n  description: yolov5 üöÄ is the world's most loved vision ai, representing ultralytics open-source research into future vision ai methods, incorporating lessons learned and best practices evolved over thousands of hours of research and development. we hope that the resources here will help you get the most out of yolov5. please browse the yolov5 docs for details, raise an issue on github for support, and join our discord community for questions and discussions! to request an enterprise license please complete the form at . yolo11 üöÄ new\n created at: 2024-11-14T09:33:32+00:00\n- model_name: tech4humans/conditional-detr-50-signature-detector\n  description: this repository presents a conditional-detr model with resnet-50 backbone, fine-tuned to detect handwritten signatures in document images. this model achieved the highest map@0.5 93.65% among all tested architectures in our comprehensive evaluation. resource links / badges details\n created at: 2025-06-18T03:39:12+00:00\n\nExamples:\n- example user query: I would like to know if there are any cats or dogs in the photos taken in my friend's backyard.\n  model_name: tensorflow/ssd_mobilenet_v2\n- example user query: I recently installed a new security camera. I need to identify the objects in the recorded video.\n  model_name: hustvl/yolos-tiny\n- example user query: I want to monitor if any car accidents happen on the street by detecting the position of vehicles and pedestrians from CCTV footage.\n  model_name: google/faster_rcnn/openimages_v4/inception_resnet_v2\n- example user query: I want to create an app which detects the license plates of any parked vehicle in my city.\n  model_name: keremberke/yolov5m-license-plate\n- example user query: Your advertising company is looking for a solution that will help them detect objects in images as part of their marketing campaigns.\n  model_name: fcakyon/yolov5s-v7.0\n- example user query: I own a pathology lab and I want to scan blood cell images using a computer vision model. How do I detect blood cells in images using an AI model?\n  model_name: keremberke/yolov8m-blood-cell-detection\n\nUser prompt:\nSurvey each element in the portrayal.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_det>\n\nCandidate Models:\n- model_name: microsoft/table-transformer-structure-recognition\n  description: Table Transformer (DETR) model trained on PubTables1M for detecting the structure (like rows, columns) in tables.\n created at: 2022-10-14\n- model_name: microsoft/table-transformer-detection\n  description: Table Transformer (DETR) model trained on PubTables1M for detecting tables in documents. Introduced in the paper PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents by Smock et al.\n created at: 2022-10-14\n- model_name: facebook/detr-resnet-50\n  description: DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: hustvl/yolos-tiny\n  description: YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN). The model is trained using a bipartite matching loss: one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a no object as class and no bounding box as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.\n created at: 2022-04-26\n- model_name: TahaDouaji/detr-doc-table-detection\n  description: detr-doc-table-detection is a model trained to detect both Bordered and Borderless tables in documents, based on facebook/detr-resnet-50.\n created at: 2022-03-11\n- model_name: facebook/detr-resnet-101\n  description: DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: keremberke/yolov8m-table-extraction\n  description: A YOLOv8 model for table extraction in images, capable of detecting both bordered and borderless tables. Trained using the keremberke/table-extraction dataset.\n created at: 2023-01-29\n- model_name: SenseTime/deformable-detr\n  description: Deformable DETR model with ResNet-50 backbone trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper Deformable DETR: Deformable Transformers for End-to-End Object Detection by Zhu et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: facebook/detr-resnet-101-dc5\n  description: DETR (End-to-End Object Detection) model with ResNet-101 backbone (dilated C5 stage). The model is trained on COCO 2017 object detection dataset and achieves an average precision (AP) of 44.9 on the COCO 2017 validation set.\n created at: 2022-03-02\n- model_name: hustvl/yolos-small\n  description: YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Fang et al. and first released in this repository. YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN).\n created at: 2022-04-26\n- model_name: keremberke/yolov8m-hard-hat-detection\n  description: A YOLOv8 model for detecting hard hats in images. The model can distinguish between 'Hardhat' and 'NO-Hardhat' classes. It can be used to ensure safety compliance in construction sites or other industrial environments where hard hats are required.\n created at: 2023-01-29\n- model_name: keremberke/yolov5m-license-plate\n  description: A YOLOv5 model for license plate detection trained on a custom dataset. The model can detect license plates in images with high accuracy.\n created at: 2023-01-01\n- model_name: keremberke/yolov8s-table-extraction\n  description: A YOLOv8 model for table extraction in documents, capable of detecting bordered and borderless tables. Trained on the table-extraction dataset, the model achieves a mAP@0.5 of 0.984 on the validation set.\n created at: 2023-01-29\n- model_name: keremberke/yolov8m-csgo-player-detection\n  description: An object detection model trained to detect Counter-Strike: Global Offensive (CS:GO) players. The model is based on the YOLOv8 architecture and can identify 'ct', 'cthead', 't', and 'thead' labels.\n created at: 2023-01-29\n- model_name: keremberke/yolov8m-nlf-head-detection\n  description: A YOLOv8 model trained for head detection in American football. The model is capable of detecting helmets, blurred helmets, difficult helmets, partial helmets, and sideline helmets.\n created at: 2023-01-29\n- model_name: keremberke/yolov8m-forklift-detection\n  description: A YOLOv8 model for detecting forklifts and persons in images.\n created at: 2023-01-22\n- model_name: google/owlvit-large-patch14\n  description: OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. It uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. OWL-ViT is trained on publicly available image-caption data and fine-tuned on publicly available object detection datasets such as COCO and OpenImages.\n created at: 2022-07-05\n- model_name: keremberke/yolov8m-plane-detection\n  description: A YOLOv8 model for plane detection trained on the keremberke/plane-detection dataset. The model is capable of detecting planes in images with high accuracy.\n created at: 2023-01-29\n- model_name: keremberke/yolov8s-csgo-player-detection\n  description: A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players. Supports the labels ['ct', 'cthead', 't', 'thead'].\n created at: 2023-01-29\n- model_name: fcakyon/yolov5s-v7.0\n  description: Yolov5s-v7.0 is an object detection model trained on the COCO dataset. It can detect objects in images and return their bounding boxes, scores, and categories.\n created at: 2022-12-13\n- model_name: keremberke/yolov8s-hard-hat-detection\n  description: An object detection model trained to detect hard hats and no-hard hats in images. The model is based on YOLOv8 architecture and can be used for safety applications.\n created at: 2023-01-29\n- model_name: keremberke/yolov8m-blood-cell-detection\n  description: A YOLOv8 model for blood cell detection, including Platelets, RBC, and WBC. Trained on the blood-cell-object-detection dataset.\n created at: 2023-01-29\n- model_name: keremberke/yolov8n-table-extraction\n  description: An object detection model for extracting tables from documents. Supports two label types: 'bordered' and 'borderless'.\n created at: 2023-01-29\n- model_name: keremberke/yolov8n-csgo-player-detection\n  description: A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players with supported labels: ['ct', 'cthead', 't', 'thead'].\n created at: 2023-01-29\n- model_name: keremberke/yolov5s-license-plate\n  description: A YOLOv5 based license plate detection model trained on a custom dataset.\n created at: 2023-01-01\n- model_name: keremberke/yolov8n-blood-cell-detection\n  description: This model detects blood cells in images, specifically Platelets, RBC, and WBC. It is based on the YOLOv8 architecture and trained on the blood-cell-object-detection dataset.\n created at: 2023-01-29\n- model_name: google/owlvit-base-patch32\n  description: OWL-ViT is a zero-shot text-conditioned object detection model that uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. The model can be used to query an image with one or multiple text queries.\n created at: 2022-07-05\n- model_name: keremberke/yolov8m-valorant-detection\n  description: A YOLOv8 model for object detection in Valorant game, trained on a custom dataset. It detects dropped spike, enemy, planted spike, and teammate objects.\n created at: 2023-01-28\n- model_name: google/owlvit-base-patch16\n  description: OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. OWL-ViT uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features.\n created at: 2022-07-05\n- model_name: google/openimages_v4/ssd/mobilenet_v2\n  description: A pre-trained image object detection model based on the SSD architecture with MobileNetV2 as the backbone. The model is capable of detecting objects in images and returning their class labels and bounding box coordinates.\n created at: 2020-10-06\n- model_name: google/faster_rcnn/openimages_v4/inception_resnet_v2\n  description: Detects objects in images using the Faster R-CNN model with Inception-ResNet V2 backbone trained on the OpenImages V4 dataset.\n created at: 2020-10-06\n- model_name: tensorflow/ssd_mobilenet_v2\n  description: This API provides a pre-trained object detection model using TensorFlow Hub. The model is based on the SSD MobileNet V2 architecture and is trained on the COCO dataset. It can be used to detect multiple objects in an image and returns their class, bounding box coordinates, and confidence scores.\n created at: 2020-10-06\n- model_name: tensorflow/efficientdet/lite0/detection\n  description: A collection of TensorFlow Hub models for detecting objects in images using the EfficientDet architecture. These models can be used for various tasks such as object detection, instance segmentation, and more.\n created at: 2020-10-06\n- model_name: tensorflow/efficientdet/lite2/detection\n  description: This API allows for object detection in images using TensorFlow Hub and the EfficientDet model. It can be used to load a pre-trained model and detect objects in a given image.\n created at: 2020-10-06\n- model_name: tensorflow/centernet/resnet50v1_fpn_512x512\n  description: Detect objects in images using TensorFlow Hub and a pre-trained CenterNet model with ResNet50V1 FPN backbone\n created at: 2020-10-06\n- model_name: tensorflow/efficientdet/d0\n  description: Load a pre-trained model from TensorFlow Hub for object detection in images.\n created at: 2020-10-06\n- model_name: tensorflow/centernet/hourglass_512x512\n  description: A pre-trained TensorFlow Hub model for detecting objects in images using the CenterNet Hourglass architecture\n created at: 2020-10-06\n- model_name: tensorflow/faster_rcnn/resnet50_v1_640x640\n  description: A pre-trained Faster R-CNN model with ResNet-50 backbone for object detection in images\n created at: 2020-10-06\n- model_name: tensorflow/ssd_mobilenet_v2/fpnlite_640x640\n  description: A pre-trained object detection model based on the SSD Mobilenet V2 architecture, capable of detecting multiple objects in an image. The model is trained on the COCO dataset and achieves a mean Average Precision (mAP) of 0.32.\n created at: 2020-10-06\n- model_name: tensorflow/centernet/hourglass_1024x1024\n  description: A pre-trained TensorFlow Hub model for detecting objects in images using the CenterNet architecture with an Hourglass backbone.\n created at: 2020-10-06\n- model_name: hustvl/yolop/yolop\n  description: YOLOP is an efficient multi-task network that can jointly handle three crucial tasks in autonomous driving: object detection, drivable area segmentation and lane detection. And it is also the first to reach real-time on embedded devices while maintaining state-of-the-art level performance on the BDD100K dataset.\n created at: 2021-08-25\n- model_name: datvuthanh/hybridnets/hybridnets\n  description: HybridNets is an end2end perception network for multi-tasks. Our work focused on traffic object detection, drivable area segmentation and lane detection. HybridNets can run real-time on embedded systems, and obtains SOTA Object Detection, Lane Detection on BDD100K Dataset.\n created at: 2021-12-23\n- model_name: ultralytics/yolov5/yolov5s\n  description: YOLOv5 is a family of compound-scaled object detection models trained on the COCO dataset, and includes simple functionality for Test Time Augmentation (TTA), model ensembling, hyperparameter evolution, and export to ONNX, CoreML and TFLite.\n created at: 2020-05-18\n- model_name: NVIDIA/DeepLearningExamples/nvidia_ssd\n  description: The SSD (Single Shot MultiBox Detector) model is an object detection model based on the paper 'SSD: Single Shot MultiBox Detector'. It uses a deep neural network for detecting objects in images. This implementation replaces the obsolete VGG model backbone with the more modern ResNet-50 model. The SSD model is trained on the COCO dataset and can be used to detect objects in images with high accuracy and efficiency.\n created at: 2018-05-02\n- model_name: valentinafeve/yolos-fashionpedia\n  description: None\n created at: 2022-11-17\n- model_name: microsoft/conditional-detr-resnet-50\n  description: Conditional DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper [Conditional DETR for Fast Training Convergence](https://arxiv.org/abs/2108.06152) by Meng et al. and first released in [this repository](https://github.com/Atten4Vis/ConditionalDETR).\n created at: 2022-09-09\n- model_name: Yifeng-Liu/rt-detr-finetuned-for-satellite-image-roofs-detection\n  description: roof detection for remote sensing task. - model type: object detection for remote sensing task. - license: mit - model type: object detection for remote sensing task. - license: mit\n created at: 2024-09-03T08:30:42+00:00\n- model_name: devxyasir/florence-finetuned-license-plate-detection\n  description: this model is a fine-tuned version of microsoft's florence-2 large `florence2-base-ft` adapted for automatic number plate detection and recognition . it processes vehicle images to localize number plates with bounding boxes and applies ocr to extract the license plate text, enabling high-accuracy license plate reading. the model leverages transformer-based vision-language architectures and is trained on a custom dataset of vehicle images with annotated license plates.\n created at: 2025-01-27T05:49:01+00:00\n- model_name: SimulaMet-HOST/HockeyAI\n  description: üîó this model is trained on the hockeyai dataset. - üìä access the dataset used for training here: - üöÄ try the model in action with our interactive hugging face space:\n created at: 2025-01-09T09:23:25+00:00\n- model_name: jameslahm/yoloe\n  description: official pytorch implementation of yoloe . comparison of performance, training cost, and inference efficiency between yoloe ours and yolo-worldv2 in terms of open text prompts. .\\\n created at: 2025-03-10T07:54:02+00:00\n- model_name: PekingU/rtdetr_v2_r18vd\n  description: the rt-detrv2 model was proposed in by wenyu lv, yian zhao, qinyao chang, kui huang, guanzhong wang, yi liu. rt-detrv2 refines rt-detr by introducing selective multi-scale feature extraction, a discrete sampling operator for broader deployment compatibility, and improved training strategies like dynamic data augmentation and scale-adaptive hyperparameters. these changes enhance flexibility and practicality while maintaining real-time performance. this model was contributed by with the help of and\n created at: 2025-01-31T17:10:44+00:00\n- model_name: ustc-community/dfine-xlarge-coco\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-02-06T15:36:03+00:00\n- model_name: ustc-community/dfine-xlarge-obj365\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-03-28T12:55:40+00:00\n- model_name: PekingU/rtdetr_v2_r101vd\n  description: the rt-detrv2 model was proposed in by wenyu lv, yian zhao, qinyao chang, kui huang, guanzhong wang, yi liu. rt-detrv2 refines rt-detr by introducing selective multi-scale feature extraction, a discrete sampling operator for broader deployment compatibility, and improved training strategies like dynamic data augmentation and scale-adaptive hyperparameters. these changes enhance flexibility and practicality while maintaining real-time performance. this model was contributed by with the help of and\n created at: 2025-01-31T18:14:53+00:00\n- model_name: PekingU/rtdetr_v2_r50vd\n  description: the rt-detrv2 model was proposed in by wenyu lv, yian zhao, qinyao chang, kui huang, guanzhong wang, yi liu. rt-detrv2 refines rt-detr by introducing selective multi-scale feature extraction, a discrete sampling operator for broader deployment compatibility, and improved training strategies like dynamic data augmentation and scale-adaptive hyperparameters. these changes enhance flexibility and practicality while maintaining real-time performance. this model was contributed by with the help of and\n created at: 2025-01-31T18:14:46+00:00\n- model_name: ustc-community/dfine-large-coco\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-02-22T12:04:35+00:00\n- model_name: ustc-community/dfine-nano-coco\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-03-28T12:48:09+00:00\n- model_name: apkonsta/table-transformer-detection-ifrs\n  description: this repository contains a fine-tuned version of the table transformer model, specifically adapted for detecting tables in ifrs international financial reporting standards pdfs. the model is based on the table transformer architecture, which is designed to extract tables from unstructured documents such as pdfs and images. base model: microsoft/table-transformer-detection library: transformers\n created at: 2024-11-24T18:13:18+00:00\n- model_name: mdefrance/yolos-tiny-signature-detection\n  description: yolos model finetuned to detect handwritten signatures in document images using dataset. original yolos was introduced in the paper by fang et al. and first released in . yolos is a vision transformer vit trained using the detr loss. despite its simplicity, a base-sized yolos model is able to achieve 42 ap on coco validation 2017 similar to detr and more complex frameworks such as faster r-cnn. yolos is a vision transformer vit trained using the detr loss. despite its simplicity, a base-sized yolos model is able to achieve 42 ap on coco validation 2017 similar to detr and more complex frameworks such as faster r-cnn. - finetuned by:  - repository:  - model type:  - license: apache 2.0 license - finetuned from model\n created at: 2025-05-06T14:15:22+00:00\n- model_name: ustc-community/dfine-small-coco\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-02-11T14:30:13+00:00\n- model_name: Adit-jain/soccana\n  description: this repository hosts a yolov8-based object detection model trained on a curated and segmented dataset derived from soccernet and other public football datasets. the model is designed to detect key entities in a football game players , referees , and the ball with high accuracy, even in challenging scenes.  - architecture : yolov8n + sahi sliced aided hyper inference\n created at: 2025-07-26T10:43:57+00:00\n- model_name: HuggingPanda/docling-layout\n  description: this is the docling model for layout detection , designed to facilitate easy importing and usage like any other hugging face model. this model is part of the , which provides document layout analysis tools. here's how you can load and use the model:\n created at: 2025-03-13T11:39:16+00:00\n- model_name: yifeihu/Florence-2-DocLayNet-Fixed\n  description: we finetuned the florence-2-large-ft hf model using the doclaynet-v1.1 dataset. to prevent the model from generating hallucinated class names, we re-mapped all class names to single tokens: original class names new class names\n created at: 2024-10-29T17:23:16+00:00\n- model_name: ustc-community/dfine-large-obj365\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-03-28T13:00:47+00:00\n- model_name: Ultralytics/YOLOv5\n  description: yolov5 üöÄ is the world's most loved vision ai, representing ultralytics open-source research into future vision ai methods, incorporating lessons learned and best practices evolved over thousands of hours of research and development. we hope that the resources here will help you get the most out of yolov5. please browse the yolov5 docs for details, raise an issue on github for support, and join our discord community for questions and discussions! to request an enterprise license please complete the form at . yolo11 üöÄ new\n created at: 2024-11-14T09:33:32+00:00\n- model_name: tech4humans/conditional-detr-50-signature-detector\n  description: this repository presents a conditional-detr model with resnet-50 backbone, fine-tuned to detect handwritten signatures in document images. this model achieved the highest map@0.5 93.65% among all tested architectures in our comprehensive evaluation. resource links / badges details\n created at: 2025-06-18T03:39:12+00:00\n\nExamples:\n- example user query: I would like to know if there are any cats or dogs in the photos taken in my friend's backyard.\n  model_name: tensorflow/ssd_mobilenet_v2\n- example user query: I recently installed a new security camera. I need to identify the objects in the recorded video.\n  model_name: hustvl/yolos-tiny\n- example user query: I want to monitor if any car accidents happen on the street by detecting the position of vehicles and pedestrians from CCTV footage.\n  model_name: google/faster_rcnn/openimages_v4/inception_resnet_v2\n- example user query: I want to create an app which detects the license plates of any parked vehicle in my city.\n  model_name: keremberke/yolov5m-license-plate\n- example user query: Your advertising company is looking for a solution that will help them detect objects in images as part of their marketing campaigns.\n  model_name: fcakyon/yolov5s-v7.0\n- example user query: I own a pathology lab and I want to scan blood cell images using a computer vision model. How do I detect blood cells in images using an AI model?\n  model_name: keremberke/yolov8m-blood-cell-detection\n\nUser prompt:\nIdentify all parts in the capture.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_det>\n\nCandidate Models:\n- model_name: microsoft/table-transformer-structure-recognition\n  description: Table Transformer (DETR) model trained on PubTables1M for detecting the structure (like rows, columns) in tables.\n created at: 2022-10-14\n- model_name: microsoft/table-transformer-detection\n  description: Table Transformer (DETR) model trained on PubTables1M for detecting tables in documents. Introduced in the paper PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents by Smock et al.\n created at: 2022-10-14\n- model_name: facebook/detr-resnet-50\n  description: DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: hustvl/yolos-tiny\n  description: YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN). The model is trained using a bipartite matching loss: one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a no object as class and no bounding box as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.\n created at: 2022-04-26\n- model_name: TahaDouaji/detr-doc-table-detection\n  description: detr-doc-table-detection is a model trained to detect both Bordered and Borderless tables in documents, based on facebook/detr-resnet-50.\n created at: 2022-03-11\n- model_name: facebook/detr-resnet-101\n  description: DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: keremberke/yolov8m-table-extraction\n  description: A YOLOv8 model for table extraction in images, capable of detecting both bordered and borderless tables. Trained using the keremberke/table-extraction dataset.\n created at: 2023-01-29\n- model_name: SenseTime/deformable-detr\n  description: Deformable DETR model with ResNet-50 backbone trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper Deformable DETR: Deformable Transformers for End-to-End Object Detection by Zhu et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: facebook/detr-resnet-101-dc5\n  description: DETR (End-to-End Object Detection) model with ResNet-101 backbone (dilated C5 stage). The model is trained on COCO 2017 object detection dataset and achieves an average precision (AP) of 44.9 on the COCO 2017 validation set.\n created at: 2022-03-02\n- model_name: hustvl/yolos-small\n  description: YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Fang et al. and first released in this repository. YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN).\n created at: 2022-04-26\n- model_name: keremberke/yolov8m-hard-hat-detection\n  description: A YOLOv8 model for detecting hard hats in images. The model can distinguish between 'Hardhat' and 'NO-Hardhat' classes. It can be used to ensure safety compliance in construction sites or other industrial environments where hard hats are required.\n created at: 2023-01-29\n- model_name: keremberke/yolov5m-license-plate\n  description: A YOLOv5 model for license plate detection trained on a custom dataset. The model can detect license plates in images with high accuracy.\n created at: 2023-01-01\n- model_name: keremberke/yolov8s-table-extraction\n  description: A YOLOv8 model for table extraction in documents, capable of detecting bordered and borderless tables. Trained on the table-extraction dataset, the model achieves a mAP@0.5 of 0.984 on the validation set.\n created at: 2023-01-29\n- model_name: keremberke/yolov8m-csgo-player-detection\n  description: An object detection model trained to detect Counter-Strike: Global Offensive (CS:GO) players. The model is based on the YOLOv8 architecture and can identify 'ct', 'cthead', 't', and 'thead' labels.\n created at: 2023-01-29\n- model_name: keremberke/yolov8m-nlf-head-detection\n  description: A YOLOv8 model trained for head detection in American football. The model is capable of detecting helmets, blurred helmets, difficult helmets, partial helmets, and sideline helmets.\n created at: 2023-01-29\n- model_name: keremberke/yolov8m-forklift-detection\n  description: A YOLOv8 model for detecting forklifts and persons in images.\n created at: 2023-01-22\n- model_name: google/owlvit-large-patch14\n  description: OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. It uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. OWL-ViT is trained on publicly available image-caption data and fine-tuned on publicly available object detection datasets such as COCO and OpenImages.\n created at: 2022-07-05\n- model_name: keremberke/yolov8m-plane-detection\n  description: A YOLOv8 model for plane detection trained on the keremberke/plane-detection dataset. The model is capable of detecting planes in images with high accuracy.\n created at: 2023-01-29\n- model_name: keremberke/yolov8s-csgo-player-detection\n  description: A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players. Supports the labels ['ct', 'cthead', 't', 'thead'].\n created at: 2023-01-29\n- model_name: fcakyon/yolov5s-v7.0\n  description: Yolov5s-v7.0 is an object detection model trained on the COCO dataset. It can detect objects in images and return their bounding boxes, scores, and categories.\n created at: 2022-12-13\n- model_name: keremberke/yolov8s-hard-hat-detection\n  description: An object detection model trained to detect hard hats and no-hard hats in images. The model is based on YOLOv8 architecture and can be used for safety applications.\n created at: 2023-01-29\n- model_name: keremberke/yolov8m-blood-cell-detection\n  description: A YOLOv8 model for blood cell detection, including Platelets, RBC, and WBC. Trained on the blood-cell-object-detection dataset.\n created at: 2023-01-29\n- model_name: keremberke/yolov8n-table-extraction\n  description: An object detection model for extracting tables from documents. Supports two label types: 'bordered' and 'borderless'.\n created at: 2023-01-29\n- model_name: keremberke/yolov8n-csgo-player-detection\n  description: A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players with supported labels: ['ct', 'cthead', 't', 'thead'].\n created at: 2023-01-29\n- model_name: keremberke/yolov5s-license-plate\n  description: A YOLOv5 based license plate detection model trained on a custom dataset.\n created at: 2023-01-01\n- model_name: keremberke/yolov8n-blood-cell-detection\n  description: This model detects blood cells in images, specifically Platelets, RBC, and WBC. It is based on the YOLOv8 architecture and trained on the blood-cell-object-detection dataset.\n created at: 2023-01-29\n- model_name: google/owlvit-base-patch32\n  description: OWL-ViT is a zero-shot text-conditioned object detection model that uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. The model can be used to query an image with one or multiple text queries.\n created at: 2022-07-05\n- model_name: keremberke/yolov8m-valorant-detection\n  description: A YOLOv8 model for object detection in Valorant game, trained on a custom dataset. It detects dropped spike, enemy, planted spike, and teammate objects.\n created at: 2023-01-28\n- model_name: google/owlvit-base-patch16\n  description: OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. OWL-ViT uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features.\n created at: 2022-07-05\n- model_name: google/openimages_v4/ssd/mobilenet_v2\n  description: A pre-trained image object detection model based on the SSD architecture with MobileNetV2 as the backbone. The model is capable of detecting objects in images and returning their class labels and bounding box coordinates.\n created at: 2020-10-06\n- model_name: google/faster_rcnn/openimages_v4/inception_resnet_v2\n  description: Detects objects in images using the Faster R-CNN model with Inception-ResNet V2 backbone trained on the OpenImages V4 dataset.\n created at: 2020-10-06\n- model_name: tensorflow/ssd_mobilenet_v2\n  description: This API provides a pre-trained object detection model using TensorFlow Hub. The model is based on the SSD MobileNet V2 architecture and is trained on the COCO dataset. It can be used to detect multiple objects in an image and returns their class, bounding box coordinates, and confidence scores.\n created at: 2020-10-06\n- model_name: tensorflow/efficientdet/lite0/detection\n  description: A collection of TensorFlow Hub models for detecting objects in images using the EfficientDet architecture. These models can be used for various tasks such as object detection, instance segmentation, and more.\n created at: 2020-10-06\n- model_name: tensorflow/efficientdet/lite2/detection\n  description: This API allows for object detection in images using TensorFlow Hub and the EfficientDet model. It can be used to load a pre-trained model and detect objects in a given image.\n created at: 2020-10-06\n- model_name: tensorflow/centernet/resnet50v1_fpn_512x512\n  description: Detect objects in images using TensorFlow Hub and a pre-trained CenterNet model with ResNet50V1 FPN backbone\n created at: 2020-10-06\n- model_name: tensorflow/efficientdet/d0\n  description: Load a pre-trained model from TensorFlow Hub for object detection in images.\n created at: 2020-10-06\n- model_name: tensorflow/centernet/hourglass_512x512\n  description: A pre-trained TensorFlow Hub model for detecting objects in images using the CenterNet Hourglass architecture\n created at: 2020-10-06\n- model_name: tensorflow/faster_rcnn/resnet50_v1_640x640\n  description: A pre-trained Faster R-CNN model with ResNet-50 backbone for object detection in images\n created at: 2020-10-06\n- model_name: tensorflow/ssd_mobilenet_v2/fpnlite_640x640\n  description: A pre-trained object detection model based on the SSD Mobilenet V2 architecture, capable of detecting multiple objects in an image. The model is trained on the COCO dataset and achieves a mean Average Precision (mAP) of 0.32.\n created at: 2020-10-06\n- model_name: tensorflow/centernet/hourglass_1024x1024\n  description: A pre-trained TensorFlow Hub model for detecting objects in images using the CenterNet architecture with an Hourglass backbone.\n created at: 2020-10-06\n- model_name: hustvl/yolop/yolop\n  description: YOLOP is an efficient multi-task network that can jointly handle three crucial tasks in autonomous driving: object detection, drivable area segmentation and lane detection. And it is also the first to reach real-time on embedded devices while maintaining state-of-the-art level performance on the BDD100K dataset.\n created at: 2021-08-25\n- model_name: datvuthanh/hybridnets/hybridnets\n  description: HybridNets is an end2end perception network for multi-tasks. Our work focused on traffic object detection, drivable area segmentation and lane detection. HybridNets can run real-time on embedded systems, and obtains SOTA Object Detection, Lane Detection on BDD100K Dataset.\n created at: 2021-12-23\n- model_name: ultralytics/yolov5/yolov5s\n  description: YOLOv5 is a family of compound-scaled object detection models trained on the COCO dataset, and includes simple functionality for Test Time Augmentation (TTA), model ensembling, hyperparameter evolution, and export to ONNX, CoreML and TFLite.\n created at: 2020-05-18\n- model_name: NVIDIA/DeepLearningExamples/nvidia_ssd\n  description: The SSD (Single Shot MultiBox Detector) model is an object detection model based on the paper 'SSD: Single Shot MultiBox Detector'. It uses a deep neural network for detecting objects in images. This implementation replaces the obsolete VGG model backbone with the more modern ResNet-50 model. The SSD model is trained on the COCO dataset and can be used to detect objects in images with high accuracy and efficiency.\n created at: 2018-05-02\n- model_name: valentinafeve/yolos-fashionpedia\n  description: None\n created at: 2022-11-17\n- model_name: microsoft/conditional-detr-resnet-50\n  description: Conditional DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper [Conditional DETR for Fast Training Convergence](https://arxiv.org/abs/2108.06152) by Meng et al. and first released in [this repository](https://github.com/Atten4Vis/ConditionalDETR).\n created at: 2022-09-09\n- model_name: Yifeng-Liu/rt-detr-finetuned-for-satellite-image-roofs-detection\n  description: roof detection for remote sensing task. - model type: object detection for remote sensing task. - license: mit - model type: object detection for remote sensing task. - license: mit\n created at: 2024-09-03T08:30:42+00:00\n- model_name: devxyasir/florence-finetuned-license-plate-detection\n  description: this model is a fine-tuned version of microsoft's florence-2 large `florence2-base-ft` adapted for automatic number plate detection and recognition . it processes vehicle images to localize number plates with bounding boxes and applies ocr to extract the license plate text, enabling high-accuracy license plate reading. the model leverages transformer-based vision-language architectures and is trained on a custom dataset of vehicle images with annotated license plates.\n created at: 2025-01-27T05:49:01+00:00\n- model_name: SimulaMet-HOST/HockeyAI\n  description: üîó this model is trained on the hockeyai dataset. - üìä access the dataset used for training here: - üöÄ try the model in action with our interactive hugging face space:\n created at: 2025-01-09T09:23:25+00:00\n- model_name: jameslahm/yoloe\n  description: official pytorch implementation of yoloe . comparison of performance, training cost, and inference efficiency between yoloe ours and yolo-worldv2 in terms of open text prompts. .\\\n created at: 2025-03-10T07:54:02+00:00\n- model_name: PekingU/rtdetr_v2_r18vd\n  description: the rt-detrv2 model was proposed in by wenyu lv, yian zhao, qinyao chang, kui huang, guanzhong wang, yi liu. rt-detrv2 refines rt-detr by introducing selective multi-scale feature extraction, a discrete sampling operator for broader deployment compatibility, and improved training strategies like dynamic data augmentation and scale-adaptive hyperparameters. these changes enhance flexibility and practicality while maintaining real-time performance. this model was contributed by with the help of and\n created at: 2025-01-31T17:10:44+00:00\n- model_name: ustc-community/dfine-xlarge-coco\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-02-06T15:36:03+00:00\n- model_name: ustc-community/dfine-xlarge-obj365\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-03-28T12:55:40+00:00\n- model_name: PekingU/rtdetr_v2_r101vd\n  description: the rt-detrv2 model was proposed in by wenyu lv, yian zhao, qinyao chang, kui huang, guanzhong wang, yi liu. rt-detrv2 refines rt-detr by introducing selective multi-scale feature extraction, a discrete sampling operator for broader deployment compatibility, and improved training strategies like dynamic data augmentation and scale-adaptive hyperparameters. these changes enhance flexibility and practicality while maintaining real-time performance. this model was contributed by with the help of and\n created at: 2025-01-31T18:14:53+00:00\n- model_name: PekingU/rtdetr_v2_r50vd\n  description: the rt-detrv2 model was proposed in by wenyu lv, yian zhao, qinyao chang, kui huang, guanzhong wang, yi liu. rt-detrv2 refines rt-detr by introducing selective multi-scale feature extraction, a discrete sampling operator for broader deployment compatibility, and improved training strategies like dynamic data augmentation and scale-adaptive hyperparameters. these changes enhance flexibility and practicality while maintaining real-time performance. this model was contributed by with the help of and\n created at: 2025-01-31T18:14:46+00:00\n- model_name: ustc-community/dfine-large-coco\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-02-22T12:04:35+00:00\n- model_name: ustc-community/dfine-nano-coco\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-03-28T12:48:09+00:00\n- model_name: apkonsta/table-transformer-detection-ifrs\n  description: this repository contains a fine-tuned version of the table transformer model, specifically adapted for detecting tables in ifrs international financial reporting standards pdfs. the model is based on the table transformer architecture, which is designed to extract tables from unstructured documents such as pdfs and images. base model: microsoft/table-transformer-detection library: transformers\n created at: 2024-11-24T18:13:18+00:00\n- model_name: mdefrance/yolos-tiny-signature-detection\n  description: yolos model finetuned to detect handwritten signatures in document images using dataset. original yolos was introduced in the paper by fang et al. and first released in . yolos is a vision transformer vit trained using the detr loss. despite its simplicity, a base-sized yolos model is able to achieve 42 ap on coco validation 2017 similar to detr and more complex frameworks such as faster r-cnn. yolos is a vision transformer vit trained using the detr loss. despite its simplicity, a base-sized yolos model is able to achieve 42 ap on coco validation 2017 similar to detr and more complex frameworks such as faster r-cnn. - finetuned by:  - repository:  - model type:  - license: apache 2.0 license - finetuned from model\n created at: 2025-05-06T14:15:22+00:00\n- model_name: ustc-community/dfine-small-coco\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-02-11T14:30:13+00:00\n- model_name: Adit-jain/soccana\n  description: this repository hosts a yolov8-based object detection model trained on a curated and segmented dataset derived from soccernet and other public football datasets. the model is designed to detect key entities in a football game players , referees , and the ball with high accuracy, even in challenging scenes.  - architecture : yolov8n + sahi sliced aided hyper inference\n created at: 2025-07-26T10:43:57+00:00\n- model_name: HuggingPanda/docling-layout\n  description: this is the docling model for layout detection , designed to facilitate easy importing and usage like any other hugging face model. this model is part of the , which provides document layout analysis tools. here's how you can load and use the model:\n created at: 2025-03-13T11:39:16+00:00\n- model_name: yifeihu/Florence-2-DocLayNet-Fixed\n  description: we finetuned the florence-2-large-ft hf model using the doclaynet-v1.1 dataset. to prevent the model from generating hallucinated class names, we re-mapped all class names to single tokens: original class names new class names\n created at: 2024-10-29T17:23:16+00:00\n- model_name: ustc-community/dfine-large-obj365\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-03-28T13:00:47+00:00\n- model_name: Ultralytics/YOLOv5\n  description: yolov5 üöÄ is the world's most loved vision ai, representing ultralytics open-source research into future vision ai methods, incorporating lessons learned and best practices evolved over thousands of hours of research and development. we hope that the resources here will help you get the most out of yolov5. please browse the yolov5 docs for details, raise an issue on github for support, and join our discord community for questions and discussions! to request an enterprise license please complete the form at . yolo11 üöÄ new\n created at: 2024-11-14T09:33:32+00:00\n- model_name: tech4humans/conditional-detr-50-signature-detector\n  description: this repository presents a conditional-detr model with resnet-50 backbone, fine-tuned to detect handwritten signatures in document images. this model achieved the highest map@0.5 93.65% among all tested architectures in our comprehensive evaluation. resource links / badges details\n created at: 2025-06-18T03:39:12+00:00\n\nExamples:\n- example user query: I would like to know if there are any cats or dogs in the photos taken in my friend's backyard.\n  model_name: tensorflow/ssd_mobilenet_v2\n- example user query: I recently installed a new security camera. I need to identify the objects in the recorded video.\n  model_name: hustvl/yolos-tiny\n- example user query: I want to monitor if any car accidents happen on the street by detecting the position of vehicles and pedestrians from CCTV footage.\n  model_name: google/faster_rcnn/openimages_v4/inception_resnet_v2\n- example user query: I want to create an app which detects the license plates of any parked vehicle in my city.\n  model_name: keremberke/yolov5m-license-plate\n- example user query: Your advertising company is looking for a solution that will help them detect objects in images as part of their marketing campaigns.\n  model_name: fcakyon/yolov5s-v7.0\n- example user query: I own a pathology lab and I want to scan blood cell images using a computer vision model. How do I detect blood cells in images using an AI model?\n  model_name: keremberke/yolov8m-blood-cell-detection\n\nUser prompt:\nScan each component in the portrayal.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_det>\n\nCandidate Models:\n- model_name: microsoft/table-transformer-structure-recognition\n  description: Table Transformer (DETR) model trained on PubTables1M for detecting the structure (like rows, columns) in tables.\n created at: 2022-10-14\n- model_name: microsoft/table-transformer-detection\n  description: Table Transformer (DETR) model trained on PubTables1M for detecting tables in documents. Introduced in the paper PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents by Smock et al.\n created at: 2022-10-14\n- model_name: facebook/detr-resnet-50\n  description: DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: hustvl/yolos-tiny\n  description: YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN). The model is trained using a bipartite matching loss: one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a no object as class and no bounding box as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.\n created at: 2022-04-26\n- model_name: TahaDouaji/detr-doc-table-detection\n  description: detr-doc-table-detection is a model trained to detect both Bordered and Borderless tables in documents, based on facebook/detr-resnet-50.\n created at: 2022-03-11\n- model_name: facebook/detr-resnet-101\n  description: DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: keremberke/yolov8m-table-extraction\n  description: A YOLOv8 model for table extraction in images, capable of detecting both bordered and borderless tables. Trained using the keremberke/table-extraction dataset.\n created at: 2023-01-29\n- model_name: SenseTime/deformable-detr\n  description: Deformable DETR model with ResNet-50 backbone trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper Deformable DETR: Deformable Transformers for End-to-End Object Detection by Zhu et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: facebook/detr-resnet-101-dc5\n  description: DETR (End-to-End Object Detection) model with ResNet-101 backbone (dilated C5 stage). The model is trained on COCO 2017 object detection dataset and achieves an average precision (AP) of 44.9 on the COCO 2017 validation set.\n created at: 2022-03-02\n- model_name: hustvl/yolos-small\n  description: YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Fang et al. and first released in this repository. YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN).\n created at: 2022-04-26\n- model_name: keremberke/yolov8m-hard-hat-detection\n  description: A YOLOv8 model for detecting hard hats in images. The model can distinguish between 'Hardhat' and 'NO-Hardhat' classes. It can be used to ensure safety compliance in construction sites or other industrial environments where hard hats are required.\n created at: 2023-01-29\n- model_name: keremberke/yolov5m-license-plate\n  description: A YOLOv5 model for license plate detection trained on a custom dataset. The model can detect license plates in images with high accuracy.\n created at: 2023-01-01\n- model_name: keremberke/yolov8s-table-extraction\n  description: A YOLOv8 model for table extraction in documents, capable of detecting bordered and borderless tables. Trained on the table-extraction dataset, the model achieves a mAP@0.5 of 0.984 on the validation set.\n created at: 2023-01-29\n- model_name: keremberke/yolov8m-csgo-player-detection\n  description: An object detection model trained to detect Counter-Strike: Global Offensive (CS:GO) players. The model is based on the YOLOv8 architecture and can identify 'ct', 'cthead', 't', and 'thead' labels.\n created at: 2023-01-29\n- model_name: keremberke/yolov8m-nlf-head-detection\n  description: A YOLOv8 model trained for head detection in American football. The model is capable of detecting helmets, blurred helmets, difficult helmets, partial helmets, and sideline helmets.\n created at: 2023-01-29\n- model_name: keremberke/yolov8m-forklift-detection\n  description: A YOLOv8 model for detecting forklifts and persons in images.\n created at: 2023-01-22\n- model_name: google/owlvit-large-patch14\n  description: OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. It uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. OWL-ViT is trained on publicly available image-caption data and fine-tuned on publicly available object detection datasets such as COCO and OpenImages.\n created at: 2022-07-05\n- model_name: keremberke/yolov8m-plane-detection\n  description: A YOLOv8 model for plane detection trained on the keremberke/plane-detection dataset. The model is capable of detecting planes in images with high accuracy.\n created at: 2023-01-29\n- model_name: keremberke/yolov8s-csgo-player-detection\n  description: A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players. Supports the labels ['ct', 'cthead', 't', 'thead'].\n created at: 2023-01-29\n- model_name: fcakyon/yolov5s-v7.0\n  description: Yolov5s-v7.0 is an object detection model trained on the COCO dataset. It can detect objects in images and return their bounding boxes, scores, and categories.\n created at: 2022-12-13\n- model_name: keremberke/yolov8s-hard-hat-detection\n  description: An object detection model trained to detect hard hats and no-hard hats in images. The model is based on YOLOv8 architecture and can be used for safety applications.\n created at: 2023-01-29\n- model_name: keremberke/yolov8m-blood-cell-detection\n  description: A YOLOv8 model for blood cell detection, including Platelets, RBC, and WBC. Trained on the blood-cell-object-detection dataset.\n created at: 2023-01-29\n- model_name: keremberke/yolov8n-table-extraction\n  description: An object detection model for extracting tables from documents. Supports two label types: 'bordered' and 'borderless'.\n created at: 2023-01-29\n- model_name: keremberke/yolov8n-csgo-player-detection\n  description: A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players with supported labels: ['ct', 'cthead', 't', 'thead'].\n created at: 2023-01-29\n- model_name: keremberke/yolov5s-license-plate\n  description: A YOLOv5 based license plate detection model trained on a custom dataset.\n created at: 2023-01-01\n- model_name: keremberke/yolov8n-blood-cell-detection\n  description: This model detects blood cells in images, specifically Platelets, RBC, and WBC. It is based on the YOLOv8 architecture and trained on the blood-cell-object-detection dataset.\n created at: 2023-01-29\n- model_name: google/owlvit-base-patch32\n  description: OWL-ViT is a zero-shot text-conditioned object detection model that uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. The model can be used to query an image with one or multiple text queries.\n created at: 2022-07-05\n- model_name: keremberke/yolov8m-valorant-detection\n  description: A YOLOv8 model for object detection in Valorant game, trained on a custom dataset. It detects dropped spike, enemy, planted spike, and teammate objects.\n created at: 2023-01-28\n- model_name: google/owlvit-base-patch16\n  description: OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. OWL-ViT uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features.\n created at: 2022-07-05\n- model_name: google/openimages_v4/ssd/mobilenet_v2\n  description: A pre-trained image object detection model based on the SSD architecture with MobileNetV2 as the backbone. The model is capable of detecting objects in images and returning their class labels and bounding box coordinates.\n created at: 2020-10-06\n- model_name: google/faster_rcnn/openimages_v4/inception_resnet_v2\n  description: Detects objects in images using the Faster R-CNN model with Inception-ResNet V2 backbone trained on the OpenImages V4 dataset.\n created at: 2020-10-06\n- model_name: tensorflow/ssd_mobilenet_v2\n  description: This API provides a pre-trained object detection model using TensorFlow Hub. The model is based on the SSD MobileNet V2 architecture and is trained on the COCO dataset. It can be used to detect multiple objects in an image and returns their class, bounding box coordinates, and confidence scores.\n created at: 2020-10-06\n- model_name: tensorflow/efficientdet/lite0/detection\n  description: A collection of TensorFlow Hub models for detecting objects in images using the EfficientDet architecture. These models can be used for various tasks such as object detection, instance segmentation, and more.\n created at: 2020-10-06\n- model_name: tensorflow/efficientdet/lite2/detection\n  description: This API allows for object detection in images using TensorFlow Hub and the EfficientDet model. It can be used to load a pre-trained model and detect objects in a given image.\n created at: 2020-10-06\n- model_name: tensorflow/centernet/resnet50v1_fpn_512x512\n  description: Detect objects in images using TensorFlow Hub and a pre-trained CenterNet model with ResNet50V1 FPN backbone\n created at: 2020-10-06\n- model_name: tensorflow/efficientdet/d0\n  description: Load a pre-trained model from TensorFlow Hub for object detection in images.\n created at: 2020-10-06\n- model_name: tensorflow/centernet/hourglass_512x512\n  description: A pre-trained TensorFlow Hub model for detecting objects in images using the CenterNet Hourglass architecture\n created at: 2020-10-06\n- model_name: tensorflow/faster_rcnn/resnet50_v1_640x640\n  description: A pre-trained Faster R-CNN model with ResNet-50 backbone for object detection in images\n created at: 2020-10-06\n- model_name: tensorflow/ssd_mobilenet_v2/fpnlite_640x640\n  description: A pre-trained object detection model based on the SSD Mobilenet V2 architecture, capable of detecting multiple objects in an image. The model is trained on the COCO dataset and achieves a mean Average Precision (mAP) of 0.32.\n created at: 2020-10-06\n- model_name: tensorflow/centernet/hourglass_1024x1024\n  description: A pre-trained TensorFlow Hub model for detecting objects in images using the CenterNet architecture with an Hourglass backbone.\n created at: 2020-10-06\n- model_name: hustvl/yolop/yolop\n  description: YOLOP is an efficient multi-task network that can jointly handle three crucial tasks in autonomous driving: object detection, drivable area segmentation and lane detection. And it is also the first to reach real-time on embedded devices while maintaining state-of-the-art level performance on the BDD100K dataset.\n created at: 2021-08-25\n- model_name: datvuthanh/hybridnets/hybridnets\n  description: HybridNets is an end2end perception network for multi-tasks. Our work focused on traffic object detection, drivable area segmentation and lane detection. HybridNets can run real-time on embedded systems, and obtains SOTA Object Detection, Lane Detection on BDD100K Dataset.\n created at: 2021-12-23\n- model_name: ultralytics/yolov5/yolov5s\n  description: YOLOv5 is a family of compound-scaled object detection models trained on the COCO dataset, and includes simple functionality for Test Time Augmentation (TTA), model ensembling, hyperparameter evolution, and export to ONNX, CoreML and TFLite.\n created at: 2020-05-18\n- model_name: NVIDIA/DeepLearningExamples/nvidia_ssd\n  description: The SSD (Single Shot MultiBox Detector) model is an object detection model based on the paper 'SSD: Single Shot MultiBox Detector'. It uses a deep neural network for detecting objects in images. This implementation replaces the obsolete VGG model backbone with the more modern ResNet-50 model. The SSD model is trained on the COCO dataset and can be used to detect objects in images with high accuracy and efficiency.\n created at: 2018-05-02\n- model_name: valentinafeve/yolos-fashionpedia\n  description: None\n created at: 2022-11-17\n- model_name: microsoft/conditional-detr-resnet-50\n  description: Conditional DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper [Conditional DETR for Fast Training Convergence](https://arxiv.org/abs/2108.06152) by Meng et al. and first released in [this repository](https://github.com/Atten4Vis/ConditionalDETR).\n created at: 2022-09-09\n- model_name: Yifeng-Liu/rt-detr-finetuned-for-satellite-image-roofs-detection\n  description: roof detection for remote sensing task. - model type: object detection for remote sensing task. - license: mit - model type: object detection for remote sensing task. - license: mit\n created at: 2024-09-03T08:30:42+00:00\n- model_name: devxyasir/florence-finetuned-license-plate-detection\n  description: this model is a fine-tuned version of microsoft's florence-2 large `florence2-base-ft` adapted for automatic number plate detection and recognition . it processes vehicle images to localize number plates with bounding boxes and applies ocr to extract the license plate text, enabling high-accuracy license plate reading. the model leverages transformer-based vision-language architectures and is trained on a custom dataset of vehicle images with annotated license plates.\n created at: 2025-01-27T05:49:01+00:00\n- model_name: SimulaMet-HOST/HockeyAI\n  description: üîó this model is trained on the hockeyai dataset. - üìä access the dataset used for training here: - üöÄ try the model in action with our interactive hugging face space:\n created at: 2025-01-09T09:23:25+00:00\n- model_name: jameslahm/yoloe\n  description: official pytorch implementation of yoloe . comparison of performance, training cost, and inference efficiency between yoloe ours and yolo-worldv2 in terms of open text prompts. .\\\n created at: 2025-03-10T07:54:02+00:00\n- model_name: PekingU/rtdetr_v2_r18vd\n  description: the rt-detrv2 model was proposed in by wenyu lv, yian zhao, qinyao chang, kui huang, guanzhong wang, yi liu. rt-detrv2 refines rt-detr by introducing selective multi-scale feature extraction, a discrete sampling operator for broader deployment compatibility, and improved training strategies like dynamic data augmentation and scale-adaptive hyperparameters. these changes enhance flexibility and practicality while maintaining real-time performance. this model was contributed by with the help of and\n created at: 2025-01-31T17:10:44+00:00\n- model_name: ustc-community/dfine-xlarge-coco\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-02-06T15:36:03+00:00\n- model_name: ustc-community/dfine-xlarge-obj365\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-03-28T12:55:40+00:00\n- model_name: PekingU/rtdetr_v2_r101vd\n  description: the rt-detrv2 model was proposed in by wenyu lv, yian zhao, qinyao chang, kui huang, guanzhong wang, yi liu. rt-detrv2 refines rt-detr by introducing selective multi-scale feature extraction, a discrete sampling operator for broader deployment compatibility, and improved training strategies like dynamic data augmentation and scale-adaptive hyperparameters. these changes enhance flexibility and practicality while maintaining real-time performance. this model was contributed by with the help of and\n created at: 2025-01-31T18:14:53+00:00\n- model_name: PekingU/rtdetr_v2_r50vd\n  description: the rt-detrv2 model was proposed in by wenyu lv, yian zhao, qinyao chang, kui huang, guanzhong wang, yi liu. rt-detrv2 refines rt-detr by introducing selective multi-scale feature extraction, a discrete sampling operator for broader deployment compatibility, and improved training strategies like dynamic data augmentation and scale-adaptive hyperparameters. these changes enhance flexibility and practicality while maintaining real-time performance. this model was contributed by with the help of and\n created at: 2025-01-31T18:14:46+00:00\n- model_name: ustc-community/dfine-large-coco\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-02-22T12:04:35+00:00\n- model_name: ustc-community/dfine-nano-coco\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-03-28T12:48:09+00:00\n- model_name: apkonsta/table-transformer-detection-ifrs\n  description: this repository contains a fine-tuned version of the table transformer model, specifically adapted for detecting tables in ifrs international financial reporting standards pdfs. the model is based on the table transformer architecture, which is designed to extract tables from unstructured documents such as pdfs and images. base model: microsoft/table-transformer-detection library: transformers\n created at: 2024-11-24T18:13:18+00:00\n- model_name: mdefrance/yolos-tiny-signature-detection\n  description: yolos model finetuned to detect handwritten signatures in document images using dataset. original yolos was introduced in the paper by fang et al. and first released in . yolos is a vision transformer vit trained using the detr loss. despite its simplicity, a base-sized yolos model is able to achieve 42 ap on coco validation 2017 similar to detr and more complex frameworks such as faster r-cnn. yolos is a vision transformer vit trained using the detr loss. despite its simplicity, a base-sized yolos model is able to achieve 42 ap on coco validation 2017 similar to detr and more complex frameworks such as faster r-cnn. - finetuned by:  - repository:  - model type:  - license: apache 2.0 license - finetuned from model\n created at: 2025-05-06T14:15:22+00:00\n- model_name: ustc-community/dfine-small-coco\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-02-11T14:30:13+00:00\n- model_name: Adit-jain/soccana\n  description: this repository hosts a yolov8-based object detection model trained on a curated and segmented dataset derived from soccernet and other public football datasets. the model is designed to detect key entities in a football game players , referees , and the ball with high accuracy, even in challenging scenes.  - architecture : yolov8n + sahi sliced aided hyper inference\n created at: 2025-07-26T10:43:57+00:00\n- model_name: HuggingPanda/docling-layout\n  description: this is the docling model for layout detection , designed to facilitate easy importing and usage like any other hugging face model. this model is part of the , which provides document layout analysis tools. here's how you can load and use the model:\n created at: 2025-03-13T11:39:16+00:00\n- model_name: yifeihu/Florence-2-DocLayNet-Fixed\n  description: we finetuned the florence-2-large-ft hf model using the doclaynet-v1.1 dataset. to prevent the model from generating hallucinated class names, we re-mapped all class names to single tokens: original class names new class names\n created at: 2024-10-29T17:23:16+00:00\n- model_name: ustc-community/dfine-large-obj365\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-03-28T13:00:47+00:00\n- model_name: Ultralytics/YOLOv5\n  description: yolov5 üöÄ is the world's most loved vision ai, representing ultralytics open-source research into future vision ai methods, incorporating lessons learned and best practices evolved over thousands of hours of research and development. we hope that the resources here will help you get the most out of yolov5. please browse the yolov5 docs for details, raise an issue on github for support, and join our discord community for questions and discussions! to request an enterprise license please complete the form at . yolo11 üöÄ new\n created at: 2024-11-14T09:33:32+00:00\n- model_name: tech4humans/conditional-detr-50-signature-detector\n  description: this repository presents a conditional-detr model with resnet-50 backbone, fine-tuned to detect handwritten signatures in document images. this model achieved the highest map@0.5 93.65% among all tested architectures in our comprehensive evaluation. resource links / badges details\n created at: 2025-06-18T03:39:12+00:00\n\nExamples:\n- example user query: I would like to know if there are any cats or dogs in the photos taken in my friend's backyard.\n  model_name: tensorflow/ssd_mobilenet_v2\n- example user query: I recently installed a new security camera. I need to identify the objects in the recorded video.\n  model_name: hustvl/yolos-tiny\n- example user query: I want to monitor if any car accidents happen on the street by detecting the position of vehicles and pedestrians from CCTV footage.\n  model_name: google/faster_rcnn/openimages_v4/inception_resnet_v2\n- example user query: I want to create an app which detects the license plates of any parked vehicle in my city.\n  model_name: keremberke/yolov5m-license-plate\n- example user query: Your advertising company is looking for a solution that will help them detect objects in images as part of their marketing campaigns.\n  model_name: fcakyon/yolov5s-v7.0\n- example user query: I own a pathology lab and I want to scan blood cell images using a computer vision model. How do I detect blood cells in images using an AI model?\n  model_name: keremberke/yolov8m-blood-cell-detection\n\nUser prompt:\nAnalyze all entities in the rendering.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_det>\n\nCandidate Models:\n- model_name: microsoft/table-transformer-structure-recognition\n  description: Table Transformer (DETR) model trained on PubTables1M for detecting the structure (like rows, columns) in tables.\n created at: 2022-10-14\n- model_name: microsoft/table-transformer-detection\n  description: Table Transformer (DETR) model trained on PubTables1M for detecting tables in documents. Introduced in the paper PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents by Smock et al.\n created at: 2022-10-14\n- model_name: facebook/detr-resnet-50\n  description: DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: hustvl/yolos-tiny\n  description: YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN). The model is trained using a bipartite matching loss: one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a no object as class and no bounding box as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.\n created at: 2022-04-26\n- model_name: TahaDouaji/detr-doc-table-detection\n  description: detr-doc-table-detection is a model trained to detect both Bordered and Borderless tables in documents, based on facebook/detr-resnet-50.\n created at: 2022-03-11\n- model_name: facebook/detr-resnet-101\n  description: DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: keremberke/yolov8m-table-extraction\n  description: A YOLOv8 model for table extraction in images, capable of detecting both bordered and borderless tables. Trained using the keremberke/table-extraction dataset.\n created at: 2023-01-29\n- model_name: SenseTime/deformable-detr\n  description: Deformable DETR model with ResNet-50 backbone trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper Deformable DETR: Deformable Transformers for End-to-End Object Detection by Zhu et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: facebook/detr-resnet-101-dc5\n  description: DETR (End-to-End Object Detection) model with ResNet-101 backbone (dilated C5 stage). The model is trained on COCO 2017 object detection dataset and achieves an average precision (AP) of 44.9 on the COCO 2017 validation set.\n created at: 2022-03-02\n- model_name: hustvl/yolos-small\n  description: YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Fang et al. and first released in this repository. YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN).\n created at: 2022-04-26\n- model_name: keremberke/yolov8m-hard-hat-detection\n  description: A YOLOv8 model for detecting hard hats in images. The model can distinguish between 'Hardhat' and 'NO-Hardhat' classes. It can be used to ensure safety compliance in construction sites or other industrial environments where hard hats are required.\n created at: 2023-01-29\n- model_name: keremberke/yolov5m-license-plate\n  description: A YOLOv5 model for license plate detection trained on a custom dataset. The model can detect license plates in images with high accuracy.\n created at: 2023-01-01\n- model_name: keremberke/yolov8s-table-extraction\n  description: A YOLOv8 model for table extraction in documents, capable of detecting bordered and borderless tables. Trained on the table-extraction dataset, the model achieves a mAP@0.5 of 0.984 on the validation set.\n created at: 2023-01-29\n- model_name: keremberke/yolov8m-csgo-player-detection\n  description: An object detection model trained to detect Counter-Strike: Global Offensive (CS:GO) players. The model is based on the YOLOv8 architecture and can identify 'ct', 'cthead', 't', and 'thead' labels.\n created at: 2023-01-29\n- model_name: keremberke/yolov8m-nlf-head-detection\n  description: A YOLOv8 model trained for head detection in American football. The model is capable of detecting helmets, blurred helmets, difficult helmets, partial helmets, and sideline helmets.\n created at: 2023-01-29\n- model_name: keremberke/yolov8m-forklift-detection\n  description: A YOLOv8 model for detecting forklifts and persons in images.\n created at: 2023-01-22\n- model_name: google/owlvit-large-patch14\n  description: OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. It uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. OWL-ViT is trained on publicly available image-caption data and fine-tuned on publicly available object detection datasets such as COCO and OpenImages.\n created at: 2022-07-05\n- model_name: keremberke/yolov8m-plane-detection\n  description: A YOLOv8 model for plane detection trained on the keremberke/plane-detection dataset. The model is capable of detecting planes in images with high accuracy.\n created at: 2023-01-29\n- model_name: keremberke/yolov8s-csgo-player-detection\n  description: A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players. Supports the labels ['ct', 'cthead', 't', 'thead'].\n created at: 2023-01-29\n- model_name: fcakyon/yolov5s-v7.0\n  description: Yolov5s-v7.0 is an object detection model trained on the COCO dataset. It can detect objects in images and return their bounding boxes, scores, and categories.\n created at: 2022-12-13\n- model_name: keremberke/yolov8s-hard-hat-detection\n  description: An object detection model trained to detect hard hats and no-hard hats in images. The model is based on YOLOv8 architecture and can be used for safety applications.\n created at: 2023-01-29\n- model_name: keremberke/yolov8m-blood-cell-detection\n  description: A YOLOv8 model for blood cell detection, including Platelets, RBC, and WBC. Trained on the blood-cell-object-detection dataset.\n created at: 2023-01-29\n- model_name: keremberke/yolov8n-table-extraction\n  description: An object detection model for extracting tables from documents. Supports two label types: 'bordered' and 'borderless'.\n created at: 2023-01-29\n- model_name: keremberke/yolov8n-csgo-player-detection\n  description: A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players with supported labels: ['ct', 'cthead', 't', 'thead'].\n created at: 2023-01-29\n- model_name: keremberke/yolov5s-license-plate\n  description: A YOLOv5 based license plate detection model trained on a custom dataset.\n created at: 2023-01-01\n- model_name: keremberke/yolov8n-blood-cell-detection\n  description: This model detects blood cells in images, specifically Platelets, RBC, and WBC. It is based on the YOLOv8 architecture and trained on the blood-cell-object-detection dataset.\n created at: 2023-01-29\n- model_name: google/owlvit-base-patch32\n  description: OWL-ViT is a zero-shot text-conditioned object detection model that uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. The model can be used to query an image with one or multiple text queries.\n created at: 2022-07-05\n- model_name: keremberke/yolov8m-valorant-detection\n  description: A YOLOv8 model for object detection in Valorant game, trained on a custom dataset. It detects dropped spike, enemy, planted spike, and teammate objects.\n created at: 2023-01-28\n- model_name: google/owlvit-base-patch16\n  description: OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. OWL-ViT uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features.\n created at: 2022-07-05\n- model_name: google/openimages_v4/ssd/mobilenet_v2\n  description: A pre-trained image object detection model based on the SSD architecture with MobileNetV2 as the backbone. The model is capable of detecting objects in images and returning their class labels and bounding box coordinates.\n created at: 2020-10-06\n- model_name: google/faster_rcnn/openimages_v4/inception_resnet_v2\n  description: Detects objects in images using the Faster R-CNN model with Inception-ResNet V2 backbone trained on the OpenImages V4 dataset.\n created at: 2020-10-06\n- model_name: tensorflow/ssd_mobilenet_v2\n  description: This API provides a pre-trained object detection model using TensorFlow Hub. The model is based on the SSD MobileNet V2 architecture and is trained on the COCO dataset. It can be used to detect multiple objects in an image and returns their class, bounding box coordinates, and confidence scores.\n created at: 2020-10-06\n- model_name: tensorflow/efficientdet/lite0/detection\n  description: A collection of TensorFlow Hub models for detecting objects in images using the EfficientDet architecture. These models can be used for various tasks such as object detection, instance segmentation, and more.\n created at: 2020-10-06\n- model_name: tensorflow/efficientdet/lite2/detection\n  description: This API allows for object detection in images using TensorFlow Hub and the EfficientDet model. It can be used to load a pre-trained model and detect objects in a given image.\n created at: 2020-10-06\n- model_name: tensorflow/centernet/resnet50v1_fpn_512x512\n  description: Detect objects in images using TensorFlow Hub and a pre-trained CenterNet model with ResNet50V1 FPN backbone\n created at: 2020-10-06\n- model_name: tensorflow/efficientdet/d0\n  description: Load a pre-trained model from TensorFlow Hub for object detection in images.\n created at: 2020-10-06\n- model_name: tensorflow/centernet/hourglass_512x512\n  description: A pre-trained TensorFlow Hub model for detecting objects in images using the CenterNet Hourglass architecture\n created at: 2020-10-06\n- model_name: tensorflow/faster_rcnn/resnet50_v1_640x640\n  description: A pre-trained Faster R-CNN model with ResNet-50 backbone for object detection in images\n created at: 2020-10-06\n- model_name: tensorflow/ssd_mobilenet_v2/fpnlite_640x640\n  description: A pre-trained object detection model based on the SSD Mobilenet V2 architecture, capable of detecting multiple objects in an image. The model is trained on the COCO dataset and achieves a mean Average Precision (mAP) of 0.32.\n created at: 2020-10-06\n- model_name: tensorflow/centernet/hourglass_1024x1024\n  description: A pre-trained TensorFlow Hub model for detecting objects in images using the CenterNet architecture with an Hourglass backbone.\n created at: 2020-10-06\n- model_name: hustvl/yolop/yolop\n  description: YOLOP is an efficient multi-task network that can jointly handle three crucial tasks in autonomous driving: object detection, drivable area segmentation and lane detection. And it is also the first to reach real-time on embedded devices while maintaining state-of-the-art level performance on the BDD100K dataset.\n created at: 2021-08-25\n- model_name: datvuthanh/hybridnets/hybridnets\n  description: HybridNets is an end2end perception network for multi-tasks. Our work focused on traffic object detection, drivable area segmentation and lane detection. HybridNets can run real-time on embedded systems, and obtains SOTA Object Detection, Lane Detection on BDD100K Dataset.\n created at: 2021-12-23\n- model_name: ultralytics/yolov5/yolov5s\n  description: YOLOv5 is a family of compound-scaled object detection models trained on the COCO dataset, and includes simple functionality for Test Time Augmentation (TTA), model ensembling, hyperparameter evolution, and export to ONNX, CoreML and TFLite.\n created at: 2020-05-18\n- model_name: NVIDIA/DeepLearningExamples/nvidia_ssd\n  description: The SSD (Single Shot MultiBox Detector) model is an object detection model based on the paper 'SSD: Single Shot MultiBox Detector'. It uses a deep neural network for detecting objects in images. This implementation replaces the obsolete VGG model backbone with the more modern ResNet-50 model. The SSD model is trained on the COCO dataset and can be used to detect objects in images with high accuracy and efficiency.\n created at: 2018-05-02\n- model_name: valentinafeve/yolos-fashionpedia\n  description: None\n created at: 2022-11-17\n- model_name: microsoft/conditional-detr-resnet-50\n  description: Conditional DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper [Conditional DETR for Fast Training Convergence](https://arxiv.org/abs/2108.06152) by Meng et al. and first released in [this repository](https://github.com/Atten4Vis/ConditionalDETR).\n created at: 2022-09-09\n- model_name: Yifeng-Liu/rt-detr-finetuned-for-satellite-image-roofs-detection\n  description: roof detection for remote sensing task. - model type: object detection for remote sensing task. - license: mit - model type: object detection for remote sensing task. - license: mit\n created at: 2024-09-03T08:30:42+00:00\n- model_name: devxyasir/florence-finetuned-license-plate-detection\n  description: this model is a fine-tuned version of microsoft's florence-2 large `florence2-base-ft` adapted for automatic number plate detection and recognition . it processes vehicle images to localize number plates with bounding boxes and applies ocr to extract the license plate text, enabling high-accuracy license plate reading. the model leverages transformer-based vision-language architectures and is trained on a custom dataset of vehicle images with annotated license plates.\n created at: 2025-01-27T05:49:01+00:00\n- model_name: SimulaMet-HOST/HockeyAI\n  description: üîó this model is trained on the hockeyai dataset. - üìä access the dataset used for training here: - üöÄ try the model in action with our interactive hugging face space:\n created at: 2025-01-09T09:23:25+00:00\n- model_name: jameslahm/yoloe\n  description: official pytorch implementation of yoloe . comparison of performance, training cost, and inference efficiency between yoloe ours and yolo-worldv2 in terms of open text prompts. .\\\n created at: 2025-03-10T07:54:02+00:00\n- model_name: PekingU/rtdetr_v2_r18vd\n  description: the rt-detrv2 model was proposed in by wenyu lv, yian zhao, qinyao chang, kui huang, guanzhong wang, yi liu. rt-detrv2 refines rt-detr by introducing selective multi-scale feature extraction, a discrete sampling operator for broader deployment compatibility, and improved training strategies like dynamic data augmentation and scale-adaptive hyperparameters. these changes enhance flexibility and practicality while maintaining real-time performance. this model was contributed by with the help of and\n created at: 2025-01-31T17:10:44+00:00\n- model_name: ustc-community/dfine-xlarge-coco\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-02-06T15:36:03+00:00\n- model_name: ustc-community/dfine-xlarge-obj365\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-03-28T12:55:40+00:00\n- model_name: PekingU/rtdetr_v2_r101vd\n  description: the rt-detrv2 model was proposed in by wenyu lv, yian zhao, qinyao chang, kui huang, guanzhong wang, yi liu. rt-detrv2 refines rt-detr by introducing selective multi-scale feature extraction, a discrete sampling operator for broader deployment compatibility, and improved training strategies like dynamic data augmentation and scale-adaptive hyperparameters. these changes enhance flexibility and practicality while maintaining real-time performance. this model was contributed by with the help of and\n created at: 2025-01-31T18:14:53+00:00\n- model_name: PekingU/rtdetr_v2_r50vd\n  description: the rt-detrv2 model was proposed in by wenyu lv, yian zhao, qinyao chang, kui huang, guanzhong wang, yi liu. rt-detrv2 refines rt-detr by introducing selective multi-scale feature extraction, a discrete sampling operator for broader deployment compatibility, and improved training strategies like dynamic data augmentation and scale-adaptive hyperparameters. these changes enhance flexibility and practicality while maintaining real-time performance. this model was contributed by with the help of and\n created at: 2025-01-31T18:14:46+00:00\n- model_name: ustc-community/dfine-large-coco\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-02-22T12:04:35+00:00\n- model_name: ustc-community/dfine-nano-coco\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-03-28T12:48:09+00:00\n- model_name: apkonsta/table-transformer-detection-ifrs\n  description: this repository contains a fine-tuned version of the table transformer model, specifically adapted for detecting tables in ifrs international financial reporting standards pdfs. the model is based on the table transformer architecture, which is designed to extract tables from unstructured documents such as pdfs and images. base model: microsoft/table-transformer-detection library: transformers\n created at: 2024-11-24T18:13:18+00:00\n- model_name: mdefrance/yolos-tiny-signature-detection\n  description: yolos model finetuned to detect handwritten signatures in document images using dataset. original yolos was introduced in the paper by fang et al. and first released in . yolos is a vision transformer vit trained using the detr loss. despite its simplicity, a base-sized yolos model is able to achieve 42 ap on coco validation 2017 similar to detr and more complex frameworks such as faster r-cnn. yolos is a vision transformer vit trained using the detr loss. despite its simplicity, a base-sized yolos model is able to achieve 42 ap on coco validation 2017 similar to detr and more complex frameworks such as faster r-cnn. - finetuned by:  - repository:  - model type:  - license: apache 2.0 license - finetuned from model\n created at: 2025-05-06T14:15:22+00:00\n- model_name: ustc-community/dfine-small-coco\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-02-11T14:30:13+00:00\n- model_name: Adit-jain/soccana\n  description: this repository hosts a yolov8-based object detection model trained on a curated and segmented dataset derived from soccernet and other public football datasets. the model is designed to detect key entities in a football game players , referees , and the ball with high accuracy, even in challenging scenes.  - architecture : yolov8n + sahi sliced aided hyper inference\n created at: 2025-07-26T10:43:57+00:00\n- model_name: HuggingPanda/docling-layout\n  description: this is the docling model for layout detection , designed to facilitate easy importing and usage like any other hugging face model. this model is part of the , which provides document layout analysis tools. here's how you can load and use the model:\n created at: 2025-03-13T11:39:16+00:00\n- model_name: yifeihu/Florence-2-DocLayNet-Fixed\n  description: we finetuned the florence-2-large-ft hf model using the doclaynet-v1.1 dataset. to prevent the model from generating hallucinated class names, we re-mapped all class names to single tokens: original class names new class names\n created at: 2024-10-29T17:23:16+00:00\n- model_name: ustc-community/dfine-large-obj365\n  description: the d-fine model was proposed in by yansong peng, hebei li, peixi wu, yueyi zhang, xiaoyan sun, feng wu this model was contributed by with the help of\n created at: 2025-03-28T13:00:47+00:00\n- model_name: Ultralytics/YOLOv5\n  description: yolov5 üöÄ is the world's most loved vision ai, representing ultralytics open-source research into future vision ai methods, incorporating lessons learned and best practices evolved over thousands of hours of research and development. we hope that the resources here will help you get the most out of yolov5. please browse the yolov5 docs for details, raise an issue on github for support, and join our discord community for questions and discussions! to request an enterprise license please complete the form at . yolo11 üöÄ new\n created at: 2024-11-14T09:33:32+00:00\n- model_name: tech4humans/conditional-detr-50-signature-detector\n  description: this repository presents a conditional-detr model with resnet-50 backbone, fine-tuned to detect handwritten signatures in document images. this model achieved the highest map@0.5 93.65% among all tested architectures in our comprehensive evaluation. resource links / badges details\n created at: 2025-06-18T03:39:12+00:00\n\nExamples:\n- example user query: I would like to know if there are any cats or dogs in the photos taken in my friend's backyard.\n  model_name: tensorflow/ssd_mobilenet_v2\n- example user query: I recently installed a new security camera. I need to identify the objects in the recorded video.\n  model_name: hustvl/yolos-tiny\n- example user query: I want to monitor if any car accidents happen on the street by detecting the position of vehicles and pedestrians from CCTV footage.\n  model_name: google/faster_rcnn/openimages_v4/inception_resnet_v2\n- example user query: I want to create an app which detects the license plates of any parked vehicle in my city.\n  model_name: keremberke/yolov5m-license-plate\n- example user query: Your advertising company is looking for a solution that will help them detect objects in images as part of their marketing campaigns.\n  model_name: fcakyon/yolov5s-v7.0\n- example user query: I own a pathology lab and I want to scan blood cell images using a computer vision model. How do I detect blood cells in images using an AI model?\n  model_name: keremberke/yolov8m-blood-cell-detection\n\nUser prompt:\nLocate every object within this view.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n"
  ],
  "<image_edit>": [
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_edit>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: black-forest-labs/FLUX.1-Kontext-dev\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-05-28T22:23:43+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-kontext-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of editing images based on text instructions. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-06-28T22:57:58+00:00\n- model_name: Qwen/Qwen-Image-Edit\n  description: üíú qwen chat&nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë tech report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp üñ•Ô∏è demo&nbsp&nbsp &nbsp&nbspüí¨ wechat &nbsp&nbsp &nbsp&nbspü´® discord&nbsp&nbsp &nbsp&nbsp github&nbsp&nbsp we are excited to introduce qwen-image-edit, the image editing version of qwen-image. built upon our 20b qwen-image model, qwen-image-edit successfully extends qwen-image‚Äôs unique text rendering capabilities to image editing tasks, enabling precise text editing. furthermore, qwen-image-edit simultaneously feeds the input image into qwen2.5-vl for visual semantic control and the vae encoder for visual appearance control, achieving capabilities in both semantic and appearance editing. to experience the latest model, visit and select the \"image editing\" feature. we are excited to introduce qwen-image-edit, the image editing version of qwen-image. built upon our 20b qwen-image model, qwen-image-edit successfully extends qwen-image‚Äôs unique text rendering capabilities to image editing tasks, enabling precise text editing. furthermore, qwen-image-edit simultaneously feeds the input image into qwen2.5-vl for visual semantic control and the vae encoder for visual appearance control, achieving capabilities in both semantic and appearance editing. to experience the latest model, visit and select the \"image editing\" feature. key features:  semantic and appearance editing : qwen-image-edit supports both low-level visual appearance editing such as adding, removing, or modifying elements, requiring all other regions of the image to remain completely unchanged and high-level visual semantic editing such as ip creation, object rotation, and style transfer, allowing overall pixel changes while maintaining semantic consistency.  precise text editing : qwen-image-edit supports bilingual chinese and english text editing, allowing direct addition, deletion, and modification of text in images while preserving the original font, size, and style.  strong benchmark performance : evaluations on multiple public benchmarks demonstrate that qwen-image-edit achieves state-of-the-art sota performance in image editing tasks, establishing it as a powerful foundation model for image editing.\n created at: 2025-08-17T08:32:01+00:00\n- model_name: bullerwins/FLUX.1-Kontext-dev-GGUF\n  description: created with example workflow added in files `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-26T16:26:45+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-kontext-dev\n  description: this repository contains nunchaku-quantized versions of , capable of editing images based on text instructions. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:52:52+00:00\n- model_name: unsloth/FLUX.1-Kontext-dev-GGUF\n  description: gguf files for . original license still applies. a huge thanks to and for help with conversion. ggufs were created with and . to run, use comfyui-gguf and place the model files in the `comfyui/models/unet` directory. for more instructions, go to the readme. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-30T10:10:09+00:00\n- model_name: calcuis/qwen-image-edit-gguf\n  description: - use 8-step lite-lora auto applied; save up to 70% loading time - run it with `gguf-connector`; simply execute the command below in console/terminal\n created at: 2025-08-19T13:14:02+00:00\n- model_name: stanfordmimi/MedVAE\n  description: the model was presented in the paper . abstract: medical images are acquired at high resolutions with large fields of view in order to capture fine-grained features necessary for clinical decision-making. consequently, training deep learning models on medical images can incur large computational costs. in this work, we address the challenge of downsizing medical images in order to improve downstream computational efficiency while preserving clinically-relevant features. we introduce medvae, a family of six large-scale 2d and 3d autoencoders capable of encoding medical images as downsized latent representations and decoding latent representations back to high-resolution images. we train medvae autoencoders using a novel two-stage training approach with 1,052,730 medical images. across diverse tasks obtained from 20 medical image datasets, we demonstrate that 1 utilizing medvae latent representations in place of high-resolution images when training downstream models can lead to efficiency benefits up to 70x improvement in throughput while simultaneously preserving clinically-relevant features and 2 medvae can decode latent representations back to high-resolution images with high fidelity. our work demonstrates that large-scale, generalizable autoencoders can help address critical efficiency challenges in the medical domain. medvae is a family of six large-scale, generalizable 2d and 3d variational autoencoders vaes designed for medical imaging. it is trained on over one million medical images across multiple anatomical regions and modalities. medvae autoencoders encode medical images as downsized latent representations and decode latent representations back to high-resolution images. across diverse tasks obtained from 20 medical image datasets, we demonstrate that utilizing medvae latent representations in place of high-resolution images when training downstream models can lead to efficiency benefits up to 70x improvement in throughput while simultaneously preserving clinically-relevant features. medvae is a family of six large-scale, generalizable 2d and 3d variational autoencoders vaes designed for medical imaging. it is trained on over one million medical images across multiple anatomical regions and modalities. medvae autoencoders encode medical images as downsized latent representations and decode latent representations back to high-resolution images. across diverse tasks obtained from 20 medical image datasets, we demonstrate that utilizing medvae latent representations in place of high-resolution images when training downstream models can lead to efficiency benefits up to 70x improvement in throughput while simultaneously preserving clinically-relevant features.  total compression factor channels dimensions modalities anatomies config file model file            16 1 2d x-ray chest, breast ffdm   16 3 2d x-ray chest, breast ffdm   64 1 2d x-ray chest, breast ffdm   64 3 2d x-ray chest, breast ffdm   64 1 3d mri, ct whole-body   512 1 3d mri, ct whole-body  note: model weights and checkpoints are located in the `model weights` folder.\n created at: 2025-01-29T02:28:23+00:00\n- model_name: ovedrive/qwen-image-edit-4bit\n  description: this is an nf4 quantized model of qwen-image-edit so it can run on gpus using 20gb vram. you can run it on lower vram like 16gb. there were other nf4 models but they made the mistake of blindly quantizing all layers in the transformer. this one does not. we retain some layers at full precision in order to ensure that we get quality output. we are excited to introduce qwen-image-edit, the image editing version of qwen-image. built upon our 20b qwen-image model, qwen-image-edit successfully extends qwen-image‚Äôs unique text rendering capabilities to image editing tasks, enabling precise text editing. furthermore, qwen-image-edit simultaneously feeds the input image into qwen2.5-vl for visual semantic control and the vae encoder for visual appearance control, achieving capabilities in both semantic and appearance editing. to experience the latest model, visit and select the \"image editing\" feature. key features:  semantic and appearance editing : qwen-image-edit supports both low-level visual appearance editing such as adding, removing, or modifying elements, requiring all other regions of the image to remain completely unchanged and high-level visual semantic editing such as ip creation, object rotation, and style transfer, allowing overall pixel changes while maintaining semantic consistency.  precise text editing : qwen-image-edit supports bilingual chinese and english text editing, allowing direct addition, deletion, and modification of text in images while preserving the original font, size, and style.  strong benchmark performance : evaluations on multiple public benchmarks demonstrate that qwen-image-edit achieves state-of-the-art sota performance in image editing tasks, establishing it as a powerful foundation model for image editing.\n created at: 2025-08-19T01:14:27+00:00\n- model_name: flymy-ai/qwen-image-edit-inscene-lora\n  description: an open-source lora low-rank adaptation model for qwen-image-edit that specializes in in-scene image editing by . agentic infra for genai. flymy.ai is a b2b infrastructure for building and running genai media agents. üîó useful links:\n created at: 2025-08-20T19:32:32+00:00\n- model_name: fuliucansheng/FLUX.1-Kontext-dev-diffusers\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-28T19:05:56+00:00\n- model_name: AIDC-AI/Ovis-U1-3B\n  description: building on the foundation of the ovis series, ovis-u1 is a 3-billion-parameter unified model that seamlessly integrates multimodal understanding, text-to-image generation, and image editing within a single powerful framework. the overall architecture of ovis-u1 cf. fig.2 in our report. ovis-u1 has been tested with python 3.10, torch 2.4.0, transformers 4.51.3, and deepspeed 0.15.4. for a comprehensive list of package dependencies, please consult the requirements.txt file.\n created at: 2025-06-28T12:00:29+00:00\n- model_name: fofr/kontext-make-person-real\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on using: you should use `make this person look real` as part of the prompt instruction for your image-to-image editing.\n created at: 2025-07-19T14:35:21+00:00\n- model_name: xiaolxl/HXHY-Asian-style-realism-Realistic-KontextLoRA\n  description: :nsfw,kontext,,„ÄÇ model statement: the training material does not contain any nsfw or minor characters, and the training material is only used to adjust the effect of the original kontext model. when you use this model, the output results will be generated based on the images you use, so please do not use it for any illegal or violation purposes.\n created at: 2025-07-07T02:38:04+00:00\n- model_name: 1038lab/SDMatte\n  description: this repository provides safetensors versions of the sdmatte models for interactive image matting , optimized for seamless use with comfyui .  sdmatte: grafting diffusion models for interactive matting is a state-of-the-art model that leverages the power of diffusion priors to achieve high-precision matting especially around fine details and complex edges.\n created at: 2025-08-17T19:04:45+00:00\n- model_name: AlekseyCalvin/QWEN_IMAGE_nf4_w_AbliteratedTE_Diffusers\n  description: this repo contains a variant of qwen's , the state-of-the-art generative model with extensive and image/text-to-image &/or instruction/control-editing capabilities. to make these cutting edge capabilities more accessible to those constrained to low-end consumer-grade hardware, we've quantized the dit diffusion transformer component of qwen-image to the 4-bit nf4 format using the bits&bytes toolkit. this optimization was derived by us directly from the bf16 base model weights released on 08/04/2025, with no other mix-ins or modifications to the dit component.\n created at: 2025-08-05T12:00:18+00:00\n- model_name: lucataco/kontext-realearth\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on using: you should use `make this a realistic drone photo` as part of the prompt instruction for your image-to-image editing.\n created at: 2025-07-22T20:28:00+00:00\n- model_name: gpustack/stable-diffusion-xl-refiner-1.0-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: diffusion-based text-to-image generative model - license:  - model description: this is a model that can be used to generate and modify images based on text prompts. it is a that uses two fixed, pretrained text encoders and . - resources for more information: check out our and the .\n created at: 2024-11-08T08:17:23+00:00\n- model_name: Aitrepreneur/text-dev\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-07-03T14:27:09+00:00\n- model_name: AlekseyCalvin/QWEN_IMAGE_fp4_w_AbliteratedTE_Diffusers\n  description: this repo contains a variant of qwen's , the state-of-the-art generative model with extensive and image/text-to-image &/or instruction/control-editing capabilities. to make these cutting edge capabilities more accessible to those constrained to low-end consumer-grade hardware, we've quantized the dit diffusion transformer component of qwen-image to the 4-bit fp4 format using the bits&bytes toolkit. this optimization was derived by us directly from the bf16 base model weights released on 08/04/2025, with no other mix-ins or modifications to the dit component.\n created at: 2025-08-06T04:07:08+00:00\n\nExamples:\n- example user query: Our customer is an advertising agency, and they need a tool to convert a sketch into a final image. They ask for a solution that incorportes suggestions for changes such as \"Add more color to the sky\" or \"Make the house look larger\".\n  model_name: lllyasviel/control_v11e_sd15_ip2p\n- example user query: I have a photo of a landscape taken during winter. I want to change the landscape style to make it look like it was taken during summer.\n  model_name: GreeneryScenery/SheepsControlV5\n- example user query: We are an online photo editing service, and one of the customers sent us a flying skate-board photo, which he said the photo was blurred. Help him to deblur the photo.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: As a magazine cover designer, we need to create some visually attractive variations of the original image for the upcoming edition.\n  model_name: lambdalabs/sd-image-variations-diffusers\n- example user query: We are a photo editing company, and we need to deblur images using AI.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: I own a travel agency and would like to create a flyer that showcases beautiful scenery. I have a few photos, but I'd like to change their style to make them more visually appealing. Can you assist me in transforming the input images into a different style?\n  model_name: GreeneryScenery/SheepsControlV5\n\nUser prompt:\nShall we erase shoe? This change will make it more eye-catching.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_edit>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: black-forest-labs/FLUX.1-Kontext-dev\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-05-28T22:23:43+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-kontext-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of editing images based on text instructions. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-06-28T22:57:58+00:00\n- model_name: Qwen/Qwen-Image-Edit\n  description: üíú qwen chat&nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë tech report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp üñ•Ô∏è demo&nbsp&nbsp &nbsp&nbspüí¨ wechat &nbsp&nbsp &nbsp&nbspü´® discord&nbsp&nbsp &nbsp&nbsp github&nbsp&nbsp we are excited to introduce qwen-image-edit, the image editing version of qwen-image. built upon our 20b qwen-image model, qwen-image-edit successfully extends qwen-image‚Äôs unique text rendering capabilities to image editing tasks, enabling precise text editing. furthermore, qwen-image-edit simultaneously feeds the input image into qwen2.5-vl for visual semantic control and the vae encoder for visual appearance control, achieving capabilities in both semantic and appearance editing. to experience the latest model, visit and select the \"image editing\" feature. we are excited to introduce qwen-image-edit, the image editing version of qwen-image. built upon our 20b qwen-image model, qwen-image-edit successfully extends qwen-image‚Äôs unique text rendering capabilities to image editing tasks, enabling precise text editing. furthermore, qwen-image-edit simultaneously feeds the input image into qwen2.5-vl for visual semantic control and the vae encoder for visual appearance control, achieving capabilities in both semantic and appearance editing. to experience the latest model, visit and select the \"image editing\" feature. key features:  semantic and appearance editing : qwen-image-edit supports both low-level visual appearance editing such as adding, removing, or modifying elements, requiring all other regions of the image to remain completely unchanged and high-level visual semantic editing such as ip creation, object rotation, and style transfer, allowing overall pixel changes while maintaining semantic consistency.  precise text editing : qwen-image-edit supports bilingual chinese and english text editing, allowing direct addition, deletion, and modification of text in images while preserving the original font, size, and style.  strong benchmark performance : evaluations on multiple public benchmarks demonstrate that qwen-image-edit achieves state-of-the-art sota performance in image editing tasks, establishing it as a powerful foundation model for image editing.\n created at: 2025-08-17T08:32:01+00:00\n- model_name: bullerwins/FLUX.1-Kontext-dev-GGUF\n  description: created with example workflow added in files `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-26T16:26:45+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-kontext-dev\n  description: this repository contains nunchaku-quantized versions of , capable of editing images based on text instructions. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:52:52+00:00\n- model_name: unsloth/FLUX.1-Kontext-dev-GGUF\n  description: gguf files for . original license still applies. a huge thanks to and for help with conversion. ggufs were created with and . to run, use comfyui-gguf and place the model files in the `comfyui/models/unet` directory. for more instructions, go to the readme. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-30T10:10:09+00:00\n- model_name: calcuis/qwen-image-edit-gguf\n  description: - use 8-step lite-lora auto applied; save up to 70% loading time - run it with `gguf-connector`; simply execute the command below in console/terminal\n created at: 2025-08-19T13:14:02+00:00\n- model_name: stanfordmimi/MedVAE\n  description: the model was presented in the paper . abstract: medical images are acquired at high resolutions with large fields of view in order to capture fine-grained features necessary for clinical decision-making. consequently, training deep learning models on medical images can incur large computational costs. in this work, we address the challenge of downsizing medical images in order to improve downstream computational efficiency while preserving clinically-relevant features. we introduce medvae, a family of six large-scale 2d and 3d autoencoders capable of encoding medical images as downsized latent representations and decoding latent representations back to high-resolution images. we train medvae autoencoders using a novel two-stage training approach with 1,052,730 medical images. across diverse tasks obtained from 20 medical image datasets, we demonstrate that 1 utilizing medvae latent representations in place of high-resolution images when training downstream models can lead to efficiency benefits up to 70x improvement in throughput while simultaneously preserving clinically-relevant features and 2 medvae can decode latent representations back to high-resolution images with high fidelity. our work demonstrates that large-scale, generalizable autoencoders can help address critical efficiency challenges in the medical domain. medvae is a family of six large-scale, generalizable 2d and 3d variational autoencoders vaes designed for medical imaging. it is trained on over one million medical images across multiple anatomical regions and modalities. medvae autoencoders encode medical images as downsized latent representations and decode latent representations back to high-resolution images. across diverse tasks obtained from 20 medical image datasets, we demonstrate that utilizing medvae latent representations in place of high-resolution images when training downstream models can lead to efficiency benefits up to 70x improvement in throughput while simultaneously preserving clinically-relevant features. medvae is a family of six large-scale, generalizable 2d and 3d variational autoencoders vaes designed for medical imaging. it is trained on over one million medical images across multiple anatomical regions and modalities. medvae autoencoders encode medical images as downsized latent representations and decode latent representations back to high-resolution images. across diverse tasks obtained from 20 medical image datasets, we demonstrate that utilizing medvae latent representations in place of high-resolution images when training downstream models can lead to efficiency benefits up to 70x improvement in throughput while simultaneously preserving clinically-relevant features.  total compression factor channels dimensions modalities anatomies config file model file            16 1 2d x-ray chest, breast ffdm   16 3 2d x-ray chest, breast ffdm   64 1 2d x-ray chest, breast ffdm   64 3 2d x-ray chest, breast ffdm   64 1 3d mri, ct whole-body   512 1 3d mri, ct whole-body  note: model weights and checkpoints are located in the `model weights` folder.\n created at: 2025-01-29T02:28:23+00:00\n- model_name: ovedrive/qwen-image-edit-4bit\n  description: this is an nf4 quantized model of qwen-image-edit so it can run on gpus using 20gb vram. you can run it on lower vram like 16gb. there were other nf4 models but they made the mistake of blindly quantizing all layers in the transformer. this one does not. we retain some layers at full precision in order to ensure that we get quality output. we are excited to introduce qwen-image-edit, the image editing version of qwen-image. built upon our 20b qwen-image model, qwen-image-edit successfully extends qwen-image‚Äôs unique text rendering capabilities to image editing tasks, enabling precise text editing. furthermore, qwen-image-edit simultaneously feeds the input image into qwen2.5-vl for visual semantic control and the vae encoder for visual appearance control, achieving capabilities in both semantic and appearance editing. to experience the latest model, visit and select the \"image editing\" feature. key features:  semantic and appearance editing : qwen-image-edit supports both low-level visual appearance editing such as adding, removing, or modifying elements, requiring all other regions of the image to remain completely unchanged and high-level visual semantic editing such as ip creation, object rotation, and style transfer, allowing overall pixel changes while maintaining semantic consistency.  precise text editing : qwen-image-edit supports bilingual chinese and english text editing, allowing direct addition, deletion, and modification of text in images while preserving the original font, size, and style.  strong benchmark performance : evaluations on multiple public benchmarks demonstrate that qwen-image-edit achieves state-of-the-art sota performance in image editing tasks, establishing it as a powerful foundation model for image editing.\n created at: 2025-08-19T01:14:27+00:00\n- model_name: flymy-ai/qwen-image-edit-inscene-lora\n  description: an open-source lora low-rank adaptation model for qwen-image-edit that specializes in in-scene image editing by . agentic infra for genai. flymy.ai is a b2b infrastructure for building and running genai media agents. üîó useful links:\n created at: 2025-08-20T19:32:32+00:00\n- model_name: fuliucansheng/FLUX.1-Kontext-dev-diffusers\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-28T19:05:56+00:00\n- model_name: AIDC-AI/Ovis-U1-3B\n  description: building on the foundation of the ovis series, ovis-u1 is a 3-billion-parameter unified model that seamlessly integrates multimodal understanding, text-to-image generation, and image editing within a single powerful framework. the overall architecture of ovis-u1 cf. fig.2 in our report. ovis-u1 has been tested with python 3.10, torch 2.4.0, transformers 4.51.3, and deepspeed 0.15.4. for a comprehensive list of package dependencies, please consult the requirements.txt file.\n created at: 2025-06-28T12:00:29+00:00\n- model_name: fofr/kontext-make-person-real\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on using: you should use `make this person look real` as part of the prompt instruction for your image-to-image editing.\n created at: 2025-07-19T14:35:21+00:00\n- model_name: xiaolxl/HXHY-Asian-style-realism-Realistic-KontextLoRA\n  description: :nsfw,kontext,,„ÄÇ model statement: the training material does not contain any nsfw or minor characters, and the training material is only used to adjust the effect of the original kontext model. when you use this model, the output results will be generated based on the images you use, so please do not use it for any illegal or violation purposes.\n created at: 2025-07-07T02:38:04+00:00\n- model_name: 1038lab/SDMatte\n  description: this repository provides safetensors versions of the sdmatte models for interactive image matting , optimized for seamless use with comfyui .  sdmatte: grafting diffusion models for interactive matting is a state-of-the-art model that leverages the power of diffusion priors to achieve high-precision matting especially around fine details and complex edges.\n created at: 2025-08-17T19:04:45+00:00\n- model_name: AlekseyCalvin/QWEN_IMAGE_nf4_w_AbliteratedTE_Diffusers\n  description: this repo contains a variant of qwen's , the state-of-the-art generative model with extensive and image/text-to-image &/or instruction/control-editing capabilities. to make these cutting edge capabilities more accessible to those constrained to low-end consumer-grade hardware, we've quantized the dit diffusion transformer component of qwen-image to the 4-bit nf4 format using the bits&bytes toolkit. this optimization was derived by us directly from the bf16 base model weights released on 08/04/2025, with no other mix-ins or modifications to the dit component.\n created at: 2025-08-05T12:00:18+00:00\n- model_name: lucataco/kontext-realearth\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on using: you should use `make this a realistic drone photo` as part of the prompt instruction for your image-to-image editing.\n created at: 2025-07-22T20:28:00+00:00\n- model_name: gpustack/stable-diffusion-xl-refiner-1.0-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: diffusion-based text-to-image generative model - license:  - model description: this is a model that can be used to generate and modify images based on text prompts. it is a that uses two fixed, pretrained text encoders and . - resources for more information: check out our and the .\n created at: 2024-11-08T08:17:23+00:00\n- model_name: Aitrepreneur/text-dev\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-07-03T14:27:09+00:00\n- model_name: AlekseyCalvin/QWEN_IMAGE_fp4_w_AbliteratedTE_Diffusers\n  description: this repo contains a variant of qwen's , the state-of-the-art generative model with extensive and image/text-to-image &/or instruction/control-editing capabilities. to make these cutting edge capabilities more accessible to those constrained to low-end consumer-grade hardware, we've quantized the dit diffusion transformer component of qwen-image to the 4-bit fp4 format using the bits&bytes toolkit. this optimization was derived by us directly from the bf16 base model weights released on 08/04/2025, with no other mix-ins or modifications to the dit component.\n created at: 2025-08-06T04:07:08+00:00\n\nExamples:\n- example user query: Our customer is an advertising agency, and they need a tool to convert a sketch into a final image. They ask for a solution that incorportes suggestions for changes such as \"Add more color to the sky\" or \"Make the house look larger\".\n  model_name: lllyasviel/control_v11e_sd15_ip2p\n- example user query: I have a photo of a landscape taken during winter. I want to change the landscape style to make it look like it was taken during summer.\n  model_name: GreeneryScenery/SheepsControlV5\n- example user query: We are an online photo editing service, and one of the customers sent us a flying skate-board photo, which he said the photo was blurred. Help him to deblur the photo.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: As a magazine cover designer, we need to create some visually attractive variations of the original image for the upcoming edition.\n  model_name: lambdalabs/sd-image-variations-diffusers\n- example user query: We are a photo editing company, and we need to deblur images using AI.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: I own a travel agency and would like to create a flyer that showcases beautiful scenery. I have a few photos, but I'd like to change their style to make them more visually appealing. Can you assist me in transforming the input images into a different style?\n  model_name: GreeneryScenery/SheepsControlV5\n\nUser prompt:\ntake the dog out of there\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_edit>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: black-forest-labs/FLUX.1-Kontext-dev\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-05-28T22:23:43+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-kontext-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of editing images based on text instructions. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-06-28T22:57:58+00:00\n- model_name: Qwen/Qwen-Image-Edit\n  description: üíú qwen chat&nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë tech report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp üñ•Ô∏è demo&nbsp&nbsp &nbsp&nbspüí¨ wechat &nbsp&nbsp &nbsp&nbspü´® discord&nbsp&nbsp &nbsp&nbsp github&nbsp&nbsp we are excited to introduce qwen-image-edit, the image editing version of qwen-image. built upon our 20b qwen-image model, qwen-image-edit successfully extends qwen-image‚Äôs unique text rendering capabilities to image editing tasks, enabling precise text editing. furthermore, qwen-image-edit simultaneously feeds the input image into qwen2.5-vl for visual semantic control and the vae encoder for visual appearance control, achieving capabilities in both semantic and appearance editing. to experience the latest model, visit and select the \"image editing\" feature. we are excited to introduce qwen-image-edit, the image editing version of qwen-image. built upon our 20b qwen-image model, qwen-image-edit successfully extends qwen-image‚Äôs unique text rendering capabilities to image editing tasks, enabling precise text editing. furthermore, qwen-image-edit simultaneously feeds the input image into qwen2.5-vl for visual semantic control and the vae encoder for visual appearance control, achieving capabilities in both semantic and appearance editing. to experience the latest model, visit and select the \"image editing\" feature. key features:  semantic and appearance editing : qwen-image-edit supports both low-level visual appearance editing such as adding, removing, or modifying elements, requiring all other regions of the image to remain completely unchanged and high-level visual semantic editing such as ip creation, object rotation, and style transfer, allowing overall pixel changes while maintaining semantic consistency.  precise text editing : qwen-image-edit supports bilingual chinese and english text editing, allowing direct addition, deletion, and modification of text in images while preserving the original font, size, and style.  strong benchmark performance : evaluations on multiple public benchmarks demonstrate that qwen-image-edit achieves state-of-the-art sota performance in image editing tasks, establishing it as a powerful foundation model for image editing.\n created at: 2025-08-17T08:32:01+00:00\n- model_name: bullerwins/FLUX.1-Kontext-dev-GGUF\n  description: created with example workflow added in files `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-26T16:26:45+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-kontext-dev\n  description: this repository contains nunchaku-quantized versions of , capable of editing images based on text instructions. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:52:52+00:00\n- model_name: unsloth/FLUX.1-Kontext-dev-GGUF\n  description: gguf files for . original license still applies. a huge thanks to and for help with conversion. ggufs were created with and . to run, use comfyui-gguf and place the model files in the `comfyui/models/unet` directory. for more instructions, go to the readme. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-30T10:10:09+00:00\n- model_name: calcuis/qwen-image-edit-gguf\n  description: - use 8-step lite-lora auto applied; save up to 70% loading time - run it with `gguf-connector`; simply execute the command below in console/terminal\n created at: 2025-08-19T13:14:02+00:00\n- model_name: stanfordmimi/MedVAE\n  description: the model was presented in the paper . abstract: medical images are acquired at high resolutions with large fields of view in order to capture fine-grained features necessary for clinical decision-making. consequently, training deep learning models on medical images can incur large computational costs. in this work, we address the challenge of downsizing medical images in order to improve downstream computational efficiency while preserving clinically-relevant features. we introduce medvae, a family of six large-scale 2d and 3d autoencoders capable of encoding medical images as downsized latent representations and decoding latent representations back to high-resolution images. we train medvae autoencoders using a novel two-stage training approach with 1,052,730 medical images. across diverse tasks obtained from 20 medical image datasets, we demonstrate that 1 utilizing medvae latent representations in place of high-resolution images when training downstream models can lead to efficiency benefits up to 70x improvement in throughput while simultaneously preserving clinically-relevant features and 2 medvae can decode latent representations back to high-resolution images with high fidelity. our work demonstrates that large-scale, generalizable autoencoders can help address critical efficiency challenges in the medical domain. medvae is a family of six large-scale, generalizable 2d and 3d variational autoencoders vaes designed for medical imaging. it is trained on over one million medical images across multiple anatomical regions and modalities. medvae autoencoders encode medical images as downsized latent representations and decode latent representations back to high-resolution images. across diverse tasks obtained from 20 medical image datasets, we demonstrate that utilizing medvae latent representations in place of high-resolution images when training downstream models can lead to efficiency benefits up to 70x improvement in throughput while simultaneously preserving clinically-relevant features. medvae is a family of six large-scale, generalizable 2d and 3d variational autoencoders vaes designed for medical imaging. it is trained on over one million medical images across multiple anatomical regions and modalities. medvae autoencoders encode medical images as downsized latent representations and decode latent representations back to high-resolution images. across diverse tasks obtained from 20 medical image datasets, we demonstrate that utilizing medvae latent representations in place of high-resolution images when training downstream models can lead to efficiency benefits up to 70x improvement in throughput while simultaneously preserving clinically-relevant features.  total compression factor channels dimensions modalities anatomies config file model file            16 1 2d x-ray chest, breast ffdm   16 3 2d x-ray chest, breast ffdm   64 1 2d x-ray chest, breast ffdm   64 3 2d x-ray chest, breast ffdm   64 1 3d mri, ct whole-body   512 1 3d mri, ct whole-body  note: model weights and checkpoints are located in the `model weights` folder.\n created at: 2025-01-29T02:28:23+00:00\n- model_name: ovedrive/qwen-image-edit-4bit\n  description: this is an nf4 quantized model of qwen-image-edit so it can run on gpus using 20gb vram. you can run it on lower vram like 16gb. there were other nf4 models but they made the mistake of blindly quantizing all layers in the transformer. this one does not. we retain some layers at full precision in order to ensure that we get quality output. we are excited to introduce qwen-image-edit, the image editing version of qwen-image. built upon our 20b qwen-image model, qwen-image-edit successfully extends qwen-image‚Äôs unique text rendering capabilities to image editing tasks, enabling precise text editing. furthermore, qwen-image-edit simultaneously feeds the input image into qwen2.5-vl for visual semantic control and the vae encoder for visual appearance control, achieving capabilities in both semantic and appearance editing. to experience the latest model, visit and select the \"image editing\" feature. key features:  semantic and appearance editing : qwen-image-edit supports both low-level visual appearance editing such as adding, removing, or modifying elements, requiring all other regions of the image to remain completely unchanged and high-level visual semantic editing such as ip creation, object rotation, and style transfer, allowing overall pixel changes while maintaining semantic consistency.  precise text editing : qwen-image-edit supports bilingual chinese and english text editing, allowing direct addition, deletion, and modification of text in images while preserving the original font, size, and style.  strong benchmark performance : evaluations on multiple public benchmarks demonstrate that qwen-image-edit achieves state-of-the-art sota performance in image editing tasks, establishing it as a powerful foundation model for image editing.\n created at: 2025-08-19T01:14:27+00:00\n- model_name: flymy-ai/qwen-image-edit-inscene-lora\n  description: an open-source lora low-rank adaptation model for qwen-image-edit that specializes in in-scene image editing by . agentic infra for genai. flymy.ai is a b2b infrastructure for building and running genai media agents. üîó useful links:\n created at: 2025-08-20T19:32:32+00:00\n- model_name: fuliucansheng/FLUX.1-Kontext-dev-diffusers\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-28T19:05:56+00:00\n- model_name: AIDC-AI/Ovis-U1-3B\n  description: building on the foundation of the ovis series, ovis-u1 is a 3-billion-parameter unified model that seamlessly integrates multimodal understanding, text-to-image generation, and image editing within a single powerful framework. the overall architecture of ovis-u1 cf. fig.2 in our report. ovis-u1 has been tested with python 3.10, torch 2.4.0, transformers 4.51.3, and deepspeed 0.15.4. for a comprehensive list of package dependencies, please consult the requirements.txt file.\n created at: 2025-06-28T12:00:29+00:00\n- model_name: fofr/kontext-make-person-real\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on using: you should use `make this person look real` as part of the prompt instruction for your image-to-image editing.\n created at: 2025-07-19T14:35:21+00:00\n- model_name: xiaolxl/HXHY-Asian-style-realism-Realistic-KontextLoRA\n  description: :nsfw,kontext,,„ÄÇ model statement: the training material does not contain any nsfw or minor characters, and the training material is only used to adjust the effect of the original kontext model. when you use this model, the output results will be generated based on the images you use, so please do not use it for any illegal or violation purposes.\n created at: 2025-07-07T02:38:04+00:00\n- model_name: 1038lab/SDMatte\n  description: this repository provides safetensors versions of the sdmatte models for interactive image matting , optimized for seamless use with comfyui .  sdmatte: grafting diffusion models for interactive matting is a state-of-the-art model that leverages the power of diffusion priors to achieve high-precision matting especially around fine details and complex edges.\n created at: 2025-08-17T19:04:45+00:00\n- model_name: AlekseyCalvin/QWEN_IMAGE_nf4_w_AbliteratedTE_Diffusers\n  description: this repo contains a variant of qwen's , the state-of-the-art generative model with extensive and image/text-to-image &/or instruction/control-editing capabilities. to make these cutting edge capabilities more accessible to those constrained to low-end consumer-grade hardware, we've quantized the dit diffusion transformer component of qwen-image to the 4-bit nf4 format using the bits&bytes toolkit. this optimization was derived by us directly from the bf16 base model weights released on 08/04/2025, with no other mix-ins or modifications to the dit component.\n created at: 2025-08-05T12:00:18+00:00\n- model_name: lucataco/kontext-realearth\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on using: you should use `make this a realistic drone photo` as part of the prompt instruction for your image-to-image editing.\n created at: 2025-07-22T20:28:00+00:00\n- model_name: gpustack/stable-diffusion-xl-refiner-1.0-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: diffusion-based text-to-image generative model - license:  - model description: this is a model that can be used to generate and modify images based on text prompts. it is a that uses two fixed, pretrained text encoders and . - resources for more information: check out our and the .\n created at: 2024-11-08T08:17:23+00:00\n- model_name: Aitrepreneur/text-dev\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-07-03T14:27:09+00:00\n- model_name: AlekseyCalvin/QWEN_IMAGE_fp4_w_AbliteratedTE_Diffusers\n  description: this repo contains a variant of qwen's , the state-of-the-art generative model with extensive and image/text-to-image &/or instruction/control-editing capabilities. to make these cutting edge capabilities more accessible to those constrained to low-end consumer-grade hardware, we've quantized the dit diffusion transformer component of qwen-image to the 4-bit fp4 format using the bits&bytes toolkit. this optimization was derived by us directly from the bf16 base model weights released on 08/04/2025, with no other mix-ins or modifications to the dit component.\n created at: 2025-08-06T04:07:08+00:00\n\nExamples:\n- example user query: Our customer is an advertising agency, and they need a tool to convert a sketch into a final image. They ask for a solution that incorportes suggestions for changes such as \"Add more color to the sky\" or \"Make the house look larger\".\n  model_name: lllyasviel/control_v11e_sd15_ip2p\n- example user query: I have a photo of a landscape taken during winter. I want to change the landscape style to make it look like it was taken during summer.\n  model_name: GreeneryScenery/SheepsControlV5\n- example user query: We are an online photo editing service, and one of the customers sent us a flying skate-board photo, which he said the photo was blurred. Help him to deblur the photo.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: As a magazine cover designer, we need to create some visually attractive variations of the original image for the upcoming edition.\n  model_name: lambdalabs/sd-image-variations-diffusers\n- example user query: We are a photo editing company, and we need to deblur images using AI.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: I own a travel agency and would like to create a flyer that showcases beautiful scenery. I have a few photos, but I'd like to change their style to make them more visually appealing. Can you assist me in transforming the input images into a different style?\n  model_name: GreeneryScenery/SheepsControlV5\n\nUser prompt:\nI would appreciate it if you could replace passport with wizard?\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_edit>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: black-forest-labs/FLUX.1-Kontext-dev\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-05-28T22:23:43+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-kontext-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of editing images based on text instructions. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-06-28T22:57:58+00:00\n- model_name: Qwen/Qwen-Image-Edit\n  description: üíú qwen chat&nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë tech report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp üñ•Ô∏è demo&nbsp&nbsp &nbsp&nbspüí¨ wechat &nbsp&nbsp &nbsp&nbspü´® discord&nbsp&nbsp &nbsp&nbsp github&nbsp&nbsp we are excited to introduce qwen-image-edit, the image editing version of qwen-image. built upon our 20b qwen-image model, qwen-image-edit successfully extends qwen-image‚Äôs unique text rendering capabilities to image editing tasks, enabling precise text editing. furthermore, qwen-image-edit simultaneously feeds the input image into qwen2.5-vl for visual semantic control and the vae encoder for visual appearance control, achieving capabilities in both semantic and appearance editing. to experience the latest model, visit and select the \"image editing\" feature. we are excited to introduce qwen-image-edit, the image editing version of qwen-image. built upon our 20b qwen-image model, qwen-image-edit successfully extends qwen-image‚Äôs unique text rendering capabilities to image editing tasks, enabling precise text editing. furthermore, qwen-image-edit simultaneously feeds the input image into qwen2.5-vl for visual semantic control and the vae encoder for visual appearance control, achieving capabilities in both semantic and appearance editing. to experience the latest model, visit and select the \"image editing\" feature. key features:  semantic and appearance editing : qwen-image-edit supports both low-level visual appearance editing such as adding, removing, or modifying elements, requiring all other regions of the image to remain completely unchanged and high-level visual semantic editing such as ip creation, object rotation, and style transfer, allowing overall pixel changes while maintaining semantic consistency.  precise text editing : qwen-image-edit supports bilingual chinese and english text editing, allowing direct addition, deletion, and modification of text in images while preserving the original font, size, and style.  strong benchmark performance : evaluations on multiple public benchmarks demonstrate that qwen-image-edit achieves state-of-the-art sota performance in image editing tasks, establishing it as a powerful foundation model for image editing.\n created at: 2025-08-17T08:32:01+00:00\n- model_name: bullerwins/FLUX.1-Kontext-dev-GGUF\n  description: created with example workflow added in files `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-26T16:26:45+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-kontext-dev\n  description: this repository contains nunchaku-quantized versions of , capable of editing images based on text instructions. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:52:52+00:00\n- model_name: unsloth/FLUX.1-Kontext-dev-GGUF\n  description: gguf files for . original license still applies. a huge thanks to and for help with conversion. ggufs were created with and . to run, use comfyui-gguf and place the model files in the `comfyui/models/unet` directory. for more instructions, go to the readme. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-30T10:10:09+00:00\n- model_name: calcuis/qwen-image-edit-gguf\n  description: - use 8-step lite-lora auto applied; save up to 70% loading time - run it with `gguf-connector`; simply execute the command below in console/terminal\n created at: 2025-08-19T13:14:02+00:00\n- model_name: stanfordmimi/MedVAE\n  description: the model was presented in the paper . abstract: medical images are acquired at high resolutions with large fields of view in order to capture fine-grained features necessary for clinical decision-making. consequently, training deep learning models on medical images can incur large computational costs. in this work, we address the challenge of downsizing medical images in order to improve downstream computational efficiency while preserving clinically-relevant features. we introduce medvae, a family of six large-scale 2d and 3d autoencoders capable of encoding medical images as downsized latent representations and decoding latent representations back to high-resolution images. we train medvae autoencoders using a novel two-stage training approach with 1,052,730 medical images. across diverse tasks obtained from 20 medical image datasets, we demonstrate that 1 utilizing medvae latent representations in place of high-resolution images when training downstream models can lead to efficiency benefits up to 70x improvement in throughput while simultaneously preserving clinically-relevant features and 2 medvae can decode latent representations back to high-resolution images with high fidelity. our work demonstrates that large-scale, generalizable autoencoders can help address critical efficiency challenges in the medical domain. medvae is a family of six large-scale, generalizable 2d and 3d variational autoencoders vaes designed for medical imaging. it is trained on over one million medical images across multiple anatomical regions and modalities. medvae autoencoders encode medical images as downsized latent representations and decode latent representations back to high-resolution images. across diverse tasks obtained from 20 medical image datasets, we demonstrate that utilizing medvae latent representations in place of high-resolution images when training downstream models can lead to efficiency benefits up to 70x improvement in throughput while simultaneously preserving clinically-relevant features. medvae is a family of six large-scale, generalizable 2d and 3d variational autoencoders vaes designed for medical imaging. it is trained on over one million medical images across multiple anatomical regions and modalities. medvae autoencoders encode medical images as downsized latent representations and decode latent representations back to high-resolution images. across diverse tasks obtained from 20 medical image datasets, we demonstrate that utilizing medvae latent representations in place of high-resolution images when training downstream models can lead to efficiency benefits up to 70x improvement in throughput while simultaneously preserving clinically-relevant features.  total compression factor channels dimensions modalities anatomies config file model file            16 1 2d x-ray chest, breast ffdm   16 3 2d x-ray chest, breast ffdm   64 1 2d x-ray chest, breast ffdm   64 3 2d x-ray chest, breast ffdm   64 1 3d mri, ct whole-body   512 1 3d mri, ct whole-body  note: model weights and checkpoints are located in the `model weights` folder.\n created at: 2025-01-29T02:28:23+00:00\n- model_name: ovedrive/qwen-image-edit-4bit\n  description: this is an nf4 quantized model of qwen-image-edit so it can run on gpus using 20gb vram. you can run it on lower vram like 16gb. there were other nf4 models but they made the mistake of blindly quantizing all layers in the transformer. this one does not. we retain some layers at full precision in order to ensure that we get quality output. we are excited to introduce qwen-image-edit, the image editing version of qwen-image. built upon our 20b qwen-image model, qwen-image-edit successfully extends qwen-image‚Äôs unique text rendering capabilities to image editing tasks, enabling precise text editing. furthermore, qwen-image-edit simultaneously feeds the input image into qwen2.5-vl for visual semantic control and the vae encoder for visual appearance control, achieving capabilities in both semantic and appearance editing. to experience the latest model, visit and select the \"image editing\" feature. key features:  semantic and appearance editing : qwen-image-edit supports both low-level visual appearance editing such as adding, removing, or modifying elements, requiring all other regions of the image to remain completely unchanged and high-level visual semantic editing such as ip creation, object rotation, and style transfer, allowing overall pixel changes while maintaining semantic consistency.  precise text editing : qwen-image-edit supports bilingual chinese and english text editing, allowing direct addition, deletion, and modification of text in images while preserving the original font, size, and style.  strong benchmark performance : evaluations on multiple public benchmarks demonstrate that qwen-image-edit achieves state-of-the-art sota performance in image editing tasks, establishing it as a powerful foundation model for image editing.\n created at: 2025-08-19T01:14:27+00:00\n- model_name: flymy-ai/qwen-image-edit-inscene-lora\n  description: an open-source lora low-rank adaptation model for qwen-image-edit that specializes in in-scene image editing by . agentic infra for genai. flymy.ai is a b2b infrastructure for building and running genai media agents. üîó useful links:\n created at: 2025-08-20T19:32:32+00:00\n- model_name: fuliucansheng/FLUX.1-Kontext-dev-diffusers\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-28T19:05:56+00:00\n- model_name: AIDC-AI/Ovis-U1-3B\n  description: building on the foundation of the ovis series, ovis-u1 is a 3-billion-parameter unified model that seamlessly integrates multimodal understanding, text-to-image generation, and image editing within a single powerful framework. the overall architecture of ovis-u1 cf. fig.2 in our report. ovis-u1 has been tested with python 3.10, torch 2.4.0, transformers 4.51.3, and deepspeed 0.15.4. for a comprehensive list of package dependencies, please consult the requirements.txt file.\n created at: 2025-06-28T12:00:29+00:00\n- model_name: fofr/kontext-make-person-real\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on using: you should use `make this person look real` as part of the prompt instruction for your image-to-image editing.\n created at: 2025-07-19T14:35:21+00:00\n- model_name: xiaolxl/HXHY-Asian-style-realism-Realistic-KontextLoRA\n  description: :nsfw,kontext,,„ÄÇ model statement: the training material does not contain any nsfw or minor characters, and the training material is only used to adjust the effect of the original kontext model. when you use this model, the output results will be generated based on the images you use, so please do not use it for any illegal or violation purposes.\n created at: 2025-07-07T02:38:04+00:00\n- model_name: 1038lab/SDMatte\n  description: this repository provides safetensors versions of the sdmatte models for interactive image matting , optimized for seamless use with comfyui .  sdmatte: grafting diffusion models for interactive matting is a state-of-the-art model that leverages the power of diffusion priors to achieve high-precision matting especially around fine details and complex edges.\n created at: 2025-08-17T19:04:45+00:00\n- model_name: AlekseyCalvin/QWEN_IMAGE_nf4_w_AbliteratedTE_Diffusers\n  description: this repo contains a variant of qwen's , the state-of-the-art generative model with extensive and image/text-to-image &/or instruction/control-editing capabilities. to make these cutting edge capabilities more accessible to those constrained to low-end consumer-grade hardware, we've quantized the dit diffusion transformer component of qwen-image to the 4-bit nf4 format using the bits&bytes toolkit. this optimization was derived by us directly from the bf16 base model weights released on 08/04/2025, with no other mix-ins or modifications to the dit component.\n created at: 2025-08-05T12:00:18+00:00\n- model_name: lucataco/kontext-realearth\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on using: you should use `make this a realistic drone photo` as part of the prompt instruction for your image-to-image editing.\n created at: 2025-07-22T20:28:00+00:00\n- model_name: gpustack/stable-diffusion-xl-refiner-1.0-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: diffusion-based text-to-image generative model - license:  - model description: this is a model that can be used to generate and modify images based on text prompts. it is a that uses two fixed, pretrained text encoders and . - resources for more information: check out our and the .\n created at: 2024-11-08T08:17:23+00:00\n- model_name: Aitrepreneur/text-dev\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-07-03T14:27:09+00:00\n- model_name: AlekseyCalvin/QWEN_IMAGE_fp4_w_AbliteratedTE_Diffusers\n  description: this repo contains a variant of qwen's , the state-of-the-art generative model with extensive and image/text-to-image &/or instruction/control-editing capabilities. to make these cutting edge capabilities more accessible to those constrained to low-end consumer-grade hardware, we've quantized the dit diffusion transformer component of qwen-image to the 4-bit fp4 format using the bits&bytes toolkit. this optimization was derived by us directly from the bf16 base model weights released on 08/04/2025, with no other mix-ins or modifications to the dit component.\n created at: 2025-08-06T04:07:08+00:00\n\nExamples:\n- example user query: Our customer is an advertising agency, and they need a tool to convert a sketch into a final image. They ask for a solution that incorportes suggestions for changes such as \"Add more color to the sky\" or \"Make the house look larger\".\n  model_name: lllyasviel/control_v11e_sd15_ip2p\n- example user query: I have a photo of a landscape taken during winter. I want to change the landscape style to make it look like it was taken during summer.\n  model_name: GreeneryScenery/SheepsControlV5\n- example user query: We are an online photo editing service, and one of the customers sent us a flying skate-board photo, which he said the photo was blurred. Help him to deblur the photo.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: As a magazine cover designer, we need to create some visually attractive variations of the original image for the upcoming edition.\n  model_name: lambdalabs/sd-image-variations-diffusers\n- example user query: We are a photo editing company, and we need to deblur images using AI.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: I own a travel agency and would like to create a flyer that showcases beautiful scenery. I have a few photos, but I'd like to change their style to make them more visually appealing. Can you assist me in transforming the input images into a different style?\n  model_name: GreeneryScenery/SheepsControlV5\n\nUser prompt:\nCould you let lamp be garden? This would introduce a dramatic effect. This alteration will allow the garden to become a central theme, enhancing the conceptual depth.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_edit>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: black-forest-labs/FLUX.1-Kontext-dev\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-05-28T22:23:43+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-kontext-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of editing images based on text instructions. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-06-28T22:57:58+00:00\n- model_name: Qwen/Qwen-Image-Edit\n  description: üíú qwen chat&nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë tech report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp üñ•Ô∏è demo&nbsp&nbsp &nbsp&nbspüí¨ wechat &nbsp&nbsp &nbsp&nbspü´® discord&nbsp&nbsp &nbsp&nbsp github&nbsp&nbsp we are excited to introduce qwen-image-edit, the image editing version of qwen-image. built upon our 20b qwen-image model, qwen-image-edit successfully extends qwen-image‚Äôs unique text rendering capabilities to image editing tasks, enabling precise text editing. furthermore, qwen-image-edit simultaneously feeds the input image into qwen2.5-vl for visual semantic control and the vae encoder for visual appearance control, achieving capabilities in both semantic and appearance editing. to experience the latest model, visit and select the \"image editing\" feature. we are excited to introduce qwen-image-edit, the image editing version of qwen-image. built upon our 20b qwen-image model, qwen-image-edit successfully extends qwen-image‚Äôs unique text rendering capabilities to image editing tasks, enabling precise text editing. furthermore, qwen-image-edit simultaneously feeds the input image into qwen2.5-vl for visual semantic control and the vae encoder for visual appearance control, achieving capabilities in both semantic and appearance editing. to experience the latest model, visit and select the \"image editing\" feature. key features:  semantic and appearance editing : qwen-image-edit supports both low-level visual appearance editing such as adding, removing, or modifying elements, requiring all other regions of the image to remain completely unchanged and high-level visual semantic editing such as ip creation, object rotation, and style transfer, allowing overall pixel changes while maintaining semantic consistency.  precise text editing : qwen-image-edit supports bilingual chinese and english text editing, allowing direct addition, deletion, and modification of text in images while preserving the original font, size, and style.  strong benchmark performance : evaluations on multiple public benchmarks demonstrate that qwen-image-edit achieves state-of-the-art sota performance in image editing tasks, establishing it as a powerful foundation model for image editing.\n created at: 2025-08-17T08:32:01+00:00\n- model_name: bullerwins/FLUX.1-Kontext-dev-GGUF\n  description: created with example workflow added in files `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-26T16:26:45+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-kontext-dev\n  description: this repository contains nunchaku-quantized versions of , capable of editing images based on text instructions. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:52:52+00:00\n- model_name: unsloth/FLUX.1-Kontext-dev-GGUF\n  description: gguf files for . original license still applies. a huge thanks to and for help with conversion. ggufs were created with and . to run, use comfyui-gguf and place the model files in the `comfyui/models/unet` directory. for more instructions, go to the readme. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-30T10:10:09+00:00\n- model_name: calcuis/qwen-image-edit-gguf\n  description: - use 8-step lite-lora auto applied; save up to 70% loading time - run it with `gguf-connector`; simply execute the command below in console/terminal\n created at: 2025-08-19T13:14:02+00:00\n- model_name: stanfordmimi/MedVAE\n  description: the model was presented in the paper . abstract: medical images are acquired at high resolutions with large fields of view in order to capture fine-grained features necessary for clinical decision-making. consequently, training deep learning models on medical images can incur large computational costs. in this work, we address the challenge of downsizing medical images in order to improve downstream computational efficiency while preserving clinically-relevant features. we introduce medvae, a family of six large-scale 2d and 3d autoencoders capable of encoding medical images as downsized latent representations and decoding latent representations back to high-resolution images. we train medvae autoencoders using a novel two-stage training approach with 1,052,730 medical images. across diverse tasks obtained from 20 medical image datasets, we demonstrate that 1 utilizing medvae latent representations in place of high-resolution images when training downstream models can lead to efficiency benefits up to 70x improvement in throughput while simultaneously preserving clinically-relevant features and 2 medvae can decode latent representations back to high-resolution images with high fidelity. our work demonstrates that large-scale, generalizable autoencoders can help address critical efficiency challenges in the medical domain. medvae is a family of six large-scale, generalizable 2d and 3d variational autoencoders vaes designed for medical imaging. it is trained on over one million medical images across multiple anatomical regions and modalities. medvae autoencoders encode medical images as downsized latent representations and decode latent representations back to high-resolution images. across diverse tasks obtained from 20 medical image datasets, we demonstrate that utilizing medvae latent representations in place of high-resolution images when training downstream models can lead to efficiency benefits up to 70x improvement in throughput while simultaneously preserving clinically-relevant features. medvae is a family of six large-scale, generalizable 2d and 3d variational autoencoders vaes designed for medical imaging. it is trained on over one million medical images across multiple anatomical regions and modalities. medvae autoencoders encode medical images as downsized latent representations and decode latent representations back to high-resolution images. across diverse tasks obtained from 20 medical image datasets, we demonstrate that utilizing medvae latent representations in place of high-resolution images when training downstream models can lead to efficiency benefits up to 70x improvement in throughput while simultaneously preserving clinically-relevant features.  total compression factor channels dimensions modalities anatomies config file model file            16 1 2d x-ray chest, breast ffdm   16 3 2d x-ray chest, breast ffdm   64 1 2d x-ray chest, breast ffdm   64 3 2d x-ray chest, breast ffdm   64 1 3d mri, ct whole-body   512 1 3d mri, ct whole-body  note: model weights and checkpoints are located in the `model weights` folder.\n created at: 2025-01-29T02:28:23+00:00\n- model_name: ovedrive/qwen-image-edit-4bit\n  description: this is an nf4 quantized model of qwen-image-edit so it can run on gpus using 20gb vram. you can run it on lower vram like 16gb. there were other nf4 models but they made the mistake of blindly quantizing all layers in the transformer. this one does not. we retain some layers at full precision in order to ensure that we get quality output. we are excited to introduce qwen-image-edit, the image editing version of qwen-image. built upon our 20b qwen-image model, qwen-image-edit successfully extends qwen-image‚Äôs unique text rendering capabilities to image editing tasks, enabling precise text editing. furthermore, qwen-image-edit simultaneously feeds the input image into qwen2.5-vl for visual semantic control and the vae encoder for visual appearance control, achieving capabilities in both semantic and appearance editing. to experience the latest model, visit and select the \"image editing\" feature. key features:  semantic and appearance editing : qwen-image-edit supports both low-level visual appearance editing such as adding, removing, or modifying elements, requiring all other regions of the image to remain completely unchanged and high-level visual semantic editing such as ip creation, object rotation, and style transfer, allowing overall pixel changes while maintaining semantic consistency.  precise text editing : qwen-image-edit supports bilingual chinese and english text editing, allowing direct addition, deletion, and modification of text in images while preserving the original font, size, and style.  strong benchmark performance : evaluations on multiple public benchmarks demonstrate that qwen-image-edit achieves state-of-the-art sota performance in image editing tasks, establishing it as a powerful foundation model for image editing.\n created at: 2025-08-19T01:14:27+00:00\n- model_name: flymy-ai/qwen-image-edit-inscene-lora\n  description: an open-source lora low-rank adaptation model for qwen-image-edit that specializes in in-scene image editing by . agentic infra for genai. flymy.ai is a b2b infrastructure for building and running genai media agents. üîó useful links:\n created at: 2025-08-20T19:32:32+00:00\n- model_name: fuliucansheng/FLUX.1-Kontext-dev-diffusers\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-28T19:05:56+00:00\n- model_name: AIDC-AI/Ovis-U1-3B\n  description: building on the foundation of the ovis series, ovis-u1 is a 3-billion-parameter unified model that seamlessly integrates multimodal understanding, text-to-image generation, and image editing within a single powerful framework. the overall architecture of ovis-u1 cf. fig.2 in our report. ovis-u1 has been tested with python 3.10, torch 2.4.0, transformers 4.51.3, and deepspeed 0.15.4. for a comprehensive list of package dependencies, please consult the requirements.txt file.\n created at: 2025-06-28T12:00:29+00:00\n- model_name: fofr/kontext-make-person-real\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on using: you should use `make this person look real` as part of the prompt instruction for your image-to-image editing.\n created at: 2025-07-19T14:35:21+00:00\n- model_name: xiaolxl/HXHY-Asian-style-realism-Realistic-KontextLoRA\n  description: :nsfw,kontext,,„ÄÇ model statement: the training material does not contain any nsfw or minor characters, and the training material is only used to adjust the effect of the original kontext model. when you use this model, the output results will be generated based on the images you use, so please do not use it for any illegal or violation purposes.\n created at: 2025-07-07T02:38:04+00:00\n- model_name: 1038lab/SDMatte\n  description: this repository provides safetensors versions of the sdmatte models for interactive image matting , optimized for seamless use with comfyui .  sdmatte: grafting diffusion models for interactive matting is a state-of-the-art model that leverages the power of diffusion priors to achieve high-precision matting especially around fine details and complex edges.\n created at: 2025-08-17T19:04:45+00:00\n- model_name: AlekseyCalvin/QWEN_IMAGE_nf4_w_AbliteratedTE_Diffusers\n  description: this repo contains a variant of qwen's , the state-of-the-art generative model with extensive and image/text-to-image &/or instruction/control-editing capabilities. to make these cutting edge capabilities more accessible to those constrained to low-end consumer-grade hardware, we've quantized the dit diffusion transformer component of qwen-image to the 4-bit nf4 format using the bits&bytes toolkit. this optimization was derived by us directly from the bf16 base model weights released on 08/04/2025, with no other mix-ins or modifications to the dit component.\n created at: 2025-08-05T12:00:18+00:00\n- model_name: lucataco/kontext-realearth\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on using: you should use `make this a realistic drone photo` as part of the prompt instruction for your image-to-image editing.\n created at: 2025-07-22T20:28:00+00:00\n- model_name: gpustack/stable-diffusion-xl-refiner-1.0-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: diffusion-based text-to-image generative model - license:  - model description: this is a model that can be used to generate and modify images based on text prompts. it is a that uses two fixed, pretrained text encoders and . - resources for more information: check out our and the .\n created at: 2024-11-08T08:17:23+00:00\n- model_name: Aitrepreneur/text-dev\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-07-03T14:27:09+00:00\n- model_name: AlekseyCalvin/QWEN_IMAGE_fp4_w_AbliteratedTE_Diffusers\n  description: this repo contains a variant of qwen's , the state-of-the-art generative model with extensive and image/text-to-image &/or instruction/control-editing capabilities. to make these cutting edge capabilities more accessible to those constrained to low-end consumer-grade hardware, we've quantized the dit diffusion transformer component of qwen-image to the 4-bit fp4 format using the bits&bytes toolkit. this optimization was derived by us directly from the bf16 base model weights released on 08/04/2025, with no other mix-ins or modifications to the dit component.\n created at: 2025-08-06T04:07:08+00:00\n\nExamples:\n- example user query: Our customer is an advertising agency, and they need a tool to convert a sketch into a final image. They ask for a solution that incorportes suggestions for changes such as \"Add more color to the sky\" or \"Make the house look larger\".\n  model_name: lllyasviel/control_v11e_sd15_ip2p\n- example user query: I have a photo of a landscape taken during winter. I want to change the landscape style to make it look like it was taken during summer.\n  model_name: GreeneryScenery/SheepsControlV5\n- example user query: We are an online photo editing service, and one of the customers sent us a flying skate-board photo, which he said the photo was blurred. Help him to deblur the photo.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: As a magazine cover designer, we need to create some visually attractive variations of the original image for the upcoming edition.\n  model_name: lambdalabs/sd-image-variations-diffusers\n- example user query: We are a photo editing company, and we need to deblur images using AI.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: I own a travel agency and would like to create a flyer that showcases beautiful scenery. I have a few photos, but I'd like to change their style to make them more visually appealing. Can you assist me in transforming the input images into a different style?\n  model_name: GreeneryScenery/SheepsControlV5\n\nUser prompt:\nWould it be okay to turn baby green? This could cool down the tones.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_edit>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: black-forest-labs/FLUX.1-Kontext-dev\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-05-28T22:23:43+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-kontext-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of editing images based on text instructions. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-06-28T22:57:58+00:00\n- model_name: Qwen/Qwen-Image-Edit\n  description: üíú qwen chat&nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë tech report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp üñ•Ô∏è demo&nbsp&nbsp &nbsp&nbspüí¨ wechat &nbsp&nbsp &nbsp&nbspü´® discord&nbsp&nbsp &nbsp&nbsp github&nbsp&nbsp we are excited to introduce qwen-image-edit, the image editing version of qwen-image. built upon our 20b qwen-image model, qwen-image-edit successfully extends qwen-image‚Äôs unique text rendering capabilities to image editing tasks, enabling precise text editing. furthermore, qwen-image-edit simultaneously feeds the input image into qwen2.5-vl for visual semantic control and the vae encoder for visual appearance control, achieving capabilities in both semantic and appearance editing. to experience the latest model, visit and select the \"image editing\" feature. we are excited to introduce qwen-image-edit, the image editing version of qwen-image. built upon our 20b qwen-image model, qwen-image-edit successfully extends qwen-image‚Äôs unique text rendering capabilities to image editing tasks, enabling precise text editing. furthermore, qwen-image-edit simultaneously feeds the input image into qwen2.5-vl for visual semantic control and the vae encoder for visual appearance control, achieving capabilities in both semantic and appearance editing. to experience the latest model, visit and select the \"image editing\" feature. key features:  semantic and appearance editing : qwen-image-edit supports both low-level visual appearance editing such as adding, removing, or modifying elements, requiring all other regions of the image to remain completely unchanged and high-level visual semantic editing such as ip creation, object rotation, and style transfer, allowing overall pixel changes while maintaining semantic consistency.  precise text editing : qwen-image-edit supports bilingual chinese and english text editing, allowing direct addition, deletion, and modification of text in images while preserving the original font, size, and style.  strong benchmark performance : evaluations on multiple public benchmarks demonstrate that qwen-image-edit achieves state-of-the-art sota performance in image editing tasks, establishing it as a powerful foundation model for image editing.\n created at: 2025-08-17T08:32:01+00:00\n- model_name: bullerwins/FLUX.1-Kontext-dev-GGUF\n  description: created with example workflow added in files `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-26T16:26:45+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-kontext-dev\n  description: this repository contains nunchaku-quantized versions of , capable of editing images based on text instructions. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:52:52+00:00\n- model_name: unsloth/FLUX.1-Kontext-dev-GGUF\n  description: gguf files for . original license still applies. a huge thanks to and for help with conversion. ggufs were created with and . to run, use comfyui-gguf and place the model files in the `comfyui/models/unet` directory. for more instructions, go to the readme. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-30T10:10:09+00:00\n- model_name: calcuis/qwen-image-edit-gguf\n  description: - use 8-step lite-lora auto applied; save up to 70% loading time - run it with `gguf-connector`; simply execute the command below in console/terminal\n created at: 2025-08-19T13:14:02+00:00\n- model_name: stanfordmimi/MedVAE\n  description: the model was presented in the paper . abstract: medical images are acquired at high resolutions with large fields of view in order to capture fine-grained features necessary for clinical decision-making. consequently, training deep learning models on medical images can incur large computational costs. in this work, we address the challenge of downsizing medical images in order to improve downstream computational efficiency while preserving clinically-relevant features. we introduce medvae, a family of six large-scale 2d and 3d autoencoders capable of encoding medical images as downsized latent representations and decoding latent representations back to high-resolution images. we train medvae autoencoders using a novel two-stage training approach with 1,052,730 medical images. across diverse tasks obtained from 20 medical image datasets, we demonstrate that 1 utilizing medvae latent representations in place of high-resolution images when training downstream models can lead to efficiency benefits up to 70x improvement in throughput while simultaneously preserving clinically-relevant features and 2 medvae can decode latent representations back to high-resolution images with high fidelity. our work demonstrates that large-scale, generalizable autoencoders can help address critical efficiency challenges in the medical domain. medvae is a family of six large-scale, generalizable 2d and 3d variational autoencoders vaes designed for medical imaging. it is trained on over one million medical images across multiple anatomical regions and modalities. medvae autoencoders encode medical images as downsized latent representations and decode latent representations back to high-resolution images. across diverse tasks obtained from 20 medical image datasets, we demonstrate that utilizing medvae latent representations in place of high-resolution images when training downstream models can lead to efficiency benefits up to 70x improvement in throughput while simultaneously preserving clinically-relevant features. medvae is a family of six large-scale, generalizable 2d and 3d variational autoencoders vaes designed for medical imaging. it is trained on over one million medical images across multiple anatomical regions and modalities. medvae autoencoders encode medical images as downsized latent representations and decode latent representations back to high-resolution images. across diverse tasks obtained from 20 medical image datasets, we demonstrate that utilizing medvae latent representations in place of high-resolution images when training downstream models can lead to efficiency benefits up to 70x improvement in throughput while simultaneously preserving clinically-relevant features.  total compression factor channels dimensions modalities anatomies config file model file            16 1 2d x-ray chest, breast ffdm   16 3 2d x-ray chest, breast ffdm   64 1 2d x-ray chest, breast ffdm   64 3 2d x-ray chest, breast ffdm   64 1 3d mri, ct whole-body   512 1 3d mri, ct whole-body  note: model weights and checkpoints are located in the `model weights` folder.\n created at: 2025-01-29T02:28:23+00:00\n- model_name: ovedrive/qwen-image-edit-4bit\n  description: this is an nf4 quantized model of qwen-image-edit so it can run on gpus using 20gb vram. you can run it on lower vram like 16gb. there were other nf4 models but they made the mistake of blindly quantizing all layers in the transformer. this one does not. we retain some layers at full precision in order to ensure that we get quality output. we are excited to introduce qwen-image-edit, the image editing version of qwen-image. built upon our 20b qwen-image model, qwen-image-edit successfully extends qwen-image‚Äôs unique text rendering capabilities to image editing tasks, enabling precise text editing. furthermore, qwen-image-edit simultaneously feeds the input image into qwen2.5-vl for visual semantic control and the vae encoder for visual appearance control, achieving capabilities in both semantic and appearance editing. to experience the latest model, visit and select the \"image editing\" feature. key features:  semantic and appearance editing : qwen-image-edit supports both low-level visual appearance editing such as adding, removing, or modifying elements, requiring all other regions of the image to remain completely unchanged and high-level visual semantic editing such as ip creation, object rotation, and style transfer, allowing overall pixel changes while maintaining semantic consistency.  precise text editing : qwen-image-edit supports bilingual chinese and english text editing, allowing direct addition, deletion, and modification of text in images while preserving the original font, size, and style.  strong benchmark performance : evaluations on multiple public benchmarks demonstrate that qwen-image-edit achieves state-of-the-art sota performance in image editing tasks, establishing it as a powerful foundation model for image editing.\n created at: 2025-08-19T01:14:27+00:00\n- model_name: flymy-ai/qwen-image-edit-inscene-lora\n  description: an open-source lora low-rank adaptation model for qwen-image-edit that specializes in in-scene image editing by . agentic infra for genai. flymy.ai is a b2b infrastructure for building and running genai media agents. üîó useful links:\n created at: 2025-08-20T19:32:32+00:00\n- model_name: fuliucansheng/FLUX.1-Kontext-dev-diffusers\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-28T19:05:56+00:00\n- model_name: AIDC-AI/Ovis-U1-3B\n  description: building on the foundation of the ovis series, ovis-u1 is a 3-billion-parameter unified model that seamlessly integrates multimodal understanding, text-to-image generation, and image editing within a single powerful framework. the overall architecture of ovis-u1 cf. fig.2 in our report. ovis-u1 has been tested with python 3.10, torch 2.4.0, transformers 4.51.3, and deepspeed 0.15.4. for a comprehensive list of package dependencies, please consult the requirements.txt file.\n created at: 2025-06-28T12:00:29+00:00\n- model_name: fofr/kontext-make-person-real\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on using: you should use `make this person look real` as part of the prompt instruction for your image-to-image editing.\n created at: 2025-07-19T14:35:21+00:00\n- model_name: xiaolxl/HXHY-Asian-style-realism-Realistic-KontextLoRA\n  description: :nsfw,kontext,,„ÄÇ model statement: the training material does not contain any nsfw or minor characters, and the training material is only used to adjust the effect of the original kontext model. when you use this model, the output results will be generated based on the images you use, so please do not use it for any illegal or violation purposes.\n created at: 2025-07-07T02:38:04+00:00\n- model_name: 1038lab/SDMatte\n  description: this repository provides safetensors versions of the sdmatte models for interactive image matting , optimized for seamless use with comfyui .  sdmatte: grafting diffusion models for interactive matting is a state-of-the-art model that leverages the power of diffusion priors to achieve high-precision matting especially around fine details and complex edges.\n created at: 2025-08-17T19:04:45+00:00\n- model_name: AlekseyCalvin/QWEN_IMAGE_nf4_w_AbliteratedTE_Diffusers\n  description: this repo contains a variant of qwen's , the state-of-the-art generative model with extensive and image/text-to-image &/or instruction/control-editing capabilities. to make these cutting edge capabilities more accessible to those constrained to low-end consumer-grade hardware, we've quantized the dit diffusion transformer component of qwen-image to the 4-bit nf4 format using the bits&bytes toolkit. this optimization was derived by us directly from the bf16 base model weights released on 08/04/2025, with no other mix-ins or modifications to the dit component.\n created at: 2025-08-05T12:00:18+00:00\n- model_name: lucataco/kontext-realearth\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on using: you should use `make this a realistic drone photo` as part of the prompt instruction for your image-to-image editing.\n created at: 2025-07-22T20:28:00+00:00\n- model_name: gpustack/stable-diffusion-xl-refiner-1.0-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: diffusion-based text-to-image generative model - license:  - model description: this is a model that can be used to generate and modify images based on text prompts. it is a that uses two fixed, pretrained text encoders and . - resources for more information: check out our and the .\n created at: 2024-11-08T08:17:23+00:00\n- model_name: Aitrepreneur/text-dev\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-07-03T14:27:09+00:00\n- model_name: AlekseyCalvin/QWEN_IMAGE_fp4_w_AbliteratedTE_Diffusers\n  description: this repo contains a variant of qwen's , the state-of-the-art generative model with extensive and image/text-to-image &/or instruction/control-editing capabilities. to make these cutting edge capabilities more accessible to those constrained to low-end consumer-grade hardware, we've quantized the dit diffusion transformer component of qwen-image to the 4-bit fp4 format using the bits&bytes toolkit. this optimization was derived by us directly from the bf16 base model weights released on 08/04/2025, with no other mix-ins or modifications to the dit component.\n created at: 2025-08-06T04:07:08+00:00\n\nExamples:\n- example user query: Our customer is an advertising agency, and they need a tool to convert a sketch into a final image. They ask for a solution that incorportes suggestions for changes such as \"Add more color to the sky\" or \"Make the house look larger\".\n  model_name: lllyasviel/control_v11e_sd15_ip2p\n- example user query: I have a photo of a landscape taken during winter. I want to change the landscape style to make it look like it was taken during summer.\n  model_name: GreeneryScenery/SheepsControlV5\n- example user query: We are an online photo editing service, and one of the customers sent us a flying skate-board photo, which he said the photo was blurred. Help him to deblur the photo.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: As a magazine cover designer, we need to create some visually attractive variations of the original image for the upcoming edition.\n  model_name: lambdalabs/sd-image-variations-diffusers\n- example user query: We are a photo editing company, and we need to deblur images using AI.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: I own a travel agency and would like to create a flyer that showcases beautiful scenery. I have a few photos, but I'd like to change their style to make them more visually appealing. Can you assist me in transforming the input images into a different style?\n  model_name: GreeneryScenery/SheepsControlV5\n\nUser prompt:\nI was wondering if you could change snow to volcano?\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_edit>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: black-forest-labs/FLUX.1-Kontext-dev\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-05-28T22:23:43+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-kontext-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of editing images based on text instructions. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-06-28T22:57:58+00:00\n- model_name: Qwen/Qwen-Image-Edit\n  description: üíú qwen chat&nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë tech report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp üñ•Ô∏è demo&nbsp&nbsp &nbsp&nbspüí¨ wechat &nbsp&nbsp &nbsp&nbspü´® discord&nbsp&nbsp &nbsp&nbsp github&nbsp&nbsp we are excited to introduce qwen-image-edit, the image editing version of qwen-image. built upon our 20b qwen-image model, qwen-image-edit successfully extends qwen-image‚Äôs unique text rendering capabilities to image editing tasks, enabling precise text editing. furthermore, qwen-image-edit simultaneously feeds the input image into qwen2.5-vl for visual semantic control and the vae encoder for visual appearance control, achieving capabilities in both semantic and appearance editing. to experience the latest model, visit and select the \"image editing\" feature. we are excited to introduce qwen-image-edit, the image editing version of qwen-image. built upon our 20b qwen-image model, qwen-image-edit successfully extends qwen-image‚Äôs unique text rendering capabilities to image editing tasks, enabling precise text editing. furthermore, qwen-image-edit simultaneously feeds the input image into qwen2.5-vl for visual semantic control and the vae encoder for visual appearance control, achieving capabilities in both semantic and appearance editing. to experience the latest model, visit and select the \"image editing\" feature. key features:  semantic and appearance editing : qwen-image-edit supports both low-level visual appearance editing such as adding, removing, or modifying elements, requiring all other regions of the image to remain completely unchanged and high-level visual semantic editing such as ip creation, object rotation, and style transfer, allowing overall pixel changes while maintaining semantic consistency.  precise text editing : qwen-image-edit supports bilingual chinese and english text editing, allowing direct addition, deletion, and modification of text in images while preserving the original font, size, and style.  strong benchmark performance : evaluations on multiple public benchmarks demonstrate that qwen-image-edit achieves state-of-the-art sota performance in image editing tasks, establishing it as a powerful foundation model for image editing.\n created at: 2025-08-17T08:32:01+00:00\n- model_name: bullerwins/FLUX.1-Kontext-dev-GGUF\n  description: created with example workflow added in files `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-26T16:26:45+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-kontext-dev\n  description: this repository contains nunchaku-quantized versions of , capable of editing images based on text instructions. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:52:52+00:00\n- model_name: unsloth/FLUX.1-Kontext-dev-GGUF\n  description: gguf files for . original license still applies. a huge thanks to and for help with conversion. ggufs were created with and . to run, use comfyui-gguf and place the model files in the `comfyui/models/unet` directory. for more instructions, go to the readme. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-30T10:10:09+00:00\n- model_name: calcuis/qwen-image-edit-gguf\n  description: - use 8-step lite-lora auto applied; save up to 70% loading time - run it with `gguf-connector`; simply execute the command below in console/terminal\n created at: 2025-08-19T13:14:02+00:00\n- model_name: stanfordmimi/MedVAE\n  description: the model was presented in the paper . abstract: medical images are acquired at high resolutions with large fields of view in order to capture fine-grained features necessary for clinical decision-making. consequently, training deep learning models on medical images can incur large computational costs. in this work, we address the challenge of downsizing medical images in order to improve downstream computational efficiency while preserving clinically-relevant features. we introduce medvae, a family of six large-scale 2d and 3d autoencoders capable of encoding medical images as downsized latent representations and decoding latent representations back to high-resolution images. we train medvae autoencoders using a novel two-stage training approach with 1,052,730 medical images. across diverse tasks obtained from 20 medical image datasets, we demonstrate that 1 utilizing medvae latent representations in place of high-resolution images when training downstream models can lead to efficiency benefits up to 70x improvement in throughput while simultaneously preserving clinically-relevant features and 2 medvae can decode latent representations back to high-resolution images with high fidelity. our work demonstrates that large-scale, generalizable autoencoders can help address critical efficiency challenges in the medical domain. medvae is a family of six large-scale, generalizable 2d and 3d variational autoencoders vaes designed for medical imaging. it is trained on over one million medical images across multiple anatomical regions and modalities. medvae autoencoders encode medical images as downsized latent representations and decode latent representations back to high-resolution images. across diverse tasks obtained from 20 medical image datasets, we demonstrate that utilizing medvae latent representations in place of high-resolution images when training downstream models can lead to efficiency benefits up to 70x improvement in throughput while simultaneously preserving clinically-relevant features. medvae is a family of six large-scale, generalizable 2d and 3d variational autoencoders vaes designed for medical imaging. it is trained on over one million medical images across multiple anatomical regions and modalities. medvae autoencoders encode medical images as downsized latent representations and decode latent representations back to high-resolution images. across diverse tasks obtained from 20 medical image datasets, we demonstrate that utilizing medvae latent representations in place of high-resolution images when training downstream models can lead to efficiency benefits up to 70x improvement in throughput while simultaneously preserving clinically-relevant features.  total compression factor channels dimensions modalities anatomies config file model file            16 1 2d x-ray chest, breast ffdm   16 3 2d x-ray chest, breast ffdm   64 1 2d x-ray chest, breast ffdm   64 3 2d x-ray chest, breast ffdm   64 1 3d mri, ct whole-body   512 1 3d mri, ct whole-body  note: model weights and checkpoints are located in the `model weights` folder.\n created at: 2025-01-29T02:28:23+00:00\n- model_name: ovedrive/qwen-image-edit-4bit\n  description: this is an nf4 quantized model of qwen-image-edit so it can run on gpus using 20gb vram. you can run it on lower vram like 16gb. there were other nf4 models but they made the mistake of blindly quantizing all layers in the transformer. this one does not. we retain some layers at full precision in order to ensure that we get quality output. we are excited to introduce qwen-image-edit, the image editing version of qwen-image. built upon our 20b qwen-image model, qwen-image-edit successfully extends qwen-image‚Äôs unique text rendering capabilities to image editing tasks, enabling precise text editing. furthermore, qwen-image-edit simultaneously feeds the input image into qwen2.5-vl for visual semantic control and the vae encoder for visual appearance control, achieving capabilities in both semantic and appearance editing. to experience the latest model, visit and select the \"image editing\" feature. key features:  semantic and appearance editing : qwen-image-edit supports both low-level visual appearance editing such as adding, removing, or modifying elements, requiring all other regions of the image to remain completely unchanged and high-level visual semantic editing such as ip creation, object rotation, and style transfer, allowing overall pixel changes while maintaining semantic consistency.  precise text editing : qwen-image-edit supports bilingual chinese and english text editing, allowing direct addition, deletion, and modification of text in images while preserving the original font, size, and style.  strong benchmark performance : evaluations on multiple public benchmarks demonstrate that qwen-image-edit achieves state-of-the-art sota performance in image editing tasks, establishing it as a powerful foundation model for image editing.\n created at: 2025-08-19T01:14:27+00:00\n- model_name: flymy-ai/qwen-image-edit-inscene-lora\n  description: an open-source lora low-rank adaptation model for qwen-image-edit that specializes in in-scene image editing by . agentic infra for genai. flymy.ai is a b2b infrastructure for building and running genai media agents. üîó useful links:\n created at: 2025-08-20T19:32:32+00:00\n- model_name: fuliucansheng/FLUX.1-Kontext-dev-diffusers\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-28T19:05:56+00:00\n- model_name: AIDC-AI/Ovis-U1-3B\n  description: building on the foundation of the ovis series, ovis-u1 is a 3-billion-parameter unified model that seamlessly integrates multimodal understanding, text-to-image generation, and image editing within a single powerful framework. the overall architecture of ovis-u1 cf. fig.2 in our report. ovis-u1 has been tested with python 3.10, torch 2.4.0, transformers 4.51.3, and deepspeed 0.15.4. for a comprehensive list of package dependencies, please consult the requirements.txt file.\n created at: 2025-06-28T12:00:29+00:00\n- model_name: fofr/kontext-make-person-real\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on using: you should use `make this person look real` as part of the prompt instruction for your image-to-image editing.\n created at: 2025-07-19T14:35:21+00:00\n- model_name: xiaolxl/HXHY-Asian-style-realism-Realistic-KontextLoRA\n  description: :nsfw,kontext,,„ÄÇ model statement: the training material does not contain any nsfw or minor characters, and the training material is only used to adjust the effect of the original kontext model. when you use this model, the output results will be generated based on the images you use, so please do not use it for any illegal or violation purposes.\n created at: 2025-07-07T02:38:04+00:00\n- model_name: 1038lab/SDMatte\n  description: this repository provides safetensors versions of the sdmatte models for interactive image matting , optimized for seamless use with comfyui .  sdmatte: grafting diffusion models for interactive matting is a state-of-the-art model that leverages the power of diffusion priors to achieve high-precision matting especially around fine details and complex edges.\n created at: 2025-08-17T19:04:45+00:00\n- model_name: AlekseyCalvin/QWEN_IMAGE_nf4_w_AbliteratedTE_Diffusers\n  description: this repo contains a variant of qwen's , the state-of-the-art generative model with extensive and image/text-to-image &/or instruction/control-editing capabilities. to make these cutting edge capabilities more accessible to those constrained to low-end consumer-grade hardware, we've quantized the dit diffusion transformer component of qwen-image to the 4-bit nf4 format using the bits&bytes toolkit. this optimization was derived by us directly from the bf16 base model weights released on 08/04/2025, with no other mix-ins or modifications to the dit component.\n created at: 2025-08-05T12:00:18+00:00\n- model_name: lucataco/kontext-realearth\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on using: you should use `make this a realistic drone photo` as part of the prompt instruction for your image-to-image editing.\n created at: 2025-07-22T20:28:00+00:00\n- model_name: gpustack/stable-diffusion-xl-refiner-1.0-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: diffusion-based text-to-image generative model - license:  - model description: this is a model that can be used to generate and modify images based on text prompts. it is a that uses two fixed, pretrained text encoders and . - resources for more information: check out our and the .\n created at: 2024-11-08T08:17:23+00:00\n- model_name: Aitrepreneur/text-dev\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-07-03T14:27:09+00:00\n- model_name: AlekseyCalvin/QWEN_IMAGE_fp4_w_AbliteratedTE_Diffusers\n  description: this repo contains a variant of qwen's , the state-of-the-art generative model with extensive and image/text-to-image &/or instruction/control-editing capabilities. to make these cutting edge capabilities more accessible to those constrained to low-end consumer-grade hardware, we've quantized the dit diffusion transformer component of qwen-image to the 4-bit fp4 format using the bits&bytes toolkit. this optimization was derived by us directly from the bf16 base model weights released on 08/04/2025, with no other mix-ins or modifications to the dit component.\n created at: 2025-08-06T04:07:08+00:00\n\nExamples:\n- example user query: Our customer is an advertising agency, and they need a tool to convert a sketch into a final image. They ask for a solution that incorportes suggestions for changes such as \"Add more color to the sky\" or \"Make the house look larger\".\n  model_name: lllyasviel/control_v11e_sd15_ip2p\n- example user query: I have a photo of a landscape taken during winter. I want to change the landscape style to make it look like it was taken during summer.\n  model_name: GreeneryScenery/SheepsControlV5\n- example user query: We are an online photo editing service, and one of the customers sent us a flying skate-board photo, which he said the photo was blurred. Help him to deblur the photo.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: As a magazine cover designer, we need to create some visually attractive variations of the original image for the upcoming edition.\n  model_name: lambdalabs/sd-image-variations-diffusers\n- example user query: We are a photo editing company, and we need to deblur images using AI.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: I own a travel agency and would like to create a flyer that showcases beautiful scenery. I have a few photos, but I'd like to change their style to make them more visually appealing. Can you assist me in transforming the input images into a different style?\n  model_name: GreeneryScenery/SheepsControlV5\n\nUser prompt:\nI would appreciate it if you could put coffee to calendar? I feel it will sharpen the focus.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_edit>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: black-forest-labs/FLUX.1-Kontext-dev\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-05-28T22:23:43+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-kontext-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of editing images based on text instructions. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-06-28T22:57:58+00:00\n- model_name: Qwen/Qwen-Image-Edit\n  description: üíú qwen chat&nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë tech report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp üñ•Ô∏è demo&nbsp&nbsp &nbsp&nbspüí¨ wechat &nbsp&nbsp &nbsp&nbspü´® discord&nbsp&nbsp &nbsp&nbsp github&nbsp&nbsp we are excited to introduce qwen-image-edit, the image editing version of qwen-image. built upon our 20b qwen-image model, qwen-image-edit successfully extends qwen-image‚Äôs unique text rendering capabilities to image editing tasks, enabling precise text editing. furthermore, qwen-image-edit simultaneously feeds the input image into qwen2.5-vl for visual semantic control and the vae encoder for visual appearance control, achieving capabilities in both semantic and appearance editing. to experience the latest model, visit and select the \"image editing\" feature. we are excited to introduce qwen-image-edit, the image editing version of qwen-image. built upon our 20b qwen-image model, qwen-image-edit successfully extends qwen-image‚Äôs unique text rendering capabilities to image editing tasks, enabling precise text editing. furthermore, qwen-image-edit simultaneously feeds the input image into qwen2.5-vl for visual semantic control and the vae encoder for visual appearance control, achieving capabilities in both semantic and appearance editing. to experience the latest model, visit and select the \"image editing\" feature. key features:  semantic and appearance editing : qwen-image-edit supports both low-level visual appearance editing such as adding, removing, or modifying elements, requiring all other regions of the image to remain completely unchanged and high-level visual semantic editing such as ip creation, object rotation, and style transfer, allowing overall pixel changes while maintaining semantic consistency.  precise text editing : qwen-image-edit supports bilingual chinese and english text editing, allowing direct addition, deletion, and modification of text in images while preserving the original font, size, and style.  strong benchmark performance : evaluations on multiple public benchmarks demonstrate that qwen-image-edit achieves state-of-the-art sota performance in image editing tasks, establishing it as a powerful foundation model for image editing.\n created at: 2025-08-17T08:32:01+00:00\n- model_name: bullerwins/FLUX.1-Kontext-dev-GGUF\n  description: created with example workflow added in files `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-26T16:26:45+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-kontext-dev\n  description: this repository contains nunchaku-quantized versions of , capable of editing images based on text instructions. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:52:52+00:00\n- model_name: unsloth/FLUX.1-Kontext-dev-GGUF\n  description: gguf files for . original license still applies. a huge thanks to and for help with conversion. ggufs were created with and . to run, use comfyui-gguf and place the model files in the `comfyui/models/unet` directory. for more instructions, go to the readme. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-30T10:10:09+00:00\n- model_name: calcuis/qwen-image-edit-gguf\n  description: - use 8-step lite-lora auto applied; save up to 70% loading time - run it with `gguf-connector`; simply execute the command below in console/terminal\n created at: 2025-08-19T13:14:02+00:00\n- model_name: stanfordmimi/MedVAE\n  description: the model was presented in the paper . abstract: medical images are acquired at high resolutions with large fields of view in order to capture fine-grained features necessary for clinical decision-making. consequently, training deep learning models on medical images can incur large computational costs. in this work, we address the challenge of downsizing medical images in order to improve downstream computational efficiency while preserving clinically-relevant features. we introduce medvae, a family of six large-scale 2d and 3d autoencoders capable of encoding medical images as downsized latent representations and decoding latent representations back to high-resolution images. we train medvae autoencoders using a novel two-stage training approach with 1,052,730 medical images. across diverse tasks obtained from 20 medical image datasets, we demonstrate that 1 utilizing medvae latent representations in place of high-resolution images when training downstream models can lead to efficiency benefits up to 70x improvement in throughput while simultaneously preserving clinically-relevant features and 2 medvae can decode latent representations back to high-resolution images with high fidelity. our work demonstrates that large-scale, generalizable autoencoders can help address critical efficiency challenges in the medical domain. medvae is a family of six large-scale, generalizable 2d and 3d variational autoencoders vaes designed for medical imaging. it is trained on over one million medical images across multiple anatomical regions and modalities. medvae autoencoders encode medical images as downsized latent representations and decode latent representations back to high-resolution images. across diverse tasks obtained from 20 medical image datasets, we demonstrate that utilizing medvae latent representations in place of high-resolution images when training downstream models can lead to efficiency benefits up to 70x improvement in throughput while simultaneously preserving clinically-relevant features. medvae is a family of six large-scale, generalizable 2d and 3d variational autoencoders vaes designed for medical imaging. it is trained on over one million medical images across multiple anatomical regions and modalities. medvae autoencoders encode medical images as downsized latent representations and decode latent representations back to high-resolution images. across diverse tasks obtained from 20 medical image datasets, we demonstrate that utilizing medvae latent representations in place of high-resolution images when training downstream models can lead to efficiency benefits up to 70x improvement in throughput while simultaneously preserving clinically-relevant features.  total compression factor channels dimensions modalities anatomies config file model file            16 1 2d x-ray chest, breast ffdm   16 3 2d x-ray chest, breast ffdm   64 1 2d x-ray chest, breast ffdm   64 3 2d x-ray chest, breast ffdm   64 1 3d mri, ct whole-body   512 1 3d mri, ct whole-body  note: model weights and checkpoints are located in the `model weights` folder.\n created at: 2025-01-29T02:28:23+00:00\n- model_name: ovedrive/qwen-image-edit-4bit\n  description: this is an nf4 quantized model of qwen-image-edit so it can run on gpus using 20gb vram. you can run it on lower vram like 16gb. there were other nf4 models but they made the mistake of blindly quantizing all layers in the transformer. this one does not. we retain some layers at full precision in order to ensure that we get quality output. we are excited to introduce qwen-image-edit, the image editing version of qwen-image. built upon our 20b qwen-image model, qwen-image-edit successfully extends qwen-image‚Äôs unique text rendering capabilities to image editing tasks, enabling precise text editing. furthermore, qwen-image-edit simultaneously feeds the input image into qwen2.5-vl for visual semantic control and the vae encoder for visual appearance control, achieving capabilities in both semantic and appearance editing. to experience the latest model, visit and select the \"image editing\" feature. key features:  semantic and appearance editing : qwen-image-edit supports both low-level visual appearance editing such as adding, removing, or modifying elements, requiring all other regions of the image to remain completely unchanged and high-level visual semantic editing such as ip creation, object rotation, and style transfer, allowing overall pixel changes while maintaining semantic consistency.  precise text editing : qwen-image-edit supports bilingual chinese and english text editing, allowing direct addition, deletion, and modification of text in images while preserving the original font, size, and style.  strong benchmark performance : evaluations on multiple public benchmarks demonstrate that qwen-image-edit achieves state-of-the-art sota performance in image editing tasks, establishing it as a powerful foundation model for image editing.\n created at: 2025-08-19T01:14:27+00:00\n- model_name: flymy-ai/qwen-image-edit-inscene-lora\n  description: an open-source lora low-rank adaptation model for qwen-image-edit that specializes in in-scene image editing by . agentic infra for genai. flymy.ai is a b2b infrastructure for building and running genai media agents. üîó useful links:\n created at: 2025-08-20T19:32:32+00:00\n- model_name: fuliucansheng/FLUX.1-Kontext-dev-diffusers\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-28T19:05:56+00:00\n- model_name: AIDC-AI/Ovis-U1-3B\n  description: building on the foundation of the ovis series, ovis-u1 is a 3-billion-parameter unified model that seamlessly integrates multimodal understanding, text-to-image generation, and image editing within a single powerful framework. the overall architecture of ovis-u1 cf. fig.2 in our report. ovis-u1 has been tested with python 3.10, torch 2.4.0, transformers 4.51.3, and deepspeed 0.15.4. for a comprehensive list of package dependencies, please consult the requirements.txt file.\n created at: 2025-06-28T12:00:29+00:00\n- model_name: fofr/kontext-make-person-real\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on using: you should use `make this person look real` as part of the prompt instruction for your image-to-image editing.\n created at: 2025-07-19T14:35:21+00:00\n- model_name: xiaolxl/HXHY-Asian-style-realism-Realistic-KontextLoRA\n  description: :nsfw,kontext,,„ÄÇ model statement: the training material does not contain any nsfw or minor characters, and the training material is only used to adjust the effect of the original kontext model. when you use this model, the output results will be generated based on the images you use, so please do not use it for any illegal or violation purposes.\n created at: 2025-07-07T02:38:04+00:00\n- model_name: 1038lab/SDMatte\n  description: this repository provides safetensors versions of the sdmatte models for interactive image matting , optimized for seamless use with comfyui .  sdmatte: grafting diffusion models for interactive matting is a state-of-the-art model that leverages the power of diffusion priors to achieve high-precision matting especially around fine details and complex edges.\n created at: 2025-08-17T19:04:45+00:00\n- model_name: AlekseyCalvin/QWEN_IMAGE_nf4_w_AbliteratedTE_Diffusers\n  description: this repo contains a variant of qwen's , the state-of-the-art generative model with extensive and image/text-to-image &/or instruction/control-editing capabilities. to make these cutting edge capabilities more accessible to those constrained to low-end consumer-grade hardware, we've quantized the dit diffusion transformer component of qwen-image to the 4-bit nf4 format using the bits&bytes toolkit. this optimization was derived by us directly from the bf16 base model weights released on 08/04/2025, with no other mix-ins or modifications to the dit component.\n created at: 2025-08-05T12:00:18+00:00\n- model_name: lucataco/kontext-realearth\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on using: you should use `make this a realistic drone photo` as part of the prompt instruction for your image-to-image editing.\n created at: 2025-07-22T20:28:00+00:00\n- model_name: gpustack/stable-diffusion-xl-refiner-1.0-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: diffusion-based text-to-image generative model - license:  - model description: this is a model that can be used to generate and modify images based on text prompts. it is a that uses two fixed, pretrained text encoders and . - resources for more information: check out our and the .\n created at: 2024-11-08T08:17:23+00:00\n- model_name: Aitrepreneur/text-dev\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-07-03T14:27:09+00:00\n- model_name: AlekseyCalvin/QWEN_IMAGE_fp4_w_AbliteratedTE_Diffusers\n  description: this repo contains a variant of qwen's , the state-of-the-art generative model with extensive and image/text-to-image &/or instruction/control-editing capabilities. to make these cutting edge capabilities more accessible to those constrained to low-end consumer-grade hardware, we've quantized the dit diffusion transformer component of qwen-image to the 4-bit fp4 format using the bits&bytes toolkit. this optimization was derived by us directly from the bf16 base model weights released on 08/04/2025, with no other mix-ins or modifications to the dit component.\n created at: 2025-08-06T04:07:08+00:00\n\nExamples:\n- example user query: Our customer is an advertising agency, and they need a tool to convert a sketch into a final image. They ask for a solution that incorportes suggestions for changes such as \"Add more color to the sky\" or \"Make the house look larger\".\n  model_name: lllyasviel/control_v11e_sd15_ip2p\n- example user query: I have a photo of a landscape taken during winter. I want to change the landscape style to make it look like it was taken during summer.\n  model_name: GreeneryScenery/SheepsControlV5\n- example user query: We are an online photo editing service, and one of the customers sent us a flying skate-board photo, which he said the photo was blurred. Help him to deblur the photo.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: As a magazine cover designer, we need to create some visually attractive variations of the original image for the upcoming edition.\n  model_name: lambdalabs/sd-image-variations-diffusers\n- example user query: We are a photo editing company, and we need to deblur images using AI.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: I own a travel agency and would like to create a flyer that showcases beautiful scenery. I have a few photos, but I'd like to change their style to make them more visually appealing. Can you assist me in transforming the input images into a different style?\n  model_name: GreeneryScenery/SheepsControlV5\n\nUser prompt:\nWould you swap lion with laptop? This change will draw attention to the background.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_edit>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: black-forest-labs/FLUX.1-Kontext-dev\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-05-28T22:23:43+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-kontext-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of editing images based on text instructions. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-06-28T22:57:58+00:00\n- model_name: Qwen/Qwen-Image-Edit\n  description: üíú qwen chat&nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë tech report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp üñ•Ô∏è demo&nbsp&nbsp &nbsp&nbspüí¨ wechat &nbsp&nbsp &nbsp&nbspü´® discord&nbsp&nbsp &nbsp&nbsp github&nbsp&nbsp we are excited to introduce qwen-image-edit, the image editing version of qwen-image. built upon our 20b qwen-image model, qwen-image-edit successfully extends qwen-image‚Äôs unique text rendering capabilities to image editing tasks, enabling precise text editing. furthermore, qwen-image-edit simultaneously feeds the input image into qwen2.5-vl for visual semantic control and the vae encoder for visual appearance control, achieving capabilities in both semantic and appearance editing. to experience the latest model, visit and select the \"image editing\" feature. we are excited to introduce qwen-image-edit, the image editing version of qwen-image. built upon our 20b qwen-image model, qwen-image-edit successfully extends qwen-image‚Äôs unique text rendering capabilities to image editing tasks, enabling precise text editing. furthermore, qwen-image-edit simultaneously feeds the input image into qwen2.5-vl for visual semantic control and the vae encoder for visual appearance control, achieving capabilities in both semantic and appearance editing. to experience the latest model, visit and select the \"image editing\" feature. key features:  semantic and appearance editing : qwen-image-edit supports both low-level visual appearance editing such as adding, removing, or modifying elements, requiring all other regions of the image to remain completely unchanged and high-level visual semantic editing such as ip creation, object rotation, and style transfer, allowing overall pixel changes while maintaining semantic consistency.  precise text editing : qwen-image-edit supports bilingual chinese and english text editing, allowing direct addition, deletion, and modification of text in images while preserving the original font, size, and style.  strong benchmark performance : evaluations on multiple public benchmarks demonstrate that qwen-image-edit achieves state-of-the-art sota performance in image editing tasks, establishing it as a powerful foundation model for image editing.\n created at: 2025-08-17T08:32:01+00:00\n- model_name: bullerwins/FLUX.1-Kontext-dev-GGUF\n  description: created with example workflow added in files `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-26T16:26:45+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-kontext-dev\n  description: this repository contains nunchaku-quantized versions of , capable of editing images based on text instructions. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:52:52+00:00\n- model_name: unsloth/FLUX.1-Kontext-dev-GGUF\n  description: gguf files for . original license still applies. a huge thanks to and for help with conversion. ggufs were created with and . to run, use comfyui-gguf and place the model files in the `comfyui/models/unet` directory. for more instructions, go to the readme. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-30T10:10:09+00:00\n- model_name: calcuis/qwen-image-edit-gguf\n  description: - use 8-step lite-lora auto applied; save up to 70% loading time - run it with `gguf-connector`; simply execute the command below in console/terminal\n created at: 2025-08-19T13:14:02+00:00\n- model_name: stanfordmimi/MedVAE\n  description: the model was presented in the paper . abstract: medical images are acquired at high resolutions with large fields of view in order to capture fine-grained features necessary for clinical decision-making. consequently, training deep learning models on medical images can incur large computational costs. in this work, we address the challenge of downsizing medical images in order to improve downstream computational efficiency while preserving clinically-relevant features. we introduce medvae, a family of six large-scale 2d and 3d autoencoders capable of encoding medical images as downsized latent representations and decoding latent representations back to high-resolution images. we train medvae autoencoders using a novel two-stage training approach with 1,052,730 medical images. across diverse tasks obtained from 20 medical image datasets, we demonstrate that 1 utilizing medvae latent representations in place of high-resolution images when training downstream models can lead to efficiency benefits up to 70x improvement in throughput while simultaneously preserving clinically-relevant features and 2 medvae can decode latent representations back to high-resolution images with high fidelity. our work demonstrates that large-scale, generalizable autoencoders can help address critical efficiency challenges in the medical domain. medvae is a family of six large-scale, generalizable 2d and 3d variational autoencoders vaes designed for medical imaging. it is trained on over one million medical images across multiple anatomical regions and modalities. medvae autoencoders encode medical images as downsized latent representations and decode latent representations back to high-resolution images. across diverse tasks obtained from 20 medical image datasets, we demonstrate that utilizing medvae latent representations in place of high-resolution images when training downstream models can lead to efficiency benefits up to 70x improvement in throughput while simultaneously preserving clinically-relevant features. medvae is a family of six large-scale, generalizable 2d and 3d variational autoencoders vaes designed for medical imaging. it is trained on over one million medical images across multiple anatomical regions and modalities. medvae autoencoders encode medical images as downsized latent representations and decode latent representations back to high-resolution images. across diverse tasks obtained from 20 medical image datasets, we demonstrate that utilizing medvae latent representations in place of high-resolution images when training downstream models can lead to efficiency benefits up to 70x improvement in throughput while simultaneously preserving clinically-relevant features.  total compression factor channels dimensions modalities anatomies config file model file            16 1 2d x-ray chest, breast ffdm   16 3 2d x-ray chest, breast ffdm   64 1 2d x-ray chest, breast ffdm   64 3 2d x-ray chest, breast ffdm   64 1 3d mri, ct whole-body   512 1 3d mri, ct whole-body  note: model weights and checkpoints are located in the `model weights` folder.\n created at: 2025-01-29T02:28:23+00:00\n- model_name: ovedrive/qwen-image-edit-4bit\n  description: this is an nf4 quantized model of qwen-image-edit so it can run on gpus using 20gb vram. you can run it on lower vram like 16gb. there were other nf4 models but they made the mistake of blindly quantizing all layers in the transformer. this one does not. we retain some layers at full precision in order to ensure that we get quality output. we are excited to introduce qwen-image-edit, the image editing version of qwen-image. built upon our 20b qwen-image model, qwen-image-edit successfully extends qwen-image‚Äôs unique text rendering capabilities to image editing tasks, enabling precise text editing. furthermore, qwen-image-edit simultaneously feeds the input image into qwen2.5-vl for visual semantic control and the vae encoder for visual appearance control, achieving capabilities in both semantic and appearance editing. to experience the latest model, visit and select the \"image editing\" feature. key features:  semantic and appearance editing : qwen-image-edit supports both low-level visual appearance editing such as adding, removing, or modifying elements, requiring all other regions of the image to remain completely unchanged and high-level visual semantic editing such as ip creation, object rotation, and style transfer, allowing overall pixel changes while maintaining semantic consistency.  precise text editing : qwen-image-edit supports bilingual chinese and english text editing, allowing direct addition, deletion, and modification of text in images while preserving the original font, size, and style.  strong benchmark performance : evaluations on multiple public benchmarks demonstrate that qwen-image-edit achieves state-of-the-art sota performance in image editing tasks, establishing it as a powerful foundation model for image editing.\n created at: 2025-08-19T01:14:27+00:00\n- model_name: flymy-ai/qwen-image-edit-inscene-lora\n  description: an open-source lora low-rank adaptation model for qwen-image-edit that specializes in in-scene image editing by . agentic infra for genai. flymy.ai is a b2b infrastructure for building and running genai media agents. üîó useful links:\n created at: 2025-08-20T19:32:32+00:00\n- model_name: fuliucansheng/FLUX.1-Kontext-dev-diffusers\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-28T19:05:56+00:00\n- model_name: AIDC-AI/Ovis-U1-3B\n  description: building on the foundation of the ovis series, ovis-u1 is a 3-billion-parameter unified model that seamlessly integrates multimodal understanding, text-to-image generation, and image editing within a single powerful framework. the overall architecture of ovis-u1 cf. fig.2 in our report. ovis-u1 has been tested with python 3.10, torch 2.4.0, transformers 4.51.3, and deepspeed 0.15.4. for a comprehensive list of package dependencies, please consult the requirements.txt file.\n created at: 2025-06-28T12:00:29+00:00\n- model_name: fofr/kontext-make-person-real\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on using: you should use `make this person look real` as part of the prompt instruction for your image-to-image editing.\n created at: 2025-07-19T14:35:21+00:00\n- model_name: xiaolxl/HXHY-Asian-style-realism-Realistic-KontextLoRA\n  description: :nsfw,kontext,,„ÄÇ model statement: the training material does not contain any nsfw or minor characters, and the training material is only used to adjust the effect of the original kontext model. when you use this model, the output results will be generated based on the images you use, so please do not use it for any illegal or violation purposes.\n created at: 2025-07-07T02:38:04+00:00\n- model_name: 1038lab/SDMatte\n  description: this repository provides safetensors versions of the sdmatte models for interactive image matting , optimized for seamless use with comfyui .  sdmatte: grafting diffusion models for interactive matting is a state-of-the-art model that leverages the power of diffusion priors to achieve high-precision matting especially around fine details and complex edges.\n created at: 2025-08-17T19:04:45+00:00\n- model_name: AlekseyCalvin/QWEN_IMAGE_nf4_w_AbliteratedTE_Diffusers\n  description: this repo contains a variant of qwen's , the state-of-the-art generative model with extensive and image/text-to-image &/or instruction/control-editing capabilities. to make these cutting edge capabilities more accessible to those constrained to low-end consumer-grade hardware, we've quantized the dit diffusion transformer component of qwen-image to the 4-bit nf4 format using the bits&bytes toolkit. this optimization was derived by us directly from the bf16 base model weights released on 08/04/2025, with no other mix-ins or modifications to the dit component.\n created at: 2025-08-05T12:00:18+00:00\n- model_name: lucataco/kontext-realearth\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on using: you should use `make this a realistic drone photo` as part of the prompt instruction for your image-to-image editing.\n created at: 2025-07-22T20:28:00+00:00\n- model_name: gpustack/stable-diffusion-xl-refiner-1.0-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: diffusion-based text-to-image generative model - license:  - model description: this is a model that can be used to generate and modify images based on text prompts. it is a that uses two fixed, pretrained text encoders and . - resources for more information: check out our and the .\n created at: 2024-11-08T08:17:23+00:00\n- model_name: Aitrepreneur/text-dev\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-07-03T14:27:09+00:00\n- model_name: AlekseyCalvin/QWEN_IMAGE_fp4_w_AbliteratedTE_Diffusers\n  description: this repo contains a variant of qwen's , the state-of-the-art generative model with extensive and image/text-to-image &/or instruction/control-editing capabilities. to make these cutting edge capabilities more accessible to those constrained to low-end consumer-grade hardware, we've quantized the dit diffusion transformer component of qwen-image to the 4-bit fp4 format using the bits&bytes toolkit. this optimization was derived by us directly from the bf16 base model weights released on 08/04/2025, with no other mix-ins or modifications to the dit component.\n created at: 2025-08-06T04:07:08+00:00\n\nExamples:\n- example user query: Our customer is an advertising agency, and they need a tool to convert a sketch into a final image. They ask for a solution that incorportes suggestions for changes such as \"Add more color to the sky\" or \"Make the house look larger\".\n  model_name: lllyasviel/control_v11e_sd15_ip2p\n- example user query: I have a photo of a landscape taken during winter. I want to change the landscape style to make it look like it was taken during summer.\n  model_name: GreeneryScenery/SheepsControlV5\n- example user query: We are an online photo editing service, and one of the customers sent us a flying skate-board photo, which he said the photo was blurred. Help him to deblur the photo.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: As a magazine cover designer, we need to create some visually attractive variations of the original image for the upcoming edition.\n  model_name: lambdalabs/sd-image-variations-diffusers\n- example user query: We are a photo editing company, and we need to deblur images using AI.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: I own a travel agency and would like to create a flyer that showcases beautiful scenery. I have a few photos, but I'd like to change their style to make them more visually appealing. Can you assist me in transforming the input images into a different style?\n  model_name: GreeneryScenery/SheepsControlV5\n\nUser prompt:\nMake the man smile.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_edit>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: black-forest-labs/FLUX.1-Kontext-dev\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-05-28T22:23:43+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-kontext-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , capable of editing images based on text instructions. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-06-28T22:57:58+00:00\n- model_name: Qwen/Qwen-Image-Edit\n  description: üíú qwen chat&nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë tech report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp üñ•Ô∏è demo&nbsp&nbsp &nbsp&nbspüí¨ wechat &nbsp&nbsp &nbsp&nbspü´® discord&nbsp&nbsp &nbsp&nbsp github&nbsp&nbsp we are excited to introduce qwen-image-edit, the image editing version of qwen-image. built upon our 20b qwen-image model, qwen-image-edit successfully extends qwen-image‚Äôs unique text rendering capabilities to image editing tasks, enabling precise text editing. furthermore, qwen-image-edit simultaneously feeds the input image into qwen2.5-vl for visual semantic control and the vae encoder for visual appearance control, achieving capabilities in both semantic and appearance editing. to experience the latest model, visit and select the \"image editing\" feature. we are excited to introduce qwen-image-edit, the image editing version of qwen-image. built upon our 20b qwen-image model, qwen-image-edit successfully extends qwen-image‚Äôs unique text rendering capabilities to image editing tasks, enabling precise text editing. furthermore, qwen-image-edit simultaneously feeds the input image into qwen2.5-vl for visual semantic control and the vae encoder for visual appearance control, achieving capabilities in both semantic and appearance editing. to experience the latest model, visit and select the \"image editing\" feature. key features:  semantic and appearance editing : qwen-image-edit supports both low-level visual appearance editing such as adding, removing, or modifying elements, requiring all other regions of the image to remain completely unchanged and high-level visual semantic editing such as ip creation, object rotation, and style transfer, allowing overall pixel changes while maintaining semantic consistency.  precise text editing : qwen-image-edit supports bilingual chinese and english text editing, allowing direct addition, deletion, and modification of text in images while preserving the original font, size, and style.  strong benchmark performance : evaluations on multiple public benchmarks demonstrate that qwen-image-edit achieves state-of-the-art sota performance in image editing tasks, establishing it as a powerful foundation model for image editing.\n created at: 2025-08-17T08:32:01+00:00\n- model_name: bullerwins/FLUX.1-Kontext-dev-GGUF\n  description: created with example workflow added in files `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-26T16:26:45+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-kontext-dev\n  description: this repository contains nunchaku-quantized versions of , capable of editing images based on text instructions. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: image-to-image - developed by: nunchaku team - model type: image-to-image - license:  - quantized from model:\n created at: 2025-07-17T00:52:52+00:00\n- model_name: unsloth/FLUX.1-Kontext-dev-GGUF\n  description: gguf files for . original license still applies. a huge thanks to and for help with conversion. ggufs were created with and . to run, use comfyui-gguf and place the model files in the `comfyui/models/unet` directory. for more instructions, go to the readme. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-30T10:10:09+00:00\n- model_name: calcuis/qwen-image-edit-gguf\n  description: - use 8-step lite-lora auto applied; save up to 70% loading time - run it with `gguf-connector`; simply execute the command below in console/terminal\n created at: 2025-08-19T13:14:02+00:00\n- model_name: stanfordmimi/MedVAE\n  description: the model was presented in the paper . abstract: medical images are acquired at high resolutions with large fields of view in order to capture fine-grained features necessary for clinical decision-making. consequently, training deep learning models on medical images can incur large computational costs. in this work, we address the challenge of downsizing medical images in order to improve downstream computational efficiency while preserving clinically-relevant features. we introduce medvae, a family of six large-scale 2d and 3d autoencoders capable of encoding medical images as downsized latent representations and decoding latent representations back to high-resolution images. we train medvae autoencoders using a novel two-stage training approach with 1,052,730 medical images. across diverse tasks obtained from 20 medical image datasets, we demonstrate that 1 utilizing medvae latent representations in place of high-resolution images when training downstream models can lead to efficiency benefits up to 70x improvement in throughput while simultaneously preserving clinically-relevant features and 2 medvae can decode latent representations back to high-resolution images with high fidelity. our work demonstrates that large-scale, generalizable autoencoders can help address critical efficiency challenges in the medical domain. medvae is a family of six large-scale, generalizable 2d and 3d variational autoencoders vaes designed for medical imaging. it is trained on over one million medical images across multiple anatomical regions and modalities. medvae autoencoders encode medical images as downsized latent representations and decode latent representations back to high-resolution images. across diverse tasks obtained from 20 medical image datasets, we demonstrate that utilizing medvae latent representations in place of high-resolution images when training downstream models can lead to efficiency benefits up to 70x improvement in throughput while simultaneously preserving clinically-relevant features. medvae is a family of six large-scale, generalizable 2d and 3d variational autoencoders vaes designed for medical imaging. it is trained on over one million medical images across multiple anatomical regions and modalities. medvae autoencoders encode medical images as downsized latent representations and decode latent representations back to high-resolution images. across diverse tasks obtained from 20 medical image datasets, we demonstrate that utilizing medvae latent representations in place of high-resolution images when training downstream models can lead to efficiency benefits up to 70x improvement in throughput while simultaneously preserving clinically-relevant features.  total compression factor channels dimensions modalities anatomies config file model file            16 1 2d x-ray chest, breast ffdm   16 3 2d x-ray chest, breast ffdm   64 1 2d x-ray chest, breast ffdm   64 3 2d x-ray chest, breast ffdm   64 1 3d mri, ct whole-body   512 1 3d mri, ct whole-body  note: model weights and checkpoints are located in the `model weights` folder.\n created at: 2025-01-29T02:28:23+00:00\n- model_name: ovedrive/qwen-image-edit-4bit\n  description: this is an nf4 quantized model of qwen-image-edit so it can run on gpus using 20gb vram. you can run it on lower vram like 16gb. there were other nf4 models but they made the mistake of blindly quantizing all layers in the transformer. this one does not. we retain some layers at full precision in order to ensure that we get quality output. we are excited to introduce qwen-image-edit, the image editing version of qwen-image. built upon our 20b qwen-image model, qwen-image-edit successfully extends qwen-image‚Äôs unique text rendering capabilities to image editing tasks, enabling precise text editing. furthermore, qwen-image-edit simultaneously feeds the input image into qwen2.5-vl for visual semantic control and the vae encoder for visual appearance control, achieving capabilities in both semantic and appearance editing. to experience the latest model, visit and select the \"image editing\" feature. key features:  semantic and appearance editing : qwen-image-edit supports both low-level visual appearance editing such as adding, removing, or modifying elements, requiring all other regions of the image to remain completely unchanged and high-level visual semantic editing such as ip creation, object rotation, and style transfer, allowing overall pixel changes while maintaining semantic consistency.  precise text editing : qwen-image-edit supports bilingual chinese and english text editing, allowing direct addition, deletion, and modification of text in images while preserving the original font, size, and style.  strong benchmark performance : evaluations on multiple public benchmarks demonstrate that qwen-image-edit achieves state-of-the-art sota performance in image editing tasks, establishing it as a powerful foundation model for image editing.\n created at: 2025-08-19T01:14:27+00:00\n- model_name: flymy-ai/qwen-image-edit-inscene-lora\n  description: an open-source lora low-rank adaptation model for qwen-image-edit that specializes in in-scene image editing by . agentic infra for genai. flymy.ai is a b2b infrastructure for building and running genai media agents. üîó useful links:\n created at: 2025-08-20T19:32:32+00:00\n- model_name: fuliucansheng/FLUX.1-Kontext-dev-diffusers\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-28T19:05:56+00:00\n- model_name: AIDC-AI/Ovis-U1-3B\n  description: building on the foundation of the ovis series, ovis-u1 is a 3-billion-parameter unified model that seamlessly integrates multimodal understanding, text-to-image generation, and image editing within a single powerful framework. the overall architecture of ovis-u1 cf. fig.2 in our report. ovis-u1 has been tested with python 3.10, torch 2.4.0, transformers 4.51.3, and deepspeed 0.15.4. for a comprehensive list of package dependencies, please consult the requirements.txt file.\n created at: 2025-06-28T12:00:29+00:00\n- model_name: fofr/kontext-make-person-real\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on using: you should use `make this person look real` as part of the prompt instruction for your image-to-image editing.\n created at: 2025-07-19T14:35:21+00:00\n- model_name: xiaolxl/HXHY-Asian-style-realism-Realistic-KontextLoRA\n  description: :nsfw,kontext,,„ÄÇ model statement: the training material does not contain any nsfw or minor characters, and the training material is only used to adjust the effect of the original kontext model. when you use this model, the output results will be generated based on the images you use, so please do not use it for any illegal or violation purposes.\n created at: 2025-07-07T02:38:04+00:00\n- model_name: 1038lab/SDMatte\n  description: this repository provides safetensors versions of the sdmatte models for interactive image matting , optimized for seamless use with comfyui .  sdmatte: grafting diffusion models for interactive matting is a state-of-the-art model that leverages the power of diffusion priors to achieve high-precision matting especially around fine details and complex edges.\n created at: 2025-08-17T19:04:45+00:00\n- model_name: AlekseyCalvin/QWEN_IMAGE_nf4_w_AbliteratedTE_Diffusers\n  description: this repo contains a variant of qwen's , the state-of-the-art generative model with extensive and image/text-to-image &/or instruction/control-editing capabilities. to make these cutting edge capabilities more accessible to those constrained to low-end consumer-grade hardware, we've quantized the dit diffusion transformer component of qwen-image to the 4-bit nf4 format using the bits&bytes toolkit. this optimization was derived by us directly from the bf16 base model weights released on 08/04/2025, with no other mix-ins or modifications to the dit component.\n created at: 2025-08-05T12:00:18+00:00\n- model_name: lucataco/kontext-realearth\n  description: this is a for the flux.1-kontext-dev image-to-image model. it can be used with diffusers or comfyui. it was trained on using: you should use `make this a realistic drone photo` as part of the prompt instruction for your image-to-image editing.\n created at: 2025-07-22T20:28:00+00:00\n- model_name: gpustack/stable-diffusion-xl-refiner-1.0-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: diffusion-based text-to-image generative model - license:  - model description: this is a model that can be used to generate and modify images based on text prompts. it is a that uses two fixed, pretrained text encoders and . - resources for more information: check out our and the .\n created at: 2024-11-08T08:17:23+00:00\n- model_name: Aitrepreneur/text-dev\n  description: !flux.1 dev grid./teaser.png `flux.1 kontext dev` is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. for more information, please read our and our . you can find information about the `pro` version in . 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-07-03T14:27:09+00:00\n- model_name: AlekseyCalvin/QWEN_IMAGE_fp4_w_AbliteratedTE_Diffusers\n  description: this repo contains a variant of qwen's , the state-of-the-art generative model with extensive and image/text-to-image &/or instruction/control-editing capabilities. to make these cutting edge capabilities more accessible to those constrained to low-end consumer-grade hardware, we've quantized the dit diffusion transformer component of qwen-image to the 4-bit fp4 format using the bits&bytes toolkit. this optimization was derived by us directly from the bf16 base model weights released on 08/04/2025, with no other mix-ins or modifications to the dit component.\n created at: 2025-08-06T04:07:08+00:00\n\nExamples:\n- example user query: Our customer is an advertising agency, and they need a tool to convert a sketch into a final image. They ask for a solution that incorportes suggestions for changes such as \"Add more color to the sky\" or \"Make the house look larger\".\n  model_name: lllyasviel/control_v11e_sd15_ip2p\n- example user query: I have a photo of a landscape taken during winter. I want to change the landscape style to make it look like it was taken during summer.\n  model_name: GreeneryScenery/SheepsControlV5\n- example user query: We are an online photo editing service, and one of the customers sent us a flying skate-board photo, which he said the photo was blurred. Help him to deblur the photo.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: As a magazine cover designer, we need to create some visually attractive variations of the original image for the upcoming edition.\n  model_name: lambdalabs/sd-image-variations-diffusers\n- example user query: We are a photo editing company, and we need to deblur images using AI.\n  model_name: google/maxim-s3-deblurring-gopro\n- example user query: I own a travel agency and would like to create a flyer that showcases beautiful scenery. I have a few photos, but I'd like to change their style to make them more visually appealing. Can you assist me in transforming the input images into a different style?\n  model_name: GreeneryScenery/SheepsControlV5\n\nUser prompt:\nGive the man a helmet.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n"
  ],
  "<image_gen>": [
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_gen>\n\nCandidate Models:\n- model_name: runwayml/stable-diffusion-v1-5\n  description: Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.\n created at: 2023-05-25\n- model_name: CompVis/stable-diffusion-v1-4\n  description: Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v1-4 checkpoint was fine-tuned on 225k steps at resolution 512x512 on laion-aesthetics v2 5+ and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. This model is intended for research purposes and can be used for generating artworks, design, educational or creative tools, and research on generative models.\n created at: 2022-08-20\n- model_name: prompthero/openjourney\n  description: Openjourney is an open source Stable Diffusion fine-tuned model on Midjourney images, by PromptHero. It can be used for generating AI art based on text prompts.\n created at: 2022-11-08\n- model_name: hakurei/waifu-diffusion\n  description: waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.\n created at: 2022-08-30\n- model_name: runwayml/stable-diffusion-inpainting\n  description: Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.\n created at: 2023-05-25\n- model_name: stabilityai/stable-diffusion-2-1\n  description: Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes.\n created at: 2022-12-06\n- model_name: SG161222/Realistic_Vision_V1.4\n  description: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images.\n created at: 2023-03-05\n- model_name: stabilityai/stable-diffusion-2-1-base\n  description: Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H). It is intended for research purposes only and can be used in areas such as safe deployment of models, understanding limitations and biases of generative models, generation of artworks, and research on generative models.\n created at: 2022-12-06\n- model_name: dreamlike-art/dreamlike-photoreal-2.0\n  description: Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art. It can be used to generate photorealistic images from text prompts.\n created at: 2023-01-04\n- model_name: prompthero/openjourney-v4\n  description: Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.\n created at: 2022-12-11\n- model_name: stabilityai/stable-diffusion-2\n  description: Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.\n created at: 2022-11-23\n- model_name: stabilityai/stable-diffusion-2-depth\n  description: Stable Diffusion v2 is a latent diffusion model that generates and modifies images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is developed by Robin Rombach and Patrick Esser. The model works with English language prompts and is intended for research purposes only.\n created at: 2022-11-23\n- model_name: eimiss/EimisAnimeDiffusion_1.0v\n  description: EimisAnimeDiffusion_1.0v is a text-to-image model trained with high-quality and detailed anime images. It works well on anime and landscape generations and supports a Gradio Web UI.\n created at: 2022-11-15\n- model_name: stabilityai/stable-diffusion-2-base\n  description: Stable Diffusion v2-base is a diffusion-based text-to-image generation model trained on a subset of LAION-5B dataset. It can be used to generate and modify images based on text prompts. The model uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is intended for research purposes only.\n created at: 2022-11-23\n- model_name: Linaqruf/anything-v3.0\n  description: A text-to-image model that generates images from text descriptions.\n created at: 2023-01-25\n- model_name: wavymulder/Analog-Diffusion\n  description: Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\n created at: 2022-12-10\n- model_name: nitrosocke/nitro-diffusion\n  description: Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use.\n created at: 2022-11-16\n- model_name: Lykon/DreamShaper\n  description: Dream Shaper is a text-to-image model that generates artistic images based on the given input text. Read more about this model here: https://civitai.com/models/4384/dreamshaper\n created at: 2023-01-12\n- model_name: dreamlike-art/dreamlike-diffusion-1.0\n  description: Dreamlike Diffusion 1.0 is SD 1.5 fine tuned on high quality art, made by dreamlike.art.\n created at: 2022-12-11\n- model_name: gsdf/Counterfeit-V2.5\n  description: Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.\n created at: 2023-02-02\n- model_name: dreamlike-art/dreamlike-anime-1.0\n  description: Dreamlike Anime 1.0 is a high quality anime model, made by dreamlike.art. It can be used to generate anime-style images based on text prompts. The model is trained on 768x768px images and works best with prompts that include 'photo anime, masterpiece, high quality, absurdres'. It can be used with the Stable Diffusion Pipeline from the diffusers library.\n created at: 2023-01-08\n- model_name: 22h/vintedois-diffusion-v0-1\n  description: Vintedois (22h) Diffusion model trained by Predogl and piEsposito with open weights, configs and prompts. This model generates beautiful images without a lot of prompt engineering. It can also generate high fidelity faces with a little amount of steps.\n created at: 2022-12-27\n- model_name: darkstorm2150/Protogen_v2.2_Official_Release\n  description: Protogen v2.2 is a text-to-image model that generates high-quality images based on text prompts. It was warm-started with Stable Diffusion v1-5 and fine-tuned on a large amount of data from large datasets new and trending on civitai.com. The model can be used with the Stable Diffusion Pipeline and supports trigger words like 'modelshoot style' to enforce camera capture.\n created at: 2022-12-31\n- model_name: stabilityai/stable-diffusion-x4-upscaler\n  description: Stable Diffusion x4 upscaler is a latent diffusion model trained on a 10M subset of LAION containing images >2048x2048. It can be used to generate and modify images based on text prompts. The model receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule. The model is trained with English captions and might not work well with other languages.\n created at: 2022-11-23\n- model_name: stabilityai/sd-x2-latent-upscaler\n  description: Stable Diffusion x2 latent upscaler is a diffusion-based upscaler model developed by Katherine Crowson in collaboration with Stability AI. It is designed to upscale Stable Diffusion's latent denoised image embeddings, allowing for fast text-to-image and upscaling pipelines. The model was trained on a high-resolution subset of the LAION-2B dataset and works with all Stable Diffusion checkpoints.\n created at: 2023-02-03\n- model_name: stabilityai/stable-diffusion-2-inpainting\n  description: A Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) to generate and modify images based on text prompts.\n created at: 2022-11-23\n- model_name: Qwen/Qwen-Image\n  description: üíú qwen chat&nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë tech report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp üñ•Ô∏è demo&nbsp&nbsp &nbsp&nbspüí¨ wechat &nbsp&nbsp &nbsp&nbspü´® discord&nbsp&nbsp we are thrilled to release qwen-image , an image generation foundation model in the qwen series that achieves significant advances in complex text rendering and precise image editing . experiments show strong general capabilities in both image generation and editing, with exceptional performance in text rendering, especially for chinese. we are thrilled to release qwen-image , an image generation foundation model in the qwen series that achieves significant advances in complex text rendering and precise image editing . experiments show strong general capabilities in both image generation and editing, with exceptional performance in text rendering, especially for chinese.\n created at: 2025-08-02T04:58:07+00:00\n- model_name: deepseek-ai/Janus-Pro-7B\n  description: janus-pro is a novel autoregressive framework that unifies multimodal understanding and generation. it addresses the limitations of previous approaches by decoupling visual encoding into separate pathways, while still utilizing a single, unified transformer architecture for processing. the decoupling not only alleviates the conflict between the visual encoder‚Äôs roles in understanding and generation, but also enhances the framework‚Äôs flexibility. janus-pro surpasses previous unified model and matches or exceeds the performance of task-specific models.\n created at: 2025-01-26T12:05:50+00:00\n- model_name: HiDream-ai/HiDream-I1-Full\n  description: `hidream-i1` is a new open-source image generative foundation model with 17b parameters that achieves state-of-the-art image generation quality within seconds. for more features and to experience the full capabilities of our product, please visit . - üåü july 16, 2025 : we've open-sourced the updated image editing model . - ‚ú® superior image quality - produces exceptional results across multiple styles including photorealistic, cartoon, artistic, and more. achieves state-of-the-art hps v2.1 score, which aligns with human preferences. - üéØ best-in-class prompt following - achieves industry-leading scores on geneval and dpg benchmarks, outperforming all other open-source models. - üîì open source - released under the mit license to foster scientific advancement and enable creative innovation. - üíº commercial-friendly - generated images can be freely used for personal projects, scientific research, and commercial applications.\n created at: 2025-04-06T14:18:31+00:00\n- model_name: stabilityai/stable-diffusion-3.5-medium\n  description: is a multimodal diffusion transformer with improvements mmdit-x text-to-image model that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency. please note: this model is released under the . visit to learn or for commercial licensing details. - developed by: stability ai - developed by: stability ai - model type: mmdit-x text-to-image generative model - model description: this model generates images based on text prompts. it is a multimodal diffusion transformer  with improvements that use three fixed, pretrained text encoders, with qk-normalization to improve training stability, and dual attention blocks in the first 12 transformer layers.\n created at: 2024-10-29T10:27:32+00:00\n- model_name: black-forest-labs/FLUX.1-Krea-dev\n  description: !flux.1 krea dev grid./teaser.png `flux.1 krea dev` is a 12 billion parameter rectified flow transformer capable of generating images from text descriptions. for more information, please read our and . 1. cutting-edge output quality, with a focus on aesthetic photography. 2. competitive prompt following, matching the performance of closed source alternatives. 3. trained using guidance distillation, making `flux.1 krea dev` more efficient. 4. open weights to drive new scientific research, and empower artists to develop innovative workflows. 5. generated outputs can be used for personal, scientific, and commercial purposes, as described in the .\n created at: 2025-07-07T10:39:11+00:00\n- model_name: OnomaAIResearch/Illustrious-xl-early-release-v0\n  description: @import url' .title-container display: flex; in summary, onoma ai plans to roll out open-source weights step by step and encourages the community to stay tuned for upcoming developments we‚Äôre just getting started.\n created at: 2024-09-20T14:56:38+00:00\n- model_name: gpustack/stable-diffusion-v3-5-large-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: mmdit text-to-image generative model - model description: this model generates images based on text prompts. it is a that use three fixed, pretrained text encoders, and with qk-normalization to improve training stability.\n created at: 2024-11-11T16:06:53+00:00\n- model_name: stabilityai/stable-diffusion-3.5-large\n  description: is a multimodal diffusion transformer mmdit text-to-image model that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency. please note: this model is released under the . visit to learn or for commercial licensing details. - developed by: stability ai - developed by: stability ai - model type: mmdit text-to-image generative model - model description: this model generates images based on text prompts. it is a that use three fixed, pretrained text encoders, and with qk-normalization to improve training stability.\n created at: 2024-10-22T07:29:57+00:00\n- model_name: gpustack/stable-diffusion-v3-5-large-turbo-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: mmdit text-to-image generative model - model description: this model generates images based on text prompts. it is an add-distilled that use three fixed, pretrained text encoders, and with qk-normalization.\n created at: 2024-11-13T00:25:51+00:00\n- model_name: gpustack/stable-diffusion-xl-base-1.0-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: diffusion-based text-to-image generative model - license:  - model description: this is a model that can be used to generate and modify images based on text prompts. it is a that uses two fixed, pretrained text encoders and . - resources for more information: check out our and the .\n created at: 2024-11-08T08:16:33+00:00\n- model_name: HiDream-ai/HiDream-I1-Fast\n  description: `hidream-i1` is a new open-source image generative foundation model with 17b parameters that achieves state-of-the-art image generation quality within seconds. for more features and to experience the full capabilities of our product, please visit . - üìù may 28, 2025 : we've released our technical report . - ‚ú® superior image quality - produces exceptional results across multiple styles including photorealistic, cartoon, artistic, and more. achieves state-of-the-art hps v2.1 score, which aligns with human preferences. - üéØ best-in-class prompt following - achieves industry-leading scores on geneval and dpg benchmarks, outperforming all other open-source models. - üîì open source - released under the mit license to foster scientific advancement and enable creative innovation. - üíº commercial-friendly - generated images can be freely used for personal projects, scientific research, and commercial applications.\n created at: 2025-04-06T14:18:51+00:00\n- model_name: gpustack/stable-diffusion-xl-inpainting-1.0-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: the diffusers team - model type: diffusion-based text-to-image generative model - license:  - model description: this is a model that can be used to generate and modify images based on text prompts. it is a that uses two fixed, pretrained text encoders and .\n created at: 2024-12-24T15:16:44+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-krea-dev\n  description: this repository contains nunchaku-quantized versions of , designed to generate high-quality images from text prompts. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: text-to-image - developed by: nunchaku team - model type: text-to-image - license:  - quantized from model:\n created at: 2025-08-01T05:11:14+00:00\n- model_name: cagliostrolab/animagine-xl-4.0\n  description: animagine xl 4.0 , also stylized as anim4gine , is the ultimate anime-themed finetuned sdxl model and the latest installment of . despite being a continuation, the model was retrained from with a massive dataset of 8.4m diverse anime-style images from various sources with the knowledge cut-off of january 7th 2025 and finetuned for approximately 2650 gpu hours. similar to the previous version, this model was trained using tag ordering method for the identity and style training. with the release of animagine xl 4.0 opt optimized , the model has been further refined with an additional dataset, improving stability , anatomy accuracy , noise reduction , color saturation , and overall color accuracy . these enhancements make animagine xl 4.0 opt more consistent and visually appealing while maintaining the signature quality of the series. - 2025-02-13 added animagine xl 4.0 opt\n created at: 2025-01-10T17:51:13+00:00\n- model_name: hoanganho0o/PMTACADEMY\n  description: you should use `pmt academy` to trigger the image generation. you should use `vietnamese` to trigger the image generation. weights for this model are available in safetensors format.\n created at: 2025-01-08T11:02:18+00:00\n- model_name: huanngzh/mv-adapter\n  description: create high-fidelity multi-view images with various base t2i models and various conditions. mv-adapter is a creative productivity tool that seamlessly transfer text-to-image models to multi-view generators. mv-adapter is a creative productivity tool that seamlessly transfer text-to-image models to multi-view generators. highlights: - 768x768 multi-view images - work well with personalized models e.g. dreamshaper, animagine, lcm, controlnet - support text or image to multi-view reconstruct 3d thereafter, or with geometry guidance for 3d texture generation - arbitrary view generation\n created at: 2024-11-29T14:38:58+00:00\n- model_name: nunchaku-tech/nunchaku-qwen-image\n  description: this repository contains nunchaku-quantized versions of , designed to generate high-quality images from text prompts, advances in complex text rendering. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: text-to-image - developed by: nunchaku team - model type: text-to-image - license: apache-2.0 - quantized from model:\n created at: 2025-08-14T08:19:05+00:00\n- model_name: stabilityai/stable-diffusion-3.5-large-turbo\n  description: is a multimodal diffusion transformer mmdit text-to-image model with that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency, with a focus on fewer inference steps. please note: this model is released under the . visit to learn or for commercial licensing details. - developed by: stability ai - developed by: stability ai - model type: mmdit text-to-image generative model - model description: this model generates images based on text prompts. it is an add-distilled that use three fixed, pretrained text encoders, and with qk-normalization.\n created at: 2024-10-22T07:32:47+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , designed to generate high-quality images from text prompts. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: text-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:27:40+00:00\n- model_name: Laxhar/noobai-XL-Vpred-1.0\n  description: noobai xl v-pred 1.0 this image generation model, based on laxhar/noobai-xl v1.0, leverages full danbooru and e621 datasets with native tags and natural language captioning. implemented as a v-prediction model distinct from eps-prediction, it requires specific parameter configurations - detailed in following sections.\n created at: 2024-12-22T06:16:42+00:00\n\nExamples:\n- example user query: Create a beautiful AI-generated image based on a given description.\n  model_name: prompthero/openjourney\n- example user query: Our gaming company is creating an anime-style game. We need to generate various anime-style characters and landscapes for the game based on text descriptions.\n  model_name: eimiss/EimisAnimeDiffusion_1.0v\n- example user query: We need a unique and detailed illustration for our science fiction book cover, specifically a futuristic city with flying cars.\n  model_name: 22h/vintedois-diffusion-v0-1\n- example user query: I have an online clothing store and I want to create ads with text descriptions. Generate images based on the text descriptions.\n  model_name: stabilityai/stable-diffusion-2\n- example user query: A fantasy novel writer needs a visual representation of their character \"A mystical wizard with a golden beard and a staff adorned with glowing jewels\" to be created from scratch.\n  model_name: nitrosocke/nitro-diffusion\n- example user query: Our team needs a unique image of a unicorn climbing the Eiffel tower for the project.\n  model_name: stabilityai/stable-diffusion-2-1\n\nUser prompt:\nCreate a visual capturing 1963 Porsche 356 B 1600 'Sunroof' Coupe by Reutter.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_gen>\n\nCandidate Models:\n- model_name: runwayml/stable-diffusion-v1-5\n  description: Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.\n created at: 2023-05-25\n- model_name: CompVis/stable-diffusion-v1-4\n  description: Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v1-4 checkpoint was fine-tuned on 225k steps at resolution 512x512 on laion-aesthetics v2 5+ and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. This model is intended for research purposes and can be used for generating artworks, design, educational or creative tools, and research on generative models.\n created at: 2022-08-20\n- model_name: prompthero/openjourney\n  description: Openjourney is an open source Stable Diffusion fine-tuned model on Midjourney images, by PromptHero. It can be used for generating AI art based on text prompts.\n created at: 2022-11-08\n- model_name: hakurei/waifu-diffusion\n  description: waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.\n created at: 2022-08-30\n- model_name: runwayml/stable-diffusion-inpainting\n  description: Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.\n created at: 2023-05-25\n- model_name: stabilityai/stable-diffusion-2-1\n  description: Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes.\n created at: 2022-12-06\n- model_name: SG161222/Realistic_Vision_V1.4\n  description: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images.\n created at: 2023-03-05\n- model_name: stabilityai/stable-diffusion-2-1-base\n  description: Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H). It is intended for research purposes only and can be used in areas such as safe deployment of models, understanding limitations and biases of generative models, generation of artworks, and research on generative models.\n created at: 2022-12-06\n- model_name: dreamlike-art/dreamlike-photoreal-2.0\n  description: Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art. It can be used to generate photorealistic images from text prompts.\n created at: 2023-01-04\n- model_name: prompthero/openjourney-v4\n  description: Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.\n created at: 2022-12-11\n- model_name: stabilityai/stable-diffusion-2\n  description: Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.\n created at: 2022-11-23\n- model_name: stabilityai/stable-diffusion-2-depth\n  description: Stable Diffusion v2 is a latent diffusion model that generates and modifies images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is developed by Robin Rombach and Patrick Esser. The model works with English language prompts and is intended for research purposes only.\n created at: 2022-11-23\n- model_name: eimiss/EimisAnimeDiffusion_1.0v\n  description: EimisAnimeDiffusion_1.0v is a text-to-image model trained with high-quality and detailed anime images. It works well on anime and landscape generations and supports a Gradio Web UI.\n created at: 2022-11-15\n- model_name: stabilityai/stable-diffusion-2-base\n  description: Stable Diffusion v2-base is a diffusion-based text-to-image generation model trained on a subset of LAION-5B dataset. It can be used to generate and modify images based on text prompts. The model uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is intended for research purposes only.\n created at: 2022-11-23\n- model_name: Linaqruf/anything-v3.0\n  description: A text-to-image model that generates images from text descriptions.\n created at: 2023-01-25\n- model_name: wavymulder/Analog-Diffusion\n  description: Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\n created at: 2022-12-10\n- model_name: nitrosocke/nitro-diffusion\n  description: Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use.\n created at: 2022-11-16\n- model_name: Lykon/DreamShaper\n  description: Dream Shaper is a text-to-image model that generates artistic images based on the given input text. Read more about this model here: https://civitai.com/models/4384/dreamshaper\n created at: 2023-01-12\n- model_name: dreamlike-art/dreamlike-diffusion-1.0\n  description: Dreamlike Diffusion 1.0 is SD 1.5 fine tuned on high quality art, made by dreamlike.art.\n created at: 2022-12-11\n- model_name: gsdf/Counterfeit-V2.5\n  description: Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.\n created at: 2023-02-02\n- model_name: dreamlike-art/dreamlike-anime-1.0\n  description: Dreamlike Anime 1.0 is a high quality anime model, made by dreamlike.art. It can be used to generate anime-style images based on text prompts. The model is trained on 768x768px images and works best with prompts that include 'photo anime, masterpiece, high quality, absurdres'. It can be used with the Stable Diffusion Pipeline from the diffusers library.\n created at: 2023-01-08\n- model_name: 22h/vintedois-diffusion-v0-1\n  description: Vintedois (22h) Diffusion model trained by Predogl and piEsposito with open weights, configs and prompts. This model generates beautiful images without a lot of prompt engineering. It can also generate high fidelity faces with a little amount of steps.\n created at: 2022-12-27\n- model_name: darkstorm2150/Protogen_v2.2_Official_Release\n  description: Protogen v2.2 is a text-to-image model that generates high-quality images based on text prompts. It was warm-started with Stable Diffusion v1-5 and fine-tuned on a large amount of data from large datasets new and trending on civitai.com. The model can be used with the Stable Diffusion Pipeline and supports trigger words like 'modelshoot style' to enforce camera capture.\n created at: 2022-12-31\n- model_name: stabilityai/stable-diffusion-x4-upscaler\n  description: Stable Diffusion x4 upscaler is a latent diffusion model trained on a 10M subset of LAION containing images >2048x2048. It can be used to generate and modify images based on text prompts. The model receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule. The model is trained with English captions and might not work well with other languages.\n created at: 2022-11-23\n- model_name: stabilityai/sd-x2-latent-upscaler\n  description: Stable Diffusion x2 latent upscaler is a diffusion-based upscaler model developed by Katherine Crowson in collaboration with Stability AI. It is designed to upscale Stable Diffusion's latent denoised image embeddings, allowing for fast text-to-image and upscaling pipelines. The model was trained on a high-resolution subset of the LAION-2B dataset and works with all Stable Diffusion checkpoints.\n created at: 2023-02-03\n- model_name: stabilityai/stable-diffusion-2-inpainting\n  description: A Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) to generate and modify images based on text prompts.\n created at: 2022-11-23\n- model_name: Qwen/Qwen-Image\n  description: üíú qwen chat&nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë tech report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp üñ•Ô∏è demo&nbsp&nbsp &nbsp&nbspüí¨ wechat &nbsp&nbsp &nbsp&nbspü´® discord&nbsp&nbsp we are thrilled to release qwen-image , an image generation foundation model in the qwen series that achieves significant advances in complex text rendering and precise image editing . experiments show strong general capabilities in both image generation and editing, with exceptional performance in text rendering, especially for chinese. we are thrilled to release qwen-image , an image generation foundation model in the qwen series that achieves significant advances in complex text rendering and precise image editing . experiments show strong general capabilities in both image generation and editing, with exceptional performance in text rendering, especially for chinese.\n created at: 2025-08-02T04:58:07+00:00\n- model_name: deepseek-ai/Janus-Pro-7B\n  description: janus-pro is a novel autoregressive framework that unifies multimodal understanding and generation. it addresses the limitations of previous approaches by decoupling visual encoding into separate pathways, while still utilizing a single, unified transformer architecture for processing. the decoupling not only alleviates the conflict between the visual encoder‚Äôs roles in understanding and generation, but also enhances the framework‚Äôs flexibility. janus-pro surpasses previous unified model and matches or exceeds the performance of task-specific models.\n created at: 2025-01-26T12:05:50+00:00\n- model_name: HiDream-ai/HiDream-I1-Full\n  description: `hidream-i1` is a new open-source image generative foundation model with 17b parameters that achieves state-of-the-art image generation quality within seconds. for more features and to experience the full capabilities of our product, please visit . - üåü july 16, 2025 : we've open-sourced the updated image editing model . - ‚ú® superior image quality - produces exceptional results across multiple styles including photorealistic, cartoon, artistic, and more. achieves state-of-the-art hps v2.1 score, which aligns with human preferences. - üéØ best-in-class prompt following - achieves industry-leading scores on geneval and dpg benchmarks, outperforming all other open-source models. - üîì open source - released under the mit license to foster scientific advancement and enable creative innovation. - üíº commercial-friendly - generated images can be freely used for personal projects, scientific research, and commercial applications.\n created at: 2025-04-06T14:18:31+00:00\n- model_name: stabilityai/stable-diffusion-3.5-medium\n  description: is a multimodal diffusion transformer with improvements mmdit-x text-to-image model that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency. please note: this model is released under the . visit to learn or for commercial licensing details. - developed by: stability ai - developed by: stability ai - model type: mmdit-x text-to-image generative model - model description: this model generates images based on text prompts. it is a multimodal diffusion transformer  with improvements that use three fixed, pretrained text encoders, with qk-normalization to improve training stability, and dual attention blocks in the first 12 transformer layers.\n created at: 2024-10-29T10:27:32+00:00\n- model_name: black-forest-labs/FLUX.1-Krea-dev\n  description: !flux.1 krea dev grid./teaser.png `flux.1 krea dev` is a 12 billion parameter rectified flow transformer capable of generating images from text descriptions. for more information, please read our and . 1. cutting-edge output quality, with a focus on aesthetic photography. 2. competitive prompt following, matching the performance of closed source alternatives. 3. trained using guidance distillation, making `flux.1 krea dev` more efficient. 4. open weights to drive new scientific research, and empower artists to develop innovative workflows. 5. generated outputs can be used for personal, scientific, and commercial purposes, as described in the .\n created at: 2025-07-07T10:39:11+00:00\n- model_name: OnomaAIResearch/Illustrious-xl-early-release-v0\n  description: @import url' .title-container display: flex; in summary, onoma ai plans to roll out open-source weights step by step and encourages the community to stay tuned for upcoming developments we‚Äôre just getting started.\n created at: 2024-09-20T14:56:38+00:00\n- model_name: gpustack/stable-diffusion-v3-5-large-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: mmdit text-to-image generative model - model description: this model generates images based on text prompts. it is a that use three fixed, pretrained text encoders, and with qk-normalization to improve training stability.\n created at: 2024-11-11T16:06:53+00:00\n- model_name: stabilityai/stable-diffusion-3.5-large\n  description: is a multimodal diffusion transformer mmdit text-to-image model that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency. please note: this model is released under the . visit to learn or for commercial licensing details. - developed by: stability ai - developed by: stability ai - model type: mmdit text-to-image generative model - model description: this model generates images based on text prompts. it is a that use three fixed, pretrained text encoders, and with qk-normalization to improve training stability.\n created at: 2024-10-22T07:29:57+00:00\n- model_name: gpustack/stable-diffusion-v3-5-large-turbo-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: mmdit text-to-image generative model - model description: this model generates images based on text prompts. it is an add-distilled that use three fixed, pretrained text encoders, and with qk-normalization.\n created at: 2024-11-13T00:25:51+00:00\n- model_name: gpustack/stable-diffusion-xl-base-1.0-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: diffusion-based text-to-image generative model - license:  - model description: this is a model that can be used to generate and modify images based on text prompts. it is a that uses two fixed, pretrained text encoders and . - resources for more information: check out our and the .\n created at: 2024-11-08T08:16:33+00:00\n- model_name: HiDream-ai/HiDream-I1-Fast\n  description: `hidream-i1` is a new open-source image generative foundation model with 17b parameters that achieves state-of-the-art image generation quality within seconds. for more features and to experience the full capabilities of our product, please visit . - üìù may 28, 2025 : we've released our technical report . - ‚ú® superior image quality - produces exceptional results across multiple styles including photorealistic, cartoon, artistic, and more. achieves state-of-the-art hps v2.1 score, which aligns with human preferences. - üéØ best-in-class prompt following - achieves industry-leading scores on geneval and dpg benchmarks, outperforming all other open-source models. - üîì open source - released under the mit license to foster scientific advancement and enable creative innovation. - üíº commercial-friendly - generated images can be freely used for personal projects, scientific research, and commercial applications.\n created at: 2025-04-06T14:18:51+00:00\n- model_name: gpustack/stable-diffusion-xl-inpainting-1.0-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: the diffusers team - model type: diffusion-based text-to-image generative model - license:  - model description: this is a model that can be used to generate and modify images based on text prompts. it is a that uses two fixed, pretrained text encoders and .\n created at: 2024-12-24T15:16:44+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-krea-dev\n  description: this repository contains nunchaku-quantized versions of , designed to generate high-quality images from text prompts. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: text-to-image - developed by: nunchaku team - model type: text-to-image - license:  - quantized from model:\n created at: 2025-08-01T05:11:14+00:00\n- model_name: cagliostrolab/animagine-xl-4.0\n  description: animagine xl 4.0 , also stylized as anim4gine , is the ultimate anime-themed finetuned sdxl model and the latest installment of . despite being a continuation, the model was retrained from with a massive dataset of 8.4m diverse anime-style images from various sources with the knowledge cut-off of january 7th 2025 and finetuned for approximately 2650 gpu hours. similar to the previous version, this model was trained using tag ordering method for the identity and style training. with the release of animagine xl 4.0 opt optimized , the model has been further refined with an additional dataset, improving stability , anatomy accuracy , noise reduction , color saturation , and overall color accuracy . these enhancements make animagine xl 4.0 opt more consistent and visually appealing while maintaining the signature quality of the series. - 2025-02-13 added animagine xl 4.0 opt\n created at: 2025-01-10T17:51:13+00:00\n- model_name: hoanganho0o/PMTACADEMY\n  description: you should use `pmt academy` to trigger the image generation. you should use `vietnamese` to trigger the image generation. weights for this model are available in safetensors format.\n created at: 2025-01-08T11:02:18+00:00\n- model_name: huanngzh/mv-adapter\n  description: create high-fidelity multi-view images with various base t2i models and various conditions. mv-adapter is a creative productivity tool that seamlessly transfer text-to-image models to multi-view generators. mv-adapter is a creative productivity tool that seamlessly transfer text-to-image models to multi-view generators. highlights: - 768x768 multi-view images - work well with personalized models e.g. dreamshaper, animagine, lcm, controlnet - support text or image to multi-view reconstruct 3d thereafter, or with geometry guidance for 3d texture generation - arbitrary view generation\n created at: 2024-11-29T14:38:58+00:00\n- model_name: nunchaku-tech/nunchaku-qwen-image\n  description: this repository contains nunchaku-quantized versions of , designed to generate high-quality images from text prompts, advances in complex text rendering. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: text-to-image - developed by: nunchaku team - model type: text-to-image - license: apache-2.0 - quantized from model:\n created at: 2025-08-14T08:19:05+00:00\n- model_name: stabilityai/stable-diffusion-3.5-large-turbo\n  description: is a multimodal diffusion transformer mmdit text-to-image model with that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency, with a focus on fewer inference steps. please note: this model is released under the . visit to learn or for commercial licensing details. - developed by: stability ai - developed by: stability ai - model type: mmdit text-to-image generative model - model description: this model generates images based on text prompts. it is an add-distilled that use three fixed, pretrained text encoders, and with qk-normalization.\n created at: 2024-10-22T07:32:47+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , designed to generate high-quality images from text prompts. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: text-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:27:40+00:00\n- model_name: Laxhar/noobai-XL-Vpred-1.0\n  description: noobai xl v-pred 1.0 this image generation model, based on laxhar/noobai-xl v1.0, leverages full danbooru and e621 datasets with native tags and natural language captioning. implemented as a v-prediction model distinct from eps-prediction, it requires specific parameter configurations - detailed in following sections.\n created at: 2024-12-22T06:16:42+00:00\n\nExamples:\n- example user query: Create a beautiful AI-generated image based on a given description.\n  model_name: prompthero/openjourney\n- example user query: Our gaming company is creating an anime-style game. We need to generate various anime-style characters and landscapes for the game based on text descriptions.\n  model_name: eimiss/EimisAnimeDiffusion_1.0v\n- example user query: We need a unique and detailed illustration for our science fiction book cover, specifically a futuristic city with flying cars.\n  model_name: 22h/vintedois-diffusion-v0-1\n- example user query: I have an online clothing store and I want to create ads with text descriptions. Generate images based on the text descriptions.\n  model_name: stabilityai/stable-diffusion-2\n- example user query: A fantasy novel writer needs a visual representation of their character \"A mystical wizard with a golden beard and a staff adorned with glowing jewels\" to be created from scratch.\n  model_name: nitrosocke/nitro-diffusion\n- example user query: Our team needs a unique image of a unicorn climbing the Eiffel tower for the project.\n  model_name: stabilityai/stable-diffusion-2-1\n\nUser prompt:\nIllustrate a tragic yet beautiful version of yurt, interior, Colorado, evening, winter, march.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_gen>\n\nCandidate Models:\n- model_name: runwayml/stable-diffusion-v1-5\n  description: Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.\n created at: 2023-05-25\n- model_name: CompVis/stable-diffusion-v1-4\n  description: Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v1-4 checkpoint was fine-tuned on 225k steps at resolution 512x512 on laion-aesthetics v2 5+ and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. This model is intended for research purposes and can be used for generating artworks, design, educational or creative tools, and research on generative models.\n created at: 2022-08-20\n- model_name: prompthero/openjourney\n  description: Openjourney is an open source Stable Diffusion fine-tuned model on Midjourney images, by PromptHero. It can be used for generating AI art based on text prompts.\n created at: 2022-11-08\n- model_name: hakurei/waifu-diffusion\n  description: waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.\n created at: 2022-08-30\n- model_name: runwayml/stable-diffusion-inpainting\n  description: Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.\n created at: 2023-05-25\n- model_name: stabilityai/stable-diffusion-2-1\n  description: Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes.\n created at: 2022-12-06\n- model_name: SG161222/Realistic_Vision_V1.4\n  description: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images.\n created at: 2023-03-05\n- model_name: stabilityai/stable-diffusion-2-1-base\n  description: Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H). It is intended for research purposes only and can be used in areas such as safe deployment of models, understanding limitations and biases of generative models, generation of artworks, and research on generative models.\n created at: 2022-12-06\n- model_name: dreamlike-art/dreamlike-photoreal-2.0\n  description: Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art. It can be used to generate photorealistic images from text prompts.\n created at: 2023-01-04\n- model_name: prompthero/openjourney-v4\n  description: Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.\n created at: 2022-12-11\n- model_name: stabilityai/stable-diffusion-2\n  description: Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.\n created at: 2022-11-23\n- model_name: stabilityai/stable-diffusion-2-depth\n  description: Stable Diffusion v2 is a latent diffusion model that generates and modifies images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is developed by Robin Rombach and Patrick Esser. The model works with English language prompts and is intended for research purposes only.\n created at: 2022-11-23\n- model_name: eimiss/EimisAnimeDiffusion_1.0v\n  description: EimisAnimeDiffusion_1.0v is a text-to-image model trained with high-quality and detailed anime images. It works well on anime and landscape generations and supports a Gradio Web UI.\n created at: 2022-11-15\n- model_name: stabilityai/stable-diffusion-2-base\n  description: Stable Diffusion v2-base is a diffusion-based text-to-image generation model trained on a subset of LAION-5B dataset. It can be used to generate and modify images based on text prompts. The model uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is intended for research purposes only.\n created at: 2022-11-23\n- model_name: Linaqruf/anything-v3.0\n  description: A text-to-image model that generates images from text descriptions.\n created at: 2023-01-25\n- model_name: wavymulder/Analog-Diffusion\n  description: Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\n created at: 2022-12-10\n- model_name: nitrosocke/nitro-diffusion\n  description: Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use.\n created at: 2022-11-16\n- model_name: Lykon/DreamShaper\n  description: Dream Shaper is a text-to-image model that generates artistic images based on the given input text. Read more about this model here: https://civitai.com/models/4384/dreamshaper\n created at: 2023-01-12\n- model_name: dreamlike-art/dreamlike-diffusion-1.0\n  description: Dreamlike Diffusion 1.0 is SD 1.5 fine tuned on high quality art, made by dreamlike.art.\n created at: 2022-12-11\n- model_name: gsdf/Counterfeit-V2.5\n  description: Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.\n created at: 2023-02-02\n- model_name: dreamlike-art/dreamlike-anime-1.0\n  description: Dreamlike Anime 1.0 is a high quality anime model, made by dreamlike.art. It can be used to generate anime-style images based on text prompts. The model is trained on 768x768px images and works best with prompts that include 'photo anime, masterpiece, high quality, absurdres'. It can be used with the Stable Diffusion Pipeline from the diffusers library.\n created at: 2023-01-08\n- model_name: 22h/vintedois-diffusion-v0-1\n  description: Vintedois (22h) Diffusion model trained by Predogl and piEsposito with open weights, configs and prompts. This model generates beautiful images without a lot of prompt engineering. It can also generate high fidelity faces with a little amount of steps.\n created at: 2022-12-27\n- model_name: darkstorm2150/Protogen_v2.2_Official_Release\n  description: Protogen v2.2 is a text-to-image model that generates high-quality images based on text prompts. It was warm-started with Stable Diffusion v1-5 and fine-tuned on a large amount of data from large datasets new and trending on civitai.com. The model can be used with the Stable Diffusion Pipeline and supports trigger words like 'modelshoot style' to enforce camera capture.\n created at: 2022-12-31\n- model_name: stabilityai/stable-diffusion-x4-upscaler\n  description: Stable Diffusion x4 upscaler is a latent diffusion model trained on a 10M subset of LAION containing images >2048x2048. It can be used to generate and modify images based on text prompts. The model receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule. The model is trained with English captions and might not work well with other languages.\n created at: 2022-11-23\n- model_name: stabilityai/sd-x2-latent-upscaler\n  description: Stable Diffusion x2 latent upscaler is a diffusion-based upscaler model developed by Katherine Crowson in collaboration with Stability AI. It is designed to upscale Stable Diffusion's latent denoised image embeddings, allowing for fast text-to-image and upscaling pipelines. The model was trained on a high-resolution subset of the LAION-2B dataset and works with all Stable Diffusion checkpoints.\n created at: 2023-02-03\n- model_name: stabilityai/stable-diffusion-2-inpainting\n  description: A Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) to generate and modify images based on text prompts.\n created at: 2022-11-23\n- model_name: Qwen/Qwen-Image\n  description: üíú qwen chat&nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë tech report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp üñ•Ô∏è demo&nbsp&nbsp &nbsp&nbspüí¨ wechat &nbsp&nbsp &nbsp&nbspü´® discord&nbsp&nbsp we are thrilled to release qwen-image , an image generation foundation model in the qwen series that achieves significant advances in complex text rendering and precise image editing . experiments show strong general capabilities in both image generation and editing, with exceptional performance in text rendering, especially for chinese. we are thrilled to release qwen-image , an image generation foundation model in the qwen series that achieves significant advances in complex text rendering and precise image editing . experiments show strong general capabilities in both image generation and editing, with exceptional performance in text rendering, especially for chinese.\n created at: 2025-08-02T04:58:07+00:00\n- model_name: deepseek-ai/Janus-Pro-7B\n  description: janus-pro is a novel autoregressive framework that unifies multimodal understanding and generation. it addresses the limitations of previous approaches by decoupling visual encoding into separate pathways, while still utilizing a single, unified transformer architecture for processing. the decoupling not only alleviates the conflict between the visual encoder‚Äôs roles in understanding and generation, but also enhances the framework‚Äôs flexibility. janus-pro surpasses previous unified model and matches or exceeds the performance of task-specific models.\n created at: 2025-01-26T12:05:50+00:00\n- model_name: HiDream-ai/HiDream-I1-Full\n  description: `hidream-i1` is a new open-source image generative foundation model with 17b parameters that achieves state-of-the-art image generation quality within seconds. for more features and to experience the full capabilities of our product, please visit . - üåü july 16, 2025 : we've open-sourced the updated image editing model . - ‚ú® superior image quality - produces exceptional results across multiple styles including photorealistic, cartoon, artistic, and more. achieves state-of-the-art hps v2.1 score, which aligns with human preferences. - üéØ best-in-class prompt following - achieves industry-leading scores on geneval and dpg benchmarks, outperforming all other open-source models. - üîì open source - released under the mit license to foster scientific advancement and enable creative innovation. - üíº commercial-friendly - generated images can be freely used for personal projects, scientific research, and commercial applications.\n created at: 2025-04-06T14:18:31+00:00\n- model_name: stabilityai/stable-diffusion-3.5-medium\n  description: is a multimodal diffusion transformer with improvements mmdit-x text-to-image model that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency. please note: this model is released under the . visit to learn or for commercial licensing details. - developed by: stability ai - developed by: stability ai - model type: mmdit-x text-to-image generative model - model description: this model generates images based on text prompts. it is a multimodal diffusion transformer  with improvements that use three fixed, pretrained text encoders, with qk-normalization to improve training stability, and dual attention blocks in the first 12 transformer layers.\n created at: 2024-10-29T10:27:32+00:00\n- model_name: black-forest-labs/FLUX.1-Krea-dev\n  description: !flux.1 krea dev grid./teaser.png `flux.1 krea dev` is a 12 billion parameter rectified flow transformer capable of generating images from text descriptions. for more information, please read our and . 1. cutting-edge output quality, with a focus on aesthetic photography. 2. competitive prompt following, matching the performance of closed source alternatives. 3. trained using guidance distillation, making `flux.1 krea dev` more efficient. 4. open weights to drive new scientific research, and empower artists to develop innovative workflows. 5. generated outputs can be used for personal, scientific, and commercial purposes, as described in the .\n created at: 2025-07-07T10:39:11+00:00\n- model_name: OnomaAIResearch/Illustrious-xl-early-release-v0\n  description: @import url' .title-container display: flex; in summary, onoma ai plans to roll out open-source weights step by step and encourages the community to stay tuned for upcoming developments we‚Äôre just getting started.\n created at: 2024-09-20T14:56:38+00:00\n- model_name: gpustack/stable-diffusion-v3-5-large-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: mmdit text-to-image generative model - model description: this model generates images based on text prompts. it is a that use three fixed, pretrained text encoders, and with qk-normalization to improve training stability.\n created at: 2024-11-11T16:06:53+00:00\n- model_name: stabilityai/stable-diffusion-3.5-large\n  description: is a multimodal diffusion transformer mmdit text-to-image model that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency. please note: this model is released under the . visit to learn or for commercial licensing details. - developed by: stability ai - developed by: stability ai - model type: mmdit text-to-image generative model - model description: this model generates images based on text prompts. it is a that use three fixed, pretrained text encoders, and with qk-normalization to improve training stability.\n created at: 2024-10-22T07:29:57+00:00\n- model_name: gpustack/stable-diffusion-v3-5-large-turbo-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: mmdit text-to-image generative model - model description: this model generates images based on text prompts. it is an add-distilled that use three fixed, pretrained text encoders, and with qk-normalization.\n created at: 2024-11-13T00:25:51+00:00\n- model_name: gpustack/stable-diffusion-xl-base-1.0-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: diffusion-based text-to-image generative model - license:  - model description: this is a model that can be used to generate and modify images based on text prompts. it is a that uses two fixed, pretrained text encoders and . - resources for more information: check out our and the .\n created at: 2024-11-08T08:16:33+00:00\n- model_name: HiDream-ai/HiDream-I1-Fast\n  description: `hidream-i1` is a new open-source image generative foundation model with 17b parameters that achieves state-of-the-art image generation quality within seconds. for more features and to experience the full capabilities of our product, please visit . - üìù may 28, 2025 : we've released our technical report . - ‚ú® superior image quality - produces exceptional results across multiple styles including photorealistic, cartoon, artistic, and more. achieves state-of-the-art hps v2.1 score, which aligns with human preferences. - üéØ best-in-class prompt following - achieves industry-leading scores on geneval and dpg benchmarks, outperforming all other open-source models. - üîì open source - released under the mit license to foster scientific advancement and enable creative innovation. - üíº commercial-friendly - generated images can be freely used for personal projects, scientific research, and commercial applications.\n created at: 2025-04-06T14:18:51+00:00\n- model_name: gpustack/stable-diffusion-xl-inpainting-1.0-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: the diffusers team - model type: diffusion-based text-to-image generative model - license:  - model description: this is a model that can be used to generate and modify images based on text prompts. it is a that uses two fixed, pretrained text encoders and .\n created at: 2024-12-24T15:16:44+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-krea-dev\n  description: this repository contains nunchaku-quantized versions of , designed to generate high-quality images from text prompts. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: text-to-image - developed by: nunchaku team - model type: text-to-image - license:  - quantized from model:\n created at: 2025-08-01T05:11:14+00:00\n- model_name: cagliostrolab/animagine-xl-4.0\n  description: animagine xl 4.0 , also stylized as anim4gine , is the ultimate anime-themed finetuned sdxl model and the latest installment of . despite being a continuation, the model was retrained from with a massive dataset of 8.4m diverse anime-style images from various sources with the knowledge cut-off of january 7th 2025 and finetuned for approximately 2650 gpu hours. similar to the previous version, this model was trained using tag ordering method for the identity and style training. with the release of animagine xl 4.0 opt optimized , the model has been further refined with an additional dataset, improving stability , anatomy accuracy , noise reduction , color saturation , and overall color accuracy . these enhancements make animagine xl 4.0 opt more consistent and visually appealing while maintaining the signature quality of the series. - 2025-02-13 added animagine xl 4.0 opt\n created at: 2025-01-10T17:51:13+00:00\n- model_name: hoanganho0o/PMTACADEMY\n  description: you should use `pmt academy` to trigger the image generation. you should use `vietnamese` to trigger the image generation. weights for this model are available in safetensors format.\n created at: 2025-01-08T11:02:18+00:00\n- model_name: huanngzh/mv-adapter\n  description: create high-fidelity multi-view images with various base t2i models and various conditions. mv-adapter is a creative productivity tool that seamlessly transfer text-to-image models to multi-view generators. mv-adapter is a creative productivity tool that seamlessly transfer text-to-image models to multi-view generators. highlights: - 768x768 multi-view images - work well with personalized models e.g. dreamshaper, animagine, lcm, controlnet - support text or image to multi-view reconstruct 3d thereafter, or with geometry guidance for 3d texture generation - arbitrary view generation\n created at: 2024-11-29T14:38:58+00:00\n- model_name: nunchaku-tech/nunchaku-qwen-image\n  description: this repository contains nunchaku-quantized versions of , designed to generate high-quality images from text prompts, advances in complex text rendering. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: text-to-image - developed by: nunchaku team - model type: text-to-image - license: apache-2.0 - quantized from model:\n created at: 2025-08-14T08:19:05+00:00\n- model_name: stabilityai/stable-diffusion-3.5-large-turbo\n  description: is a multimodal diffusion transformer mmdit text-to-image model with that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency, with a focus on fewer inference steps. please note: this model is released under the . visit to learn or for commercial licensing details. - developed by: stability ai - developed by: stability ai - model type: mmdit text-to-image generative model - model description: this model generates images based on text prompts. it is an add-distilled that use three fixed, pretrained text encoders, and with qk-normalization.\n created at: 2024-10-22T07:32:47+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , designed to generate high-quality images from text prompts. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: text-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:27:40+00:00\n- model_name: Laxhar/noobai-XL-Vpred-1.0\n  description: noobai xl v-pred 1.0 this image generation model, based on laxhar/noobai-xl v1.0, leverages full danbooru and e621 datasets with native tags and natural language captioning. implemented as a v-prediction model distinct from eps-prediction, it requires specific parameter configurations - detailed in following sections.\n created at: 2024-12-22T06:16:42+00:00\n\nExamples:\n- example user query: Create a beautiful AI-generated image based on a given description.\n  model_name: prompthero/openjourney\n- example user query: Our gaming company is creating an anime-style game. We need to generate various anime-style characters and landscapes for the game based on text descriptions.\n  model_name: eimiss/EimisAnimeDiffusion_1.0v\n- example user query: We need a unique and detailed illustration for our science fiction book cover, specifically a futuristic city with flying cars.\n  model_name: 22h/vintedois-diffusion-v0-1\n- example user query: I have an online clothing store and I want to create ads with text descriptions. Generate images based on the text descriptions.\n  model_name: stabilityai/stable-diffusion-2\n- example user query: A fantasy novel writer needs a visual representation of their character \"A mystical wizard with a golden beard and a staff adorned with glowing jewels\" to be created from scratch.\n  model_name: nitrosocke/nitro-diffusion\n- example user query: Our team needs a unique image of a unicorn climbing the Eiffel tower for the project.\n  model_name: stabilityai/stable-diffusion-2-1\n\nUser prompt:\nIllustrate a jubilant vision lake-maggiore-outdoor-wedding.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_gen>\n\nCandidate Models:\n- model_name: runwayml/stable-diffusion-v1-5\n  description: Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.\n created at: 2023-05-25\n- model_name: CompVis/stable-diffusion-v1-4\n  description: Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v1-4 checkpoint was fine-tuned on 225k steps at resolution 512x512 on laion-aesthetics v2 5+ and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. This model is intended for research purposes and can be used for generating artworks, design, educational or creative tools, and research on generative models.\n created at: 2022-08-20\n- model_name: prompthero/openjourney\n  description: Openjourney is an open source Stable Diffusion fine-tuned model on Midjourney images, by PromptHero. It can be used for generating AI art based on text prompts.\n created at: 2022-11-08\n- model_name: hakurei/waifu-diffusion\n  description: waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.\n created at: 2022-08-30\n- model_name: runwayml/stable-diffusion-inpainting\n  description: Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.\n created at: 2023-05-25\n- model_name: stabilityai/stable-diffusion-2-1\n  description: Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes.\n created at: 2022-12-06\n- model_name: SG161222/Realistic_Vision_V1.4\n  description: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images.\n created at: 2023-03-05\n- model_name: stabilityai/stable-diffusion-2-1-base\n  description: Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H). It is intended for research purposes only and can be used in areas such as safe deployment of models, understanding limitations and biases of generative models, generation of artworks, and research on generative models.\n created at: 2022-12-06\n- model_name: dreamlike-art/dreamlike-photoreal-2.0\n  description: Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art. It can be used to generate photorealistic images from text prompts.\n created at: 2023-01-04\n- model_name: prompthero/openjourney-v4\n  description: Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.\n created at: 2022-12-11\n- model_name: stabilityai/stable-diffusion-2\n  description: Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.\n created at: 2022-11-23\n- model_name: stabilityai/stable-diffusion-2-depth\n  description: Stable Diffusion v2 is a latent diffusion model that generates and modifies images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is developed by Robin Rombach and Patrick Esser. The model works with English language prompts and is intended for research purposes only.\n created at: 2022-11-23\n- model_name: eimiss/EimisAnimeDiffusion_1.0v\n  description: EimisAnimeDiffusion_1.0v is a text-to-image model trained with high-quality and detailed anime images. It works well on anime and landscape generations and supports a Gradio Web UI.\n created at: 2022-11-15\n- model_name: stabilityai/stable-diffusion-2-base\n  description: Stable Diffusion v2-base is a diffusion-based text-to-image generation model trained on a subset of LAION-5B dataset. It can be used to generate and modify images based on text prompts. The model uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is intended for research purposes only.\n created at: 2022-11-23\n- model_name: Linaqruf/anything-v3.0\n  description: A text-to-image model that generates images from text descriptions.\n created at: 2023-01-25\n- model_name: wavymulder/Analog-Diffusion\n  description: Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\n created at: 2022-12-10\n- model_name: nitrosocke/nitro-diffusion\n  description: Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use.\n created at: 2022-11-16\n- model_name: Lykon/DreamShaper\n  description: Dream Shaper is a text-to-image model that generates artistic images based on the given input text. Read more about this model here: https://civitai.com/models/4384/dreamshaper\n created at: 2023-01-12\n- model_name: dreamlike-art/dreamlike-diffusion-1.0\n  description: Dreamlike Diffusion 1.0 is SD 1.5 fine tuned on high quality art, made by dreamlike.art.\n created at: 2022-12-11\n- model_name: gsdf/Counterfeit-V2.5\n  description: Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.\n created at: 2023-02-02\n- model_name: dreamlike-art/dreamlike-anime-1.0\n  description: Dreamlike Anime 1.0 is a high quality anime model, made by dreamlike.art. It can be used to generate anime-style images based on text prompts. The model is trained on 768x768px images and works best with prompts that include 'photo anime, masterpiece, high quality, absurdres'. It can be used with the Stable Diffusion Pipeline from the diffusers library.\n created at: 2023-01-08\n- model_name: 22h/vintedois-diffusion-v0-1\n  description: Vintedois (22h) Diffusion model trained by Predogl and piEsposito with open weights, configs and prompts. This model generates beautiful images without a lot of prompt engineering. It can also generate high fidelity faces with a little amount of steps.\n created at: 2022-12-27\n- model_name: darkstorm2150/Protogen_v2.2_Official_Release\n  description: Protogen v2.2 is a text-to-image model that generates high-quality images based on text prompts. It was warm-started with Stable Diffusion v1-5 and fine-tuned on a large amount of data from large datasets new and trending on civitai.com. The model can be used with the Stable Diffusion Pipeline and supports trigger words like 'modelshoot style' to enforce camera capture.\n created at: 2022-12-31\n- model_name: stabilityai/stable-diffusion-x4-upscaler\n  description: Stable Diffusion x4 upscaler is a latent diffusion model trained on a 10M subset of LAION containing images >2048x2048. It can be used to generate and modify images based on text prompts. The model receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule. The model is trained with English captions and might not work well with other languages.\n created at: 2022-11-23\n- model_name: stabilityai/sd-x2-latent-upscaler\n  description: Stable Diffusion x2 latent upscaler is a diffusion-based upscaler model developed by Katherine Crowson in collaboration with Stability AI. It is designed to upscale Stable Diffusion's latent denoised image embeddings, allowing for fast text-to-image and upscaling pipelines. The model was trained on a high-resolution subset of the LAION-2B dataset and works with all Stable Diffusion checkpoints.\n created at: 2023-02-03\n- model_name: stabilityai/stable-diffusion-2-inpainting\n  description: A Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) to generate and modify images based on text prompts.\n created at: 2022-11-23\n- model_name: Qwen/Qwen-Image\n  description: üíú qwen chat&nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë tech report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp üñ•Ô∏è demo&nbsp&nbsp &nbsp&nbspüí¨ wechat &nbsp&nbsp &nbsp&nbspü´® discord&nbsp&nbsp we are thrilled to release qwen-image , an image generation foundation model in the qwen series that achieves significant advances in complex text rendering and precise image editing . experiments show strong general capabilities in both image generation and editing, with exceptional performance in text rendering, especially for chinese. we are thrilled to release qwen-image , an image generation foundation model in the qwen series that achieves significant advances in complex text rendering and precise image editing . experiments show strong general capabilities in both image generation and editing, with exceptional performance in text rendering, especially for chinese.\n created at: 2025-08-02T04:58:07+00:00\n- model_name: deepseek-ai/Janus-Pro-7B\n  description: janus-pro is a novel autoregressive framework that unifies multimodal understanding and generation. it addresses the limitations of previous approaches by decoupling visual encoding into separate pathways, while still utilizing a single, unified transformer architecture for processing. the decoupling not only alleviates the conflict between the visual encoder‚Äôs roles in understanding and generation, but also enhances the framework‚Äôs flexibility. janus-pro surpasses previous unified model and matches or exceeds the performance of task-specific models.\n created at: 2025-01-26T12:05:50+00:00\n- model_name: HiDream-ai/HiDream-I1-Full\n  description: `hidream-i1` is a new open-source image generative foundation model with 17b parameters that achieves state-of-the-art image generation quality within seconds. for more features and to experience the full capabilities of our product, please visit . - üåü july 16, 2025 : we've open-sourced the updated image editing model . - ‚ú® superior image quality - produces exceptional results across multiple styles including photorealistic, cartoon, artistic, and more. achieves state-of-the-art hps v2.1 score, which aligns with human preferences. - üéØ best-in-class prompt following - achieves industry-leading scores on geneval and dpg benchmarks, outperforming all other open-source models. - üîì open source - released under the mit license to foster scientific advancement and enable creative innovation. - üíº commercial-friendly - generated images can be freely used for personal projects, scientific research, and commercial applications.\n created at: 2025-04-06T14:18:31+00:00\n- model_name: stabilityai/stable-diffusion-3.5-medium\n  description: is a multimodal diffusion transformer with improvements mmdit-x text-to-image model that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency. please note: this model is released under the . visit to learn or for commercial licensing details. - developed by: stability ai - developed by: stability ai - model type: mmdit-x text-to-image generative model - model description: this model generates images based on text prompts. it is a multimodal diffusion transformer  with improvements that use three fixed, pretrained text encoders, with qk-normalization to improve training stability, and dual attention blocks in the first 12 transformer layers.\n created at: 2024-10-29T10:27:32+00:00\n- model_name: black-forest-labs/FLUX.1-Krea-dev\n  description: !flux.1 krea dev grid./teaser.png `flux.1 krea dev` is a 12 billion parameter rectified flow transformer capable of generating images from text descriptions. for more information, please read our and . 1. cutting-edge output quality, with a focus on aesthetic photography. 2. competitive prompt following, matching the performance of closed source alternatives. 3. trained using guidance distillation, making `flux.1 krea dev` more efficient. 4. open weights to drive new scientific research, and empower artists to develop innovative workflows. 5. generated outputs can be used for personal, scientific, and commercial purposes, as described in the .\n created at: 2025-07-07T10:39:11+00:00\n- model_name: OnomaAIResearch/Illustrious-xl-early-release-v0\n  description: @import url' .title-container display: flex; in summary, onoma ai plans to roll out open-source weights step by step and encourages the community to stay tuned for upcoming developments we‚Äôre just getting started.\n created at: 2024-09-20T14:56:38+00:00\n- model_name: gpustack/stable-diffusion-v3-5-large-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: mmdit text-to-image generative model - model description: this model generates images based on text prompts. it is a that use three fixed, pretrained text encoders, and with qk-normalization to improve training stability.\n created at: 2024-11-11T16:06:53+00:00\n- model_name: stabilityai/stable-diffusion-3.5-large\n  description: is a multimodal diffusion transformer mmdit text-to-image model that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency. please note: this model is released under the . visit to learn or for commercial licensing details. - developed by: stability ai - developed by: stability ai - model type: mmdit text-to-image generative model - model description: this model generates images based on text prompts. it is a that use three fixed, pretrained text encoders, and with qk-normalization to improve training stability.\n created at: 2024-10-22T07:29:57+00:00\n- model_name: gpustack/stable-diffusion-v3-5-large-turbo-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: mmdit text-to-image generative model - model description: this model generates images based on text prompts. it is an add-distilled that use three fixed, pretrained text encoders, and with qk-normalization.\n created at: 2024-11-13T00:25:51+00:00\n- model_name: gpustack/stable-diffusion-xl-base-1.0-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: diffusion-based text-to-image generative model - license:  - model description: this is a model that can be used to generate and modify images based on text prompts. it is a that uses two fixed, pretrained text encoders and . - resources for more information: check out our and the .\n created at: 2024-11-08T08:16:33+00:00\n- model_name: HiDream-ai/HiDream-I1-Fast\n  description: `hidream-i1` is a new open-source image generative foundation model with 17b parameters that achieves state-of-the-art image generation quality within seconds. for more features and to experience the full capabilities of our product, please visit . - üìù may 28, 2025 : we've released our technical report . - ‚ú® superior image quality - produces exceptional results across multiple styles including photorealistic, cartoon, artistic, and more. achieves state-of-the-art hps v2.1 score, which aligns with human preferences. - üéØ best-in-class prompt following - achieves industry-leading scores on geneval and dpg benchmarks, outperforming all other open-source models. - üîì open source - released under the mit license to foster scientific advancement and enable creative innovation. - üíº commercial-friendly - generated images can be freely used for personal projects, scientific research, and commercial applications.\n created at: 2025-04-06T14:18:51+00:00\n- model_name: gpustack/stable-diffusion-xl-inpainting-1.0-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: the diffusers team - model type: diffusion-based text-to-image generative model - license:  - model description: this is a model that can be used to generate and modify images based on text prompts. it is a that uses two fixed, pretrained text encoders and .\n created at: 2024-12-24T15:16:44+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-krea-dev\n  description: this repository contains nunchaku-quantized versions of , designed to generate high-quality images from text prompts. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: text-to-image - developed by: nunchaku team - model type: text-to-image - license:  - quantized from model:\n created at: 2025-08-01T05:11:14+00:00\n- model_name: cagliostrolab/animagine-xl-4.0\n  description: animagine xl 4.0 , also stylized as anim4gine , is the ultimate anime-themed finetuned sdxl model and the latest installment of . despite being a continuation, the model was retrained from with a massive dataset of 8.4m diverse anime-style images from various sources with the knowledge cut-off of january 7th 2025 and finetuned for approximately 2650 gpu hours. similar to the previous version, this model was trained using tag ordering method for the identity and style training. with the release of animagine xl 4.0 opt optimized , the model has been further refined with an additional dataset, improving stability , anatomy accuracy , noise reduction , color saturation , and overall color accuracy . these enhancements make animagine xl 4.0 opt more consistent and visually appealing while maintaining the signature quality of the series. - 2025-02-13 added animagine xl 4.0 opt\n created at: 2025-01-10T17:51:13+00:00\n- model_name: hoanganho0o/PMTACADEMY\n  description: you should use `pmt academy` to trigger the image generation. you should use `vietnamese` to trigger the image generation. weights for this model are available in safetensors format.\n created at: 2025-01-08T11:02:18+00:00\n- model_name: huanngzh/mv-adapter\n  description: create high-fidelity multi-view images with various base t2i models and various conditions. mv-adapter is a creative productivity tool that seamlessly transfer text-to-image models to multi-view generators. mv-adapter is a creative productivity tool that seamlessly transfer text-to-image models to multi-view generators. highlights: - 768x768 multi-view images - work well with personalized models e.g. dreamshaper, animagine, lcm, controlnet - support text or image to multi-view reconstruct 3d thereafter, or with geometry guidance for 3d texture generation - arbitrary view generation\n created at: 2024-11-29T14:38:58+00:00\n- model_name: nunchaku-tech/nunchaku-qwen-image\n  description: this repository contains nunchaku-quantized versions of , designed to generate high-quality images from text prompts, advances in complex text rendering. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: text-to-image - developed by: nunchaku team - model type: text-to-image - license: apache-2.0 - quantized from model:\n created at: 2025-08-14T08:19:05+00:00\n- model_name: stabilityai/stable-diffusion-3.5-large-turbo\n  description: is a multimodal diffusion transformer mmdit text-to-image model with that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency, with a focus on fewer inference steps. please note: this model is released under the . visit to learn or for commercial licensing details. - developed by: stability ai - developed by: stability ai - model type: mmdit text-to-image generative model - model description: this model generates images based on text prompts. it is an add-distilled that use three fixed, pretrained text encoders, and with qk-normalization.\n created at: 2024-10-22T07:32:47+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , designed to generate high-quality images from text prompts. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: text-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:27:40+00:00\n- model_name: Laxhar/noobai-XL-Vpred-1.0\n  description: noobai xl v-pred 1.0 this image generation model, based on laxhar/noobai-xl v1.0, leverages full danbooru and e621 datasets with native tags and natural language captioning. implemented as a v-prediction model distinct from eps-prediction, it requires specific parameter configurations - detailed in following sections.\n created at: 2024-12-22T06:16:42+00:00\n\nExamples:\n- example user query: Create a beautiful AI-generated image based on a given description.\n  model_name: prompthero/openjourney\n- example user query: Our gaming company is creating an anime-style game. We need to generate various anime-style characters and landscapes for the game based on text descriptions.\n  model_name: eimiss/EimisAnimeDiffusion_1.0v\n- example user query: We need a unique and detailed illustration for our science fiction book cover, specifically a futuristic city with flying cars.\n  model_name: 22h/vintedois-diffusion-v0-1\n- example user query: I have an online clothing store and I want to create ads with text descriptions. Generate images based on the text descriptions.\n  model_name: stabilityai/stable-diffusion-2\n- example user query: A fantasy novel writer needs a visual representation of their character \"A mystical wizard with a golden beard and a staff adorned with glowing jewels\" to be created from scratch.\n  model_name: nitrosocke/nitro-diffusion\n- example user query: Our team needs a unique image of a unicorn climbing the Eiffel tower for the project.\n  model_name: stabilityai/stable-diffusion-2-1\n\nUser prompt:\nIllustrate an ecosystem showing Large Picture of '40 Tudor located in Ventura California - $27,900.00 Offered by Spoke Motors - J9J9.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_gen>\n\nCandidate Models:\n- model_name: runwayml/stable-diffusion-v1-5\n  description: Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.\n created at: 2023-05-25\n- model_name: CompVis/stable-diffusion-v1-4\n  description: Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v1-4 checkpoint was fine-tuned on 225k steps at resolution 512x512 on laion-aesthetics v2 5+ and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. This model is intended for research purposes and can be used for generating artworks, design, educational or creative tools, and research on generative models.\n created at: 2022-08-20\n- model_name: prompthero/openjourney\n  description: Openjourney is an open source Stable Diffusion fine-tuned model on Midjourney images, by PromptHero. It can be used for generating AI art based on text prompts.\n created at: 2022-11-08\n- model_name: hakurei/waifu-diffusion\n  description: waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.\n created at: 2022-08-30\n- model_name: runwayml/stable-diffusion-inpainting\n  description: Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.\n created at: 2023-05-25\n- model_name: stabilityai/stable-diffusion-2-1\n  description: Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes.\n created at: 2022-12-06\n- model_name: SG161222/Realistic_Vision_V1.4\n  description: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images.\n created at: 2023-03-05\n- model_name: stabilityai/stable-diffusion-2-1-base\n  description: Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H). It is intended for research purposes only and can be used in areas such as safe deployment of models, understanding limitations and biases of generative models, generation of artworks, and research on generative models.\n created at: 2022-12-06\n- model_name: dreamlike-art/dreamlike-photoreal-2.0\n  description: Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art. It can be used to generate photorealistic images from text prompts.\n created at: 2023-01-04\n- model_name: prompthero/openjourney-v4\n  description: Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.\n created at: 2022-12-11\n- model_name: stabilityai/stable-diffusion-2\n  description: Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.\n created at: 2022-11-23\n- model_name: stabilityai/stable-diffusion-2-depth\n  description: Stable Diffusion v2 is a latent diffusion model that generates and modifies images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is developed by Robin Rombach and Patrick Esser. The model works with English language prompts and is intended for research purposes only.\n created at: 2022-11-23\n- model_name: eimiss/EimisAnimeDiffusion_1.0v\n  description: EimisAnimeDiffusion_1.0v is a text-to-image model trained with high-quality and detailed anime images. It works well on anime and landscape generations and supports a Gradio Web UI.\n created at: 2022-11-15\n- model_name: stabilityai/stable-diffusion-2-base\n  description: Stable Diffusion v2-base is a diffusion-based text-to-image generation model trained on a subset of LAION-5B dataset. It can be used to generate and modify images based on text prompts. The model uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is intended for research purposes only.\n created at: 2022-11-23\n- model_name: Linaqruf/anything-v3.0\n  description: A text-to-image model that generates images from text descriptions.\n created at: 2023-01-25\n- model_name: wavymulder/Analog-Diffusion\n  description: Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\n created at: 2022-12-10\n- model_name: nitrosocke/nitro-diffusion\n  description: Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use.\n created at: 2022-11-16\n- model_name: Lykon/DreamShaper\n  description: Dream Shaper is a text-to-image model that generates artistic images based on the given input text. Read more about this model here: https://civitai.com/models/4384/dreamshaper\n created at: 2023-01-12\n- model_name: dreamlike-art/dreamlike-diffusion-1.0\n  description: Dreamlike Diffusion 1.0 is SD 1.5 fine tuned on high quality art, made by dreamlike.art.\n created at: 2022-12-11\n- model_name: gsdf/Counterfeit-V2.5\n  description: Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.\n created at: 2023-02-02\n- model_name: dreamlike-art/dreamlike-anime-1.0\n  description: Dreamlike Anime 1.0 is a high quality anime model, made by dreamlike.art. It can be used to generate anime-style images based on text prompts. The model is trained on 768x768px images and works best with prompts that include 'photo anime, masterpiece, high quality, absurdres'. It can be used with the Stable Diffusion Pipeline from the diffusers library.\n created at: 2023-01-08\n- model_name: 22h/vintedois-diffusion-v0-1\n  description: Vintedois (22h) Diffusion model trained by Predogl and piEsposito with open weights, configs and prompts. This model generates beautiful images without a lot of prompt engineering. It can also generate high fidelity faces with a little amount of steps.\n created at: 2022-12-27\n- model_name: darkstorm2150/Protogen_v2.2_Official_Release\n  description: Protogen v2.2 is a text-to-image model that generates high-quality images based on text prompts. It was warm-started with Stable Diffusion v1-5 and fine-tuned on a large amount of data from large datasets new and trending on civitai.com. The model can be used with the Stable Diffusion Pipeline and supports trigger words like 'modelshoot style' to enforce camera capture.\n created at: 2022-12-31\n- model_name: stabilityai/stable-diffusion-x4-upscaler\n  description: Stable Diffusion x4 upscaler is a latent diffusion model trained on a 10M subset of LAION containing images >2048x2048. It can be used to generate and modify images based on text prompts. The model receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule. The model is trained with English captions and might not work well with other languages.\n created at: 2022-11-23\n- model_name: stabilityai/sd-x2-latent-upscaler\n  description: Stable Diffusion x2 latent upscaler is a diffusion-based upscaler model developed by Katherine Crowson in collaboration with Stability AI. It is designed to upscale Stable Diffusion's latent denoised image embeddings, allowing for fast text-to-image and upscaling pipelines. The model was trained on a high-resolution subset of the LAION-2B dataset and works with all Stable Diffusion checkpoints.\n created at: 2023-02-03\n- model_name: stabilityai/stable-diffusion-2-inpainting\n  description: A Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) to generate and modify images based on text prompts.\n created at: 2022-11-23\n- model_name: Qwen/Qwen-Image\n  description: üíú qwen chat&nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë tech report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp üñ•Ô∏è demo&nbsp&nbsp &nbsp&nbspüí¨ wechat &nbsp&nbsp &nbsp&nbspü´® discord&nbsp&nbsp we are thrilled to release qwen-image , an image generation foundation model in the qwen series that achieves significant advances in complex text rendering and precise image editing . experiments show strong general capabilities in both image generation and editing, with exceptional performance in text rendering, especially for chinese. we are thrilled to release qwen-image , an image generation foundation model in the qwen series that achieves significant advances in complex text rendering and precise image editing . experiments show strong general capabilities in both image generation and editing, with exceptional performance in text rendering, especially for chinese.\n created at: 2025-08-02T04:58:07+00:00\n- model_name: deepseek-ai/Janus-Pro-7B\n  description: janus-pro is a novel autoregressive framework that unifies multimodal understanding and generation. it addresses the limitations of previous approaches by decoupling visual encoding into separate pathways, while still utilizing a single, unified transformer architecture for processing. the decoupling not only alleviates the conflict between the visual encoder‚Äôs roles in understanding and generation, but also enhances the framework‚Äôs flexibility. janus-pro surpasses previous unified model and matches or exceeds the performance of task-specific models.\n created at: 2025-01-26T12:05:50+00:00\n- model_name: HiDream-ai/HiDream-I1-Full\n  description: `hidream-i1` is a new open-source image generative foundation model with 17b parameters that achieves state-of-the-art image generation quality within seconds. for more features and to experience the full capabilities of our product, please visit . - üåü july 16, 2025 : we've open-sourced the updated image editing model . - ‚ú® superior image quality - produces exceptional results across multiple styles including photorealistic, cartoon, artistic, and more. achieves state-of-the-art hps v2.1 score, which aligns with human preferences. - üéØ best-in-class prompt following - achieves industry-leading scores on geneval and dpg benchmarks, outperforming all other open-source models. - üîì open source - released under the mit license to foster scientific advancement and enable creative innovation. - üíº commercial-friendly - generated images can be freely used for personal projects, scientific research, and commercial applications.\n created at: 2025-04-06T14:18:31+00:00\n- model_name: stabilityai/stable-diffusion-3.5-medium\n  description: is a multimodal diffusion transformer with improvements mmdit-x text-to-image model that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency. please note: this model is released under the . visit to learn or for commercial licensing details. - developed by: stability ai - developed by: stability ai - model type: mmdit-x text-to-image generative model - model description: this model generates images based on text prompts. it is a multimodal diffusion transformer  with improvements that use three fixed, pretrained text encoders, with qk-normalization to improve training stability, and dual attention blocks in the first 12 transformer layers.\n created at: 2024-10-29T10:27:32+00:00\n- model_name: black-forest-labs/FLUX.1-Krea-dev\n  description: !flux.1 krea dev grid./teaser.png `flux.1 krea dev` is a 12 billion parameter rectified flow transformer capable of generating images from text descriptions. for more information, please read our and . 1. cutting-edge output quality, with a focus on aesthetic photography. 2. competitive prompt following, matching the performance of closed source alternatives. 3. trained using guidance distillation, making `flux.1 krea dev` more efficient. 4. open weights to drive new scientific research, and empower artists to develop innovative workflows. 5. generated outputs can be used for personal, scientific, and commercial purposes, as described in the .\n created at: 2025-07-07T10:39:11+00:00\n- model_name: OnomaAIResearch/Illustrious-xl-early-release-v0\n  description: @import url' .title-container display: flex; in summary, onoma ai plans to roll out open-source weights step by step and encourages the community to stay tuned for upcoming developments we‚Äôre just getting started.\n created at: 2024-09-20T14:56:38+00:00\n- model_name: gpustack/stable-diffusion-v3-5-large-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: mmdit text-to-image generative model - model description: this model generates images based on text prompts. it is a that use three fixed, pretrained text encoders, and with qk-normalization to improve training stability.\n created at: 2024-11-11T16:06:53+00:00\n- model_name: stabilityai/stable-diffusion-3.5-large\n  description: is a multimodal diffusion transformer mmdit text-to-image model that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency. please note: this model is released under the . visit to learn or for commercial licensing details. - developed by: stability ai - developed by: stability ai - model type: mmdit text-to-image generative model - model description: this model generates images based on text prompts. it is a that use three fixed, pretrained text encoders, and with qk-normalization to improve training stability.\n created at: 2024-10-22T07:29:57+00:00\n- model_name: gpustack/stable-diffusion-v3-5-large-turbo-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: mmdit text-to-image generative model - model description: this model generates images based on text prompts. it is an add-distilled that use three fixed, pretrained text encoders, and with qk-normalization.\n created at: 2024-11-13T00:25:51+00:00\n- model_name: gpustack/stable-diffusion-xl-base-1.0-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: diffusion-based text-to-image generative model - license:  - model description: this is a model that can be used to generate and modify images based on text prompts. it is a that uses two fixed, pretrained text encoders and . - resources for more information: check out our and the .\n created at: 2024-11-08T08:16:33+00:00\n- model_name: HiDream-ai/HiDream-I1-Fast\n  description: `hidream-i1` is a new open-source image generative foundation model with 17b parameters that achieves state-of-the-art image generation quality within seconds. for more features and to experience the full capabilities of our product, please visit . - üìù may 28, 2025 : we've released our technical report . - ‚ú® superior image quality - produces exceptional results across multiple styles including photorealistic, cartoon, artistic, and more. achieves state-of-the-art hps v2.1 score, which aligns with human preferences. - üéØ best-in-class prompt following - achieves industry-leading scores on geneval and dpg benchmarks, outperforming all other open-source models. - üîì open source - released under the mit license to foster scientific advancement and enable creative innovation. - üíº commercial-friendly - generated images can be freely used for personal projects, scientific research, and commercial applications.\n created at: 2025-04-06T14:18:51+00:00\n- model_name: gpustack/stable-diffusion-xl-inpainting-1.0-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: the diffusers team - model type: diffusion-based text-to-image generative model - license:  - model description: this is a model that can be used to generate and modify images based on text prompts. it is a that uses two fixed, pretrained text encoders and .\n created at: 2024-12-24T15:16:44+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-krea-dev\n  description: this repository contains nunchaku-quantized versions of , designed to generate high-quality images from text prompts. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: text-to-image - developed by: nunchaku team - model type: text-to-image - license:  - quantized from model:\n created at: 2025-08-01T05:11:14+00:00\n- model_name: cagliostrolab/animagine-xl-4.0\n  description: animagine xl 4.0 , also stylized as anim4gine , is the ultimate anime-themed finetuned sdxl model and the latest installment of . despite being a continuation, the model was retrained from with a massive dataset of 8.4m diverse anime-style images from various sources with the knowledge cut-off of january 7th 2025 and finetuned for approximately 2650 gpu hours. similar to the previous version, this model was trained using tag ordering method for the identity and style training. with the release of animagine xl 4.0 opt optimized , the model has been further refined with an additional dataset, improving stability , anatomy accuracy , noise reduction , color saturation , and overall color accuracy . these enhancements make animagine xl 4.0 opt more consistent and visually appealing while maintaining the signature quality of the series. - 2025-02-13 added animagine xl 4.0 opt\n created at: 2025-01-10T17:51:13+00:00\n- model_name: hoanganho0o/PMTACADEMY\n  description: you should use `pmt academy` to trigger the image generation. you should use `vietnamese` to trigger the image generation. weights for this model are available in safetensors format.\n created at: 2025-01-08T11:02:18+00:00\n- model_name: huanngzh/mv-adapter\n  description: create high-fidelity multi-view images with various base t2i models and various conditions. mv-adapter is a creative productivity tool that seamlessly transfer text-to-image models to multi-view generators. mv-adapter is a creative productivity tool that seamlessly transfer text-to-image models to multi-view generators. highlights: - 768x768 multi-view images - work well with personalized models e.g. dreamshaper, animagine, lcm, controlnet - support text or image to multi-view reconstruct 3d thereafter, or with geometry guidance for 3d texture generation - arbitrary view generation\n created at: 2024-11-29T14:38:58+00:00\n- model_name: nunchaku-tech/nunchaku-qwen-image\n  description: this repository contains nunchaku-quantized versions of , designed to generate high-quality images from text prompts, advances in complex text rendering. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: text-to-image - developed by: nunchaku team - model type: text-to-image - license: apache-2.0 - quantized from model:\n created at: 2025-08-14T08:19:05+00:00\n- model_name: stabilityai/stable-diffusion-3.5-large-turbo\n  description: is a multimodal diffusion transformer mmdit text-to-image model with that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency, with a focus on fewer inference steps. please note: this model is released under the . visit to learn or for commercial licensing details. - developed by: stability ai - developed by: stability ai - model type: mmdit text-to-image generative model - model description: this model generates images based on text prompts. it is an add-distilled that use three fixed, pretrained text encoders, and with qk-normalization.\n created at: 2024-10-22T07:32:47+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , designed to generate high-quality images from text prompts. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: text-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:27:40+00:00\n- model_name: Laxhar/noobai-XL-Vpred-1.0\n  description: noobai xl v-pred 1.0 this image generation model, based on laxhar/noobai-xl v1.0, leverages full danbooru and e621 datasets with native tags and natural language captioning. implemented as a v-prediction model distinct from eps-prediction, it requires specific parameter configurations - detailed in following sections.\n created at: 2024-12-22T06:16:42+00:00\n\nExamples:\n- example user query: Create a beautiful AI-generated image based on a given description.\n  model_name: prompthero/openjourney\n- example user query: Our gaming company is creating an anime-style game. We need to generate various anime-style characters and landscapes for the game based on text descriptions.\n  model_name: eimiss/EimisAnimeDiffusion_1.0v\n- example user query: We need a unique and detailed illustration for our science fiction book cover, specifically a futuristic city with flying cars.\n  model_name: 22h/vintedois-diffusion-v0-1\n- example user query: I have an online clothing store and I want to create ads with text descriptions. Generate images based on the text descriptions.\n  model_name: stabilityai/stable-diffusion-2\n- example user query: A fantasy novel writer needs a visual representation of their character \"A mystical wizard with a golden beard and a staff adorned with glowing jewels\" to be created from scratch.\n  model_name: nitrosocke/nitro-diffusion\n- example user query: Our team needs a unique image of a unicorn climbing the Eiffel tower for the project.\n  model_name: stabilityai/stable-diffusion-2-1\n\nUser prompt:\nEnvision a contemplative design Ted-2-Official-Trailer-5.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_gen>\n\nCandidate Models:\n- model_name: runwayml/stable-diffusion-v1-5\n  description: Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.\n created at: 2023-05-25\n- model_name: CompVis/stable-diffusion-v1-4\n  description: Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v1-4 checkpoint was fine-tuned on 225k steps at resolution 512x512 on laion-aesthetics v2 5+ and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. This model is intended for research purposes and can be used for generating artworks, design, educational or creative tools, and research on generative models.\n created at: 2022-08-20\n- model_name: prompthero/openjourney\n  description: Openjourney is an open source Stable Diffusion fine-tuned model on Midjourney images, by PromptHero. It can be used for generating AI art based on text prompts.\n created at: 2022-11-08\n- model_name: hakurei/waifu-diffusion\n  description: waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.\n created at: 2022-08-30\n- model_name: runwayml/stable-diffusion-inpainting\n  description: Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.\n created at: 2023-05-25\n- model_name: stabilityai/stable-diffusion-2-1\n  description: Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes.\n created at: 2022-12-06\n- model_name: SG161222/Realistic_Vision_V1.4\n  description: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images.\n created at: 2023-03-05\n- model_name: stabilityai/stable-diffusion-2-1-base\n  description: Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H). It is intended for research purposes only and can be used in areas such as safe deployment of models, understanding limitations and biases of generative models, generation of artworks, and research on generative models.\n created at: 2022-12-06\n- model_name: dreamlike-art/dreamlike-photoreal-2.0\n  description: Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art. It can be used to generate photorealistic images from text prompts.\n created at: 2023-01-04\n- model_name: prompthero/openjourney-v4\n  description: Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.\n created at: 2022-12-11\n- model_name: stabilityai/stable-diffusion-2\n  description: Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.\n created at: 2022-11-23\n- model_name: stabilityai/stable-diffusion-2-depth\n  description: Stable Diffusion v2 is a latent diffusion model that generates and modifies images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is developed by Robin Rombach and Patrick Esser. The model works with English language prompts and is intended for research purposes only.\n created at: 2022-11-23\n- model_name: eimiss/EimisAnimeDiffusion_1.0v\n  description: EimisAnimeDiffusion_1.0v is a text-to-image model trained with high-quality and detailed anime images. It works well on anime and landscape generations and supports a Gradio Web UI.\n created at: 2022-11-15\n- model_name: stabilityai/stable-diffusion-2-base\n  description: Stable Diffusion v2-base is a diffusion-based text-to-image generation model trained on a subset of LAION-5B dataset. It can be used to generate and modify images based on text prompts. The model uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is intended for research purposes only.\n created at: 2022-11-23\n- model_name: Linaqruf/anything-v3.0\n  description: A text-to-image model that generates images from text descriptions.\n created at: 2023-01-25\n- model_name: wavymulder/Analog-Diffusion\n  description: Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\n created at: 2022-12-10\n- model_name: nitrosocke/nitro-diffusion\n  description: Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use.\n created at: 2022-11-16\n- model_name: Lykon/DreamShaper\n  description: Dream Shaper is a text-to-image model that generates artistic images based on the given input text. Read more about this model here: https://civitai.com/models/4384/dreamshaper\n created at: 2023-01-12\n- model_name: dreamlike-art/dreamlike-diffusion-1.0\n  description: Dreamlike Diffusion 1.0 is SD 1.5 fine tuned on high quality art, made by dreamlike.art.\n created at: 2022-12-11\n- model_name: gsdf/Counterfeit-V2.5\n  description: Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.\n created at: 2023-02-02\n- model_name: dreamlike-art/dreamlike-anime-1.0\n  description: Dreamlike Anime 1.0 is a high quality anime model, made by dreamlike.art. It can be used to generate anime-style images based on text prompts. The model is trained on 768x768px images and works best with prompts that include 'photo anime, masterpiece, high quality, absurdres'. It can be used with the Stable Diffusion Pipeline from the diffusers library.\n created at: 2023-01-08\n- model_name: 22h/vintedois-diffusion-v0-1\n  description: Vintedois (22h) Diffusion model trained by Predogl and piEsposito with open weights, configs and prompts. This model generates beautiful images without a lot of prompt engineering. It can also generate high fidelity faces with a little amount of steps.\n created at: 2022-12-27\n- model_name: darkstorm2150/Protogen_v2.2_Official_Release\n  description: Protogen v2.2 is a text-to-image model that generates high-quality images based on text prompts. It was warm-started with Stable Diffusion v1-5 and fine-tuned on a large amount of data from large datasets new and trending on civitai.com. The model can be used with the Stable Diffusion Pipeline and supports trigger words like 'modelshoot style' to enforce camera capture.\n created at: 2022-12-31\n- model_name: stabilityai/stable-diffusion-x4-upscaler\n  description: Stable Diffusion x4 upscaler is a latent diffusion model trained on a 10M subset of LAION containing images >2048x2048. It can be used to generate and modify images based on text prompts. The model receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule. The model is trained with English captions and might not work well with other languages.\n created at: 2022-11-23\n- model_name: stabilityai/sd-x2-latent-upscaler\n  description: Stable Diffusion x2 latent upscaler is a diffusion-based upscaler model developed by Katherine Crowson in collaboration with Stability AI. It is designed to upscale Stable Diffusion's latent denoised image embeddings, allowing for fast text-to-image and upscaling pipelines. The model was trained on a high-resolution subset of the LAION-2B dataset and works with all Stable Diffusion checkpoints.\n created at: 2023-02-03\n- model_name: stabilityai/stable-diffusion-2-inpainting\n  description: A Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) to generate and modify images based on text prompts.\n created at: 2022-11-23\n- model_name: Qwen/Qwen-Image\n  description: üíú qwen chat&nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë tech report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp üñ•Ô∏è demo&nbsp&nbsp &nbsp&nbspüí¨ wechat &nbsp&nbsp &nbsp&nbspü´® discord&nbsp&nbsp we are thrilled to release qwen-image , an image generation foundation model in the qwen series that achieves significant advances in complex text rendering and precise image editing . experiments show strong general capabilities in both image generation and editing, with exceptional performance in text rendering, especially for chinese. we are thrilled to release qwen-image , an image generation foundation model in the qwen series that achieves significant advances in complex text rendering and precise image editing . experiments show strong general capabilities in both image generation and editing, with exceptional performance in text rendering, especially for chinese.\n created at: 2025-08-02T04:58:07+00:00\n- model_name: deepseek-ai/Janus-Pro-7B\n  description: janus-pro is a novel autoregressive framework that unifies multimodal understanding and generation. it addresses the limitations of previous approaches by decoupling visual encoding into separate pathways, while still utilizing a single, unified transformer architecture for processing. the decoupling not only alleviates the conflict between the visual encoder‚Äôs roles in understanding and generation, but also enhances the framework‚Äôs flexibility. janus-pro surpasses previous unified model and matches or exceeds the performance of task-specific models.\n created at: 2025-01-26T12:05:50+00:00\n- model_name: HiDream-ai/HiDream-I1-Full\n  description: `hidream-i1` is a new open-source image generative foundation model with 17b parameters that achieves state-of-the-art image generation quality within seconds. for more features and to experience the full capabilities of our product, please visit . - üåü july 16, 2025 : we've open-sourced the updated image editing model . - ‚ú® superior image quality - produces exceptional results across multiple styles including photorealistic, cartoon, artistic, and more. achieves state-of-the-art hps v2.1 score, which aligns with human preferences. - üéØ best-in-class prompt following - achieves industry-leading scores on geneval and dpg benchmarks, outperforming all other open-source models. - üîì open source - released under the mit license to foster scientific advancement and enable creative innovation. - üíº commercial-friendly - generated images can be freely used for personal projects, scientific research, and commercial applications.\n created at: 2025-04-06T14:18:31+00:00\n- model_name: stabilityai/stable-diffusion-3.5-medium\n  description: is a multimodal diffusion transformer with improvements mmdit-x text-to-image model that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency. please note: this model is released under the . visit to learn or for commercial licensing details. - developed by: stability ai - developed by: stability ai - model type: mmdit-x text-to-image generative model - model description: this model generates images based on text prompts. it is a multimodal diffusion transformer  with improvements that use three fixed, pretrained text encoders, with qk-normalization to improve training stability, and dual attention blocks in the first 12 transformer layers.\n created at: 2024-10-29T10:27:32+00:00\n- model_name: black-forest-labs/FLUX.1-Krea-dev\n  description: !flux.1 krea dev grid./teaser.png `flux.1 krea dev` is a 12 billion parameter rectified flow transformer capable of generating images from text descriptions. for more information, please read our and . 1. cutting-edge output quality, with a focus on aesthetic photography. 2. competitive prompt following, matching the performance of closed source alternatives. 3. trained using guidance distillation, making `flux.1 krea dev` more efficient. 4. open weights to drive new scientific research, and empower artists to develop innovative workflows. 5. generated outputs can be used for personal, scientific, and commercial purposes, as described in the .\n created at: 2025-07-07T10:39:11+00:00\n- model_name: OnomaAIResearch/Illustrious-xl-early-release-v0\n  description: @import url' .title-container display: flex; in summary, onoma ai plans to roll out open-source weights step by step and encourages the community to stay tuned for upcoming developments we‚Äôre just getting started.\n created at: 2024-09-20T14:56:38+00:00\n- model_name: gpustack/stable-diffusion-v3-5-large-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: mmdit text-to-image generative model - model description: this model generates images based on text prompts. it is a that use three fixed, pretrained text encoders, and with qk-normalization to improve training stability.\n created at: 2024-11-11T16:06:53+00:00\n- model_name: stabilityai/stable-diffusion-3.5-large\n  description: is a multimodal diffusion transformer mmdit text-to-image model that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency. please note: this model is released under the . visit to learn or for commercial licensing details. - developed by: stability ai - developed by: stability ai - model type: mmdit text-to-image generative model - model description: this model generates images based on text prompts. it is a that use three fixed, pretrained text encoders, and with qk-normalization to improve training stability.\n created at: 2024-10-22T07:29:57+00:00\n- model_name: gpustack/stable-diffusion-v3-5-large-turbo-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: mmdit text-to-image generative model - model description: this model generates images based on text prompts. it is an add-distilled that use three fixed, pretrained text encoders, and with qk-normalization.\n created at: 2024-11-13T00:25:51+00:00\n- model_name: gpustack/stable-diffusion-xl-base-1.0-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: diffusion-based text-to-image generative model - license:  - model description: this is a model that can be used to generate and modify images based on text prompts. it is a that uses two fixed, pretrained text encoders and . - resources for more information: check out our and the .\n created at: 2024-11-08T08:16:33+00:00\n- model_name: HiDream-ai/HiDream-I1-Fast\n  description: `hidream-i1` is a new open-source image generative foundation model with 17b parameters that achieves state-of-the-art image generation quality within seconds. for more features and to experience the full capabilities of our product, please visit . - üìù may 28, 2025 : we've released our technical report . - ‚ú® superior image quality - produces exceptional results across multiple styles including photorealistic, cartoon, artistic, and more. achieves state-of-the-art hps v2.1 score, which aligns with human preferences. - üéØ best-in-class prompt following - achieves industry-leading scores on geneval and dpg benchmarks, outperforming all other open-source models. - üîì open source - released under the mit license to foster scientific advancement and enable creative innovation. - üíº commercial-friendly - generated images can be freely used for personal projects, scientific research, and commercial applications.\n created at: 2025-04-06T14:18:51+00:00\n- model_name: gpustack/stable-diffusion-xl-inpainting-1.0-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: the diffusers team - model type: diffusion-based text-to-image generative model - license:  - model description: this is a model that can be used to generate and modify images based on text prompts. it is a that uses two fixed, pretrained text encoders and .\n created at: 2024-12-24T15:16:44+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-krea-dev\n  description: this repository contains nunchaku-quantized versions of , designed to generate high-quality images from text prompts. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: text-to-image - developed by: nunchaku team - model type: text-to-image - license:  - quantized from model:\n created at: 2025-08-01T05:11:14+00:00\n- model_name: cagliostrolab/animagine-xl-4.0\n  description: animagine xl 4.0 , also stylized as anim4gine , is the ultimate anime-themed finetuned sdxl model and the latest installment of . despite being a continuation, the model was retrained from with a massive dataset of 8.4m diverse anime-style images from various sources with the knowledge cut-off of january 7th 2025 and finetuned for approximately 2650 gpu hours. similar to the previous version, this model was trained using tag ordering method for the identity and style training. with the release of animagine xl 4.0 opt optimized , the model has been further refined with an additional dataset, improving stability , anatomy accuracy , noise reduction , color saturation , and overall color accuracy . these enhancements make animagine xl 4.0 opt more consistent and visually appealing while maintaining the signature quality of the series. - 2025-02-13 added animagine xl 4.0 opt\n created at: 2025-01-10T17:51:13+00:00\n- model_name: hoanganho0o/PMTACADEMY\n  description: you should use `pmt academy` to trigger the image generation. you should use `vietnamese` to trigger the image generation. weights for this model are available in safetensors format.\n created at: 2025-01-08T11:02:18+00:00\n- model_name: huanngzh/mv-adapter\n  description: create high-fidelity multi-view images with various base t2i models and various conditions. mv-adapter is a creative productivity tool that seamlessly transfer text-to-image models to multi-view generators. mv-adapter is a creative productivity tool that seamlessly transfer text-to-image models to multi-view generators. highlights: - 768x768 multi-view images - work well with personalized models e.g. dreamshaper, animagine, lcm, controlnet - support text or image to multi-view reconstruct 3d thereafter, or with geometry guidance for 3d texture generation - arbitrary view generation\n created at: 2024-11-29T14:38:58+00:00\n- model_name: nunchaku-tech/nunchaku-qwen-image\n  description: this repository contains nunchaku-quantized versions of , designed to generate high-quality images from text prompts, advances in complex text rendering. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: text-to-image - developed by: nunchaku team - model type: text-to-image - license: apache-2.0 - quantized from model:\n created at: 2025-08-14T08:19:05+00:00\n- model_name: stabilityai/stable-diffusion-3.5-large-turbo\n  description: is a multimodal diffusion transformer mmdit text-to-image model with that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency, with a focus on fewer inference steps. please note: this model is released under the . visit to learn or for commercial licensing details. - developed by: stability ai - developed by: stability ai - model type: mmdit text-to-image generative model - model description: this model generates images based on text prompts. it is an add-distilled that use three fixed, pretrained text encoders, and with qk-normalization.\n created at: 2024-10-22T07:32:47+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , designed to generate high-quality images from text prompts. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: text-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:27:40+00:00\n- model_name: Laxhar/noobai-XL-Vpred-1.0\n  description: noobai xl v-pred 1.0 this image generation model, based on laxhar/noobai-xl v1.0, leverages full danbooru and e621 datasets with native tags and natural language captioning. implemented as a v-prediction model distinct from eps-prediction, it requires specific parameter configurations - detailed in following sections.\n created at: 2024-12-22T06:16:42+00:00\n\nExamples:\n- example user query: Create a beautiful AI-generated image based on a given description.\n  model_name: prompthero/openjourney\n- example user query: Our gaming company is creating an anime-style game. We need to generate various anime-style characters and landscapes for the game based on text descriptions.\n  model_name: eimiss/EimisAnimeDiffusion_1.0v\n- example user query: We need a unique and detailed illustration for our science fiction book cover, specifically a futuristic city with flying cars.\n  model_name: 22h/vintedois-diffusion-v0-1\n- example user query: I have an online clothing store and I want to create ads with text descriptions. Generate images based on the text descriptions.\n  model_name: stabilityai/stable-diffusion-2\n- example user query: A fantasy novel writer needs a visual representation of their character \"A mystical wizard with a golden beard and a staff adorned with glowing jewels\" to be created from scratch.\n  model_name: nitrosocke/nitro-diffusion\n- example user query: Our team needs a unique image of a unicorn climbing the Eiffel tower for the project.\n  model_name: stabilityai/stable-diffusion-2-1\n\nUser prompt:\nImagine a scenario where Beautiful wedding cakes by The Frostery - trends and ideas for 2019 (14).\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_gen>\n\nCandidate Models:\n- model_name: runwayml/stable-diffusion-v1-5\n  description: Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.\n created at: 2023-05-25\n- model_name: CompVis/stable-diffusion-v1-4\n  description: Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v1-4 checkpoint was fine-tuned on 225k steps at resolution 512x512 on laion-aesthetics v2 5+ and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. This model is intended for research purposes and can be used for generating artworks, design, educational or creative tools, and research on generative models.\n created at: 2022-08-20\n- model_name: prompthero/openjourney\n  description: Openjourney is an open source Stable Diffusion fine-tuned model on Midjourney images, by PromptHero. It can be used for generating AI art based on text prompts.\n created at: 2022-11-08\n- model_name: hakurei/waifu-diffusion\n  description: waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.\n created at: 2022-08-30\n- model_name: runwayml/stable-diffusion-inpainting\n  description: Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.\n created at: 2023-05-25\n- model_name: stabilityai/stable-diffusion-2-1\n  description: Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes.\n created at: 2022-12-06\n- model_name: SG161222/Realistic_Vision_V1.4\n  description: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images.\n created at: 2023-03-05\n- model_name: stabilityai/stable-diffusion-2-1-base\n  description: Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H). It is intended for research purposes only and can be used in areas such as safe deployment of models, understanding limitations and biases of generative models, generation of artworks, and research on generative models.\n created at: 2022-12-06\n- model_name: dreamlike-art/dreamlike-photoreal-2.0\n  description: Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art. It can be used to generate photorealistic images from text prompts.\n created at: 2023-01-04\n- model_name: prompthero/openjourney-v4\n  description: Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.\n created at: 2022-12-11\n- model_name: stabilityai/stable-diffusion-2\n  description: Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.\n created at: 2022-11-23\n- model_name: stabilityai/stable-diffusion-2-depth\n  description: Stable Diffusion v2 is a latent diffusion model that generates and modifies images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is developed by Robin Rombach and Patrick Esser. The model works with English language prompts and is intended for research purposes only.\n created at: 2022-11-23\n- model_name: eimiss/EimisAnimeDiffusion_1.0v\n  description: EimisAnimeDiffusion_1.0v is a text-to-image model trained with high-quality and detailed anime images. It works well on anime and landscape generations and supports a Gradio Web UI.\n created at: 2022-11-15\n- model_name: stabilityai/stable-diffusion-2-base\n  description: Stable Diffusion v2-base is a diffusion-based text-to-image generation model trained on a subset of LAION-5B dataset. It can be used to generate and modify images based on text prompts. The model uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is intended for research purposes only.\n created at: 2022-11-23\n- model_name: Linaqruf/anything-v3.0\n  description: A text-to-image model that generates images from text descriptions.\n created at: 2023-01-25\n- model_name: wavymulder/Analog-Diffusion\n  description: Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\n created at: 2022-12-10\n- model_name: nitrosocke/nitro-diffusion\n  description: Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use.\n created at: 2022-11-16\n- model_name: Lykon/DreamShaper\n  description: Dream Shaper is a text-to-image model that generates artistic images based on the given input text. Read more about this model here: https://civitai.com/models/4384/dreamshaper\n created at: 2023-01-12\n- model_name: dreamlike-art/dreamlike-diffusion-1.0\n  description: Dreamlike Diffusion 1.0 is SD 1.5 fine tuned on high quality art, made by dreamlike.art.\n created at: 2022-12-11\n- model_name: gsdf/Counterfeit-V2.5\n  description: Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.\n created at: 2023-02-02\n- model_name: dreamlike-art/dreamlike-anime-1.0\n  description: Dreamlike Anime 1.0 is a high quality anime model, made by dreamlike.art. It can be used to generate anime-style images based on text prompts. The model is trained on 768x768px images and works best with prompts that include 'photo anime, masterpiece, high quality, absurdres'. It can be used with the Stable Diffusion Pipeline from the diffusers library.\n created at: 2023-01-08\n- model_name: 22h/vintedois-diffusion-v0-1\n  description: Vintedois (22h) Diffusion model trained by Predogl and piEsposito with open weights, configs and prompts. This model generates beautiful images without a lot of prompt engineering. It can also generate high fidelity faces with a little amount of steps.\n created at: 2022-12-27\n- model_name: darkstorm2150/Protogen_v2.2_Official_Release\n  description: Protogen v2.2 is a text-to-image model that generates high-quality images based on text prompts. It was warm-started with Stable Diffusion v1-5 and fine-tuned on a large amount of data from large datasets new and trending on civitai.com. The model can be used with the Stable Diffusion Pipeline and supports trigger words like 'modelshoot style' to enforce camera capture.\n created at: 2022-12-31\n- model_name: stabilityai/stable-diffusion-x4-upscaler\n  description: Stable Diffusion x4 upscaler is a latent diffusion model trained on a 10M subset of LAION containing images >2048x2048. It can be used to generate and modify images based on text prompts. The model receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule. The model is trained with English captions and might not work well with other languages.\n created at: 2022-11-23\n- model_name: stabilityai/sd-x2-latent-upscaler\n  description: Stable Diffusion x2 latent upscaler is a diffusion-based upscaler model developed by Katherine Crowson in collaboration with Stability AI. It is designed to upscale Stable Diffusion's latent denoised image embeddings, allowing for fast text-to-image and upscaling pipelines. The model was trained on a high-resolution subset of the LAION-2B dataset and works with all Stable Diffusion checkpoints.\n created at: 2023-02-03\n- model_name: stabilityai/stable-diffusion-2-inpainting\n  description: A Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) to generate and modify images based on text prompts.\n created at: 2022-11-23\n- model_name: Qwen/Qwen-Image\n  description: üíú qwen chat&nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë tech report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp üñ•Ô∏è demo&nbsp&nbsp &nbsp&nbspüí¨ wechat &nbsp&nbsp &nbsp&nbspü´® discord&nbsp&nbsp we are thrilled to release qwen-image , an image generation foundation model in the qwen series that achieves significant advances in complex text rendering and precise image editing . experiments show strong general capabilities in both image generation and editing, with exceptional performance in text rendering, especially for chinese. we are thrilled to release qwen-image , an image generation foundation model in the qwen series that achieves significant advances in complex text rendering and precise image editing . experiments show strong general capabilities in both image generation and editing, with exceptional performance in text rendering, especially for chinese.\n created at: 2025-08-02T04:58:07+00:00\n- model_name: deepseek-ai/Janus-Pro-7B\n  description: janus-pro is a novel autoregressive framework that unifies multimodal understanding and generation. it addresses the limitations of previous approaches by decoupling visual encoding into separate pathways, while still utilizing a single, unified transformer architecture for processing. the decoupling not only alleviates the conflict between the visual encoder‚Äôs roles in understanding and generation, but also enhances the framework‚Äôs flexibility. janus-pro surpasses previous unified model and matches or exceeds the performance of task-specific models.\n created at: 2025-01-26T12:05:50+00:00\n- model_name: HiDream-ai/HiDream-I1-Full\n  description: `hidream-i1` is a new open-source image generative foundation model with 17b parameters that achieves state-of-the-art image generation quality within seconds. for more features and to experience the full capabilities of our product, please visit . - üåü july 16, 2025 : we've open-sourced the updated image editing model . - ‚ú® superior image quality - produces exceptional results across multiple styles including photorealistic, cartoon, artistic, and more. achieves state-of-the-art hps v2.1 score, which aligns with human preferences. - üéØ best-in-class prompt following - achieves industry-leading scores on geneval and dpg benchmarks, outperforming all other open-source models. - üîì open source - released under the mit license to foster scientific advancement and enable creative innovation. - üíº commercial-friendly - generated images can be freely used for personal projects, scientific research, and commercial applications.\n created at: 2025-04-06T14:18:31+00:00\n- model_name: stabilityai/stable-diffusion-3.5-medium\n  description: is a multimodal diffusion transformer with improvements mmdit-x text-to-image model that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency. please note: this model is released under the . visit to learn or for commercial licensing details. - developed by: stability ai - developed by: stability ai - model type: mmdit-x text-to-image generative model - model description: this model generates images based on text prompts. it is a multimodal diffusion transformer  with improvements that use three fixed, pretrained text encoders, with qk-normalization to improve training stability, and dual attention blocks in the first 12 transformer layers.\n created at: 2024-10-29T10:27:32+00:00\n- model_name: black-forest-labs/FLUX.1-Krea-dev\n  description: !flux.1 krea dev grid./teaser.png `flux.1 krea dev` is a 12 billion parameter rectified flow transformer capable of generating images from text descriptions. for more information, please read our and . 1. cutting-edge output quality, with a focus on aesthetic photography. 2. competitive prompt following, matching the performance of closed source alternatives. 3. trained using guidance distillation, making `flux.1 krea dev` more efficient. 4. open weights to drive new scientific research, and empower artists to develop innovative workflows. 5. generated outputs can be used for personal, scientific, and commercial purposes, as described in the .\n created at: 2025-07-07T10:39:11+00:00\n- model_name: OnomaAIResearch/Illustrious-xl-early-release-v0\n  description: @import url' .title-container display: flex; in summary, onoma ai plans to roll out open-source weights step by step and encourages the community to stay tuned for upcoming developments we‚Äôre just getting started.\n created at: 2024-09-20T14:56:38+00:00\n- model_name: gpustack/stable-diffusion-v3-5-large-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: mmdit text-to-image generative model - model description: this model generates images based on text prompts. it is a that use three fixed, pretrained text encoders, and with qk-normalization to improve training stability.\n created at: 2024-11-11T16:06:53+00:00\n- model_name: stabilityai/stable-diffusion-3.5-large\n  description: is a multimodal diffusion transformer mmdit text-to-image model that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency. please note: this model is released under the . visit to learn or for commercial licensing details. - developed by: stability ai - developed by: stability ai - model type: mmdit text-to-image generative model - model description: this model generates images based on text prompts. it is a that use three fixed, pretrained text encoders, and with qk-normalization to improve training stability.\n created at: 2024-10-22T07:29:57+00:00\n- model_name: gpustack/stable-diffusion-v3-5-large-turbo-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: mmdit text-to-image generative model - model description: this model generates images based on text prompts. it is an add-distilled that use three fixed, pretrained text encoders, and with qk-normalization.\n created at: 2024-11-13T00:25:51+00:00\n- model_name: gpustack/stable-diffusion-xl-base-1.0-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: diffusion-based text-to-image generative model - license:  - model description: this is a model that can be used to generate and modify images based on text prompts. it is a that uses two fixed, pretrained text encoders and . - resources for more information: check out our and the .\n created at: 2024-11-08T08:16:33+00:00\n- model_name: HiDream-ai/HiDream-I1-Fast\n  description: `hidream-i1` is a new open-source image generative foundation model with 17b parameters that achieves state-of-the-art image generation quality within seconds. for more features and to experience the full capabilities of our product, please visit . - üìù may 28, 2025 : we've released our technical report . - ‚ú® superior image quality - produces exceptional results across multiple styles including photorealistic, cartoon, artistic, and more. achieves state-of-the-art hps v2.1 score, which aligns with human preferences. - üéØ best-in-class prompt following - achieves industry-leading scores on geneval and dpg benchmarks, outperforming all other open-source models. - üîì open source - released under the mit license to foster scientific advancement and enable creative innovation. - üíº commercial-friendly - generated images can be freely used for personal projects, scientific research, and commercial applications.\n created at: 2025-04-06T14:18:51+00:00\n- model_name: gpustack/stable-diffusion-xl-inpainting-1.0-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: the diffusers team - model type: diffusion-based text-to-image generative model - license:  - model description: this is a model that can be used to generate and modify images based on text prompts. it is a that uses two fixed, pretrained text encoders and .\n created at: 2024-12-24T15:16:44+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-krea-dev\n  description: this repository contains nunchaku-quantized versions of , designed to generate high-quality images from text prompts. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: text-to-image - developed by: nunchaku team - model type: text-to-image - license:  - quantized from model:\n created at: 2025-08-01T05:11:14+00:00\n- model_name: cagliostrolab/animagine-xl-4.0\n  description: animagine xl 4.0 , also stylized as anim4gine , is the ultimate anime-themed finetuned sdxl model and the latest installment of . despite being a continuation, the model was retrained from with a massive dataset of 8.4m diverse anime-style images from various sources with the knowledge cut-off of january 7th 2025 and finetuned for approximately 2650 gpu hours. similar to the previous version, this model was trained using tag ordering method for the identity and style training. with the release of animagine xl 4.0 opt optimized , the model has been further refined with an additional dataset, improving stability , anatomy accuracy , noise reduction , color saturation , and overall color accuracy . these enhancements make animagine xl 4.0 opt more consistent and visually appealing while maintaining the signature quality of the series. - 2025-02-13 added animagine xl 4.0 opt\n created at: 2025-01-10T17:51:13+00:00\n- model_name: hoanganho0o/PMTACADEMY\n  description: you should use `pmt academy` to trigger the image generation. you should use `vietnamese` to trigger the image generation. weights for this model are available in safetensors format.\n created at: 2025-01-08T11:02:18+00:00\n- model_name: huanngzh/mv-adapter\n  description: create high-fidelity multi-view images with various base t2i models and various conditions. mv-adapter is a creative productivity tool that seamlessly transfer text-to-image models to multi-view generators. mv-adapter is a creative productivity tool that seamlessly transfer text-to-image models to multi-view generators. highlights: - 768x768 multi-view images - work well with personalized models e.g. dreamshaper, animagine, lcm, controlnet - support text or image to multi-view reconstruct 3d thereafter, or with geometry guidance for 3d texture generation - arbitrary view generation\n created at: 2024-11-29T14:38:58+00:00\n- model_name: nunchaku-tech/nunchaku-qwen-image\n  description: this repository contains nunchaku-quantized versions of , designed to generate high-quality images from text prompts, advances in complex text rendering. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: text-to-image - developed by: nunchaku team - model type: text-to-image - license: apache-2.0 - quantized from model:\n created at: 2025-08-14T08:19:05+00:00\n- model_name: stabilityai/stable-diffusion-3.5-large-turbo\n  description: is a multimodal diffusion transformer mmdit text-to-image model with that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency, with a focus on fewer inference steps. please note: this model is released under the . visit to learn or for commercial licensing details. - developed by: stability ai - developed by: stability ai - model type: mmdit text-to-image generative model - model description: this model generates images based on text prompts. it is an add-distilled that use three fixed, pretrained text encoders, and with qk-normalization.\n created at: 2024-10-22T07:32:47+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , designed to generate high-quality images from text prompts. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: text-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:27:40+00:00\n- model_name: Laxhar/noobai-XL-Vpred-1.0\n  description: noobai xl v-pred 1.0 this image generation model, based on laxhar/noobai-xl v1.0, leverages full danbooru and e621 datasets with native tags and natural language captioning. implemented as a v-prediction model distinct from eps-prediction, it requires specific parameter configurations - detailed in following sections.\n created at: 2024-12-22T06:16:42+00:00\n\nExamples:\n- example user query: Create a beautiful AI-generated image based on a given description.\n  model_name: prompthero/openjourney\n- example user query: Our gaming company is creating an anime-style game. We need to generate various anime-style characters and landscapes for the game based on text descriptions.\n  model_name: eimiss/EimisAnimeDiffusion_1.0v\n- example user query: We need a unique and detailed illustration for our science fiction book cover, specifically a futuristic city with flying cars.\n  model_name: 22h/vintedois-diffusion-v0-1\n- example user query: I have an online clothing store and I want to create ads with text descriptions. Generate images based on the text descriptions.\n  model_name: stabilityai/stable-diffusion-2\n- example user query: A fantasy novel writer needs a visual representation of their character \"A mystical wizard with a golden beard and a staff adorned with glowing jewels\" to be created from scratch.\n  model_name: nitrosocke/nitro-diffusion\n- example user query: Our team needs a unique image of a unicorn climbing the Eiffel tower for the project.\n  model_name: stabilityai/stable-diffusion-2-1\n\nUser prompt:\nCompose a stylized illustration of 2018 Indian Chief Vintage at Fort Myers.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_gen>\n\nCandidate Models:\n- model_name: runwayml/stable-diffusion-v1-5\n  description: Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.\n created at: 2023-05-25\n- model_name: CompVis/stable-diffusion-v1-4\n  description: Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v1-4 checkpoint was fine-tuned on 225k steps at resolution 512x512 on laion-aesthetics v2 5+ and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. This model is intended for research purposes and can be used for generating artworks, design, educational or creative tools, and research on generative models.\n created at: 2022-08-20\n- model_name: prompthero/openjourney\n  description: Openjourney is an open source Stable Diffusion fine-tuned model on Midjourney images, by PromptHero. It can be used for generating AI art based on text prompts.\n created at: 2022-11-08\n- model_name: hakurei/waifu-diffusion\n  description: waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.\n created at: 2022-08-30\n- model_name: runwayml/stable-diffusion-inpainting\n  description: Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.\n created at: 2023-05-25\n- model_name: stabilityai/stable-diffusion-2-1\n  description: Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes.\n created at: 2022-12-06\n- model_name: SG161222/Realistic_Vision_V1.4\n  description: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images.\n created at: 2023-03-05\n- model_name: stabilityai/stable-diffusion-2-1-base\n  description: Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H). It is intended for research purposes only and can be used in areas such as safe deployment of models, understanding limitations and biases of generative models, generation of artworks, and research on generative models.\n created at: 2022-12-06\n- model_name: dreamlike-art/dreamlike-photoreal-2.0\n  description: Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art. It can be used to generate photorealistic images from text prompts.\n created at: 2023-01-04\n- model_name: prompthero/openjourney-v4\n  description: Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.\n created at: 2022-12-11\n- model_name: stabilityai/stable-diffusion-2\n  description: Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.\n created at: 2022-11-23\n- model_name: stabilityai/stable-diffusion-2-depth\n  description: Stable Diffusion v2 is a latent diffusion model that generates and modifies images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is developed by Robin Rombach and Patrick Esser. The model works with English language prompts and is intended for research purposes only.\n created at: 2022-11-23\n- model_name: eimiss/EimisAnimeDiffusion_1.0v\n  description: EimisAnimeDiffusion_1.0v is a text-to-image model trained with high-quality and detailed anime images. It works well on anime and landscape generations and supports a Gradio Web UI.\n created at: 2022-11-15\n- model_name: stabilityai/stable-diffusion-2-base\n  description: Stable Diffusion v2-base is a diffusion-based text-to-image generation model trained on a subset of LAION-5B dataset. It can be used to generate and modify images based on text prompts. The model uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is intended for research purposes only.\n created at: 2022-11-23\n- model_name: Linaqruf/anything-v3.0\n  description: A text-to-image model that generates images from text descriptions.\n created at: 2023-01-25\n- model_name: wavymulder/Analog-Diffusion\n  description: Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\n created at: 2022-12-10\n- model_name: nitrosocke/nitro-diffusion\n  description: Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use.\n created at: 2022-11-16\n- model_name: Lykon/DreamShaper\n  description: Dream Shaper is a text-to-image model that generates artistic images based on the given input text. Read more about this model here: https://civitai.com/models/4384/dreamshaper\n created at: 2023-01-12\n- model_name: dreamlike-art/dreamlike-diffusion-1.0\n  description: Dreamlike Diffusion 1.0 is SD 1.5 fine tuned on high quality art, made by dreamlike.art.\n created at: 2022-12-11\n- model_name: gsdf/Counterfeit-V2.5\n  description: Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.\n created at: 2023-02-02\n- model_name: dreamlike-art/dreamlike-anime-1.0\n  description: Dreamlike Anime 1.0 is a high quality anime model, made by dreamlike.art. It can be used to generate anime-style images based on text prompts. The model is trained on 768x768px images and works best with prompts that include 'photo anime, masterpiece, high quality, absurdres'. It can be used with the Stable Diffusion Pipeline from the diffusers library.\n created at: 2023-01-08\n- model_name: 22h/vintedois-diffusion-v0-1\n  description: Vintedois (22h) Diffusion model trained by Predogl and piEsposito with open weights, configs and prompts. This model generates beautiful images without a lot of prompt engineering. It can also generate high fidelity faces with a little amount of steps.\n created at: 2022-12-27\n- model_name: darkstorm2150/Protogen_v2.2_Official_Release\n  description: Protogen v2.2 is a text-to-image model that generates high-quality images based on text prompts. It was warm-started with Stable Diffusion v1-5 and fine-tuned on a large amount of data from large datasets new and trending on civitai.com. The model can be used with the Stable Diffusion Pipeline and supports trigger words like 'modelshoot style' to enforce camera capture.\n created at: 2022-12-31\n- model_name: stabilityai/stable-diffusion-x4-upscaler\n  description: Stable Diffusion x4 upscaler is a latent diffusion model trained on a 10M subset of LAION containing images >2048x2048. It can be used to generate and modify images based on text prompts. The model receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule. The model is trained with English captions and might not work well with other languages.\n created at: 2022-11-23\n- model_name: stabilityai/sd-x2-latent-upscaler\n  description: Stable Diffusion x2 latent upscaler is a diffusion-based upscaler model developed by Katherine Crowson in collaboration with Stability AI. It is designed to upscale Stable Diffusion's latent denoised image embeddings, allowing for fast text-to-image and upscaling pipelines. The model was trained on a high-resolution subset of the LAION-2B dataset and works with all Stable Diffusion checkpoints.\n created at: 2023-02-03\n- model_name: stabilityai/stable-diffusion-2-inpainting\n  description: A Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) to generate and modify images based on text prompts.\n created at: 2022-11-23\n- model_name: Qwen/Qwen-Image\n  description: üíú qwen chat&nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë tech report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp üñ•Ô∏è demo&nbsp&nbsp &nbsp&nbspüí¨ wechat &nbsp&nbsp &nbsp&nbspü´® discord&nbsp&nbsp we are thrilled to release qwen-image , an image generation foundation model in the qwen series that achieves significant advances in complex text rendering and precise image editing . experiments show strong general capabilities in both image generation and editing, with exceptional performance in text rendering, especially for chinese. we are thrilled to release qwen-image , an image generation foundation model in the qwen series that achieves significant advances in complex text rendering and precise image editing . experiments show strong general capabilities in both image generation and editing, with exceptional performance in text rendering, especially for chinese.\n created at: 2025-08-02T04:58:07+00:00\n- model_name: deepseek-ai/Janus-Pro-7B\n  description: janus-pro is a novel autoregressive framework that unifies multimodal understanding and generation. it addresses the limitations of previous approaches by decoupling visual encoding into separate pathways, while still utilizing a single, unified transformer architecture for processing. the decoupling not only alleviates the conflict between the visual encoder‚Äôs roles in understanding and generation, but also enhances the framework‚Äôs flexibility. janus-pro surpasses previous unified model and matches or exceeds the performance of task-specific models.\n created at: 2025-01-26T12:05:50+00:00\n- model_name: HiDream-ai/HiDream-I1-Full\n  description: `hidream-i1` is a new open-source image generative foundation model with 17b parameters that achieves state-of-the-art image generation quality within seconds. for more features and to experience the full capabilities of our product, please visit . - üåü july 16, 2025 : we've open-sourced the updated image editing model . - ‚ú® superior image quality - produces exceptional results across multiple styles including photorealistic, cartoon, artistic, and more. achieves state-of-the-art hps v2.1 score, which aligns with human preferences. - üéØ best-in-class prompt following - achieves industry-leading scores on geneval and dpg benchmarks, outperforming all other open-source models. - üîì open source - released under the mit license to foster scientific advancement and enable creative innovation. - üíº commercial-friendly - generated images can be freely used for personal projects, scientific research, and commercial applications.\n created at: 2025-04-06T14:18:31+00:00\n- model_name: stabilityai/stable-diffusion-3.5-medium\n  description: is a multimodal diffusion transformer with improvements mmdit-x text-to-image model that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency. please note: this model is released under the . visit to learn or for commercial licensing details. - developed by: stability ai - developed by: stability ai - model type: mmdit-x text-to-image generative model - model description: this model generates images based on text prompts. it is a multimodal diffusion transformer  with improvements that use three fixed, pretrained text encoders, with qk-normalization to improve training stability, and dual attention blocks in the first 12 transformer layers.\n created at: 2024-10-29T10:27:32+00:00\n- model_name: black-forest-labs/FLUX.1-Krea-dev\n  description: !flux.1 krea dev grid./teaser.png `flux.1 krea dev` is a 12 billion parameter rectified flow transformer capable of generating images from text descriptions. for more information, please read our and . 1. cutting-edge output quality, with a focus on aesthetic photography. 2. competitive prompt following, matching the performance of closed source alternatives. 3. trained using guidance distillation, making `flux.1 krea dev` more efficient. 4. open weights to drive new scientific research, and empower artists to develop innovative workflows. 5. generated outputs can be used for personal, scientific, and commercial purposes, as described in the .\n created at: 2025-07-07T10:39:11+00:00\n- model_name: OnomaAIResearch/Illustrious-xl-early-release-v0\n  description: @import url' .title-container display: flex; in summary, onoma ai plans to roll out open-source weights step by step and encourages the community to stay tuned for upcoming developments we‚Äôre just getting started.\n created at: 2024-09-20T14:56:38+00:00\n- model_name: gpustack/stable-diffusion-v3-5-large-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: mmdit text-to-image generative model - model description: this model generates images based on text prompts. it is a that use three fixed, pretrained text encoders, and with qk-normalization to improve training stability.\n created at: 2024-11-11T16:06:53+00:00\n- model_name: stabilityai/stable-diffusion-3.5-large\n  description: is a multimodal diffusion transformer mmdit text-to-image model that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency. please note: this model is released under the . visit to learn or for commercial licensing details. - developed by: stability ai - developed by: stability ai - model type: mmdit text-to-image generative model - model description: this model generates images based on text prompts. it is a that use three fixed, pretrained text encoders, and with qk-normalization to improve training stability.\n created at: 2024-10-22T07:29:57+00:00\n- model_name: gpustack/stable-diffusion-v3-5-large-turbo-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: mmdit text-to-image generative model - model description: this model generates images based on text prompts. it is an add-distilled that use three fixed, pretrained text encoders, and with qk-normalization.\n created at: 2024-11-13T00:25:51+00:00\n- model_name: gpustack/stable-diffusion-xl-base-1.0-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: diffusion-based text-to-image generative model - license:  - model description: this is a model that can be used to generate and modify images based on text prompts. it is a that uses two fixed, pretrained text encoders and . - resources for more information: check out our and the .\n created at: 2024-11-08T08:16:33+00:00\n- model_name: HiDream-ai/HiDream-I1-Fast\n  description: `hidream-i1` is a new open-source image generative foundation model with 17b parameters that achieves state-of-the-art image generation quality within seconds. for more features and to experience the full capabilities of our product, please visit . - üìù may 28, 2025 : we've released our technical report . - ‚ú® superior image quality - produces exceptional results across multiple styles including photorealistic, cartoon, artistic, and more. achieves state-of-the-art hps v2.1 score, which aligns with human preferences. - üéØ best-in-class prompt following - achieves industry-leading scores on geneval and dpg benchmarks, outperforming all other open-source models. - üîì open source - released under the mit license to foster scientific advancement and enable creative innovation. - üíº commercial-friendly - generated images can be freely used for personal projects, scientific research, and commercial applications.\n created at: 2025-04-06T14:18:51+00:00\n- model_name: gpustack/stable-diffusion-xl-inpainting-1.0-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: the diffusers team - model type: diffusion-based text-to-image generative model - license:  - model description: this is a model that can be used to generate and modify images based on text prompts. it is a that uses two fixed, pretrained text encoders and .\n created at: 2024-12-24T15:16:44+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-krea-dev\n  description: this repository contains nunchaku-quantized versions of , designed to generate high-quality images from text prompts. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: text-to-image - developed by: nunchaku team - model type: text-to-image - license:  - quantized from model:\n created at: 2025-08-01T05:11:14+00:00\n- model_name: cagliostrolab/animagine-xl-4.0\n  description: animagine xl 4.0 , also stylized as anim4gine , is the ultimate anime-themed finetuned sdxl model and the latest installment of . despite being a continuation, the model was retrained from with a massive dataset of 8.4m diverse anime-style images from various sources with the knowledge cut-off of january 7th 2025 and finetuned for approximately 2650 gpu hours. similar to the previous version, this model was trained using tag ordering method for the identity and style training. with the release of animagine xl 4.0 opt optimized , the model has been further refined with an additional dataset, improving stability , anatomy accuracy , noise reduction , color saturation , and overall color accuracy . these enhancements make animagine xl 4.0 opt more consistent and visually appealing while maintaining the signature quality of the series. - 2025-02-13 added animagine xl 4.0 opt\n created at: 2025-01-10T17:51:13+00:00\n- model_name: hoanganho0o/PMTACADEMY\n  description: you should use `pmt academy` to trigger the image generation. you should use `vietnamese` to trigger the image generation. weights for this model are available in safetensors format.\n created at: 2025-01-08T11:02:18+00:00\n- model_name: huanngzh/mv-adapter\n  description: create high-fidelity multi-view images with various base t2i models and various conditions. mv-adapter is a creative productivity tool that seamlessly transfer text-to-image models to multi-view generators. mv-adapter is a creative productivity tool that seamlessly transfer text-to-image models to multi-view generators. highlights: - 768x768 multi-view images - work well with personalized models e.g. dreamshaper, animagine, lcm, controlnet - support text or image to multi-view reconstruct 3d thereafter, or with geometry guidance for 3d texture generation - arbitrary view generation\n created at: 2024-11-29T14:38:58+00:00\n- model_name: nunchaku-tech/nunchaku-qwen-image\n  description: this repository contains nunchaku-quantized versions of , designed to generate high-quality images from text prompts, advances in complex text rendering. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: text-to-image - developed by: nunchaku team - model type: text-to-image - license: apache-2.0 - quantized from model:\n created at: 2025-08-14T08:19:05+00:00\n- model_name: stabilityai/stable-diffusion-3.5-large-turbo\n  description: is a multimodal diffusion transformer mmdit text-to-image model with that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency, with a focus on fewer inference steps. please note: this model is released under the . visit to learn or for commercial licensing details. - developed by: stability ai - developed by: stability ai - model type: mmdit text-to-image generative model - model description: this model generates images based on text prompts. it is an add-distilled that use three fixed, pretrained text encoders, and with qk-normalization.\n created at: 2024-10-22T07:32:47+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , designed to generate high-quality images from text prompts. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: text-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:27:40+00:00\n- model_name: Laxhar/noobai-XL-Vpred-1.0\n  description: noobai xl v-pred 1.0 this image generation model, based on laxhar/noobai-xl v1.0, leverages full danbooru and e621 datasets with native tags and natural language captioning. implemented as a v-prediction model distinct from eps-prediction, it requires specific parameter configurations - detailed in following sections.\n created at: 2024-12-22T06:16:42+00:00\n\nExamples:\n- example user query: Create a beautiful AI-generated image based on a given description.\n  model_name: prompthero/openjourney\n- example user query: Our gaming company is creating an anime-style game. We need to generate various anime-style characters and landscapes for the game based on text descriptions.\n  model_name: eimiss/EimisAnimeDiffusion_1.0v\n- example user query: We need a unique and detailed illustration for our science fiction book cover, specifically a futuristic city with flying cars.\n  model_name: 22h/vintedois-diffusion-v0-1\n- example user query: I have an online clothing store and I want to create ads with text descriptions. Generate images based on the text descriptions.\n  model_name: stabilityai/stable-diffusion-2\n- example user query: A fantasy novel writer needs a visual representation of their character \"A mystical wizard with a golden beard and a staff adorned with glowing jewels\" to be created from scratch.\n  model_name: nitrosocke/nitro-diffusion\n- example user query: Our team needs a unique image of a unicorn climbing the Eiffel tower for the project.\n  model_name: stabilityai/stable-diffusion-2-1\n\nUser prompt:\nDesign an infographic explaining Deluxe Rainbow Backpack.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_gen>\n\nCandidate Models:\n- model_name: runwayml/stable-diffusion-v1-5\n  description: Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.\n created at: 2023-05-25\n- model_name: CompVis/stable-diffusion-v1-4\n  description: Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v1-4 checkpoint was fine-tuned on 225k steps at resolution 512x512 on laion-aesthetics v2 5+ and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. This model is intended for research purposes and can be used for generating artworks, design, educational or creative tools, and research on generative models.\n created at: 2022-08-20\n- model_name: prompthero/openjourney\n  description: Openjourney is an open source Stable Diffusion fine-tuned model on Midjourney images, by PromptHero. It can be used for generating AI art based on text prompts.\n created at: 2022-11-08\n- model_name: hakurei/waifu-diffusion\n  description: waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.\n created at: 2022-08-30\n- model_name: runwayml/stable-diffusion-inpainting\n  description: Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.\n created at: 2023-05-25\n- model_name: stabilityai/stable-diffusion-2-1\n  description: Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes.\n created at: 2022-12-06\n- model_name: SG161222/Realistic_Vision_V1.4\n  description: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images.\n created at: 2023-03-05\n- model_name: stabilityai/stable-diffusion-2-1-base\n  description: Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H). It is intended for research purposes only and can be used in areas such as safe deployment of models, understanding limitations and biases of generative models, generation of artworks, and research on generative models.\n created at: 2022-12-06\n- model_name: dreamlike-art/dreamlike-photoreal-2.0\n  description: Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art. It can be used to generate photorealistic images from text prompts.\n created at: 2023-01-04\n- model_name: prompthero/openjourney-v4\n  description: Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.\n created at: 2022-12-11\n- model_name: stabilityai/stable-diffusion-2\n  description: Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.\n created at: 2022-11-23\n- model_name: stabilityai/stable-diffusion-2-depth\n  description: Stable Diffusion v2 is a latent diffusion model that generates and modifies images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is developed by Robin Rombach and Patrick Esser. The model works with English language prompts and is intended for research purposes only.\n created at: 2022-11-23\n- model_name: eimiss/EimisAnimeDiffusion_1.0v\n  description: EimisAnimeDiffusion_1.0v is a text-to-image model trained with high-quality and detailed anime images. It works well on anime and landscape generations and supports a Gradio Web UI.\n created at: 2022-11-15\n- model_name: stabilityai/stable-diffusion-2-base\n  description: Stable Diffusion v2-base is a diffusion-based text-to-image generation model trained on a subset of LAION-5B dataset. It can be used to generate and modify images based on text prompts. The model uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is intended for research purposes only.\n created at: 2022-11-23\n- model_name: Linaqruf/anything-v3.0\n  description: A text-to-image model that generates images from text descriptions.\n created at: 2023-01-25\n- model_name: wavymulder/Analog-Diffusion\n  description: Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\n created at: 2022-12-10\n- model_name: nitrosocke/nitro-diffusion\n  description: Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use.\n created at: 2022-11-16\n- model_name: Lykon/DreamShaper\n  description: Dream Shaper is a text-to-image model that generates artistic images based on the given input text. Read more about this model here: https://civitai.com/models/4384/dreamshaper\n created at: 2023-01-12\n- model_name: dreamlike-art/dreamlike-diffusion-1.0\n  description: Dreamlike Diffusion 1.0 is SD 1.5 fine tuned on high quality art, made by dreamlike.art.\n created at: 2022-12-11\n- model_name: gsdf/Counterfeit-V2.5\n  description: Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.\n created at: 2023-02-02\n- model_name: dreamlike-art/dreamlike-anime-1.0\n  description: Dreamlike Anime 1.0 is a high quality anime model, made by dreamlike.art. It can be used to generate anime-style images based on text prompts. The model is trained on 768x768px images and works best with prompts that include 'photo anime, masterpiece, high quality, absurdres'. It can be used with the Stable Diffusion Pipeline from the diffusers library.\n created at: 2023-01-08\n- model_name: 22h/vintedois-diffusion-v0-1\n  description: Vintedois (22h) Diffusion model trained by Predogl and piEsposito with open weights, configs and prompts. This model generates beautiful images without a lot of prompt engineering. It can also generate high fidelity faces with a little amount of steps.\n created at: 2022-12-27\n- model_name: darkstorm2150/Protogen_v2.2_Official_Release\n  description: Protogen v2.2 is a text-to-image model that generates high-quality images based on text prompts. It was warm-started with Stable Diffusion v1-5 and fine-tuned on a large amount of data from large datasets new and trending on civitai.com. The model can be used with the Stable Diffusion Pipeline and supports trigger words like 'modelshoot style' to enforce camera capture.\n created at: 2022-12-31\n- model_name: stabilityai/stable-diffusion-x4-upscaler\n  description: Stable Diffusion x4 upscaler is a latent diffusion model trained on a 10M subset of LAION containing images >2048x2048. It can be used to generate and modify images based on text prompts. The model receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule. The model is trained with English captions and might not work well with other languages.\n created at: 2022-11-23\n- model_name: stabilityai/sd-x2-latent-upscaler\n  description: Stable Diffusion x2 latent upscaler is a diffusion-based upscaler model developed by Katherine Crowson in collaboration with Stability AI. It is designed to upscale Stable Diffusion's latent denoised image embeddings, allowing for fast text-to-image and upscaling pipelines. The model was trained on a high-resolution subset of the LAION-2B dataset and works with all Stable Diffusion checkpoints.\n created at: 2023-02-03\n- model_name: stabilityai/stable-diffusion-2-inpainting\n  description: A Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) to generate and modify images based on text prompts.\n created at: 2022-11-23\n- model_name: Qwen/Qwen-Image\n  description: üíú qwen chat&nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë tech report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp üñ•Ô∏è demo&nbsp&nbsp &nbsp&nbspüí¨ wechat &nbsp&nbsp &nbsp&nbspü´® discord&nbsp&nbsp we are thrilled to release qwen-image , an image generation foundation model in the qwen series that achieves significant advances in complex text rendering and precise image editing . experiments show strong general capabilities in both image generation and editing, with exceptional performance in text rendering, especially for chinese. we are thrilled to release qwen-image , an image generation foundation model in the qwen series that achieves significant advances in complex text rendering and precise image editing . experiments show strong general capabilities in both image generation and editing, with exceptional performance in text rendering, especially for chinese.\n created at: 2025-08-02T04:58:07+00:00\n- model_name: deepseek-ai/Janus-Pro-7B\n  description: janus-pro is a novel autoregressive framework that unifies multimodal understanding and generation. it addresses the limitations of previous approaches by decoupling visual encoding into separate pathways, while still utilizing a single, unified transformer architecture for processing. the decoupling not only alleviates the conflict between the visual encoder‚Äôs roles in understanding and generation, but also enhances the framework‚Äôs flexibility. janus-pro surpasses previous unified model and matches or exceeds the performance of task-specific models.\n created at: 2025-01-26T12:05:50+00:00\n- model_name: HiDream-ai/HiDream-I1-Full\n  description: `hidream-i1` is a new open-source image generative foundation model with 17b parameters that achieves state-of-the-art image generation quality within seconds. for more features and to experience the full capabilities of our product, please visit . - üåü july 16, 2025 : we've open-sourced the updated image editing model . - ‚ú® superior image quality - produces exceptional results across multiple styles including photorealistic, cartoon, artistic, and more. achieves state-of-the-art hps v2.1 score, which aligns with human preferences. - üéØ best-in-class prompt following - achieves industry-leading scores on geneval and dpg benchmarks, outperforming all other open-source models. - üîì open source - released under the mit license to foster scientific advancement and enable creative innovation. - üíº commercial-friendly - generated images can be freely used for personal projects, scientific research, and commercial applications.\n created at: 2025-04-06T14:18:31+00:00\n- model_name: stabilityai/stable-diffusion-3.5-medium\n  description: is a multimodal diffusion transformer with improvements mmdit-x text-to-image model that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency. please note: this model is released under the . visit to learn or for commercial licensing details. - developed by: stability ai - developed by: stability ai - model type: mmdit-x text-to-image generative model - model description: this model generates images based on text prompts. it is a multimodal diffusion transformer  with improvements that use three fixed, pretrained text encoders, with qk-normalization to improve training stability, and dual attention blocks in the first 12 transformer layers.\n created at: 2024-10-29T10:27:32+00:00\n- model_name: black-forest-labs/FLUX.1-Krea-dev\n  description: !flux.1 krea dev grid./teaser.png `flux.1 krea dev` is a 12 billion parameter rectified flow transformer capable of generating images from text descriptions. for more information, please read our and . 1. cutting-edge output quality, with a focus on aesthetic photography. 2. competitive prompt following, matching the performance of closed source alternatives. 3. trained using guidance distillation, making `flux.1 krea dev` more efficient. 4. open weights to drive new scientific research, and empower artists to develop innovative workflows. 5. generated outputs can be used for personal, scientific, and commercial purposes, as described in the .\n created at: 2025-07-07T10:39:11+00:00\n- model_name: OnomaAIResearch/Illustrious-xl-early-release-v0\n  description: @import url' .title-container display: flex; in summary, onoma ai plans to roll out open-source weights step by step and encourages the community to stay tuned for upcoming developments we‚Äôre just getting started.\n created at: 2024-09-20T14:56:38+00:00\n- model_name: gpustack/stable-diffusion-v3-5-large-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: mmdit text-to-image generative model - model description: this model generates images based on text prompts. it is a that use three fixed, pretrained text encoders, and with qk-normalization to improve training stability.\n created at: 2024-11-11T16:06:53+00:00\n- model_name: stabilityai/stable-diffusion-3.5-large\n  description: is a multimodal diffusion transformer mmdit text-to-image model that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency. please note: this model is released under the . visit to learn or for commercial licensing details. - developed by: stability ai - developed by: stability ai - model type: mmdit text-to-image generative model - model description: this model generates images based on text prompts. it is a that use three fixed, pretrained text encoders, and with qk-normalization to improve training stability.\n created at: 2024-10-22T07:29:57+00:00\n- model_name: gpustack/stable-diffusion-v3-5-large-turbo-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: mmdit text-to-image generative model - model description: this model generates images based on text prompts. it is an add-distilled that use three fixed, pretrained text encoders, and with qk-normalization.\n created at: 2024-11-13T00:25:51+00:00\n- model_name: gpustack/stable-diffusion-xl-base-1.0-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: diffusion-based text-to-image generative model - license:  - model description: this is a model that can be used to generate and modify images based on text prompts. it is a that uses two fixed, pretrained text encoders and . - resources for more information: check out our and the .\n created at: 2024-11-08T08:16:33+00:00\n- model_name: HiDream-ai/HiDream-I1-Fast\n  description: `hidream-i1` is a new open-source image generative foundation model with 17b parameters that achieves state-of-the-art image generation quality within seconds. for more features and to experience the full capabilities of our product, please visit . - üìù may 28, 2025 : we've released our technical report . - ‚ú® superior image quality - produces exceptional results across multiple styles including photorealistic, cartoon, artistic, and more. achieves state-of-the-art hps v2.1 score, which aligns with human preferences. - üéØ best-in-class prompt following - achieves industry-leading scores on geneval and dpg benchmarks, outperforming all other open-source models. - üîì open source - released under the mit license to foster scientific advancement and enable creative innovation. - üíº commercial-friendly - generated images can be freely used for personal projects, scientific research, and commercial applications.\n created at: 2025-04-06T14:18:51+00:00\n- model_name: gpustack/stable-diffusion-xl-inpainting-1.0-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: the diffusers team - model type: diffusion-based text-to-image generative model - license:  - model description: this is a model that can be used to generate and modify images based on text prompts. it is a that uses two fixed, pretrained text encoders and .\n created at: 2024-12-24T15:16:44+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-krea-dev\n  description: this repository contains nunchaku-quantized versions of , designed to generate high-quality images from text prompts. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: text-to-image - developed by: nunchaku team - model type: text-to-image - license:  - quantized from model:\n created at: 2025-08-01T05:11:14+00:00\n- model_name: cagliostrolab/animagine-xl-4.0\n  description: animagine xl 4.0 , also stylized as anim4gine , is the ultimate anime-themed finetuned sdxl model and the latest installment of . despite being a continuation, the model was retrained from with a massive dataset of 8.4m diverse anime-style images from various sources with the knowledge cut-off of january 7th 2025 and finetuned for approximately 2650 gpu hours. similar to the previous version, this model was trained using tag ordering method for the identity and style training. with the release of animagine xl 4.0 opt optimized , the model has been further refined with an additional dataset, improving stability , anatomy accuracy , noise reduction , color saturation , and overall color accuracy . these enhancements make animagine xl 4.0 opt more consistent and visually appealing while maintaining the signature quality of the series. - 2025-02-13 added animagine xl 4.0 opt\n created at: 2025-01-10T17:51:13+00:00\n- model_name: hoanganho0o/PMTACADEMY\n  description: you should use `pmt academy` to trigger the image generation. you should use `vietnamese` to trigger the image generation. weights for this model are available in safetensors format.\n created at: 2025-01-08T11:02:18+00:00\n- model_name: huanngzh/mv-adapter\n  description: create high-fidelity multi-view images with various base t2i models and various conditions. mv-adapter is a creative productivity tool that seamlessly transfer text-to-image models to multi-view generators. mv-adapter is a creative productivity tool that seamlessly transfer text-to-image models to multi-view generators. highlights: - 768x768 multi-view images - work well with personalized models e.g. dreamshaper, animagine, lcm, controlnet - support text or image to multi-view reconstruct 3d thereafter, or with geometry guidance for 3d texture generation - arbitrary view generation\n created at: 2024-11-29T14:38:58+00:00\n- model_name: nunchaku-tech/nunchaku-qwen-image\n  description: this repository contains nunchaku-quantized versions of , designed to generate high-quality images from text prompts, advances in complex text rendering. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: text-to-image - developed by: nunchaku team - model type: text-to-image - license: apache-2.0 - quantized from model:\n created at: 2025-08-14T08:19:05+00:00\n- model_name: stabilityai/stable-diffusion-3.5-large-turbo\n  description: is a multimodal diffusion transformer mmdit text-to-image model with that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency, with a focus on fewer inference steps. please note: this model is released under the . visit to learn or for commercial licensing details. - developed by: stability ai - developed by: stability ai - model type: mmdit text-to-image generative model - model description: this model generates images based on text prompts. it is an add-distilled that use three fixed, pretrained text encoders, and with qk-normalization.\n created at: 2024-10-22T07:32:47+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , designed to generate high-quality images from text prompts. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: text-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:27:40+00:00\n- model_name: Laxhar/noobai-XL-Vpred-1.0\n  description: noobai xl v-pred 1.0 this image generation model, based on laxhar/noobai-xl v1.0, leverages full danbooru and e621 datasets with native tags and natural language captioning. implemented as a v-prediction model distinct from eps-prediction, it requires specific parameter configurations - detailed in following sections.\n created at: 2024-12-22T06:16:42+00:00\n\nExamples:\n- example user query: Create a beautiful AI-generated image based on a given description.\n  model_name: prompthero/openjourney\n- example user query: Our gaming company is creating an anime-style game. We need to generate various anime-style characters and landscapes for the game based on text descriptions.\n  model_name: eimiss/EimisAnimeDiffusion_1.0v\n- example user query: We need a unique and detailed illustration for our science fiction book cover, specifically a futuristic city with flying cars.\n  model_name: 22h/vintedois-diffusion-v0-1\n- example user query: I have an online clothing store and I want to create ads with text descriptions. Generate images based on the text descriptions.\n  model_name: stabilityai/stable-diffusion-2\n- example user query: A fantasy novel writer needs a visual representation of their character \"A mystical wizard with a golden beard and a staff adorned with glowing jewels\" to be created from scratch.\n  model_name: nitrosocke/nitro-diffusion\n- example user query: Our team needs a unique image of a unicorn climbing the Eiffel tower for the project.\n  model_name: stabilityai/stable-diffusion-2-1\n\nUser prompt:\nSculpt an image displaying Planet over the hill - Fantasy art wallpaper.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_gen>\n\nCandidate Models:\n- model_name: runwayml/stable-diffusion-v1-5\n  description: Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input.\n created at: 2023-05-25\n- model_name: CompVis/stable-diffusion-v1-4\n  description: Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v1-4 checkpoint was fine-tuned on 225k steps at resolution 512x512 on laion-aesthetics v2 5+ and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. This model is intended for research purposes and can be used for generating artworks, design, educational or creative tools, and research on generative models.\n created at: 2022-08-20\n- model_name: prompthero/openjourney\n  description: Openjourney is an open source Stable Diffusion fine-tuned model on Midjourney images, by PromptHero. It can be used for generating AI art based on text prompts.\n created at: 2022-11-08\n- model_name: hakurei/waifu-diffusion\n  description: waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.\n created at: 2022-08-30\n- model_name: runwayml/stable-diffusion-inpainting\n  description: Stable Diffusion Inpainting is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input, with the extra capability of inpainting the pictures by using a mask.\n created at: 2023-05-25\n- model_name: stabilityai/stable-diffusion-2-1\n  description: Stable Diffusion v2-1 is a diffusion-based text-to-image generation model developed by Robin Rombach and Patrick Esser. It is capable of generating and modifying images based on text prompts in English. The model is trained on a subset of the LAION-5B dataset and is primarily intended for research purposes.\n created at: 2022-12-06\n- model_name: SG161222/Realistic_Vision_V1.4\n  description: Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images.\n created at: 2023-03-05\n- model_name: stabilityai/stable-diffusion-2-1-base\n  description: Stable Diffusion v2-1-base is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It is a Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H). It is intended for research purposes only and can be used in areas such as safe deployment of models, understanding limitations and biases of generative models, generation of artworks, and research on generative models.\n created at: 2022-12-06\n- model_name: dreamlike-art/dreamlike-photoreal-2.0\n  description: Dreamlike Photoreal 2.0 is a photorealistic model based on Stable Diffusion 1.5, made by dreamlike.art. It can be used to generate photorealistic images from text prompts.\n created at: 2023-01-04\n- model_name: prompthero/openjourney-v4\n  description: Openjourney v4 is trained on +124k Midjourney v4 images by PromptHero. It is used for generating images based on text inputs.\n created at: 2022-12-11\n- model_name: stabilityai/stable-diffusion-2\n  description: Stable Diffusion v2 is a diffusion-based text-to-image generation model that can generate and modify images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is primarily intended for research purposes, such as safe deployment of models with potential to generate harmful content, understanding limitations and biases of generative models, and generation of artworks for design and artistic processes.\n created at: 2022-11-23\n- model_name: stabilityai/stable-diffusion-2-depth\n  description: Stable Diffusion v2 is a latent diffusion model that generates and modifies images based on text prompts. It uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is developed by Robin Rombach and Patrick Esser. The model works with English language prompts and is intended for research purposes only.\n created at: 2022-11-23\n- model_name: eimiss/EimisAnimeDiffusion_1.0v\n  description: EimisAnimeDiffusion_1.0v is a text-to-image model trained with high-quality and detailed anime images. It works well on anime and landscape generations and supports a Gradio Web UI.\n created at: 2022-11-15\n- model_name: stabilityai/stable-diffusion-2-base\n  description: Stable Diffusion v2-base is a diffusion-based text-to-image generation model trained on a subset of LAION-5B dataset. It can be used to generate and modify images based on text prompts. The model uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) and is intended for research purposes only.\n created at: 2022-11-23\n- model_name: Linaqruf/anything-v3.0\n  description: A text-to-image model that generates images from text descriptions.\n created at: 2023-01-25\n- model_name: wavymulder/Analog-Diffusion\n  description: Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.\n created at: 2022-12-10\n- model_name: nitrosocke/nitro-diffusion\n  description: Nitro Diffusion is a fine-tuned Stable Diffusion model trained on three artstyles simultaneously while keeping each style separate from the others. It allows for high control of mixing, weighting, and single style use.\n created at: 2022-11-16\n- model_name: Lykon/DreamShaper\n  description: Dream Shaper is a text-to-image model that generates artistic images based on the given input text. Read more about this model here: https://civitai.com/models/4384/dreamshaper\n created at: 2023-01-12\n- model_name: dreamlike-art/dreamlike-diffusion-1.0\n  description: Dreamlike Diffusion 1.0 is SD 1.5 fine tuned on high quality art, made by dreamlike.art.\n created at: 2022-12-11\n- model_name: gsdf/Counterfeit-V2.5\n  description: Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.\n created at: 2023-02-02\n- model_name: dreamlike-art/dreamlike-anime-1.0\n  description: Dreamlike Anime 1.0 is a high quality anime model, made by dreamlike.art. It can be used to generate anime-style images based on text prompts. The model is trained on 768x768px images and works best with prompts that include 'photo anime, masterpiece, high quality, absurdres'. It can be used with the Stable Diffusion Pipeline from the diffusers library.\n created at: 2023-01-08\n- model_name: 22h/vintedois-diffusion-v0-1\n  description: Vintedois (22h) Diffusion model trained by Predogl and piEsposito with open weights, configs and prompts. This model generates beautiful images without a lot of prompt engineering. It can also generate high fidelity faces with a little amount of steps.\n created at: 2022-12-27\n- model_name: darkstorm2150/Protogen_v2.2_Official_Release\n  description: Protogen v2.2 is a text-to-image model that generates high-quality images based on text prompts. It was warm-started with Stable Diffusion v1-5 and fine-tuned on a large amount of data from large datasets new and trending on civitai.com. The model can be used with the Stable Diffusion Pipeline and supports trigger words like 'modelshoot style' to enforce camera capture.\n created at: 2022-12-31\n- model_name: stabilityai/stable-diffusion-x4-upscaler\n  description: Stable Diffusion x4 upscaler is a latent diffusion model trained on a 10M subset of LAION containing images >2048x2048. It can be used to generate and modify images based on text prompts. The model receives a noise_level as an input parameter, which can be used to add noise to the low-resolution input according to a predefined diffusion schedule. The model is trained with English captions and might not work well with other languages.\n created at: 2022-11-23\n- model_name: stabilityai/sd-x2-latent-upscaler\n  description: Stable Diffusion x2 latent upscaler is a diffusion-based upscaler model developed by Katherine Crowson in collaboration with Stability AI. It is designed to upscale Stable Diffusion's latent denoised image embeddings, allowing for fast text-to-image and upscaling pipelines. The model was trained on a high-resolution subset of the LAION-2B dataset and works with all Stable Diffusion checkpoints.\n created at: 2023-02-03\n- model_name: stabilityai/stable-diffusion-2-inpainting\n  description: A Latent Diffusion Model that uses a fixed, pretrained text encoder (OpenCLIP-ViT/H) to generate and modify images based on text prompts.\n created at: 2022-11-23\n- model_name: Qwen/Qwen-Image\n  description: üíú qwen chat&nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë tech report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp üñ•Ô∏è demo&nbsp&nbsp &nbsp&nbspüí¨ wechat &nbsp&nbsp &nbsp&nbspü´® discord&nbsp&nbsp we are thrilled to release qwen-image , an image generation foundation model in the qwen series that achieves significant advances in complex text rendering and precise image editing . experiments show strong general capabilities in both image generation and editing, with exceptional performance in text rendering, especially for chinese. we are thrilled to release qwen-image , an image generation foundation model in the qwen series that achieves significant advances in complex text rendering and precise image editing . experiments show strong general capabilities in both image generation and editing, with exceptional performance in text rendering, especially for chinese.\n created at: 2025-08-02T04:58:07+00:00\n- model_name: deepseek-ai/Janus-Pro-7B\n  description: janus-pro is a novel autoregressive framework that unifies multimodal understanding and generation. it addresses the limitations of previous approaches by decoupling visual encoding into separate pathways, while still utilizing a single, unified transformer architecture for processing. the decoupling not only alleviates the conflict between the visual encoder‚Äôs roles in understanding and generation, but also enhances the framework‚Äôs flexibility. janus-pro surpasses previous unified model and matches or exceeds the performance of task-specific models.\n created at: 2025-01-26T12:05:50+00:00\n- model_name: HiDream-ai/HiDream-I1-Full\n  description: `hidream-i1` is a new open-source image generative foundation model with 17b parameters that achieves state-of-the-art image generation quality within seconds. for more features and to experience the full capabilities of our product, please visit . - üåü july 16, 2025 : we've open-sourced the updated image editing model . - ‚ú® superior image quality - produces exceptional results across multiple styles including photorealistic, cartoon, artistic, and more. achieves state-of-the-art hps v2.1 score, which aligns with human preferences. - üéØ best-in-class prompt following - achieves industry-leading scores on geneval and dpg benchmarks, outperforming all other open-source models. - üîì open source - released under the mit license to foster scientific advancement and enable creative innovation. - üíº commercial-friendly - generated images can be freely used for personal projects, scientific research, and commercial applications.\n created at: 2025-04-06T14:18:31+00:00\n- model_name: stabilityai/stable-diffusion-3.5-medium\n  description: is a multimodal diffusion transformer with improvements mmdit-x text-to-image model that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency. please note: this model is released under the . visit to learn or for commercial licensing details. - developed by: stability ai - developed by: stability ai - model type: mmdit-x text-to-image generative model - model description: this model generates images based on text prompts. it is a multimodal diffusion transformer  with improvements that use three fixed, pretrained text encoders, with qk-normalization to improve training stability, and dual attention blocks in the first 12 transformer layers.\n created at: 2024-10-29T10:27:32+00:00\n- model_name: black-forest-labs/FLUX.1-Krea-dev\n  description: !flux.1 krea dev grid./teaser.png `flux.1 krea dev` is a 12 billion parameter rectified flow transformer capable of generating images from text descriptions. for more information, please read our and . 1. cutting-edge output quality, with a focus on aesthetic photography. 2. competitive prompt following, matching the performance of closed source alternatives. 3. trained using guidance distillation, making `flux.1 krea dev` more efficient. 4. open weights to drive new scientific research, and empower artists to develop innovative workflows. 5. generated outputs can be used for personal, scientific, and commercial purposes, as described in the .\n created at: 2025-07-07T10:39:11+00:00\n- model_name: OnomaAIResearch/Illustrious-xl-early-release-v0\n  description: @import url' .title-container display: flex; in summary, onoma ai plans to roll out open-source weights step by step and encourages the community to stay tuned for upcoming developments we‚Äôre just getting started.\n created at: 2024-09-20T14:56:38+00:00\n- model_name: gpustack/stable-diffusion-v3-5-large-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: mmdit text-to-image generative model - model description: this model generates images based on text prompts. it is a that use three fixed, pretrained text encoders, and with qk-normalization to improve training stability.\n created at: 2024-11-11T16:06:53+00:00\n- model_name: stabilityai/stable-diffusion-3.5-large\n  description: is a multimodal diffusion transformer mmdit text-to-image model that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency. please note: this model is released under the . visit to learn or for commercial licensing details. - developed by: stability ai - developed by: stability ai - model type: mmdit text-to-image generative model - model description: this model generates images based on text prompts. it is a that use three fixed, pretrained text encoders, and with qk-normalization to improve training stability.\n created at: 2024-10-22T07:29:57+00:00\n- model_name: gpustack/stable-diffusion-v3-5-large-turbo-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: mmdit text-to-image generative model - model description: this model generates images based on text prompts. it is an add-distilled that use three fixed, pretrained text encoders, and with qk-normalization.\n created at: 2024-11-13T00:25:51+00:00\n- model_name: gpustack/stable-diffusion-xl-base-1.0-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: stability ai - model type: diffusion-based text-to-image generative model - license:  - model description: this is a model that can be used to generate and modify images based on text prompts. it is a that uses two fixed, pretrained text encoders and . - resources for more information: check out our and the .\n created at: 2024-11-08T08:16:33+00:00\n- model_name: HiDream-ai/HiDream-I1-Fast\n  description: `hidream-i1` is a new open-source image generative foundation model with 17b parameters that achieves state-of-the-art image generation quality within seconds. for more features and to experience the full capabilities of our product, please visit . - üìù may 28, 2025 : we've released our technical report . - ‚ú® superior image quality - produces exceptional results across multiple styles including photorealistic, cartoon, artistic, and more. achieves state-of-the-art hps v2.1 score, which aligns with human preferences. - üéØ best-in-class prompt following - achieves industry-leading scores on geneval and dpg benchmarks, outperforming all other open-source models. - üîì open source - released under the mit license to foster scientific advancement and enable creative innovation. - üíº commercial-friendly - generated images can be freely used for personal projects, scientific research, and commercial applications.\n created at: 2025-04-06T14:18:51+00:00\n- model_name: gpustack/stable-diffusion-xl-inpainting-1.0-GGUF\n  description: !!! experimental supported by only !!! model creator : original model : - developed by: the diffusers team - model type: diffusion-based text-to-image generative model - license:  - model description: this is a model that can be used to generate and modify images based on text prompts. it is a that uses two fixed, pretrained text encoders and .\n created at: 2024-12-24T15:16:44+00:00\n- model_name: nunchaku-tech/nunchaku-flux.1-krea-dev\n  description: this repository contains nunchaku-quantized versions of , designed to generate high-quality images from text prompts. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: text-to-image - developed by: nunchaku team - model type: text-to-image - license:  - quantized from model:\n created at: 2025-08-01T05:11:14+00:00\n- model_name: cagliostrolab/animagine-xl-4.0\n  description: animagine xl 4.0 , also stylized as anim4gine , is the ultimate anime-themed finetuned sdxl model and the latest installment of . despite being a continuation, the model was retrained from with a massive dataset of 8.4m diverse anime-style images from various sources with the knowledge cut-off of january 7th 2025 and finetuned for approximately 2650 gpu hours. similar to the previous version, this model was trained using tag ordering method for the identity and style training. with the release of animagine xl 4.0 opt optimized , the model has been further refined with an additional dataset, improving stability , anatomy accuracy , noise reduction , color saturation , and overall color accuracy . these enhancements make animagine xl 4.0 opt more consistent and visually appealing while maintaining the signature quality of the series. - 2025-02-13 added animagine xl 4.0 opt\n created at: 2025-01-10T17:51:13+00:00\n- model_name: hoanganho0o/PMTACADEMY\n  description: you should use `pmt academy` to trigger the image generation. you should use `vietnamese` to trigger the image generation. weights for this model are available in safetensors format.\n created at: 2025-01-08T11:02:18+00:00\n- model_name: huanngzh/mv-adapter\n  description: create high-fidelity multi-view images with various base t2i models and various conditions. mv-adapter is a creative productivity tool that seamlessly transfer text-to-image models to multi-view generators. mv-adapter is a creative productivity tool that seamlessly transfer text-to-image models to multi-view generators. highlights: - 768x768 multi-view images - work well with personalized models e.g. dreamshaper, animagine, lcm, controlnet - support text or image to multi-view reconstruct 3d thereafter, or with geometry guidance for 3d texture generation - arbitrary view generation\n created at: 2024-11-29T14:38:58+00:00\n- model_name: nunchaku-tech/nunchaku-qwen-image\n  description: this repository contains nunchaku-quantized versions of , designed to generate high-quality images from text prompts, advances in complex text rendering. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - model type: text-to-image - developed by: nunchaku team - model type: text-to-image - license: apache-2.0 - quantized from model:\n created at: 2025-08-14T08:19:05+00:00\n- model_name: stabilityai/stable-diffusion-3.5-large-turbo\n  description: is a multimodal diffusion transformer mmdit text-to-image model with that features improved performance in image quality, typography, complex prompt understanding, and resource-efficiency, with a focus on fewer inference steps. please note: this model is released under the . visit to learn or for commercial licensing details. - developed by: stability ai - developed by: stability ai - model type: mmdit text-to-image generative model - model description: this model generates images based on text prompts. it is an add-distilled that use three fixed, pretrained text encoders, and with qk-normalization.\n created at: 2024-10-22T07:32:47+00:00\n- model_name: mit-han-lab/nunchaku-flux.1-dev\n  description: this repository has been migrated to and will be hidden in december 2025. this repository contains nunchaku-quantized versions of , designed to generate high-quality images from text prompts. it is optimized for efficient inference while maintaining minimal loss in performance. - developed by: nunchaku team - developed by: nunchaku team - model type: text-to-image - license:  - quantized from model:\n created at: 2025-05-29T06:27:40+00:00\n- model_name: Laxhar/noobai-XL-Vpred-1.0\n  description: noobai xl v-pred 1.0 this image generation model, based on laxhar/noobai-xl v1.0, leverages full danbooru and e621 datasets with native tags and natural language captioning. implemented as a v-prediction model distinct from eps-prediction, it requires specific parameter configurations - detailed in following sections.\n created at: 2024-12-22T06:16:42+00:00\n\nExamples:\n- example user query: Create a beautiful AI-generated image based on a given description.\n  model_name: prompthero/openjourney\n- example user query: Our gaming company is creating an anime-style game. We need to generate various anime-style characters and landscapes for the game based on text descriptions.\n  model_name: eimiss/EimisAnimeDiffusion_1.0v\n- example user query: We need a unique and detailed illustration for our science fiction book cover, specifically a futuristic city with flying cars.\n  model_name: 22h/vintedois-diffusion-v0-1\n- example user query: I have an online clothing store and I want to create ads with text descriptions. Generate images based on the text descriptions.\n  model_name: stabilityai/stable-diffusion-2\n- example user query: A fantasy novel writer needs a visual representation of their character \"A mystical wizard with a golden beard and a staff adorned with glowing jewels\" to be created from scratch.\n  model_name: nitrosocke/nitro-diffusion\n- example user query: Our team needs a unique image of a unicorn climbing the Eiffel tower for the project.\n  model_name: stabilityai/stable-diffusion-2-1\n\nUser prompt:\nIllustrate an element of two grilled corn cobs.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n"
  ],
  "<image_normal>": [
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_normal>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: prs-eth/marigold-normals-v1-1\n  description: marigold normals v1-1 model card this is a model card for the `marigold-normals-v1-1` model for monocular normals estimation from a single image. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-10-21T19:16:05+00:00\n- model_name: Stable-X/yoso-normal-v1-8-1\n  description: this repository contains the weights of stablenormal: reducing diffusion variance for stable and sharp normal see the github repository: regarding installation instructions. the model can then be used as follows:\n created at: 2025-01-20T16:28:01+00:00\n- model_name: Stable-X/yoso-normal-v1-5\n  description: this repository contains the weights of stablenormal: reducing diffusion variance for stable and sharp normal see the github repository: regarding installation instructions. the model can then be used as follows:\n created at: 2024-11-07T22:30:16+00:00\n- model_name: jasperai/Flux.1-dev-Controlnet-Surface-Normals\n  description: this is controlnet for surface normals map developed by jasper research team. this model can be used directly with the `diffusers` library üí° note: you can compute the conditioning map using the `normalbaedetector` from the `controlnet aux` library\n created at: 2024-09-23T13:00:04+00:00\n- model_name: ilkerzgi/Glittering-Portrait-Kontext-Dev-Lora\n  description: this lora model transforms normal portraits into artistic, glittering portres with dramatic lighting and effects. perfect for creating stunning, artistic portrait variations. - -\n created at: 2025-07-17T18:17:40+00:00\n\nExamples:\n- example user query: We want to create a 3D model from a photo of our latest product. The idea is to estimate the normal map of the model based on the image.\n  model_name: lllyasviel/sd-controlnet-normal\n- example user query: A high-end art gallery is interested in transforming a normal image of a painting into a unique version with edge details to use on their marketing materials.\n  model_name: lllyasviel/sd-controlnet-hed\n- example user query: We are a game design company. We need to estimate the normal map from depth map.\n  model_name: lllyasviel/sd-controlnet-normal\n- example user query: I need an image of a car and estimate its 3D normal map using an AI model.\n  model_name: lllyasviel/sd-controlnet-normal\n- example user query: We are a robotics company and we need to create a map of the environment in 3D using the camera.\n  model_name: lllyasviel/sd-controlnet-normal\n- example user query: I need a method to generate high-quality normal maps from input images such as an image of a rock.\n  model_name: lllyasviel/sd-controlnet-normal\n\nUser prompt:\nPlease give the normal map image with the provided visual.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_normal>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: prs-eth/marigold-normals-v1-1\n  description: marigold normals v1-1 model card this is a model card for the `marigold-normals-v1-1` model for monocular normals estimation from a single image. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-10-21T19:16:05+00:00\n- model_name: Stable-X/yoso-normal-v1-8-1\n  description: this repository contains the weights of stablenormal: reducing diffusion variance for stable and sharp normal see the github repository: regarding installation instructions. the model can then be used as follows:\n created at: 2025-01-20T16:28:01+00:00\n- model_name: Stable-X/yoso-normal-v1-5\n  description: this repository contains the weights of stablenormal: reducing diffusion variance for stable and sharp normal see the github repository: regarding installation instructions. the model can then be used as follows:\n created at: 2024-11-07T22:30:16+00:00\n- model_name: jasperai/Flux.1-dev-Controlnet-Surface-Normals\n  description: this is controlnet for surface normals map developed by jasper research team. this model can be used directly with the `diffusers` library üí° note: you can compute the conditioning map using the `normalbaedetector` from the `controlnet aux` library\n created at: 2024-09-23T13:00:04+00:00\n- model_name: ilkerzgi/Glittering-Portrait-Kontext-Dev-Lora\n  description: this lora model transforms normal portraits into artistic, glittering portres with dramatic lighting and effects. perfect for creating stunning, artistic portrait variations. - -\n created at: 2025-07-17T18:17:40+00:00\n\nExamples:\n- example user query: We want to create a 3D model from a photo of our latest product. The idea is to estimate the normal map of the model based on the image.\n  model_name: lllyasviel/sd-controlnet-normal\n- example user query: A high-end art gallery is interested in transforming a normal image of a painting into a unique version with edge details to use on their marketing materials.\n  model_name: lllyasviel/sd-controlnet-hed\n- example user query: We are a game design company. We need to estimate the normal map from depth map.\n  model_name: lllyasviel/sd-controlnet-normal\n- example user query: I need an image of a car and estimate its 3D normal map using an AI model.\n  model_name: lllyasviel/sd-controlnet-normal\n- example user query: We are a robotics company and we need to create a map of the environment in 3D using the camera.\n  model_name: lllyasviel/sd-controlnet-normal\n- example user query: I need a method to generate high-quality normal maps from input images such as an image of a rock.\n  model_name: lllyasviel/sd-controlnet-normal\n\nUser prompt:\nWould you be able to generate a normal map file on this artwork.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_normal>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: prs-eth/marigold-normals-v1-1\n  description: marigold normals v1-1 model card this is a model card for the `marigold-normals-v1-1` model for monocular normals estimation from a single image. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-10-21T19:16:05+00:00\n- model_name: Stable-X/yoso-normal-v1-8-1\n  description: this repository contains the weights of stablenormal: reducing diffusion variance for stable and sharp normal see the github repository: regarding installation instructions. the model can then be used as follows:\n created at: 2025-01-20T16:28:01+00:00\n- model_name: Stable-X/yoso-normal-v1-5\n  description: this repository contains the weights of stablenormal: reducing diffusion variance for stable and sharp normal see the github repository: regarding installation instructions. the model can then be used as follows:\n created at: 2024-11-07T22:30:16+00:00\n- model_name: jasperai/Flux.1-dev-Controlnet-Surface-Normals\n  description: this is controlnet for surface normals map developed by jasper research team. this model can be used directly with the `diffusers` library üí° note: you can compute the conditioning map using the `normalbaedetector` from the `controlnet aux` library\n created at: 2024-09-23T13:00:04+00:00\n- model_name: ilkerzgi/Glittering-Portrait-Kontext-Dev-Lora\n  description: this lora model transforms normal portraits into artistic, glittering portres with dramatic lighting and effects. perfect for creating stunning, artistic portrait variations. - -\n created at: 2025-07-17T18:17:40+00:00\n\nExamples:\n- example user query: We want to create a 3D model from a photo of our latest product. The idea is to estimate the normal map of the model based on the image.\n  model_name: lllyasviel/sd-controlnet-normal\n- example user query: A high-end art gallery is interested in transforming a normal image of a painting into a unique version with edge details to use on their marketing materials.\n  model_name: lllyasviel/sd-controlnet-hed\n- example user query: We are a game design company. We need to estimate the normal map from depth map.\n  model_name: lllyasviel/sd-controlnet-normal\n- example user query: I need an image of a car and estimate its 3D normal map using an AI model.\n  model_name: lllyasviel/sd-controlnet-normal\n- example user query: We are a robotics company and we need to create a map of the environment in 3D using the camera.\n  model_name: lllyasviel/sd-controlnet-normal\n- example user query: I need a method to generate high-quality normal maps from input images such as an image of a rock.\n  model_name: lllyasviel/sd-controlnet-normal\n\nUser prompt:\nPlease give the normal map image on this artwork.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_normal>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: prs-eth/marigold-normals-v1-1\n  description: marigold normals v1-1 model card this is a model card for the `marigold-normals-v1-1` model for monocular normals estimation from a single image. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-10-21T19:16:05+00:00\n- model_name: Stable-X/yoso-normal-v1-8-1\n  description: this repository contains the weights of stablenormal: reducing diffusion variance for stable and sharp normal see the github repository: regarding installation instructions. the model can then be used as follows:\n created at: 2025-01-20T16:28:01+00:00\n- model_name: Stable-X/yoso-normal-v1-5\n  description: this repository contains the weights of stablenormal: reducing diffusion variance for stable and sharp normal see the github repository: regarding installation instructions. the model can then be used as follows:\n created at: 2024-11-07T22:30:16+00:00\n- model_name: jasperai/Flux.1-dev-Controlnet-Surface-Normals\n  description: this is controlnet for surface normals map developed by jasper research team. this model can be used directly with the `diffusers` library üí° note: you can compute the conditioning map using the `normalbaedetector` from the `controlnet aux` library\n created at: 2024-09-23T13:00:04+00:00\n- model_name: ilkerzgi/Glittering-Portrait-Kontext-Dev-Lora\n  description: this lora model transforms normal portraits into artistic, glittering portres with dramatic lighting and effects. perfect for creating stunning, artistic portrait variations. - -\n created at: 2025-07-17T18:17:40+00:00\n\nExamples:\n- example user query: We want to create a 3D model from a photo of our latest product. The idea is to estimate the normal map of the model based on the image.\n  model_name: lllyasviel/sd-controlnet-normal\n- example user query: A high-end art gallery is interested in transforming a normal image of a painting into a unique version with edge details to use on their marketing materials.\n  model_name: lllyasviel/sd-controlnet-hed\n- example user query: We are a game design company. We need to estimate the normal map from depth map.\n  model_name: lllyasviel/sd-controlnet-normal\n- example user query: I need an image of a car and estimate its 3D normal map using an AI model.\n  model_name: lllyasviel/sd-controlnet-normal\n- example user query: We are a robotics company and we need to create a map of the environment in 3D using the camera.\n  model_name: lllyasviel/sd-controlnet-normal\n- example user query: I need a method to generate high-quality normal maps from input images such as an image of a rock.\n  model_name: lllyasviel/sd-controlnet-normal\n\nUser prompt:\nDo you mind producing a normal outputting based on this snapshot.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_normal>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: prs-eth/marigold-normals-v1-1\n  description: marigold normals v1-1 model card this is a model card for the `marigold-normals-v1-1` model for monocular normals estimation from a single image. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-10-21T19:16:05+00:00\n- model_name: Stable-X/yoso-normal-v1-8-1\n  description: this repository contains the weights of stablenormal: reducing diffusion variance for stable and sharp normal see the github repository: regarding installation instructions. the model can then be used as follows:\n created at: 2025-01-20T16:28:01+00:00\n- model_name: Stable-X/yoso-normal-v1-5\n  description: this repository contains the weights of stablenormal: reducing diffusion variance for stable and sharp normal see the github repository: regarding installation instructions. the model can then be used as follows:\n created at: 2024-11-07T22:30:16+00:00\n- model_name: jasperai/Flux.1-dev-Controlnet-Surface-Normals\n  description: this is controlnet for surface normals map developed by jasper research team. this model can be used directly with the `diffusers` library üí° note: you can compute the conditioning map using the `normalbaedetector` from the `controlnet aux` library\n created at: 2024-09-23T13:00:04+00:00\n- model_name: ilkerzgi/Glittering-Portrait-Kontext-Dev-Lora\n  description: this lora model transforms normal portraits into artistic, glittering portres with dramatic lighting and effects. perfect for creating stunning, artistic portrait variations. - -\n created at: 2025-07-17T18:17:40+00:00\n\nExamples:\n- example user query: We want to create a 3D model from a photo of our latest product. The idea is to estimate the normal map of the model based on the image.\n  model_name: lllyasviel/sd-controlnet-normal\n- example user query: A high-end art gallery is interested in transforming a normal image of a painting into a unique version with edge details to use on their marketing materials.\n  model_name: lllyasviel/sd-controlnet-hed\n- example user query: We are a game design company. We need to estimate the normal map from depth map.\n  model_name: lllyasviel/sd-controlnet-normal\n- example user query: I need an image of a car and estimate its 3D normal map using an AI model.\n  model_name: lllyasviel/sd-controlnet-normal\n- example user query: We are a robotics company and we need to create a map of the environment in 3D using the camera.\n  model_name: lllyasviel/sd-controlnet-normal\n- example user query: I need a method to generate high-quality normal maps from input images such as an image of a rock.\n  model_name: lllyasviel/sd-controlnet-normal\n\nUser prompt:\nKindly produce a normal detail image from this detailed shot.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_normal>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: prs-eth/marigold-normals-v1-1\n  description: marigold normals v1-1 model card this is a model card for the `marigold-normals-v1-1` model for monocular normals estimation from a single image. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-10-21T19:16:05+00:00\n- model_name: Stable-X/yoso-normal-v1-8-1\n  description: this repository contains the weights of stablenormal: reducing diffusion variance for stable and sharp normal see the github repository: regarding installation instructions. the model can then be used as follows:\n created at: 2025-01-20T16:28:01+00:00\n- model_name: Stable-X/yoso-normal-v1-5\n  description: this repository contains the weights of stablenormal: reducing diffusion variance for stable and sharp normal see the github repository: regarding installation instructions. the model can then be used as follows:\n created at: 2024-11-07T22:30:16+00:00\n- model_name: jasperai/Flux.1-dev-Controlnet-Surface-Normals\n  description: this is controlnet for surface normals map developed by jasper research team. this model can be used directly with the `diffusers` library üí° note: you can compute the conditioning map using the `normalbaedetector` from the `controlnet aux` library\n created at: 2024-09-23T13:00:04+00:00\n- model_name: ilkerzgi/Glittering-Portrait-Kontext-Dev-Lora\n  description: this lora model transforms normal portraits into artistic, glittering portres with dramatic lighting and effects. perfect for creating stunning, artistic portrait variations. - -\n created at: 2025-07-17T18:17:40+00:00\n\nExamples:\n- example user query: We want to create a 3D model from a photo of our latest product. The idea is to estimate the normal map of the model based on the image.\n  model_name: lllyasviel/sd-controlnet-normal\n- example user query: A high-end art gallery is interested in transforming a normal image of a painting into a unique version with edge details to use on their marketing materials.\n  model_name: lllyasviel/sd-controlnet-hed\n- example user query: We are a game design company. We need to estimate the normal map from depth map.\n  model_name: lllyasviel/sd-controlnet-normal\n- example user query: I need an image of a car and estimate its 3D normal map using an AI model.\n  model_name: lllyasviel/sd-controlnet-normal\n- example user query: We are a robotics company and we need to create a map of the environment in 3D using the camera.\n  model_name: lllyasviel/sd-controlnet-normal\n- example user query: I need a method to generate high-quality normal maps from input images such as an image of a rock.\n  model_name: lllyasviel/sd-controlnet-normal\n\nUser prompt:\nI would appreciate producing a normal detail image based on this snapshot.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_normal>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: prs-eth/marigold-normals-v1-1\n  description: marigold normals v1-1 model card this is a model card for the `marigold-normals-v1-1` model for monocular normals estimation from a single image. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-10-21T19:16:05+00:00\n- model_name: Stable-X/yoso-normal-v1-8-1\n  description: this repository contains the weights of stablenormal: reducing diffusion variance for stable and sharp normal see the github repository: regarding installation instructions. the model can then be used as follows:\n created at: 2025-01-20T16:28:01+00:00\n- model_name: Stable-X/yoso-normal-v1-5\n  description: this repository contains the weights of stablenormal: reducing diffusion variance for stable and sharp normal see the github repository: regarding installation instructions. the model can then be used as follows:\n created at: 2024-11-07T22:30:16+00:00\n- model_name: jasperai/Flux.1-dev-Controlnet-Surface-Normals\n  description: this is controlnet for surface normals map developed by jasper research team. this model can be used directly with the `diffusers` library üí° note: you can compute the conditioning map using the `normalbaedetector` from the `controlnet aux` library\n created at: 2024-09-23T13:00:04+00:00\n- model_name: ilkerzgi/Glittering-Portrait-Kontext-Dev-Lora\n  description: this lora model transforms normal portraits into artistic, glittering portres with dramatic lighting and effects. perfect for creating stunning, artistic portrait variations. - -\n created at: 2025-07-17T18:17:40+00:00\n\nExamples:\n- example user query: We want to create a 3D model from a photo of our latest product. The idea is to estimate the normal map of the model based on the image.\n  model_name: lllyasviel/sd-controlnet-normal\n- example user query: A high-end art gallery is interested in transforming a normal image of a painting into a unique version with edge details to use on their marketing materials.\n  model_name: lllyasviel/sd-controlnet-hed\n- example user query: We are a game design company. We need to estimate the normal map from depth map.\n  model_name: lllyasviel/sd-controlnet-normal\n- example user query: I need an image of a car and estimate its 3D normal map using an AI model.\n  model_name: lllyasviel/sd-controlnet-normal\n- example user query: We are a robotics company and we need to create a map of the environment in 3D using the camera.\n  model_name: lllyasviel/sd-controlnet-normal\n- example user query: I need a method to generate high-quality normal maps from input images such as an image of a rock.\n  model_name: lllyasviel/sd-controlnet-normal\n\nUser prompt:\nI want you to generate a normal map file for this specific snapshot.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_normal>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: prs-eth/marigold-normals-v1-1\n  description: marigold normals v1-1 model card this is a model card for the `marigold-normals-v1-1` model for monocular normals estimation from a single image. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-10-21T19:16:05+00:00\n- model_name: Stable-X/yoso-normal-v1-8-1\n  description: this repository contains the weights of stablenormal: reducing diffusion variance for stable and sharp normal see the github repository: regarding installation instructions. the model can then be used as follows:\n created at: 2025-01-20T16:28:01+00:00\n- model_name: Stable-X/yoso-normal-v1-5\n  description: this repository contains the weights of stablenormal: reducing diffusion variance for stable and sharp normal see the github repository: regarding installation instructions. the model can then be used as follows:\n created at: 2024-11-07T22:30:16+00:00\n- model_name: jasperai/Flux.1-dev-Controlnet-Surface-Normals\n  description: this is controlnet for surface normals map developed by jasper research team. this model can be used directly with the `diffusers` library üí° note: you can compute the conditioning map using the `normalbaedetector` from the `controlnet aux` library\n created at: 2024-09-23T13:00:04+00:00\n- model_name: ilkerzgi/Glittering-Portrait-Kontext-Dev-Lora\n  description: this lora model transforms normal portraits into artistic, glittering portres with dramatic lighting and effects. perfect for creating stunning, artistic portrait variations. - -\n created at: 2025-07-17T18:17:40+00:00\n\nExamples:\n- example user query: We want to create a 3D model from a photo of our latest product. The idea is to estimate the normal map of the model based on the image.\n  model_name: lllyasviel/sd-controlnet-normal\n- example user query: A high-end art gallery is interested in transforming a normal image of a painting into a unique version with edge details to use on their marketing materials.\n  model_name: lllyasviel/sd-controlnet-hed\n- example user query: We are a game design company. We need to estimate the normal map from depth map.\n  model_name: lllyasviel/sd-controlnet-normal\n- example user query: I need an image of a car and estimate its 3D normal map using an AI model.\n  model_name: lllyasviel/sd-controlnet-normal\n- example user query: We are a robotics company and we need to create a map of the environment in 3D using the camera.\n  model_name: lllyasviel/sd-controlnet-normal\n- example user query: I need a method to generate high-quality normal maps from input images such as an image of a rock.\n  model_name: lllyasviel/sd-controlnet-normal\n\nUser prompt:\nWould you mind rendering a normal map from the supplied visual.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_normal>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: prs-eth/marigold-normals-v1-1\n  description: marigold normals v1-1 model card this is a model card for the `marigold-normals-v1-1` model for monocular normals estimation from a single image. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-10-21T19:16:05+00:00\n- model_name: Stable-X/yoso-normal-v1-8-1\n  description: this repository contains the weights of stablenormal: reducing diffusion variance for stable and sharp normal see the github repository: regarding installation instructions. the model can then be used as follows:\n created at: 2025-01-20T16:28:01+00:00\n- model_name: Stable-X/yoso-normal-v1-5\n  description: this repository contains the weights of stablenormal: reducing diffusion variance for stable and sharp normal see the github repository: regarding installation instructions. the model can then be used as follows:\n created at: 2024-11-07T22:30:16+00:00\n- model_name: jasperai/Flux.1-dev-Controlnet-Surface-Normals\n  description: this is controlnet for surface normals map developed by jasper research team. this model can be used directly with the `diffusers` library üí° note: you can compute the conditioning map using the `normalbaedetector` from the `controlnet aux` library\n created at: 2024-09-23T13:00:04+00:00\n- model_name: ilkerzgi/Glittering-Portrait-Kontext-Dev-Lora\n  description: this lora model transforms normal portraits into artistic, glittering portres with dramatic lighting and effects. perfect for creating stunning, artistic portrait variations. - -\n created at: 2025-07-17T18:17:40+00:00\n\nExamples:\n- example user query: We want to create a 3D model from a photo of our latest product. The idea is to estimate the normal map of the model based on the image.\n  model_name: lllyasviel/sd-controlnet-normal\n- example user query: A high-end art gallery is interested in transforming a normal image of a painting into a unique version with edge details to use on their marketing materials.\n  model_name: lllyasviel/sd-controlnet-hed\n- example user query: We are a game design company. We need to estimate the normal map from depth map.\n  model_name: lllyasviel/sd-controlnet-normal\n- example user query: I need an image of a car and estimate its 3D normal map using an AI model.\n  model_name: lllyasviel/sd-controlnet-normal\n- example user query: We are a robotics company and we need to create a map of the environment in 3D using the camera.\n  model_name: lllyasviel/sd-controlnet-normal\n- example user query: I need a method to generate high-quality normal maps from input images such as an image of a rock.\n  model_name: lllyasviel/sd-controlnet-normal\n\nUser prompt:\nLet‚Äôs generate a normal map for object tracking and analysis.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_normal>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: prs-eth/marigold-normals-v1-1\n  description: marigold normals v1-1 model card this is a model card for the `marigold-normals-v1-1` model for monocular normals estimation from a single image. the model is fine-tuned from the `stable-diffusion-2` as\n created at: 2024-10-21T19:16:05+00:00\n- model_name: Stable-X/yoso-normal-v1-8-1\n  description: this repository contains the weights of stablenormal: reducing diffusion variance for stable and sharp normal see the github repository: regarding installation instructions. the model can then be used as follows:\n created at: 2025-01-20T16:28:01+00:00\n- model_name: Stable-X/yoso-normal-v1-5\n  description: this repository contains the weights of stablenormal: reducing diffusion variance for stable and sharp normal see the github repository: regarding installation instructions. the model can then be used as follows:\n created at: 2024-11-07T22:30:16+00:00\n- model_name: jasperai/Flux.1-dev-Controlnet-Surface-Normals\n  description: this is controlnet for surface normals map developed by jasper research team. this model can be used directly with the `diffusers` library üí° note: you can compute the conditioning map using the `normalbaedetector` from the `controlnet aux` library\n created at: 2024-09-23T13:00:04+00:00\n- model_name: ilkerzgi/Glittering-Portrait-Kontext-Dev-Lora\n  description: this lora model transforms normal portraits into artistic, glittering portres with dramatic lighting and effects. perfect for creating stunning, artistic portrait variations. - -\n created at: 2025-07-17T18:17:40+00:00\n\nExamples:\n- example user query: We want to create a 3D model from a photo of our latest product. The idea is to estimate the normal map of the model based on the image.\n  model_name: lllyasviel/sd-controlnet-normal\n- example user query: A high-end art gallery is interested in transforming a normal image of a painting into a unique version with edge details to use on their marketing materials.\n  model_name: lllyasviel/sd-controlnet-hed\n- example user query: We are a game design company. We need to estimate the normal map from depth map.\n  model_name: lllyasviel/sd-controlnet-normal\n- example user query: I need an image of a car and estimate its 3D normal map using an AI model.\n  model_name: lllyasviel/sd-controlnet-normal\n- example user query: We are a robotics company and we need to create a map of the environment in 3D using the camera.\n  model_name: lllyasviel/sd-controlnet-normal\n- example user query: I need a method to generate high-quality normal maps from input images such as an image of a rock.\n  model_name: lllyasviel/sd-controlnet-normal\n\nUser prompt:\nI want you to create a normal image to extract the important edges.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n"
  ],
  "<image_pose>": [
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_pose>\n\nCandidate Models:\n- model_name: google/movenet/singlepose/lightning\n  description: MoveNet is a family of pose detection models designed for efficient and accurate detection of human poses in images.\n created at: 2021-04-01\n- model_name: ETH-CVG/lightglue_superpoint\n  description: the lightglue model was proposed in by philipp lindenberger, paul-edouard sarlin and marc pollefeys. this model consists of matching two sets of interest points detected in an image. paired with the lightglue is a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. building on the success of superglue, this model has the ability to introspect the confidence of its own predictions. it adapts the amount of computation to the difficulty of each image pair to match. both its depth and width are adaptive : 1. the inference can stop at an early layer if all predictions are ready 2. points that are deemed not matchable are discarded early from further steps. the resulting model, lightglue, is finally faster, more accurate, and easier to train than the long-unrivaled superglue. - developed by: eth zurich - computer vision and geometry lab - model type: image matching - license: academic or non-profit organization noncommercial research use only implied by the use of superpoint as its keypoint detector\n created at: 2025-02-20T15:52:56+00:00\n- model_name: stanfordmimi/synthpose-vitpose-base-hf\n  description: the synthpose model was proposed in by yoni gozlan, antoine falisse, scott uhlrich, anthony gatti, michael black, akshay chaudhari. this model was contributed by this model uses a vitpose base backbone.\n created at: 2025-01-08T17:35:23+00:00\n- model_name: stanfordmimi/synthpose-vitpose-huge-hf\n  description: the synthpose model was proposed in by yoni gozlan, antoine falisse, scott uhlrich, anthony gatti, michael black, akshay chaudhari. this model was contributed by this model uses a vitpose huge backbone.\n created at: 2025-01-10T17:53:40+00:00\n- model_name: ETH-CVG/lightglue_disk\n  description: this model has been pushed to the hub using the integration: this is a lightglue variant trained on disk, with a commecially permissive license, which requires `kornia` to be installed and is usable with transformers with the following lines of code also, the commit allowing disk to work with lightglue is not yet included in a version of transformers, please install transformers from the main branch  lightglue is a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. building on the success of superglue, this model has the ability to introspect the confidence of its own predictions. it adapts the amount of computation to the difficulty of each image pair to match. both its depth and width are adaptive : 1. the inference can stop at an early layer if all predictions are ready 2. points that are deemed not matchable are discarded early from further steps. the resulting model, lightglue, is finally faster, more accurate, and easier to train than the long-unrivaled superglue. - developed by: eth zurich - computer vision and geometry lab - model type: image matching - license: apache 2.0\n created at: 2025-06-24T19:48:47+00:00\n- model_name: qualcomm/Facial-Landmark-Detection\n  description: detects facial landmarks eg, nose, mouth, etc.. this model's architecture was developed by qualcomm. the model was trained by qualcomm on a proprietary dataset of faces, but can be used on any image. this repository provides scripts to run facial-landmark-detection on qualcomm¬Æ devices. more details on model performance across various devices, can be found\n created at: 2024-10-21T23:02:15+00:00\n- model_name: facebook/sapiens-pose-1b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T19:52:56+00:00\n- model_name: facebook/sapiens-pose-0.3b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.3b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-13T20:07:29+00:00\n- model_name: facebook/sapiens-pose-1b\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-10T02:03:29+00:00\n- model_name: qualcomm/Movenet\n  description: movenet performs pose estimation on human images. this model is an implementation of movenet found . this repository provides scripts to run movenet on qualcomm¬Æ devices.\n created at: 2025-02-28T19:13:04+00:00\n- model_name: facebook/sapiens-pose-0.6b\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.6b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-18T18:44:23+00:00\n- model_name: facebook/sapiens-pose-0.3b\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.3b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-13T20:06:09+00:00\n- model_name: qualcomm/RTMPose-Body2d\n  description: rtmpose is a machine learning model that detects human pose and returns a location and confidence for each of 133 joints. this model is an implementation of rtmpose-body2d found . this repository provides scripts to run rtmpose-body2d on qualcomm¬Æ devices.\n created at: 2025-06-23T22:07:55+00:00\n- model_name: facebook/sapiens-pose-0.3b-bfloat16\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.3b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-13T20:09:25+00:00\n- model_name: facebook/sapiens-pose-0.6b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.6b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-18T18:42:35+00:00\n- model_name: facebook/sapiens-pose-1b-bfloat16\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-10T17:13:39+00:00\n- model_name: onnx-community/vitpose-base-simple\n  description: with onnx weights to be compatible with transformers.js. if you haven't already, you can install the javascript library from using: example: pose estimation w/ `onnx-community/vitpose-base-simple`.\n created at: 2024-11-18T22:37:19+00:00\n- model_name: facebook/sapiens-pose-0.6b-bfloat16\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.6b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-18T18:45:55+00:00\n- model_name: STMicroelectronics/hand_landmarks\n  description: hand landmarks is a single pose estimation model targeted for real-time processing implemented in tensorflow. the model is quantized in int8 format using tensorflow lite converter. network information value  hand landmarks is a single pose estimation model targeted for real-time processing implemented in tensorflow. the model is quantized in int8 format using tensorflow lite converter.\n created at: 2024-12-04T15:37:01+00:00\n- model_name: STMicroelectronics/movenet\n  description: movenet is a single pose estimation model targeted for real-time processing implemented in tensorflow. the model is quantized in int8 format using tensorflow lite converter. network information value  movenet is a single pose estimation model targeted for real-time processing implemented in tensorflow. the model is quantized in int8 format using tensorflow lite converter.\n created at: 2024-12-04T15:44:08+00:00\n- model_name: STMicroelectronics/yolov8n_pose\n  description: yolov8n pose is a lightweight and efficient model designed for multi pose estimation tasks. it is part of the yolo you only look once family of models, known for their real-time object detection capabilities. the \"n\" in yolov8n pose indicates that it is a nano version, optimized for speed and resource efficiency, making it suitable for deployment on devices with limited computational power, such as mobile devices and embedded systems. yolov8n pose is implemented in pytorch by ultralytics and is quantized in int8 format using tensorflow lite converter. network information value  yolov8n pose is a lightweight and efficient model designed for multi pose estimation tasks. it is part of the yolo you only look once family of models, known for their real-time object detection capabilities. the \"n\" in yolov8n pose indicates that it is a nano version, optimized for speed and resource efficiency, making it suitable for deployment on devices with limited computational power, such as mobile devices and embedded systems. yolov8n pose is implemented in pytorch by ultralytics and is quantized in int8 format using tensorflow lite converter.\n created at: 2024-12-04T15:51:46+00:00\n\nExamples:\n- example user query: Figure out where people are standing in a photograph by detecting their body poses.\n  model_name: google/movenet/singlepose/lightning\n- example user query: My yoga instructor wants me to send a picture of my new yoga pose, Can this model help us analyze my posture correctly?\n  model_name: google/movenet/singlepose/lightning\n- example user query: Our fitness app needs to recognize poses from pictures to analyze the user's workout technique.\n  model_name: google/movenet/singlepose/lightning\n- example user query: Teach us how we can estimate human poses within a picture of athletes playing basketball.\n  model_name: google/movenet/singlepose/lightning\n- example user query: We need to detect the pose of a person while performing exercise.\n  model_name: google/movenet/singlepose/lightning\n- example user query: Track the posture of each exercise to give advice to gym users.\n  model_name: google/movenet/singlepose/lightning\n\nUser prompt:\nAssist me in providing a pose estimation based on the given file.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_pose>\n\nCandidate Models:\n- model_name: google/movenet/singlepose/lightning\n  description: MoveNet is a family of pose detection models designed for efficient and accurate detection of human poses in images.\n created at: 2021-04-01\n- model_name: ETH-CVG/lightglue_superpoint\n  description: the lightglue model was proposed in by philipp lindenberger, paul-edouard sarlin and marc pollefeys. this model consists of matching two sets of interest points detected in an image. paired with the lightglue is a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. building on the success of superglue, this model has the ability to introspect the confidence of its own predictions. it adapts the amount of computation to the difficulty of each image pair to match. both its depth and width are adaptive : 1. the inference can stop at an early layer if all predictions are ready 2. points that are deemed not matchable are discarded early from further steps. the resulting model, lightglue, is finally faster, more accurate, and easier to train than the long-unrivaled superglue. - developed by: eth zurich - computer vision and geometry lab - model type: image matching - license: academic or non-profit organization noncommercial research use only implied by the use of superpoint as its keypoint detector\n created at: 2025-02-20T15:52:56+00:00\n- model_name: stanfordmimi/synthpose-vitpose-base-hf\n  description: the synthpose model was proposed in by yoni gozlan, antoine falisse, scott uhlrich, anthony gatti, michael black, akshay chaudhari. this model was contributed by this model uses a vitpose base backbone.\n created at: 2025-01-08T17:35:23+00:00\n- model_name: stanfordmimi/synthpose-vitpose-huge-hf\n  description: the synthpose model was proposed in by yoni gozlan, antoine falisse, scott uhlrich, anthony gatti, michael black, akshay chaudhari. this model was contributed by this model uses a vitpose huge backbone.\n created at: 2025-01-10T17:53:40+00:00\n- model_name: ETH-CVG/lightglue_disk\n  description: this model has been pushed to the hub using the integration: this is a lightglue variant trained on disk, with a commecially permissive license, which requires `kornia` to be installed and is usable with transformers with the following lines of code also, the commit allowing disk to work with lightglue is not yet included in a version of transformers, please install transformers from the main branch  lightglue is a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. building on the success of superglue, this model has the ability to introspect the confidence of its own predictions. it adapts the amount of computation to the difficulty of each image pair to match. both its depth and width are adaptive : 1. the inference can stop at an early layer if all predictions are ready 2. points that are deemed not matchable are discarded early from further steps. the resulting model, lightglue, is finally faster, more accurate, and easier to train than the long-unrivaled superglue. - developed by: eth zurich - computer vision and geometry lab - model type: image matching - license: apache 2.0\n created at: 2025-06-24T19:48:47+00:00\n- model_name: qualcomm/Facial-Landmark-Detection\n  description: detects facial landmarks eg, nose, mouth, etc.. this model's architecture was developed by qualcomm. the model was trained by qualcomm on a proprietary dataset of faces, but can be used on any image. this repository provides scripts to run facial-landmark-detection on qualcomm¬Æ devices. more details on model performance across various devices, can be found\n created at: 2024-10-21T23:02:15+00:00\n- model_name: facebook/sapiens-pose-1b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T19:52:56+00:00\n- model_name: facebook/sapiens-pose-0.3b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.3b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-13T20:07:29+00:00\n- model_name: facebook/sapiens-pose-1b\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-10T02:03:29+00:00\n- model_name: qualcomm/Movenet\n  description: movenet performs pose estimation on human images. this model is an implementation of movenet found . this repository provides scripts to run movenet on qualcomm¬Æ devices.\n created at: 2025-02-28T19:13:04+00:00\n- model_name: facebook/sapiens-pose-0.6b\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.6b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-18T18:44:23+00:00\n- model_name: facebook/sapiens-pose-0.3b\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.3b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-13T20:06:09+00:00\n- model_name: qualcomm/RTMPose-Body2d\n  description: rtmpose is a machine learning model that detects human pose and returns a location and confidence for each of 133 joints. this model is an implementation of rtmpose-body2d found . this repository provides scripts to run rtmpose-body2d on qualcomm¬Æ devices.\n created at: 2025-06-23T22:07:55+00:00\n- model_name: facebook/sapiens-pose-0.3b-bfloat16\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.3b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-13T20:09:25+00:00\n- model_name: facebook/sapiens-pose-0.6b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.6b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-18T18:42:35+00:00\n- model_name: facebook/sapiens-pose-1b-bfloat16\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-10T17:13:39+00:00\n- model_name: onnx-community/vitpose-base-simple\n  description: with onnx weights to be compatible with transformers.js. if you haven't already, you can install the javascript library from using: example: pose estimation w/ `onnx-community/vitpose-base-simple`.\n created at: 2024-11-18T22:37:19+00:00\n- model_name: facebook/sapiens-pose-0.6b-bfloat16\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.6b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-18T18:45:55+00:00\n- model_name: STMicroelectronics/hand_landmarks\n  description: hand landmarks is a single pose estimation model targeted for real-time processing implemented in tensorflow. the model is quantized in int8 format using tensorflow lite converter. network information value  hand landmarks is a single pose estimation model targeted for real-time processing implemented in tensorflow. the model is quantized in int8 format using tensorflow lite converter.\n created at: 2024-12-04T15:37:01+00:00\n- model_name: STMicroelectronics/movenet\n  description: movenet is a single pose estimation model targeted for real-time processing implemented in tensorflow. the model is quantized in int8 format using tensorflow lite converter. network information value  movenet is a single pose estimation model targeted for real-time processing implemented in tensorflow. the model is quantized in int8 format using tensorflow lite converter.\n created at: 2024-12-04T15:44:08+00:00\n- model_name: STMicroelectronics/yolov8n_pose\n  description: yolov8n pose is a lightweight and efficient model designed for multi pose estimation tasks. it is part of the yolo you only look once family of models, known for their real-time object detection capabilities. the \"n\" in yolov8n pose indicates that it is a nano version, optimized for speed and resource efficiency, making it suitable for deployment on devices with limited computational power, such as mobile devices and embedded systems. yolov8n pose is implemented in pytorch by ultralytics and is quantized in int8 format using tensorflow lite converter. network information value  yolov8n pose is a lightweight and efficient model designed for multi pose estimation tasks. it is part of the yolo you only look once family of models, known for their real-time object detection capabilities. the \"n\" in yolov8n pose indicates that it is a nano version, optimized for speed and resource efficiency, making it suitable for deployment on devices with limited computational power, such as mobile devices and embedded systems. yolov8n pose is implemented in pytorch by ultralytics and is quantized in int8 format using tensorflow lite converter.\n created at: 2024-12-04T15:51:46+00:00\n\nExamples:\n- example user query: Figure out where people are standing in a photograph by detecting their body poses.\n  model_name: google/movenet/singlepose/lightning\n- example user query: My yoga instructor wants me to send a picture of my new yoga pose, Can this model help us analyze my posture correctly?\n  model_name: google/movenet/singlepose/lightning\n- example user query: Our fitness app needs to recognize poses from pictures to analyze the user's workout technique.\n  model_name: google/movenet/singlepose/lightning\n- example user query: Teach us how we can estimate human poses within a picture of athletes playing basketball.\n  model_name: google/movenet/singlepose/lightning\n- example user query: We need to detect the pose of a person while performing exercise.\n  model_name: google/movenet/singlepose/lightning\n- example user query: Track the posture of each exercise to give advice to gym users.\n  model_name: google/movenet/singlepose/lightning\n\nUser prompt:\nCould you please produce a pose estimate for the given image.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_pose>\n\nCandidate Models:\n- model_name: google/movenet/singlepose/lightning\n  description: MoveNet is a family of pose detection models designed for efficient and accurate detection of human poses in images.\n created at: 2021-04-01\n- model_name: ETH-CVG/lightglue_superpoint\n  description: the lightglue model was proposed in by philipp lindenberger, paul-edouard sarlin and marc pollefeys. this model consists of matching two sets of interest points detected in an image. paired with the lightglue is a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. building on the success of superglue, this model has the ability to introspect the confidence of its own predictions. it adapts the amount of computation to the difficulty of each image pair to match. both its depth and width are adaptive : 1. the inference can stop at an early layer if all predictions are ready 2. points that are deemed not matchable are discarded early from further steps. the resulting model, lightglue, is finally faster, more accurate, and easier to train than the long-unrivaled superglue. - developed by: eth zurich - computer vision and geometry lab - model type: image matching - license: academic or non-profit organization noncommercial research use only implied by the use of superpoint as its keypoint detector\n created at: 2025-02-20T15:52:56+00:00\n- model_name: stanfordmimi/synthpose-vitpose-base-hf\n  description: the synthpose model was proposed in by yoni gozlan, antoine falisse, scott uhlrich, anthony gatti, michael black, akshay chaudhari. this model was contributed by this model uses a vitpose base backbone.\n created at: 2025-01-08T17:35:23+00:00\n- model_name: stanfordmimi/synthpose-vitpose-huge-hf\n  description: the synthpose model was proposed in by yoni gozlan, antoine falisse, scott uhlrich, anthony gatti, michael black, akshay chaudhari. this model was contributed by this model uses a vitpose huge backbone.\n created at: 2025-01-10T17:53:40+00:00\n- model_name: ETH-CVG/lightglue_disk\n  description: this model has been pushed to the hub using the integration: this is a lightglue variant trained on disk, with a commecially permissive license, which requires `kornia` to be installed and is usable with transformers with the following lines of code also, the commit allowing disk to work with lightglue is not yet included in a version of transformers, please install transformers from the main branch  lightglue is a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. building on the success of superglue, this model has the ability to introspect the confidence of its own predictions. it adapts the amount of computation to the difficulty of each image pair to match. both its depth and width are adaptive : 1. the inference can stop at an early layer if all predictions are ready 2. points that are deemed not matchable are discarded early from further steps. the resulting model, lightglue, is finally faster, more accurate, and easier to train than the long-unrivaled superglue. - developed by: eth zurich - computer vision and geometry lab - model type: image matching - license: apache 2.0\n created at: 2025-06-24T19:48:47+00:00\n- model_name: qualcomm/Facial-Landmark-Detection\n  description: detects facial landmarks eg, nose, mouth, etc.. this model's architecture was developed by qualcomm. the model was trained by qualcomm on a proprietary dataset of faces, but can be used on any image. this repository provides scripts to run facial-landmark-detection on qualcomm¬Æ devices. more details on model performance across various devices, can be found\n created at: 2024-10-21T23:02:15+00:00\n- model_name: facebook/sapiens-pose-1b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T19:52:56+00:00\n- model_name: facebook/sapiens-pose-0.3b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.3b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-13T20:07:29+00:00\n- model_name: facebook/sapiens-pose-1b\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-10T02:03:29+00:00\n- model_name: qualcomm/Movenet\n  description: movenet performs pose estimation on human images. this model is an implementation of movenet found . this repository provides scripts to run movenet on qualcomm¬Æ devices.\n created at: 2025-02-28T19:13:04+00:00\n- model_name: facebook/sapiens-pose-0.6b\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.6b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-18T18:44:23+00:00\n- model_name: facebook/sapiens-pose-0.3b\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.3b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-13T20:06:09+00:00\n- model_name: qualcomm/RTMPose-Body2d\n  description: rtmpose is a machine learning model that detects human pose and returns a location and confidence for each of 133 joints. this model is an implementation of rtmpose-body2d found . this repository provides scripts to run rtmpose-body2d on qualcomm¬Æ devices.\n created at: 2025-06-23T22:07:55+00:00\n- model_name: facebook/sapiens-pose-0.3b-bfloat16\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.3b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-13T20:09:25+00:00\n- model_name: facebook/sapiens-pose-0.6b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.6b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-18T18:42:35+00:00\n- model_name: facebook/sapiens-pose-1b-bfloat16\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-10T17:13:39+00:00\n- model_name: onnx-community/vitpose-base-simple\n  description: with onnx weights to be compatible with transformers.js. if you haven't already, you can install the javascript library from using: example: pose estimation w/ `onnx-community/vitpose-base-simple`.\n created at: 2024-11-18T22:37:19+00:00\n- model_name: facebook/sapiens-pose-0.6b-bfloat16\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.6b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-18T18:45:55+00:00\n- model_name: STMicroelectronics/hand_landmarks\n  description: hand landmarks is a single pose estimation model targeted for real-time processing implemented in tensorflow. the model is quantized in int8 format using tensorflow lite converter. network information value  hand landmarks is a single pose estimation model targeted for real-time processing implemented in tensorflow. the model is quantized in int8 format using tensorflow lite converter.\n created at: 2024-12-04T15:37:01+00:00\n- model_name: STMicroelectronics/movenet\n  description: movenet is a single pose estimation model targeted for real-time processing implemented in tensorflow. the model is quantized in int8 format using tensorflow lite converter. network information value  movenet is a single pose estimation model targeted for real-time processing implemented in tensorflow. the model is quantized in int8 format using tensorflow lite converter.\n created at: 2024-12-04T15:44:08+00:00\n- model_name: STMicroelectronics/yolov8n_pose\n  description: yolov8n pose is a lightweight and efficient model designed for multi pose estimation tasks. it is part of the yolo you only look once family of models, known for their real-time object detection capabilities. the \"n\" in yolov8n pose indicates that it is a nano version, optimized for speed and resource efficiency, making it suitable for deployment on devices with limited computational power, such as mobile devices and embedded systems. yolov8n pose is implemented in pytorch by ultralytics and is quantized in int8 format using tensorflow lite converter. network information value  yolov8n pose is a lightweight and efficient model designed for multi pose estimation tasks. it is part of the yolo you only look once family of models, known for their real-time object detection capabilities. the \"n\" in yolov8n pose indicates that it is a nano version, optimized for speed and resource efficiency, making it suitable for deployment on devices with limited computational power, such as mobile devices and embedded systems. yolov8n pose is implemented in pytorch by ultralytics and is quantized in int8 format using tensorflow lite converter.\n created at: 2024-12-04T15:51:46+00:00\n\nExamples:\n- example user query: Figure out where people are standing in a photograph by detecting their body poses.\n  model_name: google/movenet/singlepose/lightning\n- example user query: My yoga instructor wants me to send a picture of my new yoga pose, Can this model help us analyze my posture correctly?\n  model_name: google/movenet/singlepose/lightning\n- example user query: Our fitness app needs to recognize poses from pictures to analyze the user's workout technique.\n  model_name: google/movenet/singlepose/lightning\n- example user query: Teach us how we can estimate human poses within a picture of athletes playing basketball.\n  model_name: google/movenet/singlepose/lightning\n- example user query: We need to detect the pose of a person while performing exercise.\n  model_name: google/movenet/singlepose/lightning\n- example user query: Track the posture of each exercise to give advice to gym users.\n  model_name: google/movenet/singlepose/lightning\n\nUser prompt:\nCould you kindly generate a pose estimation image from this image file.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_pose>\n\nCandidate Models:\n- model_name: google/movenet/singlepose/lightning\n  description: MoveNet is a family of pose detection models designed for efficient and accurate detection of human poses in images.\n created at: 2021-04-01\n- model_name: ETH-CVG/lightglue_superpoint\n  description: the lightglue model was proposed in by philipp lindenberger, paul-edouard sarlin and marc pollefeys. this model consists of matching two sets of interest points detected in an image. paired with the lightglue is a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. building on the success of superglue, this model has the ability to introspect the confidence of its own predictions. it adapts the amount of computation to the difficulty of each image pair to match. both its depth and width are adaptive : 1. the inference can stop at an early layer if all predictions are ready 2. points that are deemed not matchable are discarded early from further steps. the resulting model, lightglue, is finally faster, more accurate, and easier to train than the long-unrivaled superglue. - developed by: eth zurich - computer vision and geometry lab - model type: image matching - license: academic or non-profit organization noncommercial research use only implied by the use of superpoint as its keypoint detector\n created at: 2025-02-20T15:52:56+00:00\n- model_name: stanfordmimi/synthpose-vitpose-base-hf\n  description: the synthpose model was proposed in by yoni gozlan, antoine falisse, scott uhlrich, anthony gatti, michael black, akshay chaudhari. this model was contributed by this model uses a vitpose base backbone.\n created at: 2025-01-08T17:35:23+00:00\n- model_name: stanfordmimi/synthpose-vitpose-huge-hf\n  description: the synthpose model was proposed in by yoni gozlan, antoine falisse, scott uhlrich, anthony gatti, michael black, akshay chaudhari. this model was contributed by this model uses a vitpose huge backbone.\n created at: 2025-01-10T17:53:40+00:00\n- model_name: ETH-CVG/lightglue_disk\n  description: this model has been pushed to the hub using the integration: this is a lightglue variant trained on disk, with a commecially permissive license, which requires `kornia` to be installed and is usable with transformers with the following lines of code also, the commit allowing disk to work with lightglue is not yet included in a version of transformers, please install transformers from the main branch  lightglue is a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. building on the success of superglue, this model has the ability to introspect the confidence of its own predictions. it adapts the amount of computation to the difficulty of each image pair to match. both its depth and width are adaptive : 1. the inference can stop at an early layer if all predictions are ready 2. points that are deemed not matchable are discarded early from further steps. the resulting model, lightglue, is finally faster, more accurate, and easier to train than the long-unrivaled superglue. - developed by: eth zurich - computer vision and geometry lab - model type: image matching - license: apache 2.0\n created at: 2025-06-24T19:48:47+00:00\n- model_name: qualcomm/Facial-Landmark-Detection\n  description: detects facial landmarks eg, nose, mouth, etc.. this model's architecture was developed by qualcomm. the model was trained by qualcomm on a proprietary dataset of faces, but can be used on any image. this repository provides scripts to run facial-landmark-detection on qualcomm¬Æ devices. more details on model performance across various devices, can be found\n created at: 2024-10-21T23:02:15+00:00\n- model_name: facebook/sapiens-pose-1b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T19:52:56+00:00\n- model_name: facebook/sapiens-pose-0.3b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.3b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-13T20:07:29+00:00\n- model_name: facebook/sapiens-pose-1b\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-10T02:03:29+00:00\n- model_name: qualcomm/Movenet\n  description: movenet performs pose estimation on human images. this model is an implementation of movenet found . this repository provides scripts to run movenet on qualcomm¬Æ devices.\n created at: 2025-02-28T19:13:04+00:00\n- model_name: facebook/sapiens-pose-0.6b\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.6b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-18T18:44:23+00:00\n- model_name: facebook/sapiens-pose-0.3b\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.3b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-13T20:06:09+00:00\n- model_name: qualcomm/RTMPose-Body2d\n  description: rtmpose is a machine learning model that detects human pose and returns a location and confidence for each of 133 joints. this model is an implementation of rtmpose-body2d found . this repository provides scripts to run rtmpose-body2d on qualcomm¬Æ devices.\n created at: 2025-06-23T22:07:55+00:00\n- model_name: facebook/sapiens-pose-0.3b-bfloat16\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.3b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-13T20:09:25+00:00\n- model_name: facebook/sapiens-pose-0.6b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.6b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-18T18:42:35+00:00\n- model_name: facebook/sapiens-pose-1b-bfloat16\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-10T17:13:39+00:00\n- model_name: onnx-community/vitpose-base-simple\n  description: with onnx weights to be compatible with transformers.js. if you haven't already, you can install the javascript library from using: example: pose estimation w/ `onnx-community/vitpose-base-simple`.\n created at: 2024-11-18T22:37:19+00:00\n- model_name: facebook/sapiens-pose-0.6b-bfloat16\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.6b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-18T18:45:55+00:00\n- model_name: STMicroelectronics/hand_landmarks\n  description: hand landmarks is a single pose estimation model targeted for real-time processing implemented in tensorflow. the model is quantized in int8 format using tensorflow lite converter. network information value  hand landmarks is a single pose estimation model targeted for real-time processing implemented in tensorflow. the model is quantized in int8 format using tensorflow lite converter.\n created at: 2024-12-04T15:37:01+00:00\n- model_name: STMicroelectronics/movenet\n  description: movenet is a single pose estimation model targeted for real-time processing implemented in tensorflow. the model is quantized in int8 format using tensorflow lite converter. network information value  movenet is a single pose estimation model targeted for real-time processing implemented in tensorflow. the model is quantized in int8 format using tensorflow lite converter.\n created at: 2024-12-04T15:44:08+00:00\n- model_name: STMicroelectronics/yolov8n_pose\n  description: yolov8n pose is a lightweight and efficient model designed for multi pose estimation tasks. it is part of the yolo you only look once family of models, known for their real-time object detection capabilities. the \"n\" in yolov8n pose indicates that it is a nano version, optimized for speed and resource efficiency, making it suitable for deployment on devices with limited computational power, such as mobile devices and embedded systems. yolov8n pose is implemented in pytorch by ultralytics and is quantized in int8 format using tensorflow lite converter. network information value  yolov8n pose is a lightweight and efficient model designed for multi pose estimation tasks. it is part of the yolo you only look once family of models, known for their real-time object detection capabilities. the \"n\" in yolov8n pose indicates that it is a nano version, optimized for speed and resource efficiency, making it suitable for deployment on devices with limited computational power, such as mobile devices and embedded systems. yolov8n pose is implemented in pytorch by ultralytics and is quantized in int8 format using tensorflow lite converter.\n created at: 2024-12-04T15:51:46+00:00\n\nExamples:\n- example user query: Figure out where people are standing in a photograph by detecting their body poses.\n  model_name: google/movenet/singlepose/lightning\n- example user query: My yoga instructor wants me to send a picture of my new yoga pose, Can this model help us analyze my posture correctly?\n  model_name: google/movenet/singlepose/lightning\n- example user query: Our fitness app needs to recognize poses from pictures to analyze the user's workout technique.\n  model_name: google/movenet/singlepose/lightning\n- example user query: Teach us how we can estimate human poses within a picture of athletes playing basketball.\n  model_name: google/movenet/singlepose/lightning\n- example user query: We need to detect the pose of a person while performing exercise.\n  model_name: google/movenet/singlepose/lightning\n- example user query: Track the posture of each exercise to give advice to gym users.\n  model_name: google/movenet/singlepose/lightning\n\nUser prompt:\nPlease ensure to render a pose image to enhance the visual quality.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_pose>\n\nCandidate Models:\n- model_name: google/movenet/singlepose/lightning\n  description: MoveNet is a family of pose detection models designed for efficient and accurate detection of human poses in images.\n created at: 2021-04-01\n- model_name: ETH-CVG/lightglue_superpoint\n  description: the lightglue model was proposed in by philipp lindenberger, paul-edouard sarlin and marc pollefeys. this model consists of matching two sets of interest points detected in an image. paired with the lightglue is a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. building on the success of superglue, this model has the ability to introspect the confidence of its own predictions. it adapts the amount of computation to the difficulty of each image pair to match. both its depth and width are adaptive : 1. the inference can stop at an early layer if all predictions are ready 2. points that are deemed not matchable are discarded early from further steps. the resulting model, lightglue, is finally faster, more accurate, and easier to train than the long-unrivaled superglue. - developed by: eth zurich - computer vision and geometry lab - model type: image matching - license: academic or non-profit organization noncommercial research use only implied by the use of superpoint as its keypoint detector\n created at: 2025-02-20T15:52:56+00:00\n- model_name: stanfordmimi/synthpose-vitpose-base-hf\n  description: the synthpose model was proposed in by yoni gozlan, antoine falisse, scott uhlrich, anthony gatti, michael black, akshay chaudhari. this model was contributed by this model uses a vitpose base backbone.\n created at: 2025-01-08T17:35:23+00:00\n- model_name: stanfordmimi/synthpose-vitpose-huge-hf\n  description: the synthpose model was proposed in by yoni gozlan, antoine falisse, scott uhlrich, anthony gatti, michael black, akshay chaudhari. this model was contributed by this model uses a vitpose huge backbone.\n created at: 2025-01-10T17:53:40+00:00\n- model_name: ETH-CVG/lightglue_disk\n  description: this model has been pushed to the hub using the integration: this is a lightglue variant trained on disk, with a commecially permissive license, which requires `kornia` to be installed and is usable with transformers with the following lines of code also, the commit allowing disk to work with lightglue is not yet included in a version of transformers, please install transformers from the main branch  lightglue is a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. building on the success of superglue, this model has the ability to introspect the confidence of its own predictions. it adapts the amount of computation to the difficulty of each image pair to match. both its depth and width are adaptive : 1. the inference can stop at an early layer if all predictions are ready 2. points that are deemed not matchable are discarded early from further steps. the resulting model, lightglue, is finally faster, more accurate, and easier to train than the long-unrivaled superglue. - developed by: eth zurich - computer vision and geometry lab - model type: image matching - license: apache 2.0\n created at: 2025-06-24T19:48:47+00:00\n- model_name: qualcomm/Facial-Landmark-Detection\n  description: detects facial landmarks eg, nose, mouth, etc.. this model's architecture was developed by qualcomm. the model was trained by qualcomm on a proprietary dataset of faces, but can be used on any image. this repository provides scripts to run facial-landmark-detection on qualcomm¬Æ devices. more details on model performance across various devices, can be found\n created at: 2024-10-21T23:02:15+00:00\n- model_name: facebook/sapiens-pose-1b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T19:52:56+00:00\n- model_name: facebook/sapiens-pose-0.3b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.3b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-13T20:07:29+00:00\n- model_name: facebook/sapiens-pose-1b\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-10T02:03:29+00:00\n- model_name: qualcomm/Movenet\n  description: movenet performs pose estimation on human images. this model is an implementation of movenet found . this repository provides scripts to run movenet on qualcomm¬Æ devices.\n created at: 2025-02-28T19:13:04+00:00\n- model_name: facebook/sapiens-pose-0.6b\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.6b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-18T18:44:23+00:00\n- model_name: facebook/sapiens-pose-0.3b\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.3b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-13T20:06:09+00:00\n- model_name: qualcomm/RTMPose-Body2d\n  description: rtmpose is a machine learning model that detects human pose and returns a location and confidence for each of 133 joints. this model is an implementation of rtmpose-body2d found . this repository provides scripts to run rtmpose-body2d on qualcomm¬Æ devices.\n created at: 2025-06-23T22:07:55+00:00\n- model_name: facebook/sapiens-pose-0.3b-bfloat16\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.3b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-13T20:09:25+00:00\n- model_name: facebook/sapiens-pose-0.6b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.6b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-18T18:42:35+00:00\n- model_name: facebook/sapiens-pose-1b-bfloat16\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-10T17:13:39+00:00\n- model_name: onnx-community/vitpose-base-simple\n  description: with onnx weights to be compatible with transformers.js. if you haven't already, you can install the javascript library from using: example: pose estimation w/ `onnx-community/vitpose-base-simple`.\n created at: 2024-11-18T22:37:19+00:00\n- model_name: facebook/sapiens-pose-0.6b-bfloat16\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.6b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-18T18:45:55+00:00\n- model_name: STMicroelectronics/hand_landmarks\n  description: hand landmarks is a single pose estimation model targeted for real-time processing implemented in tensorflow. the model is quantized in int8 format using tensorflow lite converter. network information value  hand landmarks is a single pose estimation model targeted for real-time processing implemented in tensorflow. the model is quantized in int8 format using tensorflow lite converter.\n created at: 2024-12-04T15:37:01+00:00\n- model_name: STMicroelectronics/movenet\n  description: movenet is a single pose estimation model targeted for real-time processing implemented in tensorflow. the model is quantized in int8 format using tensorflow lite converter. network information value  movenet is a single pose estimation model targeted for real-time processing implemented in tensorflow. the model is quantized in int8 format using tensorflow lite converter.\n created at: 2024-12-04T15:44:08+00:00\n- model_name: STMicroelectronics/yolov8n_pose\n  description: yolov8n pose is a lightweight and efficient model designed for multi pose estimation tasks. it is part of the yolo you only look once family of models, known for their real-time object detection capabilities. the \"n\" in yolov8n pose indicates that it is a nano version, optimized for speed and resource efficiency, making it suitable for deployment on devices with limited computational power, such as mobile devices and embedded systems. yolov8n pose is implemented in pytorch by ultralytics and is quantized in int8 format using tensorflow lite converter. network information value  yolov8n pose is a lightweight and efficient model designed for multi pose estimation tasks. it is part of the yolo you only look once family of models, known for their real-time object detection capabilities. the \"n\" in yolov8n pose indicates that it is a nano version, optimized for speed and resource efficiency, making it suitable for deployment on devices with limited computational power, such as mobile devices and embedded systems. yolov8n pose is implemented in pytorch by ultralytics and is quantized in int8 format using tensorflow lite converter.\n created at: 2024-12-04T15:51:46+00:00\n\nExamples:\n- example user query: Figure out where people are standing in a photograph by detecting their body poses.\n  model_name: google/movenet/singlepose/lightning\n- example user query: My yoga instructor wants me to send a picture of my new yoga pose, Can this model help us analyze my posture correctly?\n  model_name: google/movenet/singlepose/lightning\n- example user query: Our fitness app needs to recognize poses from pictures to analyze the user's workout technique.\n  model_name: google/movenet/singlepose/lightning\n- example user query: Teach us how we can estimate human poses within a picture of athletes playing basketball.\n  model_name: google/movenet/singlepose/lightning\n- example user query: We need to detect the pose of a person while performing exercise.\n  model_name: google/movenet/singlepose/lightning\n- example user query: Track the posture of each exercise to give advice to gym users.\n  model_name: google/movenet/singlepose/lightning\n\nUser prompt:\nPlease make sure to generate a pose estimation image to extract the important edges.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_pose>\n\nCandidate Models:\n- model_name: google/movenet/singlepose/lightning\n  description: MoveNet is a family of pose detection models designed for efficient and accurate detection of human poses in images.\n created at: 2021-04-01\n- model_name: ETH-CVG/lightglue_superpoint\n  description: the lightglue model was proposed in by philipp lindenberger, paul-edouard sarlin and marc pollefeys. this model consists of matching two sets of interest points detected in an image. paired with the lightglue is a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. building on the success of superglue, this model has the ability to introspect the confidence of its own predictions. it adapts the amount of computation to the difficulty of each image pair to match. both its depth and width are adaptive : 1. the inference can stop at an early layer if all predictions are ready 2. points that are deemed not matchable are discarded early from further steps. the resulting model, lightglue, is finally faster, more accurate, and easier to train than the long-unrivaled superglue. - developed by: eth zurich - computer vision and geometry lab - model type: image matching - license: academic or non-profit organization noncommercial research use only implied by the use of superpoint as its keypoint detector\n created at: 2025-02-20T15:52:56+00:00\n- model_name: stanfordmimi/synthpose-vitpose-base-hf\n  description: the synthpose model was proposed in by yoni gozlan, antoine falisse, scott uhlrich, anthony gatti, michael black, akshay chaudhari. this model was contributed by this model uses a vitpose base backbone.\n created at: 2025-01-08T17:35:23+00:00\n- model_name: stanfordmimi/synthpose-vitpose-huge-hf\n  description: the synthpose model was proposed in by yoni gozlan, antoine falisse, scott uhlrich, anthony gatti, michael black, akshay chaudhari. this model was contributed by this model uses a vitpose huge backbone.\n created at: 2025-01-10T17:53:40+00:00\n- model_name: ETH-CVG/lightglue_disk\n  description: this model has been pushed to the hub using the integration: this is a lightglue variant trained on disk, with a commecially permissive license, which requires `kornia` to be installed and is usable with transformers with the following lines of code also, the commit allowing disk to work with lightglue is not yet included in a version of transformers, please install transformers from the main branch  lightglue is a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. building on the success of superglue, this model has the ability to introspect the confidence of its own predictions. it adapts the amount of computation to the difficulty of each image pair to match. both its depth and width are adaptive : 1. the inference can stop at an early layer if all predictions are ready 2. points that are deemed not matchable are discarded early from further steps. the resulting model, lightglue, is finally faster, more accurate, and easier to train than the long-unrivaled superglue. - developed by: eth zurich - computer vision and geometry lab - model type: image matching - license: apache 2.0\n created at: 2025-06-24T19:48:47+00:00\n- model_name: qualcomm/Facial-Landmark-Detection\n  description: detects facial landmarks eg, nose, mouth, etc.. this model's architecture was developed by qualcomm. the model was trained by qualcomm on a proprietary dataset of faces, but can be used on any image. this repository provides scripts to run facial-landmark-detection on qualcomm¬Æ devices. more details on model performance across various devices, can be found\n created at: 2024-10-21T23:02:15+00:00\n- model_name: facebook/sapiens-pose-1b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T19:52:56+00:00\n- model_name: facebook/sapiens-pose-0.3b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.3b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-13T20:07:29+00:00\n- model_name: facebook/sapiens-pose-1b\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-10T02:03:29+00:00\n- model_name: qualcomm/Movenet\n  description: movenet performs pose estimation on human images. this model is an implementation of movenet found . this repository provides scripts to run movenet on qualcomm¬Æ devices.\n created at: 2025-02-28T19:13:04+00:00\n- model_name: facebook/sapiens-pose-0.6b\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.6b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-18T18:44:23+00:00\n- model_name: facebook/sapiens-pose-0.3b\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.3b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-13T20:06:09+00:00\n- model_name: qualcomm/RTMPose-Body2d\n  description: rtmpose is a machine learning model that detects human pose and returns a location and confidence for each of 133 joints. this model is an implementation of rtmpose-body2d found . this repository provides scripts to run rtmpose-body2d on qualcomm¬Æ devices.\n created at: 2025-06-23T22:07:55+00:00\n- model_name: facebook/sapiens-pose-0.3b-bfloat16\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.3b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-13T20:09:25+00:00\n- model_name: facebook/sapiens-pose-0.6b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.6b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-18T18:42:35+00:00\n- model_name: facebook/sapiens-pose-1b-bfloat16\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-10T17:13:39+00:00\n- model_name: onnx-community/vitpose-base-simple\n  description: with onnx weights to be compatible with transformers.js. if you haven't already, you can install the javascript library from using: example: pose estimation w/ `onnx-community/vitpose-base-simple`.\n created at: 2024-11-18T22:37:19+00:00\n- model_name: facebook/sapiens-pose-0.6b-bfloat16\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.6b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-18T18:45:55+00:00\n- model_name: STMicroelectronics/hand_landmarks\n  description: hand landmarks is a single pose estimation model targeted for real-time processing implemented in tensorflow. the model is quantized in int8 format using tensorflow lite converter. network information value  hand landmarks is a single pose estimation model targeted for real-time processing implemented in tensorflow. the model is quantized in int8 format using tensorflow lite converter.\n created at: 2024-12-04T15:37:01+00:00\n- model_name: STMicroelectronics/movenet\n  description: movenet is a single pose estimation model targeted for real-time processing implemented in tensorflow. the model is quantized in int8 format using tensorflow lite converter. network information value  movenet is a single pose estimation model targeted for real-time processing implemented in tensorflow. the model is quantized in int8 format using tensorflow lite converter.\n created at: 2024-12-04T15:44:08+00:00\n- model_name: STMicroelectronics/yolov8n_pose\n  description: yolov8n pose is a lightweight and efficient model designed for multi pose estimation tasks. it is part of the yolo you only look once family of models, known for their real-time object detection capabilities. the \"n\" in yolov8n pose indicates that it is a nano version, optimized for speed and resource efficiency, making it suitable for deployment on devices with limited computational power, such as mobile devices and embedded systems. yolov8n pose is implemented in pytorch by ultralytics and is quantized in int8 format using tensorflow lite converter. network information value  yolov8n pose is a lightweight and efficient model designed for multi pose estimation tasks. it is part of the yolo you only look once family of models, known for their real-time object detection capabilities. the \"n\" in yolov8n pose indicates that it is a nano version, optimized for speed and resource efficiency, making it suitable for deployment on devices with limited computational power, such as mobile devices and embedded systems. yolov8n pose is implemented in pytorch by ultralytics and is quantized in int8 format using tensorflow lite converter.\n created at: 2024-12-04T15:51:46+00:00\n\nExamples:\n- example user query: Figure out where people are standing in a photograph by detecting their body poses.\n  model_name: google/movenet/singlepose/lightning\n- example user query: My yoga instructor wants me to send a picture of my new yoga pose, Can this model help us analyze my posture correctly?\n  model_name: google/movenet/singlepose/lightning\n- example user query: Our fitness app needs to recognize poses from pictures to analyze the user's workout technique.\n  model_name: google/movenet/singlepose/lightning\n- example user query: Teach us how we can estimate human poses within a picture of athletes playing basketball.\n  model_name: google/movenet/singlepose/lightning\n- example user query: We need to detect the pose of a person while performing exercise.\n  model_name: google/movenet/singlepose/lightning\n- example user query: Track the posture of each exercise to give advice to gym users.\n  model_name: google/movenet/singlepose/lightning\n\nUser prompt:\nI require you to output a pose detection result for a 3D reconstruction process.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_pose>\n\nCandidate Models:\n- model_name: google/movenet/singlepose/lightning\n  description: MoveNet is a family of pose detection models designed for efficient and accurate detection of human poses in images.\n created at: 2021-04-01\n- model_name: ETH-CVG/lightglue_superpoint\n  description: the lightglue model was proposed in by philipp lindenberger, paul-edouard sarlin and marc pollefeys. this model consists of matching two sets of interest points detected in an image. paired with the lightglue is a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. building on the success of superglue, this model has the ability to introspect the confidence of its own predictions. it adapts the amount of computation to the difficulty of each image pair to match. both its depth and width are adaptive : 1. the inference can stop at an early layer if all predictions are ready 2. points that are deemed not matchable are discarded early from further steps. the resulting model, lightglue, is finally faster, more accurate, and easier to train than the long-unrivaled superglue. - developed by: eth zurich - computer vision and geometry lab - model type: image matching - license: academic or non-profit organization noncommercial research use only implied by the use of superpoint as its keypoint detector\n created at: 2025-02-20T15:52:56+00:00\n- model_name: stanfordmimi/synthpose-vitpose-base-hf\n  description: the synthpose model was proposed in by yoni gozlan, antoine falisse, scott uhlrich, anthony gatti, michael black, akshay chaudhari. this model was contributed by this model uses a vitpose base backbone.\n created at: 2025-01-08T17:35:23+00:00\n- model_name: stanfordmimi/synthpose-vitpose-huge-hf\n  description: the synthpose model was proposed in by yoni gozlan, antoine falisse, scott uhlrich, anthony gatti, michael black, akshay chaudhari. this model was contributed by this model uses a vitpose huge backbone.\n created at: 2025-01-10T17:53:40+00:00\n- model_name: ETH-CVG/lightglue_disk\n  description: this model has been pushed to the hub using the integration: this is a lightglue variant trained on disk, with a commecially permissive license, which requires `kornia` to be installed and is usable with transformers with the following lines of code also, the commit allowing disk to work with lightglue is not yet included in a version of transformers, please install transformers from the main branch  lightglue is a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. building on the success of superglue, this model has the ability to introspect the confidence of its own predictions. it adapts the amount of computation to the difficulty of each image pair to match. both its depth and width are adaptive : 1. the inference can stop at an early layer if all predictions are ready 2. points that are deemed not matchable are discarded early from further steps. the resulting model, lightglue, is finally faster, more accurate, and easier to train than the long-unrivaled superglue. - developed by: eth zurich - computer vision and geometry lab - model type: image matching - license: apache 2.0\n created at: 2025-06-24T19:48:47+00:00\n- model_name: qualcomm/Facial-Landmark-Detection\n  description: detects facial landmarks eg, nose, mouth, etc.. this model's architecture was developed by qualcomm. the model was trained by qualcomm on a proprietary dataset of faces, but can be used on any image. this repository provides scripts to run facial-landmark-detection on qualcomm¬Æ devices. more details on model performance across various devices, can be found\n created at: 2024-10-21T23:02:15+00:00\n- model_name: facebook/sapiens-pose-1b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T19:52:56+00:00\n- model_name: facebook/sapiens-pose-0.3b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.3b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-13T20:07:29+00:00\n- model_name: facebook/sapiens-pose-1b\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-10T02:03:29+00:00\n- model_name: qualcomm/Movenet\n  description: movenet performs pose estimation on human images. this model is an implementation of movenet found . this repository provides scripts to run movenet on qualcomm¬Æ devices.\n created at: 2025-02-28T19:13:04+00:00\n- model_name: facebook/sapiens-pose-0.6b\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.6b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-18T18:44:23+00:00\n- model_name: facebook/sapiens-pose-0.3b\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.3b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-13T20:06:09+00:00\n- model_name: qualcomm/RTMPose-Body2d\n  description: rtmpose is a machine learning model that detects human pose and returns a location and confidence for each of 133 joints. this model is an implementation of rtmpose-body2d found . this repository provides scripts to run rtmpose-body2d on qualcomm¬Æ devices.\n created at: 2025-06-23T22:07:55+00:00\n- model_name: facebook/sapiens-pose-0.3b-bfloat16\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.3b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-13T20:09:25+00:00\n- model_name: facebook/sapiens-pose-0.6b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.6b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-18T18:42:35+00:00\n- model_name: facebook/sapiens-pose-1b-bfloat16\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-10T17:13:39+00:00\n- model_name: onnx-community/vitpose-base-simple\n  description: with onnx weights to be compatible with transformers.js. if you haven't already, you can install the javascript library from using: example: pose estimation w/ `onnx-community/vitpose-base-simple`.\n created at: 2024-11-18T22:37:19+00:00\n- model_name: facebook/sapiens-pose-0.6b-bfloat16\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.6b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-18T18:45:55+00:00\n- model_name: STMicroelectronics/hand_landmarks\n  description: hand landmarks is a single pose estimation model targeted for real-time processing implemented in tensorflow. the model is quantized in int8 format using tensorflow lite converter. network information value  hand landmarks is a single pose estimation model targeted for real-time processing implemented in tensorflow. the model is quantized in int8 format using tensorflow lite converter.\n created at: 2024-12-04T15:37:01+00:00\n- model_name: STMicroelectronics/movenet\n  description: movenet is a single pose estimation model targeted for real-time processing implemented in tensorflow. the model is quantized in int8 format using tensorflow lite converter. network information value  movenet is a single pose estimation model targeted for real-time processing implemented in tensorflow. the model is quantized in int8 format using tensorflow lite converter.\n created at: 2024-12-04T15:44:08+00:00\n- model_name: STMicroelectronics/yolov8n_pose\n  description: yolov8n pose is a lightweight and efficient model designed for multi pose estimation tasks. it is part of the yolo you only look once family of models, known for their real-time object detection capabilities. the \"n\" in yolov8n pose indicates that it is a nano version, optimized for speed and resource efficiency, making it suitable for deployment on devices with limited computational power, such as mobile devices and embedded systems. yolov8n pose is implemented in pytorch by ultralytics and is quantized in int8 format using tensorflow lite converter. network information value  yolov8n pose is a lightweight and efficient model designed for multi pose estimation tasks. it is part of the yolo you only look once family of models, known for their real-time object detection capabilities. the \"n\" in yolov8n pose indicates that it is a nano version, optimized for speed and resource efficiency, making it suitable for deployment on devices with limited computational power, such as mobile devices and embedded systems. yolov8n pose is implemented in pytorch by ultralytics and is quantized in int8 format using tensorflow lite converter.\n created at: 2024-12-04T15:51:46+00:00\n\nExamples:\n- example user query: Figure out where people are standing in a photograph by detecting their body poses.\n  model_name: google/movenet/singlepose/lightning\n- example user query: My yoga instructor wants me to send a picture of my new yoga pose, Can this model help us analyze my posture correctly?\n  model_name: google/movenet/singlepose/lightning\n- example user query: Our fitness app needs to recognize poses from pictures to analyze the user's workout technique.\n  model_name: google/movenet/singlepose/lightning\n- example user query: Teach us how we can estimate human poses within a picture of athletes playing basketball.\n  model_name: google/movenet/singlepose/lightning\n- example user query: We need to detect the pose of a person while performing exercise.\n  model_name: google/movenet/singlepose/lightning\n- example user query: Track the posture of each exercise to give advice to gym users.\n  model_name: google/movenet/singlepose/lightning\n\nUser prompt:\nWould you kindly produce a pose estimate using the current scene.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_pose>\n\nCandidate Models:\n- model_name: google/movenet/singlepose/lightning\n  description: MoveNet is a family of pose detection models designed for efficient and accurate detection of human poses in images.\n created at: 2021-04-01\n- model_name: ETH-CVG/lightglue_superpoint\n  description: the lightglue model was proposed in by philipp lindenberger, paul-edouard sarlin and marc pollefeys. this model consists of matching two sets of interest points detected in an image. paired with the lightglue is a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. building on the success of superglue, this model has the ability to introspect the confidence of its own predictions. it adapts the amount of computation to the difficulty of each image pair to match. both its depth and width are adaptive : 1. the inference can stop at an early layer if all predictions are ready 2. points that are deemed not matchable are discarded early from further steps. the resulting model, lightglue, is finally faster, more accurate, and easier to train than the long-unrivaled superglue. - developed by: eth zurich - computer vision and geometry lab - model type: image matching - license: academic or non-profit organization noncommercial research use only implied by the use of superpoint as its keypoint detector\n created at: 2025-02-20T15:52:56+00:00\n- model_name: stanfordmimi/synthpose-vitpose-base-hf\n  description: the synthpose model was proposed in by yoni gozlan, antoine falisse, scott uhlrich, anthony gatti, michael black, akshay chaudhari. this model was contributed by this model uses a vitpose base backbone.\n created at: 2025-01-08T17:35:23+00:00\n- model_name: stanfordmimi/synthpose-vitpose-huge-hf\n  description: the synthpose model was proposed in by yoni gozlan, antoine falisse, scott uhlrich, anthony gatti, michael black, akshay chaudhari. this model was contributed by this model uses a vitpose huge backbone.\n created at: 2025-01-10T17:53:40+00:00\n- model_name: ETH-CVG/lightglue_disk\n  description: this model has been pushed to the hub using the integration: this is a lightglue variant trained on disk, with a commecially permissive license, which requires `kornia` to be installed and is usable with transformers with the following lines of code also, the commit allowing disk to work with lightglue is not yet included in a version of transformers, please install transformers from the main branch  lightglue is a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. building on the success of superglue, this model has the ability to introspect the confidence of its own predictions. it adapts the amount of computation to the difficulty of each image pair to match. both its depth and width are adaptive : 1. the inference can stop at an early layer if all predictions are ready 2. points that are deemed not matchable are discarded early from further steps. the resulting model, lightglue, is finally faster, more accurate, and easier to train than the long-unrivaled superglue. - developed by: eth zurich - computer vision and geometry lab - model type: image matching - license: apache 2.0\n created at: 2025-06-24T19:48:47+00:00\n- model_name: qualcomm/Facial-Landmark-Detection\n  description: detects facial landmarks eg, nose, mouth, etc.. this model's architecture was developed by qualcomm. the model was trained by qualcomm on a proprietary dataset of faces, but can be used on any image. this repository provides scripts to run facial-landmark-detection on qualcomm¬Æ devices. more details on model performance across various devices, can be found\n created at: 2024-10-21T23:02:15+00:00\n- model_name: facebook/sapiens-pose-1b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T19:52:56+00:00\n- model_name: facebook/sapiens-pose-0.3b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.3b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-13T20:07:29+00:00\n- model_name: facebook/sapiens-pose-1b\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-10T02:03:29+00:00\n- model_name: qualcomm/Movenet\n  description: movenet performs pose estimation on human images. this model is an implementation of movenet found . this repository provides scripts to run movenet on qualcomm¬Æ devices.\n created at: 2025-02-28T19:13:04+00:00\n- model_name: facebook/sapiens-pose-0.6b\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.6b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-18T18:44:23+00:00\n- model_name: facebook/sapiens-pose-0.3b\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.3b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-13T20:06:09+00:00\n- model_name: qualcomm/RTMPose-Body2d\n  description: rtmpose is a machine learning model that detects human pose and returns a location and confidence for each of 133 joints. this model is an implementation of rtmpose-body2d found . this repository provides scripts to run rtmpose-body2d on qualcomm¬Æ devices.\n created at: 2025-06-23T22:07:55+00:00\n- model_name: facebook/sapiens-pose-0.3b-bfloat16\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.3b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-13T20:09:25+00:00\n- model_name: facebook/sapiens-pose-0.6b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.6b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-18T18:42:35+00:00\n- model_name: facebook/sapiens-pose-1b-bfloat16\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-10T17:13:39+00:00\n- model_name: onnx-community/vitpose-base-simple\n  description: with onnx weights to be compatible with transformers.js. if you haven't already, you can install the javascript library from using: example: pose estimation w/ `onnx-community/vitpose-base-simple`.\n created at: 2024-11-18T22:37:19+00:00\n- model_name: facebook/sapiens-pose-0.6b-bfloat16\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.6b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-18T18:45:55+00:00\n- model_name: STMicroelectronics/hand_landmarks\n  description: hand landmarks is a single pose estimation model targeted for real-time processing implemented in tensorflow. the model is quantized in int8 format using tensorflow lite converter. network information value  hand landmarks is a single pose estimation model targeted for real-time processing implemented in tensorflow. the model is quantized in int8 format using tensorflow lite converter.\n created at: 2024-12-04T15:37:01+00:00\n- model_name: STMicroelectronics/movenet\n  description: movenet is a single pose estimation model targeted for real-time processing implemented in tensorflow. the model is quantized in int8 format using tensorflow lite converter. network information value  movenet is a single pose estimation model targeted for real-time processing implemented in tensorflow. the model is quantized in int8 format using tensorflow lite converter.\n created at: 2024-12-04T15:44:08+00:00\n- model_name: STMicroelectronics/yolov8n_pose\n  description: yolov8n pose is a lightweight and efficient model designed for multi pose estimation tasks. it is part of the yolo you only look once family of models, known for their real-time object detection capabilities. the \"n\" in yolov8n pose indicates that it is a nano version, optimized for speed and resource efficiency, making it suitable for deployment on devices with limited computational power, such as mobile devices and embedded systems. yolov8n pose is implemented in pytorch by ultralytics and is quantized in int8 format using tensorflow lite converter. network information value  yolov8n pose is a lightweight and efficient model designed for multi pose estimation tasks. it is part of the yolo you only look once family of models, known for their real-time object detection capabilities. the \"n\" in yolov8n pose indicates that it is a nano version, optimized for speed and resource efficiency, making it suitable for deployment on devices with limited computational power, such as mobile devices and embedded systems. yolov8n pose is implemented in pytorch by ultralytics and is quantized in int8 format using tensorflow lite converter.\n created at: 2024-12-04T15:51:46+00:00\n\nExamples:\n- example user query: Figure out where people are standing in a photograph by detecting their body poses.\n  model_name: google/movenet/singlepose/lightning\n- example user query: My yoga instructor wants me to send a picture of my new yoga pose, Can this model help us analyze my posture correctly?\n  model_name: google/movenet/singlepose/lightning\n- example user query: Our fitness app needs to recognize poses from pictures to analyze the user's workout technique.\n  model_name: google/movenet/singlepose/lightning\n- example user query: Teach us how we can estimate human poses within a picture of athletes playing basketball.\n  model_name: google/movenet/singlepose/lightning\n- example user query: We need to detect the pose of a person while performing exercise.\n  model_name: google/movenet/singlepose/lightning\n- example user query: Track the posture of each exercise to give advice to gym users.\n  model_name: google/movenet/singlepose/lightning\n\nUser prompt:\nI‚Äôm asking you to produce a pose output from this scene.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_pose>\n\nCandidate Models:\n- model_name: google/movenet/singlepose/lightning\n  description: MoveNet is a family of pose detection models designed for efficient and accurate detection of human poses in images.\n created at: 2021-04-01\n- model_name: ETH-CVG/lightglue_superpoint\n  description: the lightglue model was proposed in by philipp lindenberger, paul-edouard sarlin and marc pollefeys. this model consists of matching two sets of interest points detected in an image. paired with the lightglue is a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. building on the success of superglue, this model has the ability to introspect the confidence of its own predictions. it adapts the amount of computation to the difficulty of each image pair to match. both its depth and width are adaptive : 1. the inference can stop at an early layer if all predictions are ready 2. points that are deemed not matchable are discarded early from further steps. the resulting model, lightglue, is finally faster, more accurate, and easier to train than the long-unrivaled superglue. - developed by: eth zurich - computer vision and geometry lab - model type: image matching - license: academic or non-profit organization noncommercial research use only implied by the use of superpoint as its keypoint detector\n created at: 2025-02-20T15:52:56+00:00\n- model_name: stanfordmimi/synthpose-vitpose-base-hf\n  description: the synthpose model was proposed in by yoni gozlan, antoine falisse, scott uhlrich, anthony gatti, michael black, akshay chaudhari. this model was contributed by this model uses a vitpose base backbone.\n created at: 2025-01-08T17:35:23+00:00\n- model_name: stanfordmimi/synthpose-vitpose-huge-hf\n  description: the synthpose model was proposed in by yoni gozlan, antoine falisse, scott uhlrich, anthony gatti, michael black, akshay chaudhari. this model was contributed by this model uses a vitpose huge backbone.\n created at: 2025-01-10T17:53:40+00:00\n- model_name: ETH-CVG/lightglue_disk\n  description: this model has been pushed to the hub using the integration: this is a lightglue variant trained on disk, with a commecially permissive license, which requires `kornia` to be installed and is usable with transformers with the following lines of code also, the commit allowing disk to work with lightglue is not yet included in a version of transformers, please install transformers from the main branch  lightglue is a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. building on the success of superglue, this model has the ability to introspect the confidence of its own predictions. it adapts the amount of computation to the difficulty of each image pair to match. both its depth and width are adaptive : 1. the inference can stop at an early layer if all predictions are ready 2. points that are deemed not matchable are discarded early from further steps. the resulting model, lightglue, is finally faster, more accurate, and easier to train than the long-unrivaled superglue. - developed by: eth zurich - computer vision and geometry lab - model type: image matching - license: apache 2.0\n created at: 2025-06-24T19:48:47+00:00\n- model_name: qualcomm/Facial-Landmark-Detection\n  description: detects facial landmarks eg, nose, mouth, etc.. this model's architecture was developed by qualcomm. the model was trained by qualcomm on a proprietary dataset of faces, but can be used on any image. this repository provides scripts to run facial-landmark-detection on qualcomm¬Æ devices. more details on model performance across various devices, can be found\n created at: 2024-10-21T23:02:15+00:00\n- model_name: facebook/sapiens-pose-1b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T19:52:56+00:00\n- model_name: facebook/sapiens-pose-0.3b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.3b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-13T20:07:29+00:00\n- model_name: facebook/sapiens-pose-1b\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-10T02:03:29+00:00\n- model_name: qualcomm/Movenet\n  description: movenet performs pose estimation on human images. this model is an implementation of movenet found . this repository provides scripts to run movenet on qualcomm¬Æ devices.\n created at: 2025-02-28T19:13:04+00:00\n- model_name: facebook/sapiens-pose-0.6b\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.6b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-18T18:44:23+00:00\n- model_name: facebook/sapiens-pose-0.3b\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.3b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-13T20:06:09+00:00\n- model_name: qualcomm/RTMPose-Body2d\n  description: rtmpose is a machine learning model that detects human pose and returns a location and confidence for each of 133 joints. this model is an implementation of rtmpose-body2d found . this repository provides scripts to run rtmpose-body2d on qualcomm¬Æ devices.\n created at: 2025-06-23T22:07:55+00:00\n- model_name: facebook/sapiens-pose-0.3b-bfloat16\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.3b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-13T20:09:25+00:00\n- model_name: facebook/sapiens-pose-0.6b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.6b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-18T18:42:35+00:00\n- model_name: facebook/sapiens-pose-1b-bfloat16\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-10T17:13:39+00:00\n- model_name: onnx-community/vitpose-base-simple\n  description: with onnx weights to be compatible with transformers.js. if you haven't already, you can install the javascript library from using: example: pose estimation w/ `onnx-community/vitpose-base-simple`.\n created at: 2024-11-18T22:37:19+00:00\n- model_name: facebook/sapiens-pose-0.6b-bfloat16\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.6b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-18T18:45:55+00:00\n- model_name: STMicroelectronics/hand_landmarks\n  description: hand landmarks is a single pose estimation model targeted for real-time processing implemented in tensorflow. the model is quantized in int8 format using tensorflow lite converter. network information value  hand landmarks is a single pose estimation model targeted for real-time processing implemented in tensorflow. the model is quantized in int8 format using tensorflow lite converter.\n created at: 2024-12-04T15:37:01+00:00\n- model_name: STMicroelectronics/movenet\n  description: movenet is a single pose estimation model targeted for real-time processing implemented in tensorflow. the model is quantized in int8 format using tensorflow lite converter. network information value  movenet is a single pose estimation model targeted for real-time processing implemented in tensorflow. the model is quantized in int8 format using tensorflow lite converter.\n created at: 2024-12-04T15:44:08+00:00\n- model_name: STMicroelectronics/yolov8n_pose\n  description: yolov8n pose is a lightweight and efficient model designed for multi pose estimation tasks. it is part of the yolo you only look once family of models, known for their real-time object detection capabilities. the \"n\" in yolov8n pose indicates that it is a nano version, optimized for speed and resource efficiency, making it suitable for deployment on devices with limited computational power, such as mobile devices and embedded systems. yolov8n pose is implemented in pytorch by ultralytics and is quantized in int8 format using tensorflow lite converter. network information value  yolov8n pose is a lightweight and efficient model designed for multi pose estimation tasks. it is part of the yolo you only look once family of models, known for their real-time object detection capabilities. the \"n\" in yolov8n pose indicates that it is a nano version, optimized for speed and resource efficiency, making it suitable for deployment on devices with limited computational power, such as mobile devices and embedded systems. yolov8n pose is implemented in pytorch by ultralytics and is quantized in int8 format using tensorflow lite converter.\n created at: 2024-12-04T15:51:46+00:00\n\nExamples:\n- example user query: Figure out where people are standing in a photograph by detecting their body poses.\n  model_name: google/movenet/singlepose/lightning\n- example user query: My yoga instructor wants me to send a picture of my new yoga pose, Can this model help us analyze my posture correctly?\n  model_name: google/movenet/singlepose/lightning\n- example user query: Our fitness app needs to recognize poses from pictures to analyze the user's workout technique.\n  model_name: google/movenet/singlepose/lightning\n- example user query: Teach us how we can estimate human poses within a picture of athletes playing basketball.\n  model_name: google/movenet/singlepose/lightning\n- example user query: We need to detect the pose of a person while performing exercise.\n  model_name: google/movenet/singlepose/lightning\n- example user query: Track the posture of each exercise to give advice to gym users.\n  model_name: google/movenet/singlepose/lightning\n\nUser prompt:\nI would be grateful if you could produce a pose estimate to show the object's pose.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_pose>\n\nCandidate Models:\n- model_name: google/movenet/singlepose/lightning\n  description: MoveNet is a family of pose detection models designed for efficient and accurate detection of human poses in images.\n created at: 2021-04-01\n- model_name: ETH-CVG/lightglue_superpoint\n  description: the lightglue model was proposed in by philipp lindenberger, paul-edouard sarlin and marc pollefeys. this model consists of matching two sets of interest points detected in an image. paired with the lightglue is a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. building on the success of superglue, this model has the ability to introspect the confidence of its own predictions. it adapts the amount of computation to the difficulty of each image pair to match. both its depth and width are adaptive : 1. the inference can stop at an early layer if all predictions are ready 2. points that are deemed not matchable are discarded early from further steps. the resulting model, lightglue, is finally faster, more accurate, and easier to train than the long-unrivaled superglue. - developed by: eth zurich - computer vision and geometry lab - model type: image matching - license: academic or non-profit organization noncommercial research use only implied by the use of superpoint as its keypoint detector\n created at: 2025-02-20T15:52:56+00:00\n- model_name: stanfordmimi/synthpose-vitpose-base-hf\n  description: the synthpose model was proposed in by yoni gozlan, antoine falisse, scott uhlrich, anthony gatti, michael black, akshay chaudhari. this model was contributed by this model uses a vitpose base backbone.\n created at: 2025-01-08T17:35:23+00:00\n- model_name: stanfordmimi/synthpose-vitpose-huge-hf\n  description: the synthpose model was proposed in by yoni gozlan, antoine falisse, scott uhlrich, anthony gatti, michael black, akshay chaudhari. this model was contributed by this model uses a vitpose huge backbone.\n created at: 2025-01-10T17:53:40+00:00\n- model_name: ETH-CVG/lightglue_disk\n  description: this model has been pushed to the hub using the integration: this is a lightglue variant trained on disk, with a commecially permissive license, which requires `kornia` to be installed and is usable with transformers with the following lines of code also, the commit allowing disk to work with lightglue is not yet included in a version of transformers, please install transformers from the main branch  lightglue is a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. building on the success of superglue, this model has the ability to introspect the confidence of its own predictions. it adapts the amount of computation to the difficulty of each image pair to match. both its depth and width are adaptive : 1. the inference can stop at an early layer if all predictions are ready 2. points that are deemed not matchable are discarded early from further steps. the resulting model, lightglue, is finally faster, more accurate, and easier to train than the long-unrivaled superglue. - developed by: eth zurich - computer vision and geometry lab - model type: image matching - license: apache 2.0\n created at: 2025-06-24T19:48:47+00:00\n- model_name: qualcomm/Facial-Landmark-Detection\n  description: detects facial landmarks eg, nose, mouth, etc.. this model's architecture was developed by qualcomm. the model was trained by qualcomm on a proprietary dataset of faces, but can be used on any image. this repository provides scripts to run facial-landmark-detection on qualcomm¬Æ devices. more details on model performance across various devices, can be found\n created at: 2024-10-21T23:02:15+00:00\n- model_name: facebook/sapiens-pose-1b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T19:52:56+00:00\n- model_name: facebook/sapiens-pose-0.3b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.3b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-13T20:07:29+00:00\n- model_name: facebook/sapiens-pose-1b\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-10T02:03:29+00:00\n- model_name: qualcomm/Movenet\n  description: movenet performs pose estimation on human images. this model is an implementation of movenet found . this repository provides scripts to run movenet on qualcomm¬Æ devices.\n created at: 2025-02-28T19:13:04+00:00\n- model_name: facebook/sapiens-pose-0.6b\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.6b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-18T18:44:23+00:00\n- model_name: facebook/sapiens-pose-0.3b\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.3b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-13T20:06:09+00:00\n- model_name: qualcomm/RTMPose-Body2d\n  description: rtmpose is a machine learning model that detects human pose and returns a location and confidence for each of 133 joints. this model is an implementation of rtmpose-body2d found . this repository provides scripts to run rtmpose-body2d on qualcomm¬Æ devices.\n created at: 2025-06-23T22:07:55+00:00\n- model_name: facebook/sapiens-pose-0.3b-bfloat16\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.3b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-13T20:09:25+00:00\n- model_name: facebook/sapiens-pose-0.6b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.6b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-18T18:42:35+00:00\n- model_name: facebook/sapiens-pose-1b-bfloat16\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-10T17:13:39+00:00\n- model_name: onnx-community/vitpose-base-simple\n  description: with onnx weights to be compatible with transformers.js. if you haven't already, you can install the javascript library from using: example: pose estimation w/ `onnx-community/vitpose-base-simple`.\n created at: 2024-11-18T22:37:19+00:00\n- model_name: facebook/sapiens-pose-0.6b-bfloat16\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-0.6b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-18T18:45:55+00:00\n- model_name: STMicroelectronics/hand_landmarks\n  description: hand landmarks is a single pose estimation model targeted for real-time processing implemented in tensorflow. the model is quantized in int8 format using tensorflow lite converter. network information value  hand landmarks is a single pose estimation model targeted for real-time processing implemented in tensorflow. the model is quantized in int8 format using tensorflow lite converter.\n created at: 2024-12-04T15:37:01+00:00\n- model_name: STMicroelectronics/movenet\n  description: movenet is a single pose estimation model targeted for real-time processing implemented in tensorflow. the model is quantized in int8 format using tensorflow lite converter. network information value  movenet is a single pose estimation model targeted for real-time processing implemented in tensorflow. the model is quantized in int8 format using tensorflow lite converter.\n created at: 2024-12-04T15:44:08+00:00\n- model_name: STMicroelectronics/yolov8n_pose\n  description: yolov8n pose is a lightweight and efficient model designed for multi pose estimation tasks. it is part of the yolo you only look once family of models, known for their real-time object detection capabilities. the \"n\" in yolov8n pose indicates that it is a nano version, optimized for speed and resource efficiency, making it suitable for deployment on devices with limited computational power, such as mobile devices and embedded systems. yolov8n pose is implemented in pytorch by ultralytics and is quantized in int8 format using tensorflow lite converter. network information value  yolov8n pose is a lightweight and efficient model designed for multi pose estimation tasks. it is part of the yolo you only look once family of models, known for their real-time object detection capabilities. the \"n\" in yolov8n pose indicates that it is a nano version, optimized for speed and resource efficiency, making it suitable for deployment on devices with limited computational power, such as mobile devices and embedded systems. yolov8n pose is implemented in pytorch by ultralytics and is quantized in int8 format using tensorflow lite converter.\n created at: 2024-12-04T15:51:46+00:00\n\nExamples:\n- example user query: Figure out where people are standing in a photograph by detecting their body poses.\n  model_name: google/movenet/singlepose/lightning\n- example user query: My yoga instructor wants me to send a picture of my new yoga pose, Can this model help us analyze my posture correctly?\n  model_name: google/movenet/singlepose/lightning\n- example user query: Our fitness app needs to recognize poses from pictures to analyze the user's workout technique.\n  model_name: google/movenet/singlepose/lightning\n- example user query: Teach us how we can estimate human poses within a picture of athletes playing basketball.\n  model_name: google/movenet/singlepose/lightning\n- example user query: We need to detect the pose of a person while performing exercise.\n  model_name: google/movenet/singlepose/lightning\n- example user query: Track the posture of each exercise to give advice to gym users.\n  model_name: google/movenet/singlepose/lightning\n\nUser prompt:\nCan you render a pose image for this specific snapshot.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n"
  ],
  "<image_seg>": [
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_seg>\n\nCandidate Models:\n- model_name: CIDAS/clipseg-rd64-refined\n  description: CLIPSeg model with reduce dimension 64, refined (using a more complex convolution). It was introduced in the paper Image Segmentation Using Text and Image Prompts by L√ºddecke et al. and first released in this repository. This model is intended for zero-shot and one-shot image segmentation.\n created at: 2022-11-01\n- model_name: openmmlab/upernet-convnext-small\n  description: UperNet framework for semantic segmentation, leveraging a ConvNeXt backbone. UperNet was introduced in the paper Unified Perceptual Parsing for Scene Understanding by Xiao et al. Combining UperNet with a ConvNeXt backbone was introduced in the paper A ConvNet for the 2020s.\n created at: 2023-01-13\n- model_name: nvidia/segformer-b0-finetuned-ade-512-512\n  description: SegFormer model fine-tuned on ADE20k at resolution 512x512. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: nvidia/segformer-b5-finetuned-ade-640-640\n  description: SegFormer model fine-tuned on ADE20k at resolution 640x640. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: nvidia/segformer-b2-finetuned-cityscapes-1024-1024\n  description: SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: nvidia/segformer-b0-finetuned-cityscapes-1024-1024\n  description: SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: facebook/maskformer-swin-base-coco\n  description: MaskFormer model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.\n created at: 2022-03-02\n- model_name: facebook/mask2former-swin-large-cityscapes-semantic\n  description: Mask2Former model trained on Cityscapes semantic segmentation (large-sized version, Swin backbone). It addresses instance, semantic and panoptic segmentation by predicting a set of masks and corresponding labels. The model outperforms the previous SOTA, MaskFormer, in terms of performance and efficiency.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-base-coco-panoptic\n  description: Mask2Former model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.\n created at: 2023-01-02\n- model_name: mattmdjaga/segformer_b2_clothes\n  description: SegFormer model fine-tuned on ATR dataset for clothes segmentation.\n created at: 2022-11-24\n- model_name: shi-labs/oneformer_coco_swin_large\n  description: OneFormer model trained on the COCO dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.\n created at: 2022-11-15\n- model_name: facebook/mask2former-swin-large-coco-panoptic\n  description: Mask2Former model trained on COCO panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.\n created at: 2023-01-02\n- model_name: shi-labs/oneformer_ade20k_swin_large\n  description: OneFormer model trained on the ADE20k dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.\n created at: 2022-11-15\n- model_name: facebook/mask2former-swin-small-coco-instance\n  description: Mask2Former model trained on COCO instance segmentation (small-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.\n created at: 2022-12-26\n- model_name: nvidia/segformer-b5-finetuned-cityscapes-1024-1024\n  description: SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: shi-labs/oneformer_ade20k_swin_tiny\n  description: OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.\n created at: 2022-11-16\n- model_name: keremberke/yolov8m-building-segmentation\n  description: A YOLOv8 model for building segmentation in satellite images. It can detect and segment buildings in the input images.\n created at: 2023-01-27\n- model_name: facebook/maskformer-swin-base-ade\n  description: MaskFormer model trained on ADE20k semantic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.\n created at: 2022-03-02\n- model_name: facebook/mask2former-swin-tiny-coco-instance\n  description: Mask2Former model trained on COCO instance segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. You can use this particular checkpoint for instance segmentation.\n created at: 2022-12-23\n- model_name: keremberke/yolov8m-pcb-defect-segmentation\n  description: A YOLOv8 model for PCB defect segmentation trained on the pcb-defect-segmentation dataset. The model can detect and segment defects in PCB images, such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.\n created at: 2023-01-28\n- model_name: facebook/maskformer-swin-tiny-coco\n  description: MaskFormer model trained on COCO panoptic segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.\n created at: 2022-03-02\n- model_name: keremberke/yolov8m-pothole-segmentation\n  description: A YOLOv8 model for pothole segmentation trained on keremberke/pothole-segmentation dataset. It can detect potholes in images and provide segmentation masks for the detected potholes.\n created at: 2023-01-26\n- model_name: keremberke/yolov8s-building-segmentation\n  description: A YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can detect and segment buildings with high accuracy.\n created at: 2023-01-26\n- model_name: keremberke/yolov8s-pothole-segmentation\n  description: A YOLOv8 model for pothole segmentation. This model detects potholes in images and outputs bounding boxes and masks for the detected potholes.\n created at: 2023-01-26\n- model_name: keremberke/yolov8s-pcb-defect-segmentation\n  description: YOLOv8s model for PCB defect segmentation. The model is trained to detect and segment PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.\n created at: 2023-01-28\n- model_name: keremberke/yolov8n-pcb-defect-segmentation\n  description: A YOLOv8 model for detecting and segmenting PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.\n created at: 2023-01-28\n- model_name: keremberke/yolov8n-pothole-segmentation\n  description: A YOLOv8 model for pothole segmentation in images. The model is trained on the pothole-segmentation dataset and achieves high accuracy in detecting potholes.\n created at: 2023-01-15\n- model_name: facebook/maskformer-swin-large-ade\n  description: MaskFormer model trained on ADE20k semantic segmentation (large-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.\n created at: 2022-03-02\n- model_name: facebook/detr-resnet-50-panoptic\n  description: DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 panoptic (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: tensorflow/lite-model/deeplabv3/1/default\n  description: TensorFlow Hub provides pre-trained image segmentation models that can be loaded and used for various tasks such as semantic segmentation. The example code demonstrates how to load a DeepLabV3 model and perform segmentation on an input image tensor.\n created at: 2020-10-06\n- model_name: pytorch/vision/deeplabv3_resnet50\n  description: DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: pytorch/vision/deeplabv3_mobilenet_v3_large\n  description: DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: pytorch/vision/deeplabv3_resnet101\n  description: DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: mateuszbuda/brain-segmentation-pytorch/unet\n  description: U-Net with batch normalization for biomedical image segmentation with pretrained weights for abnormality segmentation in brain MRI. The model comprises four levels of blocks containing two convolutional layers with batch normalization and ReLU activation function, and one max pooling layer in the encoding part and up-convolutional layers instead in the decoding part. The number of convolutional filters in each block is 32, 64, 128, and 256. The bottleneck layer has 512 convolutional filters. From the encoding layers, skip connections are used to the corresponding layers in the decoding part. Input image is a 3-channel brain MRI slice from pre-contrast, FLAIR, and post-contrast sequences, respectively. Output is a one-channel probability map of abnormality regions with the same size as the input image.\n created at: 2019-05-26\n- model_name: pytorch/vision/fcn_resnet101\n  description: FCN-ResNet is a Fully-Convolutional Network model using a ResNet-50 or a ResNet-101 backbone. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: pytorch/fairseq\n  description: Transformer (NMT) is a powerful sequence-to-sequence modeling architecture that produces state-of-the-art neural machine translation systems. It is based on the paper 'Attention Is All You Need' and has been improved using techniques such as large-scale semi-supervised training, back-translation, and noisy-channel reranking. It supports English-French and English-German translation as well as round-trip translation for paraphrasing.\n created at: 2017-08-29\n- model_name: pytorch/vision/fcn_resnet50\n  description: FCN-ResNet is a Fully-Convolutional Network model using a ResNet-50 or a ResNet-101 backbone. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: facebook/mask2former-swin-large-ade-semantic\n  description: Mask2Former model trained on ADE20k semantic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-large-ade-panoptic\n  description: Mask2Former model trained on ADE20k panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-large-mapillary-vistas-panoptic\n  description: Mask2Former model trained on Mapillary Vistas panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-large-mapillary-vistas-semantic\n  description: Mask2Former model trained on Mapillary Vistas semantic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-large-cityscapes-panoptic\n  description: Mask2Former model trained on Cityscapes panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-03\n- model_name: shi-labs/oneformer_cityscapes_swin_large\n  description: OneFormer model trained on the Cityscapes dataset (large-sized version, Swin backbone). It was introduced in the paper [OneFormer: One Transformer to Rule Universal Image Segmentation](https://arxiv.org/abs/2211.06220) by Jain et al. and first released in [this repository](https://github.com/SHI-Labs/OneFormer).\n\n![model image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/oneformer_teaser.png)\n created at: 2022-11-15\n- model_name: google/deeplabv3_mobilenet_v2_1.0_513\n  description: MobileNet V2 model pre-trained on PASCAL VOC at resolution 513x513. It was introduced in [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381) by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen. It was first released in [this repository](https://github.com/tensorflow/models/tree/master/research/deeplab).\n\nDisclaimer: The team releasing MobileNet V2 did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2022-11-10\n- model_name: apple/deeplabv3-mobilevit-small\n  description: MobileViT model pre-trained on PASCAL VOC at resolution 512x512. It was introduced in [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer](https://arxiv.org/abs/2110.02178) by Sachin Mehta and Mohammad Rastegari, and first released in [this repository](https://github.com/apple/ml-cvnets). The license used is [Apple sample code license](https://github.com/apple/ml-cvnets/blob/main/LICENSE).\n\nDisclaimer: The team releasing MobileViT did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2022-05-30\n- model_name: eugenesiow/edsr-base\n  description: The goal of image super resolution is to restore a high resolution (HR) image from a single low resolution (LR) image. The image below shows the ground truth (HR), the bicubic upscaling x2 and EDSR upscaling x2.\n\n![Comparing Bicubic upscaling against EDSR x2 upscaling on Set5 Image 4](images/Set5_4_compare.png \"Comparing Bicubic upscaling against EDSR x2 upscaling on Set5 Image 4\")\n## Model description\nEDSR is a model that uses both deeper and wider architecture (32 ResBlocks and 256 channels) to improve performance. It uses both global and local skip connections, and up-scaling is done at the end of the network. It doesn't use batch normalization layers (input and output have similar distributions, normalizing intermediate features may not be desirable) instead it uses constant scaling layers to ensure stable training. An L1 loss function (absolute error) is used instead of L2 (MSE), the authors showed better performance empirically and it requires less computation.\n\nThis is a base model (~5mb vs ~100mb) that includes just 16 ResBlocks and 64 channels.\n## Intended uses & limitations\nYou can use the pre-trained models for upscaling your images 2x, 3x and 4x. You can also use the trainer to train a model on your own dataset.\n### How to use\nThe model can be used with the [super_image](https://github.com/eugenesiow/super-image) library:\n```bash\npip install super-image\n```\nHere is how to use a pre-trained model to upscale your image:\n```python\nfrom super_image import EdsrModel, ImageLoader\nfrom PIL import Image\nimport requests\n\nurl = 'https://paperswithcode.com/media/datasets/Set5-0000002728-07a9793f_zA3bDjj.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel = EdsrModel.from_pretrained('eugenesiow/edsr-base', scale=2)      # scale 2, 3 and 4 models available\ninputs = ImageLoader.load_image(image)\npreds = model(inputs)\n\nImageLoader.save_image(preds, './scaled_2x.png')                        # save the output 2x scaled image to `./scaled_2x.png`\nImageLoader.save_compare(inputs, preds, './scaled_2x_compare.png')      # save an output comparing the super-image with a bicubic scaling\n```\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Upscale_Images_with_Pretrained_super_image_Models.ipynb \"Open in Colab\")\n## Training data\nThe models for 2x, 3x and 4x image super resolution were pretrained on [DIV2K](https://huggingface.co/datasets/eugenesiow/Div2k), a dataset of 800 high-quality (2K resolution) images for training, augmented to 4000 images and uses a dev set of  100 validation images (images numbered 801 to 900). \n## Training procedure\n### Preprocessing\nWe follow the pre-processing and training method of [Wang et al.](https://arxiv.org/abs/2104.07566).\nLow Resolution (LR) images are created by using bicubic interpolation as the resizing method to reduce the size of the High Resolution (HR) images by x2, x3 and x4 times.\nDuring training, RGB patches with size of 64√ó64 from the LR input are used together with their corresponding HR patches. \nData augmentation is applied to the training set in the pre-processing stage where five images are created from the four corners and center of the original image. \n\nWe need the huggingface [datasets](https://huggingface.co/datasets?filter=task_ids:other-other-image-super-resolution) library to download the data:\n```bash\npip install datasets\n```\nThe following code gets the data and preprocesses/augments the data.\n\n```python\nfrom datasets import load_dataset\nfrom super_image.data import EvalDataset, TrainDataset, augment_five_crop\n\naugmented_dataset = load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='train')\\\n    .map(augment_five_crop, batched=True, desc=\"Augmenting Dataset\")                                # download and augment the data with the five_crop method\ntrain_dataset = TrainDataset(augmented_dataset)                                                     # prepare the train dataset for loading PyTorch DataLoader\neval_dataset = EvalDataset(load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='validation'))      # prepare the eval dataset for the PyTorch DataLoader\n```\n### Pretraining\nThe model was trained on GPU. The training code is provided below:\n```python\nfrom super_image import Trainer, TrainingArguments, EdsrModel, EdsrConfig\n\ntraining_args = TrainingArguments(\n    output_dir='./results',                 # output directory\n    num_train_epochs=1000,                  # total number of training epochs\n)\n\nconfig = EdsrConfig(\n    scale=4,                                # train a model to upscale 4x\n)\nmodel = EdsrModel(config)\n\ntrainer = Trainer(\n    model=model,                         # the instantiated model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=eval_dataset            # evaluation dataset\n)\n\ntrainer.train()\n```\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Train_super_image_Models.ipynb \"Open in Colab\")\n## Evaluation results\nThe evaluation metrics include [PSNR](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio#Quality_estimation_with_PSNR) and [SSIM](https://en.wikipedia.org/wiki/Structural_similarity#Algorithm). \n\nEvaluation datasets include:\n- Set5 - [Bevilacqua et al. (2012)](https://huggingface.co/datasets/eugenesiow/Set5)\n- Set14 - [Zeyde et al. (2010)](https://huggingface.co/datasets/eugenesiow/Set14)\n- BSD100 - [Martin et al. (2001)](https://huggingface.co/datasets/eugenesiow/BSD100)\n- Urban100 - [Huang et al. (2015)](https://huggingface.co/datasets/eugenesiow/Urban100)\n\nThe results columns below are represented below as `PSNR/SSIM`. They are compared against a Bicubic baseline.\n\n|Dataset  \t    |Scale      |Bicubic  \t        |edsr-base  \t        |\n|---\t        |---\t    |---\t            |---\t                |\n|Set5  \t        |2x         |33.64/0.9292       |**38.02/0.9607**       |\n|Set5  \t        |3x  \t    |30.39/0.8678  \t    |**35.04/0.9403**  \t    |\n|Set5  \t        |4x  \t    |28.42/0.8101  \t    |**32.12/0.8947**       |\n|Set14  \t    |2x         |30.22/0.8683  \t    |**33.57/0.9172**  \t    |\n|Set14  \t    |3x         |27.53/0.7737  \t    |**30.93/0.8567**  \t    |\n|Set14  \t    |4x         |25.99/0.7023  \t    |**28.60/0.7815**  \t    |\n|BSD100  \t    |2x  \t    |29.55/0.8425  \t    |**32.21/0.8999**  \t    |\n|BSD100  \t    |3x  \t    |27.20/0.7382  \t    |**29.65/0.8204**  \t    |\n|BSD100  \t    |4x  \t    |25.96/0.6672  \t    |**27.61/0.7363**  \t    |\n|Urban100  \t    |2x  \t    |26.66/0.8408  \t    |**32.04/0.9276**  \t    |\n|Urban100  \t    |3x  \t    |  \t                |**29.23/0.8723**  \t    |\n|Urban100  \t    |4x  \t    |23.14/0.6573  \t    |**26.02/0.7832**  \t    |\n\n![Comparing Bicubic upscaling against x2 upscaling on Set5 Image 2](images/Set5_2_compare.png \"Comparing Bicubic upscaling against x2 upscaling on Set5 Image 2\")\n\nYou can find a notebook to easily run evaluation on pretrained models below:\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Evaluate_Pretrained_super_image_Models.ipynb \"Open in Colab\")\n created at: 2022-03-02\n- model_name: eugenesiow/msrn-bam\n  description: The goal of image super resolution is to restore a high resolution (HR) image from a single low resolution (LR) image. The image below shows the ground truth (HR), the bicubic upscaling x2 and model upscaling x2.\n\n![Comparing Bicubic upscaling against the models x4 upscaling on Set5 Image 4](images/msrn_4_4_compare.png \"Comparing Bicubic upscaling against the models x4 upscaling on Set5 Image 4\")\n## Model description\nThe MSRN model proposes a feature extraction structure called the multi-scale residual block. This module can \"adaptively detect image features at different scales\" and \"exploit the potential features of the image\".\n\nThis model also applies the balanced attention (BAM) method invented by [Wang et al. (2021)](https://arxiv.org/abs/2104.07566) to further improve the results.\n## Intended uses & limitations\nYou can use the pre-trained models for upscaling your images 2x, 3x and 4x. You can also use the trainer to train a model on your own dataset.\n### How to use\nThe model can be used with the [super_image](https://github.com/eugenesiow/super-image) library:\n```bash\npip install super-image\n```\nHere is how to use a pre-trained model to upscale your image:\n```python\nfrom super_image import MsrnModel, ImageLoader\nfrom PIL import Image\nimport requests\n\nurl = 'https://paperswithcode.com/media/datasets/Set5-0000002728-07a9793f_zA3bDjj.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel = MsrnModel.from_pretrained('eugenesiow/msrn-bam', scale=2)      # scale 2, 3 and 4 models available\ninputs = ImageLoader.load_image(image)\npreds = model(inputs)\n\nImageLoader.save_image(preds, './scaled_2x.png')                        # save the output 2x scaled image to `./scaled_2x.png`\nImageLoader.save_compare(inputs, preds, './scaled_2x_compare.png')      # save an output comparing the super-image with a bicubic scaling\n```\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Upscale_Images_with_Pretrained_super_image_Models.ipynb \"Open in Colab\")\n## Training data\nThe models for 2x, 3x and 4x image super resolution were pretrained on [DIV2K](https://huggingface.co/datasets/eugenesiow/Div2k), a dataset of 800 high-quality (2K resolution) images for training, augmented to 4000 images and uses a dev set of  100 validation images (images numbered 801 to 900). \n## Training procedure\n### Preprocessing\nWe follow the pre-processing and training method of [Wang et al.](https://arxiv.org/abs/2104.07566).\nLow Resolution (LR) images are created by using bicubic interpolation as the resizing method to reduce the size of the High Resolution (HR) images by x2, x3 and x4 times.\nDuring training, RGB patches with size of 64√ó64 from the LR input are used together with their corresponding HR patches. \nData augmentation is applied to the training set in the pre-processing stage where five images are created from the four corners and center of the original image. \n\nWe need the huggingface [datasets](https://huggingface.co/datasets?filter=task_ids:other-other-image-super-resolution) library to download the data:\n```bash\npip install datasets\n```\nThe following code gets the data and preprocesses/augments the data.\n\n```python\nfrom datasets import load_dataset\nfrom super_image.data import EvalDataset, TrainDataset, augment_five_crop\n\naugmented_dataset = load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='train')\\\n    .map(augment_five_crop, batched=True, desc=\"Augmenting Dataset\")                                # download and augment the data with the five_crop method\ntrain_dataset = TrainDataset(augmented_dataset)                                                     # prepare the train dataset for loading PyTorch DataLoader\neval_dataset = EvalDataset(load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='validation'))      # prepare the eval dataset for the PyTorch DataLoader\n```\n### Pretraining\nThe model was trained on GPU. The training code is provided below:\n```python\nfrom super_image import Trainer, TrainingArguments, MsrnModel, MsrnConfig\n\ntraining_args = TrainingArguments(\n    output_dir='./results',                 # output directory\n    num_train_epochs=1000,                  # total number of training epochs\n)\n\nconfig = MsrnConfig(\n    scale=4,                                # train a model to upscale 4x\n    bam=True,                               # apply balanced attention to the network\n)\nmodel = MsrnModel(config)\n\ntrainer = Trainer(\n    model=model,                         # the instantiated model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=eval_dataset            # evaluation dataset\n)\n\ntrainer.train()\n```\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Train_super_image_Models.ipynb \"Open in Colab\")\n## Evaluation results\nThe evaluation metrics include [PSNR](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio#Quality_estimation_with_PSNR) and [SSIM](https://en.wikipedia.org/wiki/Structural_similarity#Algorithm). \n\nEvaluation datasets include:\n- Set5 - [Bevilacqua et al. (2012)](https://huggingface.co/datasets/eugenesiow/Set5)\n- Set14 - [Zeyde et al. (2010)](https://huggingface.co/datasets/eugenesiow/Set14)\n- BSD100 - [Martin et al. (2001)](https://huggingface.co/datasets/eugenesiow/BSD100)\n- Urban100 - [Huang et al. (2015)](https://huggingface.co/datasets/eugenesiow/Urban100)\n\nThe results columns below are represented below as `PSNR/SSIM`. They are compared against a Bicubic baseline.\n\n|Dataset  \t    |Scale      |Bicubic  \t        |msrn-bam  \t                    |\n|---\t        |---\t    |---\t            |---\t                        |\n|Set5  \t        |2x         |33.64/0.9292       |**38.02/0.9608**       |\n|Set5  \t        |3x  \t    |30.39/0.8678  \t    |**35.13/0.9408**  \t    |\n|Set5  \t        |4x  \t    |28.42/0.8101  \t    |**32.26/0.8955**       |\n|Set14  \t    |2x         |30.22/0.8683  \t    |**33.73/0.9186**  \t    |\n|Set14  \t    |3x         |27.53/0.7737  \t    |**31.06/0.8588**  \t    |\n|Set14  \t    |4x         |25.99/0.7023  \t    |**28.78/0.7859**  \t    |\n|BSD100  \t    |2x  \t    |29.55/0.8425  \t    |**33.78/0.9253**  \t    |\n|BSD100  \t    |3x  \t    |27.20/0.7382  \t    |**29.65/0.8196**  \t    |\n|BSD100  \t    |4x  \t    |25.96/0.6672  \t    |**28.51/0.7651**  \t    |\n|Urban100  \t    |2x  \t    |26.66/0.8408  \t    |**32.08/0.9276**  \t    |\n|Urban100  \t    |3x  \t    |  \t                |**29.26/0.8736**  \t    |\n|Urban100  \t    |4x  \t    |23.14/0.6573  \t    |**26.10/0.7857**  \t    |\n\n![Comparing Bicubic upscaling against the models x4 upscaling on Set5 Image 2](images/msrn_2_4_compare.png \"Comparing Bicubic upscaling against the models x4 upscaling on Set5 Image 2\")\n\nYou can find a notebook to easily run evaluation on pretrained models below:\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Evaluate_Pretrained_super_image_Models.ipynb \"Open in Colab\")\n created at: 2022-03-02\n- model_name: ZhengPeng7/BiRefNet_HR-matting\n  description: this birefnet was trained with images in `2048x2048` for higher resolution image matting with transparency. all tested in fp16 mode. dataset method resolution maxfm wfmeasure mae smeasure meanem hce maxem meanfm adpem adpfm mba maxbiou meanbiou\n created at: 2025-02-12T16:19:23+00:00\n- model_name: PramaLLC/BEN2\n  description: ben2 background erase network introduces a novel approach to foreground segmentation through its innovative confidence guided matting cgm pipeline. the architecture employs a refiner network that targets and processes pixels where the base model exhibits lower confidence levels, resulting in more precise and reliable matting results. this model is built on ben: ben2 was trained on the dis5k and our 22k proprietary segmentation dataset. our enhanced model delivers superior performance in hair matting, 4k processing, object segmentation, and edge refinement. our base model is open source. to try the full model through our free web demo or integrate ben2 into your project with our api: - üåê\n created at: 2025-01-22T14:39:05+00:00\n- model_name: ZhengPeng7/BiRefNet_HR\n  description: this birefnet was trained with images in `2048x2048` for higher resolution inference. all tested in fp16 mode. dataset method resolution maxfm wfmeasure mae smeasure meanem hce maxem meanfm adpem adpfm mba maxbiou meanbiou\n created at: 2025-02-01T17:52:14+00:00\n- model_name: briaai/RMBG-2.0\n  description: rmbg v2.0 is our new state-of-the-art background removal model significantly improves rmbg v1.4. the model is designed to effectively separate foreground from background in a range of categories and image types. this model has been trained on a carefully selected dataset, which includes: general stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale. - developed by:  - model type: background removal - license:  - the model is released under a cc by-nc 4.0 license for non-commercial use. - commercial use is subject to a commercial agreement with bria. available  purchase: to purchase a commercial license simply click . - model description: bria rmbg-2.0 is a dichotomous image segmentation model trained exclusively on a professional-grade dataset. the model output includes a single-channel 8-bit grayscale alpha matte, where each pixel value indicates the opacity level of the corresponding pixel in the original image. this non-binary output approach offers developers the flexibility to define custom thresholds for foreground-background separation, catering to varied use cases requirements and enhancing integration into complex pipelines. - bria: resources for more information:\n created at: 2024-10-29T12:14:36+00:00\n- model_name: ZhengPeng7/BiRefNet_dynamic\n  description: bilateral reference for high-resolution dichotomous image segmentation an arbitrary shape adaptable birefnet for general segmentation. this model was trained on arbitrary shapes 256x256 2304x2304 and shows great robustness on inputs with any shape.\n created at: 2025-03-31T03:37:03+00:00\n- model_name: Ricky06662/Seg-Zero-7B\n  description: this model is based on the paper . it uses a decoupled architecture with a reasoning model and a segmentation model. it's trained via reinforcement learning using grpo without explicit reasoning data, leading to robust zero-shot generalization and emergent test-time reasoning. code: this is a seg-zero-7b model. it introduces a decoupled architecture consisting of a reasoning model and a segmentation model. the reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate pixel-level masks. this is a seg-zero-7b model. it introduces a decoupled architecture consisting of a reasoning model and a segmentation model. the reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate pixel-level masks.\n created at: 2025-03-07T15:08:24+00:00\n- model_name: cocktailpeanut/rm\n  description: rmbg v2.0 is our new state-of-the-art background removal model significantly improves rmbg v1.4. the model is designed to effectively separate foreground from background in a range of categories and image types. this model has been trained on a carefully selected dataset, which includes: general stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale. - developed by:  - model type: background removal - license:  - the model is released under a cc by-nc 4.0 license for non-commercial use. - commercial use is subject to a commercial agreement with bria. available  purchase: to purchase a commercial license simply click . - model description: bria rmbg-2.0 is a dichotomous image segmentation model trained exclusively on a professional-grade dataset. - bria: resources for more information:\n created at: 2025-04-12T06:16:07+00:00\n- model_name: yuvraj108c/RMBG-2.0\n  description: rmbg v2.0 is our new state-of-the-art background removal model significantly improves rmbg v1.4. the model is designed to effectively separate foreground from background in a range of categories and image types. this model has been trained on a carefully selected dataset, which includes: general stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale. - developed by:  - model type: background removal - license:  - the model is released under a cc by-nc 4.0 license for non-commercial use. - commercial use is subject to a commercial agreement with bria. available  purchase: to purchase a commercial license simply click . - model description: bria rmbg-2.0 is a dichotomous image segmentation model trained exclusively on a professional-grade dataset. the model output includes a single-channel 8-bit grayscale alpha matte, where each pixel value indicates the opacity level of the corresponding pixel in the original image. this non-binary output approach offers developers the flexibility to define custom thresholds for foreground-background separation, catering to varied use cases requirements and enhancing integration into complex pipelines. - bria: resources for more information:\n created at: 2025-07-28T10:18:49+00:00\n- model_name: Ricky06662/VisionReasoner-7B\n  description: this repository contains the visionreasoner-7b model, developed as part of the novel seg-zero framework, presented in the paper . this model is also associated with the paper . code: project page:  seg-zero is a novel framework that demonstrates remarkable generalizability and derives explicit chain-of-thought reasoning through cognitive reinforcement for reasoning segmentation. this visionreasoner-7b model employs a decoupled architecture consisting of a reasoning model and a segmentation model. the reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate precise pixel-level masks. trained exclusively via reinforcement learning with grpo and without explicit reasoning data, seg-zero achieves robust zero-shot generalization and exhibits emergent test-time reasoning capabilities. experiments show that seg-zero-7b achieves a zero-shot performance of 57.5 on the reasonseg benchmark, surpassing the prior lisa-7b by 18%. this significant improvement highlights seg-zero's ability to generalize across domains while presenting an explicit reasoning process.\n created at: 2025-05-18T05:46:01+00:00\n- model_name: tue-mps/coco_panoptic_eomt_large_640\n  description: eomt encoder-only mask transformer is a vision transformer vit architecture designed for high-quality and efficient image segmentation. it was introduced in the cvpr 2025 highlight paper: by tommie kerssies, niccol√≤ cavagnero, alexander hermans, narges norouzi, giuseppe averta, bastian leibe, gijs dubbelman, and daan de geus.\n created at: 2025-03-26T17:28:05+00:00\n- model_name: tue-mps/ade20k_semantic_eomt_large_512\n  description: eomt encoder-only mask transformer is a vision transformer vit architecture designed for high-quality and efficient image segmentation. it was introduced in the cvpr 2025 highlight paper: by tommie kerssies, niccol√≤ cavagnero, alexander hermans, narges norouzi, giuseppe averta, bastian leibe, gijs dubbelman, and daan de geus.\n created at: 2025-03-26T19:29:47+00:00\n- model_name: bwittmann/vesselFM\n  description: tl;dr : vesselfm is a foundation model for universal 3d blood vessel segmentation in arbitrary imaging domains. for details, please refer to our preprint and our github repo\n created at: 2024-12-02T12:45:08+00:00\n- model_name: Ricky06662/Seg-Zero-7B-Best-on-ReasonSegTest\n  description: this model is based on the paper . code: seg-zero introduces a novel framework for reasoning segmentation that addresses the limitations of traditional supervised fine-tuning methods, which often struggle with out-of-domain generalization and lack explicit reasoning processes. the framework features a decoupled architecture consisting of a reasoning model and a segmentation model. the reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate precise pixel-level masks.\n created at: 2025-04-09T16:12:22+00:00\n- model_name: tue-mps/coco_instance_eomt_large_640\n  description: eomt encoder-only mask transformer is a vision transformer vit architecture designed for high-quality and efficient image segmentation. it was introduced in the cvpr 2025 highlight paper: by tommie kerssies, niccol√≤ cavagnero, alexander hermans, narges norouzi, giuseppe averta, bastian leibe, gijs dubbelman, and daan de geus.\n created at: 2025-03-26T17:16:21+00:00\n- model_name: facebook/sapiens-seg-1b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T20:26:24+00:00\n- model_name: tue-mps/coco_instance_eomt_large_1280\n  description: eomt encoder-only mask transformer is a vision transformer vit architecture designed for high-quality and efficient image segmentation. it was introduced in the cvpr 2025 highlight paper: by tommie kerssies, niccol√≤ cavagnero, alexander hermans, narges norouzi, giuseppe averta, bastian leibe, gijs dubbelman, and daan de geus.\n created at: 2025-03-26T19:34:47+00:00\n- model_name: nevernever69/dit-doclaynet-segmentation\n  description: this model is a fine-tuned version of for document layout semantic segmentation on the dataset small subset: `nevernever69/small-doclaynet-v1.1`. it segments scanned document images into 11 layout categories such as title, paragraph, table, and footer. - segment document images into structured layout elements - assist in downstream tasks like document ocr, archiving, and automatic annotation\n created at: 2025-03-12T11:45:04+00:00\n- model_name: Pushti/BirefNetHR\n  description: this birefnet was trained with images in `2048x2048` for higher resolution inference. all tested in fp16 mode. dataset method resolution maxfm wfmeasure mae smeasure meanem hce maxem meanfm adpem adpfm mba maxbiou meanbiou\n created at: 2025-04-18T07:28:32+00:00\n- model_name: ZQL9711/RMBG-2-Matting\n  description: rmbg v2.0 is our new state-of-the-art background removal model significantly improves rmbg v1.4. the model is designed to effectively separate foreground from background in a range of categories and image types. this model has been trained on a carefully selected dataset, which includes: general stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale. - developed by:  - model type: background removal - license:  - the model is released under a cc by-nc 4.0 license for non-commercial use. - commercial use is subject to a commercial agreement with bria. available  purchase: to purchase a commercial license simply click . - model description: bria rmbg-2.0 is a dichotomous image segmentation model trained exclusively on a professional-grade dataset. - bria: resources for more information:\n created at: 2025-07-05T08:31:53+00:00\n- model_name: Acly/BiRefNet-GGUF\n  description: birefnet is a model for dichotomous image segmentation background removal. the weights in this repository are converted for lightweight inference on consumer hardware with .\n created at: 2025-07-20T10:33:31+00:00\n\nExamples:\n- example user query: Our company is building a software to analyze satellite images for land use. We need to segment different regions in the images.\n  model_name: nvidia/segformer-b0-finetuned-ade-512-512\n- example user query: Identify an API to perform semantic segmentation of the street labeled with road, sidewalk, and building for a given picture.\n  model_name: pytorch/vision/deeplabv3_resnet50\n- example user query: A drone is flying over the neighborhood and is recording video. How can I process the video and perform semantic segmentation on each frame?\n  model_name: pytorch/vision/deeplabv3_resnet101\n- example user query: To identify and separate different objects in an image, create an image segmentation model.\n  model_name: openmmlab/upernet-convnext-small\n- example user query: The company is developing a fashion application which needs to identify the clothing items in pictures.\n  model_name: mattmdjaga/segformer_b2_clothes\n- example user query: An urban development company wants to analyze city images to inform infrastructure plans. Extract features using a semantic segmentation model.\n  model_name: nvidia/segformer-b5-finetuned-cityscapes-1024-1024\n\nUser prompt:\nClassify every part in the picture.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_seg>\n\nCandidate Models:\n- model_name: CIDAS/clipseg-rd64-refined\n  description: CLIPSeg model with reduce dimension 64, refined (using a more complex convolution). It was introduced in the paper Image Segmentation Using Text and Image Prompts by L√ºddecke et al. and first released in this repository. This model is intended for zero-shot and one-shot image segmentation.\n created at: 2022-11-01\n- model_name: openmmlab/upernet-convnext-small\n  description: UperNet framework for semantic segmentation, leveraging a ConvNeXt backbone. UperNet was introduced in the paper Unified Perceptual Parsing for Scene Understanding by Xiao et al. Combining UperNet with a ConvNeXt backbone was introduced in the paper A ConvNet for the 2020s.\n created at: 2023-01-13\n- model_name: nvidia/segformer-b0-finetuned-ade-512-512\n  description: SegFormer model fine-tuned on ADE20k at resolution 512x512. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: nvidia/segformer-b5-finetuned-ade-640-640\n  description: SegFormer model fine-tuned on ADE20k at resolution 640x640. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: nvidia/segformer-b2-finetuned-cityscapes-1024-1024\n  description: SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: nvidia/segformer-b0-finetuned-cityscapes-1024-1024\n  description: SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: facebook/maskformer-swin-base-coco\n  description: MaskFormer model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.\n created at: 2022-03-02\n- model_name: facebook/mask2former-swin-large-cityscapes-semantic\n  description: Mask2Former model trained on Cityscapes semantic segmentation (large-sized version, Swin backbone). It addresses instance, semantic and panoptic segmentation by predicting a set of masks and corresponding labels. The model outperforms the previous SOTA, MaskFormer, in terms of performance and efficiency.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-base-coco-panoptic\n  description: Mask2Former model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.\n created at: 2023-01-02\n- model_name: mattmdjaga/segformer_b2_clothes\n  description: SegFormer model fine-tuned on ATR dataset for clothes segmentation.\n created at: 2022-11-24\n- model_name: shi-labs/oneformer_coco_swin_large\n  description: OneFormer model trained on the COCO dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.\n created at: 2022-11-15\n- model_name: facebook/mask2former-swin-large-coco-panoptic\n  description: Mask2Former model trained on COCO panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.\n created at: 2023-01-02\n- model_name: shi-labs/oneformer_ade20k_swin_large\n  description: OneFormer model trained on the ADE20k dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.\n created at: 2022-11-15\n- model_name: facebook/mask2former-swin-small-coco-instance\n  description: Mask2Former model trained on COCO instance segmentation (small-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.\n created at: 2022-12-26\n- model_name: nvidia/segformer-b5-finetuned-cityscapes-1024-1024\n  description: SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: shi-labs/oneformer_ade20k_swin_tiny\n  description: OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.\n created at: 2022-11-16\n- model_name: keremberke/yolov8m-building-segmentation\n  description: A YOLOv8 model for building segmentation in satellite images. It can detect and segment buildings in the input images.\n created at: 2023-01-27\n- model_name: facebook/maskformer-swin-base-ade\n  description: MaskFormer model trained on ADE20k semantic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.\n created at: 2022-03-02\n- model_name: facebook/mask2former-swin-tiny-coco-instance\n  description: Mask2Former model trained on COCO instance segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. You can use this particular checkpoint for instance segmentation.\n created at: 2022-12-23\n- model_name: keremberke/yolov8m-pcb-defect-segmentation\n  description: A YOLOv8 model for PCB defect segmentation trained on the pcb-defect-segmentation dataset. The model can detect and segment defects in PCB images, such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.\n created at: 2023-01-28\n- model_name: facebook/maskformer-swin-tiny-coco\n  description: MaskFormer model trained on COCO panoptic segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.\n created at: 2022-03-02\n- model_name: keremberke/yolov8m-pothole-segmentation\n  description: A YOLOv8 model for pothole segmentation trained on keremberke/pothole-segmentation dataset. It can detect potholes in images and provide segmentation masks for the detected potholes.\n created at: 2023-01-26\n- model_name: keremberke/yolov8s-building-segmentation\n  description: A YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can detect and segment buildings with high accuracy.\n created at: 2023-01-26\n- model_name: keremberke/yolov8s-pothole-segmentation\n  description: A YOLOv8 model for pothole segmentation. This model detects potholes in images and outputs bounding boxes and masks for the detected potholes.\n created at: 2023-01-26\n- model_name: keremberke/yolov8s-pcb-defect-segmentation\n  description: YOLOv8s model for PCB defect segmentation. The model is trained to detect and segment PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.\n created at: 2023-01-28\n- model_name: keremberke/yolov8n-pcb-defect-segmentation\n  description: A YOLOv8 model for detecting and segmenting PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.\n created at: 2023-01-28\n- model_name: keremberke/yolov8n-pothole-segmentation\n  description: A YOLOv8 model for pothole segmentation in images. The model is trained on the pothole-segmentation dataset and achieves high accuracy in detecting potholes.\n created at: 2023-01-15\n- model_name: facebook/maskformer-swin-large-ade\n  description: MaskFormer model trained on ADE20k semantic segmentation (large-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.\n created at: 2022-03-02\n- model_name: facebook/detr-resnet-50-panoptic\n  description: DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 panoptic (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: tensorflow/lite-model/deeplabv3/1/default\n  description: TensorFlow Hub provides pre-trained image segmentation models that can be loaded and used for various tasks such as semantic segmentation. The example code demonstrates how to load a DeepLabV3 model and perform segmentation on an input image tensor.\n created at: 2020-10-06\n- model_name: pytorch/vision/deeplabv3_resnet50\n  description: DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: pytorch/vision/deeplabv3_mobilenet_v3_large\n  description: DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: pytorch/vision/deeplabv3_resnet101\n  description: DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: mateuszbuda/brain-segmentation-pytorch/unet\n  description: U-Net with batch normalization for biomedical image segmentation with pretrained weights for abnormality segmentation in brain MRI. The model comprises four levels of blocks containing two convolutional layers with batch normalization and ReLU activation function, and one max pooling layer in the encoding part and up-convolutional layers instead in the decoding part. The number of convolutional filters in each block is 32, 64, 128, and 256. The bottleneck layer has 512 convolutional filters. From the encoding layers, skip connections are used to the corresponding layers in the decoding part. Input image is a 3-channel brain MRI slice from pre-contrast, FLAIR, and post-contrast sequences, respectively. Output is a one-channel probability map of abnormality regions with the same size as the input image.\n created at: 2019-05-26\n- model_name: pytorch/vision/fcn_resnet101\n  description: FCN-ResNet is a Fully-Convolutional Network model using a ResNet-50 or a ResNet-101 backbone. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: pytorch/fairseq\n  description: Transformer (NMT) is a powerful sequence-to-sequence modeling architecture that produces state-of-the-art neural machine translation systems. It is based on the paper 'Attention Is All You Need' and has been improved using techniques such as large-scale semi-supervised training, back-translation, and noisy-channel reranking. It supports English-French and English-German translation as well as round-trip translation for paraphrasing.\n created at: 2017-08-29\n- model_name: pytorch/vision/fcn_resnet50\n  description: FCN-ResNet is a Fully-Convolutional Network model using a ResNet-50 or a ResNet-101 backbone. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: facebook/mask2former-swin-large-ade-semantic\n  description: Mask2Former model trained on ADE20k semantic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-large-ade-panoptic\n  description: Mask2Former model trained on ADE20k panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-large-mapillary-vistas-panoptic\n  description: Mask2Former model trained on Mapillary Vistas panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-large-mapillary-vistas-semantic\n  description: Mask2Former model trained on Mapillary Vistas semantic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-large-cityscapes-panoptic\n  description: Mask2Former model trained on Cityscapes panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-03\n- model_name: shi-labs/oneformer_cityscapes_swin_large\n  description: OneFormer model trained on the Cityscapes dataset (large-sized version, Swin backbone). It was introduced in the paper [OneFormer: One Transformer to Rule Universal Image Segmentation](https://arxiv.org/abs/2211.06220) by Jain et al. and first released in [this repository](https://github.com/SHI-Labs/OneFormer).\n\n![model image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/oneformer_teaser.png)\n created at: 2022-11-15\n- model_name: google/deeplabv3_mobilenet_v2_1.0_513\n  description: MobileNet V2 model pre-trained on PASCAL VOC at resolution 513x513. It was introduced in [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381) by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen. It was first released in [this repository](https://github.com/tensorflow/models/tree/master/research/deeplab).\n\nDisclaimer: The team releasing MobileNet V2 did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2022-11-10\n- model_name: apple/deeplabv3-mobilevit-small\n  description: MobileViT model pre-trained on PASCAL VOC at resolution 512x512. It was introduced in [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer](https://arxiv.org/abs/2110.02178) by Sachin Mehta and Mohammad Rastegari, and first released in [this repository](https://github.com/apple/ml-cvnets). The license used is [Apple sample code license](https://github.com/apple/ml-cvnets/blob/main/LICENSE).\n\nDisclaimer: The team releasing MobileViT did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2022-05-30\n- model_name: eugenesiow/edsr-base\n  description: The goal of image super resolution is to restore a high resolution (HR) image from a single low resolution (LR) image. The image below shows the ground truth (HR), the bicubic upscaling x2 and EDSR upscaling x2.\n\n![Comparing Bicubic upscaling against EDSR x2 upscaling on Set5 Image 4](images/Set5_4_compare.png \"Comparing Bicubic upscaling against EDSR x2 upscaling on Set5 Image 4\")\n## Model description\nEDSR is a model that uses both deeper and wider architecture (32 ResBlocks and 256 channels) to improve performance. It uses both global and local skip connections, and up-scaling is done at the end of the network. It doesn't use batch normalization layers (input and output have similar distributions, normalizing intermediate features may not be desirable) instead it uses constant scaling layers to ensure stable training. An L1 loss function (absolute error) is used instead of L2 (MSE), the authors showed better performance empirically and it requires less computation.\n\nThis is a base model (~5mb vs ~100mb) that includes just 16 ResBlocks and 64 channels.\n## Intended uses & limitations\nYou can use the pre-trained models for upscaling your images 2x, 3x and 4x. You can also use the trainer to train a model on your own dataset.\n### How to use\nThe model can be used with the [super_image](https://github.com/eugenesiow/super-image) library:\n```bash\npip install super-image\n```\nHere is how to use a pre-trained model to upscale your image:\n```python\nfrom super_image import EdsrModel, ImageLoader\nfrom PIL import Image\nimport requests\n\nurl = 'https://paperswithcode.com/media/datasets/Set5-0000002728-07a9793f_zA3bDjj.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel = EdsrModel.from_pretrained('eugenesiow/edsr-base', scale=2)      # scale 2, 3 and 4 models available\ninputs = ImageLoader.load_image(image)\npreds = model(inputs)\n\nImageLoader.save_image(preds, './scaled_2x.png')                        # save the output 2x scaled image to `./scaled_2x.png`\nImageLoader.save_compare(inputs, preds, './scaled_2x_compare.png')      # save an output comparing the super-image with a bicubic scaling\n```\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Upscale_Images_with_Pretrained_super_image_Models.ipynb \"Open in Colab\")\n## Training data\nThe models for 2x, 3x and 4x image super resolution were pretrained on [DIV2K](https://huggingface.co/datasets/eugenesiow/Div2k), a dataset of 800 high-quality (2K resolution) images for training, augmented to 4000 images and uses a dev set of  100 validation images (images numbered 801 to 900). \n## Training procedure\n### Preprocessing\nWe follow the pre-processing and training method of [Wang et al.](https://arxiv.org/abs/2104.07566).\nLow Resolution (LR) images are created by using bicubic interpolation as the resizing method to reduce the size of the High Resolution (HR) images by x2, x3 and x4 times.\nDuring training, RGB patches with size of 64√ó64 from the LR input are used together with their corresponding HR patches. \nData augmentation is applied to the training set in the pre-processing stage where five images are created from the four corners and center of the original image. \n\nWe need the huggingface [datasets](https://huggingface.co/datasets?filter=task_ids:other-other-image-super-resolution) library to download the data:\n```bash\npip install datasets\n```\nThe following code gets the data and preprocesses/augments the data.\n\n```python\nfrom datasets import load_dataset\nfrom super_image.data import EvalDataset, TrainDataset, augment_five_crop\n\naugmented_dataset = load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='train')\\\n    .map(augment_five_crop, batched=True, desc=\"Augmenting Dataset\")                                # download and augment the data with the five_crop method\ntrain_dataset = TrainDataset(augmented_dataset)                                                     # prepare the train dataset for loading PyTorch DataLoader\neval_dataset = EvalDataset(load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='validation'))      # prepare the eval dataset for the PyTorch DataLoader\n```\n### Pretraining\nThe model was trained on GPU. The training code is provided below:\n```python\nfrom super_image import Trainer, TrainingArguments, EdsrModel, EdsrConfig\n\ntraining_args = TrainingArguments(\n    output_dir='./results',                 # output directory\n    num_train_epochs=1000,                  # total number of training epochs\n)\n\nconfig = EdsrConfig(\n    scale=4,                                # train a model to upscale 4x\n)\nmodel = EdsrModel(config)\n\ntrainer = Trainer(\n    model=model,                         # the instantiated model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=eval_dataset            # evaluation dataset\n)\n\ntrainer.train()\n```\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Train_super_image_Models.ipynb \"Open in Colab\")\n## Evaluation results\nThe evaluation metrics include [PSNR](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio#Quality_estimation_with_PSNR) and [SSIM](https://en.wikipedia.org/wiki/Structural_similarity#Algorithm). \n\nEvaluation datasets include:\n- Set5 - [Bevilacqua et al. (2012)](https://huggingface.co/datasets/eugenesiow/Set5)\n- Set14 - [Zeyde et al. (2010)](https://huggingface.co/datasets/eugenesiow/Set14)\n- BSD100 - [Martin et al. (2001)](https://huggingface.co/datasets/eugenesiow/BSD100)\n- Urban100 - [Huang et al. (2015)](https://huggingface.co/datasets/eugenesiow/Urban100)\n\nThe results columns below are represented below as `PSNR/SSIM`. They are compared against a Bicubic baseline.\n\n|Dataset  \t    |Scale      |Bicubic  \t        |edsr-base  \t        |\n|---\t        |---\t    |---\t            |---\t                |\n|Set5  \t        |2x         |33.64/0.9292       |**38.02/0.9607**       |\n|Set5  \t        |3x  \t    |30.39/0.8678  \t    |**35.04/0.9403**  \t    |\n|Set5  \t        |4x  \t    |28.42/0.8101  \t    |**32.12/0.8947**       |\n|Set14  \t    |2x         |30.22/0.8683  \t    |**33.57/0.9172**  \t    |\n|Set14  \t    |3x         |27.53/0.7737  \t    |**30.93/0.8567**  \t    |\n|Set14  \t    |4x         |25.99/0.7023  \t    |**28.60/0.7815**  \t    |\n|BSD100  \t    |2x  \t    |29.55/0.8425  \t    |**32.21/0.8999**  \t    |\n|BSD100  \t    |3x  \t    |27.20/0.7382  \t    |**29.65/0.8204**  \t    |\n|BSD100  \t    |4x  \t    |25.96/0.6672  \t    |**27.61/0.7363**  \t    |\n|Urban100  \t    |2x  \t    |26.66/0.8408  \t    |**32.04/0.9276**  \t    |\n|Urban100  \t    |3x  \t    |  \t                |**29.23/0.8723**  \t    |\n|Urban100  \t    |4x  \t    |23.14/0.6573  \t    |**26.02/0.7832**  \t    |\n\n![Comparing Bicubic upscaling against x2 upscaling on Set5 Image 2](images/Set5_2_compare.png \"Comparing Bicubic upscaling against x2 upscaling on Set5 Image 2\")\n\nYou can find a notebook to easily run evaluation on pretrained models below:\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Evaluate_Pretrained_super_image_Models.ipynb \"Open in Colab\")\n created at: 2022-03-02\n- model_name: eugenesiow/msrn-bam\n  description: The goal of image super resolution is to restore a high resolution (HR) image from a single low resolution (LR) image. The image below shows the ground truth (HR), the bicubic upscaling x2 and model upscaling x2.\n\n![Comparing Bicubic upscaling against the models x4 upscaling on Set5 Image 4](images/msrn_4_4_compare.png \"Comparing Bicubic upscaling against the models x4 upscaling on Set5 Image 4\")\n## Model description\nThe MSRN model proposes a feature extraction structure called the multi-scale residual block. This module can \"adaptively detect image features at different scales\" and \"exploit the potential features of the image\".\n\nThis model also applies the balanced attention (BAM) method invented by [Wang et al. (2021)](https://arxiv.org/abs/2104.07566) to further improve the results.\n## Intended uses & limitations\nYou can use the pre-trained models for upscaling your images 2x, 3x and 4x. You can also use the trainer to train a model on your own dataset.\n### How to use\nThe model can be used with the [super_image](https://github.com/eugenesiow/super-image) library:\n```bash\npip install super-image\n```\nHere is how to use a pre-trained model to upscale your image:\n```python\nfrom super_image import MsrnModel, ImageLoader\nfrom PIL import Image\nimport requests\n\nurl = 'https://paperswithcode.com/media/datasets/Set5-0000002728-07a9793f_zA3bDjj.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel = MsrnModel.from_pretrained('eugenesiow/msrn-bam', scale=2)      # scale 2, 3 and 4 models available\ninputs = ImageLoader.load_image(image)\npreds = model(inputs)\n\nImageLoader.save_image(preds, './scaled_2x.png')                        # save the output 2x scaled image to `./scaled_2x.png`\nImageLoader.save_compare(inputs, preds, './scaled_2x_compare.png')      # save an output comparing the super-image with a bicubic scaling\n```\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Upscale_Images_with_Pretrained_super_image_Models.ipynb \"Open in Colab\")\n## Training data\nThe models for 2x, 3x and 4x image super resolution were pretrained on [DIV2K](https://huggingface.co/datasets/eugenesiow/Div2k), a dataset of 800 high-quality (2K resolution) images for training, augmented to 4000 images and uses a dev set of  100 validation images (images numbered 801 to 900). \n## Training procedure\n### Preprocessing\nWe follow the pre-processing and training method of [Wang et al.](https://arxiv.org/abs/2104.07566).\nLow Resolution (LR) images are created by using bicubic interpolation as the resizing method to reduce the size of the High Resolution (HR) images by x2, x3 and x4 times.\nDuring training, RGB patches with size of 64√ó64 from the LR input are used together with their corresponding HR patches. \nData augmentation is applied to the training set in the pre-processing stage where five images are created from the four corners and center of the original image. \n\nWe need the huggingface [datasets](https://huggingface.co/datasets?filter=task_ids:other-other-image-super-resolution) library to download the data:\n```bash\npip install datasets\n```\nThe following code gets the data and preprocesses/augments the data.\n\n```python\nfrom datasets import load_dataset\nfrom super_image.data import EvalDataset, TrainDataset, augment_five_crop\n\naugmented_dataset = load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='train')\\\n    .map(augment_five_crop, batched=True, desc=\"Augmenting Dataset\")                                # download and augment the data with the five_crop method\ntrain_dataset = TrainDataset(augmented_dataset)                                                     # prepare the train dataset for loading PyTorch DataLoader\neval_dataset = EvalDataset(load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='validation'))      # prepare the eval dataset for the PyTorch DataLoader\n```\n### Pretraining\nThe model was trained on GPU. The training code is provided below:\n```python\nfrom super_image import Trainer, TrainingArguments, MsrnModel, MsrnConfig\n\ntraining_args = TrainingArguments(\n    output_dir='./results',                 # output directory\n    num_train_epochs=1000,                  # total number of training epochs\n)\n\nconfig = MsrnConfig(\n    scale=4,                                # train a model to upscale 4x\n    bam=True,                               # apply balanced attention to the network\n)\nmodel = MsrnModel(config)\n\ntrainer = Trainer(\n    model=model,                         # the instantiated model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=eval_dataset            # evaluation dataset\n)\n\ntrainer.train()\n```\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Train_super_image_Models.ipynb \"Open in Colab\")\n## Evaluation results\nThe evaluation metrics include [PSNR](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio#Quality_estimation_with_PSNR) and [SSIM](https://en.wikipedia.org/wiki/Structural_similarity#Algorithm). \n\nEvaluation datasets include:\n- Set5 - [Bevilacqua et al. (2012)](https://huggingface.co/datasets/eugenesiow/Set5)\n- Set14 - [Zeyde et al. (2010)](https://huggingface.co/datasets/eugenesiow/Set14)\n- BSD100 - [Martin et al. (2001)](https://huggingface.co/datasets/eugenesiow/BSD100)\n- Urban100 - [Huang et al. (2015)](https://huggingface.co/datasets/eugenesiow/Urban100)\n\nThe results columns below are represented below as `PSNR/SSIM`. They are compared against a Bicubic baseline.\n\n|Dataset  \t    |Scale      |Bicubic  \t        |msrn-bam  \t                    |\n|---\t        |---\t    |---\t            |---\t                        |\n|Set5  \t        |2x         |33.64/0.9292       |**38.02/0.9608**       |\n|Set5  \t        |3x  \t    |30.39/0.8678  \t    |**35.13/0.9408**  \t    |\n|Set5  \t        |4x  \t    |28.42/0.8101  \t    |**32.26/0.8955**       |\n|Set14  \t    |2x         |30.22/0.8683  \t    |**33.73/0.9186**  \t    |\n|Set14  \t    |3x         |27.53/0.7737  \t    |**31.06/0.8588**  \t    |\n|Set14  \t    |4x         |25.99/0.7023  \t    |**28.78/0.7859**  \t    |\n|BSD100  \t    |2x  \t    |29.55/0.8425  \t    |**33.78/0.9253**  \t    |\n|BSD100  \t    |3x  \t    |27.20/0.7382  \t    |**29.65/0.8196**  \t    |\n|BSD100  \t    |4x  \t    |25.96/0.6672  \t    |**28.51/0.7651**  \t    |\n|Urban100  \t    |2x  \t    |26.66/0.8408  \t    |**32.08/0.9276**  \t    |\n|Urban100  \t    |3x  \t    |  \t                |**29.26/0.8736**  \t    |\n|Urban100  \t    |4x  \t    |23.14/0.6573  \t    |**26.10/0.7857**  \t    |\n\n![Comparing Bicubic upscaling against the models x4 upscaling on Set5 Image 2](images/msrn_2_4_compare.png \"Comparing Bicubic upscaling against the models x4 upscaling on Set5 Image 2\")\n\nYou can find a notebook to easily run evaluation on pretrained models below:\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Evaluate_Pretrained_super_image_Models.ipynb \"Open in Colab\")\n created at: 2022-03-02\n- model_name: ZhengPeng7/BiRefNet_HR-matting\n  description: this birefnet was trained with images in `2048x2048` for higher resolution image matting with transparency. all tested in fp16 mode. dataset method resolution maxfm wfmeasure mae smeasure meanem hce maxem meanfm adpem adpfm mba maxbiou meanbiou\n created at: 2025-02-12T16:19:23+00:00\n- model_name: PramaLLC/BEN2\n  description: ben2 background erase network introduces a novel approach to foreground segmentation through its innovative confidence guided matting cgm pipeline. the architecture employs a refiner network that targets and processes pixels where the base model exhibits lower confidence levels, resulting in more precise and reliable matting results. this model is built on ben: ben2 was trained on the dis5k and our 22k proprietary segmentation dataset. our enhanced model delivers superior performance in hair matting, 4k processing, object segmentation, and edge refinement. our base model is open source. to try the full model through our free web demo or integrate ben2 into your project with our api: - üåê\n created at: 2025-01-22T14:39:05+00:00\n- model_name: ZhengPeng7/BiRefNet_HR\n  description: this birefnet was trained with images in `2048x2048` for higher resolution inference. all tested in fp16 mode. dataset method resolution maxfm wfmeasure mae smeasure meanem hce maxem meanfm adpem adpfm mba maxbiou meanbiou\n created at: 2025-02-01T17:52:14+00:00\n- model_name: briaai/RMBG-2.0\n  description: rmbg v2.0 is our new state-of-the-art background removal model significantly improves rmbg v1.4. the model is designed to effectively separate foreground from background in a range of categories and image types. this model has been trained on a carefully selected dataset, which includes: general stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale. - developed by:  - model type: background removal - license:  - the model is released under a cc by-nc 4.0 license for non-commercial use. - commercial use is subject to a commercial agreement with bria. available  purchase: to purchase a commercial license simply click . - model description: bria rmbg-2.0 is a dichotomous image segmentation model trained exclusively on a professional-grade dataset. the model output includes a single-channel 8-bit grayscale alpha matte, where each pixel value indicates the opacity level of the corresponding pixel in the original image. this non-binary output approach offers developers the flexibility to define custom thresholds for foreground-background separation, catering to varied use cases requirements and enhancing integration into complex pipelines. - bria: resources for more information:\n created at: 2024-10-29T12:14:36+00:00\n- model_name: ZhengPeng7/BiRefNet_dynamic\n  description: bilateral reference for high-resolution dichotomous image segmentation an arbitrary shape adaptable birefnet for general segmentation. this model was trained on arbitrary shapes 256x256 2304x2304 and shows great robustness on inputs with any shape.\n created at: 2025-03-31T03:37:03+00:00\n- model_name: Ricky06662/Seg-Zero-7B\n  description: this model is based on the paper . it uses a decoupled architecture with a reasoning model and a segmentation model. it's trained via reinforcement learning using grpo without explicit reasoning data, leading to robust zero-shot generalization and emergent test-time reasoning. code: this is a seg-zero-7b model. it introduces a decoupled architecture consisting of a reasoning model and a segmentation model. the reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate pixel-level masks. this is a seg-zero-7b model. it introduces a decoupled architecture consisting of a reasoning model and a segmentation model. the reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate pixel-level masks.\n created at: 2025-03-07T15:08:24+00:00\n- model_name: cocktailpeanut/rm\n  description: rmbg v2.0 is our new state-of-the-art background removal model significantly improves rmbg v1.4. the model is designed to effectively separate foreground from background in a range of categories and image types. this model has been trained on a carefully selected dataset, which includes: general stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale. - developed by:  - model type: background removal - license:  - the model is released under a cc by-nc 4.0 license for non-commercial use. - commercial use is subject to a commercial agreement with bria. available  purchase: to purchase a commercial license simply click . - model description: bria rmbg-2.0 is a dichotomous image segmentation model trained exclusively on a professional-grade dataset. - bria: resources for more information:\n created at: 2025-04-12T06:16:07+00:00\n- model_name: yuvraj108c/RMBG-2.0\n  description: rmbg v2.0 is our new state-of-the-art background removal model significantly improves rmbg v1.4. the model is designed to effectively separate foreground from background in a range of categories and image types. this model has been trained on a carefully selected dataset, which includes: general stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale. - developed by:  - model type: background removal - license:  - the model is released under a cc by-nc 4.0 license for non-commercial use. - commercial use is subject to a commercial agreement with bria. available  purchase: to purchase a commercial license simply click . - model description: bria rmbg-2.0 is a dichotomous image segmentation model trained exclusively on a professional-grade dataset. the model output includes a single-channel 8-bit grayscale alpha matte, where each pixel value indicates the opacity level of the corresponding pixel in the original image. this non-binary output approach offers developers the flexibility to define custom thresholds for foreground-background separation, catering to varied use cases requirements and enhancing integration into complex pipelines. - bria: resources for more information:\n created at: 2025-07-28T10:18:49+00:00\n- model_name: Ricky06662/VisionReasoner-7B\n  description: this repository contains the visionreasoner-7b model, developed as part of the novel seg-zero framework, presented in the paper . this model is also associated with the paper . code: project page:  seg-zero is a novel framework that demonstrates remarkable generalizability and derives explicit chain-of-thought reasoning through cognitive reinforcement for reasoning segmentation. this visionreasoner-7b model employs a decoupled architecture consisting of a reasoning model and a segmentation model. the reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate precise pixel-level masks. trained exclusively via reinforcement learning with grpo and without explicit reasoning data, seg-zero achieves robust zero-shot generalization and exhibits emergent test-time reasoning capabilities. experiments show that seg-zero-7b achieves a zero-shot performance of 57.5 on the reasonseg benchmark, surpassing the prior lisa-7b by 18%. this significant improvement highlights seg-zero's ability to generalize across domains while presenting an explicit reasoning process.\n created at: 2025-05-18T05:46:01+00:00\n- model_name: tue-mps/coco_panoptic_eomt_large_640\n  description: eomt encoder-only mask transformer is a vision transformer vit architecture designed for high-quality and efficient image segmentation. it was introduced in the cvpr 2025 highlight paper: by tommie kerssies, niccol√≤ cavagnero, alexander hermans, narges norouzi, giuseppe averta, bastian leibe, gijs dubbelman, and daan de geus.\n created at: 2025-03-26T17:28:05+00:00\n- model_name: tue-mps/ade20k_semantic_eomt_large_512\n  description: eomt encoder-only mask transformer is a vision transformer vit architecture designed for high-quality and efficient image segmentation. it was introduced in the cvpr 2025 highlight paper: by tommie kerssies, niccol√≤ cavagnero, alexander hermans, narges norouzi, giuseppe averta, bastian leibe, gijs dubbelman, and daan de geus.\n created at: 2025-03-26T19:29:47+00:00\n- model_name: bwittmann/vesselFM\n  description: tl;dr : vesselfm is a foundation model for universal 3d blood vessel segmentation in arbitrary imaging domains. for details, please refer to our preprint and our github repo\n created at: 2024-12-02T12:45:08+00:00\n- model_name: Ricky06662/Seg-Zero-7B-Best-on-ReasonSegTest\n  description: this model is based on the paper . code: seg-zero introduces a novel framework for reasoning segmentation that addresses the limitations of traditional supervised fine-tuning methods, which often struggle with out-of-domain generalization and lack explicit reasoning processes. the framework features a decoupled architecture consisting of a reasoning model and a segmentation model. the reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate precise pixel-level masks.\n created at: 2025-04-09T16:12:22+00:00\n- model_name: tue-mps/coco_instance_eomt_large_640\n  description: eomt encoder-only mask transformer is a vision transformer vit architecture designed for high-quality and efficient image segmentation. it was introduced in the cvpr 2025 highlight paper: by tommie kerssies, niccol√≤ cavagnero, alexander hermans, narges norouzi, giuseppe averta, bastian leibe, gijs dubbelman, and daan de geus.\n created at: 2025-03-26T17:16:21+00:00\n- model_name: facebook/sapiens-seg-1b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T20:26:24+00:00\n- model_name: tue-mps/coco_instance_eomt_large_1280\n  description: eomt encoder-only mask transformer is a vision transformer vit architecture designed for high-quality and efficient image segmentation. it was introduced in the cvpr 2025 highlight paper: by tommie kerssies, niccol√≤ cavagnero, alexander hermans, narges norouzi, giuseppe averta, bastian leibe, gijs dubbelman, and daan de geus.\n created at: 2025-03-26T19:34:47+00:00\n- model_name: nevernever69/dit-doclaynet-segmentation\n  description: this model is a fine-tuned version of for document layout semantic segmentation on the dataset small subset: `nevernever69/small-doclaynet-v1.1`. it segments scanned document images into 11 layout categories such as title, paragraph, table, and footer. - segment document images into structured layout elements - assist in downstream tasks like document ocr, archiving, and automatic annotation\n created at: 2025-03-12T11:45:04+00:00\n- model_name: Pushti/BirefNetHR\n  description: this birefnet was trained with images in `2048x2048` for higher resolution inference. all tested in fp16 mode. dataset method resolution maxfm wfmeasure mae smeasure meanem hce maxem meanfm adpem adpfm mba maxbiou meanbiou\n created at: 2025-04-18T07:28:32+00:00\n- model_name: ZQL9711/RMBG-2-Matting\n  description: rmbg v2.0 is our new state-of-the-art background removal model significantly improves rmbg v1.4. the model is designed to effectively separate foreground from background in a range of categories and image types. this model has been trained on a carefully selected dataset, which includes: general stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale. - developed by:  - model type: background removal - license:  - the model is released under a cc by-nc 4.0 license for non-commercial use. - commercial use is subject to a commercial agreement with bria. available  purchase: to purchase a commercial license simply click . - model description: bria rmbg-2.0 is a dichotomous image segmentation model trained exclusively on a professional-grade dataset. - bria: resources for more information:\n created at: 2025-07-05T08:31:53+00:00\n- model_name: Acly/BiRefNet-GGUF\n  description: birefnet is a model for dichotomous image segmentation background removal. the weights in this repository are converted for lightweight inference on consumer hardware with .\n created at: 2025-07-20T10:33:31+00:00\n\nExamples:\n- example user query: Our company is building a software to analyze satellite images for land use. We need to segment different regions in the images.\n  model_name: nvidia/segformer-b0-finetuned-ade-512-512\n- example user query: Identify an API to perform semantic segmentation of the street labeled with road, sidewalk, and building for a given picture.\n  model_name: pytorch/vision/deeplabv3_resnet50\n- example user query: A drone is flying over the neighborhood and is recording video. How can I process the video and perform semantic segmentation on each frame?\n  model_name: pytorch/vision/deeplabv3_resnet101\n- example user query: To identify and separate different objects in an image, create an image segmentation model.\n  model_name: openmmlab/upernet-convnext-small\n- example user query: The company is developing a fashion application which needs to identify the clothing items in pictures.\n  model_name: mattmdjaga/segformer_b2_clothes\n- example user query: An urban development company wants to analyze city images to inform infrastructure plans. Extract features using a semantic segmentation model.\n  model_name: nvidia/segformer-b5-finetuned-cityscapes-1024-1024\n\nUser prompt:\nClassify all components in this image.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_seg>\n\nCandidate Models:\n- model_name: CIDAS/clipseg-rd64-refined\n  description: CLIPSeg model with reduce dimension 64, refined (using a more complex convolution). It was introduced in the paper Image Segmentation Using Text and Image Prompts by L√ºddecke et al. and first released in this repository. This model is intended for zero-shot and one-shot image segmentation.\n created at: 2022-11-01\n- model_name: openmmlab/upernet-convnext-small\n  description: UperNet framework for semantic segmentation, leveraging a ConvNeXt backbone. UperNet was introduced in the paper Unified Perceptual Parsing for Scene Understanding by Xiao et al. Combining UperNet with a ConvNeXt backbone was introduced in the paper A ConvNet for the 2020s.\n created at: 2023-01-13\n- model_name: nvidia/segformer-b0-finetuned-ade-512-512\n  description: SegFormer model fine-tuned on ADE20k at resolution 512x512. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: nvidia/segformer-b5-finetuned-ade-640-640\n  description: SegFormer model fine-tuned on ADE20k at resolution 640x640. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: nvidia/segformer-b2-finetuned-cityscapes-1024-1024\n  description: SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: nvidia/segformer-b0-finetuned-cityscapes-1024-1024\n  description: SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: facebook/maskformer-swin-base-coco\n  description: MaskFormer model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.\n created at: 2022-03-02\n- model_name: facebook/mask2former-swin-large-cityscapes-semantic\n  description: Mask2Former model trained on Cityscapes semantic segmentation (large-sized version, Swin backbone). It addresses instance, semantic and panoptic segmentation by predicting a set of masks and corresponding labels. The model outperforms the previous SOTA, MaskFormer, in terms of performance and efficiency.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-base-coco-panoptic\n  description: Mask2Former model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.\n created at: 2023-01-02\n- model_name: mattmdjaga/segformer_b2_clothes\n  description: SegFormer model fine-tuned on ATR dataset for clothes segmentation.\n created at: 2022-11-24\n- model_name: shi-labs/oneformer_coco_swin_large\n  description: OneFormer model trained on the COCO dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.\n created at: 2022-11-15\n- model_name: facebook/mask2former-swin-large-coco-panoptic\n  description: Mask2Former model trained on COCO panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.\n created at: 2023-01-02\n- model_name: shi-labs/oneformer_ade20k_swin_large\n  description: OneFormer model trained on the ADE20k dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.\n created at: 2022-11-15\n- model_name: facebook/mask2former-swin-small-coco-instance\n  description: Mask2Former model trained on COCO instance segmentation (small-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.\n created at: 2022-12-26\n- model_name: nvidia/segformer-b5-finetuned-cityscapes-1024-1024\n  description: SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: shi-labs/oneformer_ade20k_swin_tiny\n  description: OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.\n created at: 2022-11-16\n- model_name: keremberke/yolov8m-building-segmentation\n  description: A YOLOv8 model for building segmentation in satellite images. It can detect and segment buildings in the input images.\n created at: 2023-01-27\n- model_name: facebook/maskformer-swin-base-ade\n  description: MaskFormer model trained on ADE20k semantic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.\n created at: 2022-03-02\n- model_name: facebook/mask2former-swin-tiny-coco-instance\n  description: Mask2Former model trained on COCO instance segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. You can use this particular checkpoint for instance segmentation.\n created at: 2022-12-23\n- model_name: keremberke/yolov8m-pcb-defect-segmentation\n  description: A YOLOv8 model for PCB defect segmentation trained on the pcb-defect-segmentation dataset. The model can detect and segment defects in PCB images, such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.\n created at: 2023-01-28\n- model_name: facebook/maskformer-swin-tiny-coco\n  description: MaskFormer model trained on COCO panoptic segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.\n created at: 2022-03-02\n- model_name: keremberke/yolov8m-pothole-segmentation\n  description: A YOLOv8 model for pothole segmentation trained on keremberke/pothole-segmentation dataset. It can detect potholes in images and provide segmentation masks for the detected potholes.\n created at: 2023-01-26\n- model_name: keremberke/yolov8s-building-segmentation\n  description: A YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can detect and segment buildings with high accuracy.\n created at: 2023-01-26\n- model_name: keremberke/yolov8s-pothole-segmentation\n  description: A YOLOv8 model for pothole segmentation. This model detects potholes in images and outputs bounding boxes and masks for the detected potholes.\n created at: 2023-01-26\n- model_name: keremberke/yolov8s-pcb-defect-segmentation\n  description: YOLOv8s model for PCB defect segmentation. The model is trained to detect and segment PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.\n created at: 2023-01-28\n- model_name: keremberke/yolov8n-pcb-defect-segmentation\n  description: A YOLOv8 model for detecting and segmenting PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.\n created at: 2023-01-28\n- model_name: keremberke/yolov8n-pothole-segmentation\n  description: A YOLOv8 model for pothole segmentation in images. The model is trained on the pothole-segmentation dataset and achieves high accuracy in detecting potholes.\n created at: 2023-01-15\n- model_name: facebook/maskformer-swin-large-ade\n  description: MaskFormer model trained on ADE20k semantic segmentation (large-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.\n created at: 2022-03-02\n- model_name: facebook/detr-resnet-50-panoptic\n  description: DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 panoptic (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: tensorflow/lite-model/deeplabv3/1/default\n  description: TensorFlow Hub provides pre-trained image segmentation models that can be loaded and used for various tasks such as semantic segmentation. The example code demonstrates how to load a DeepLabV3 model and perform segmentation on an input image tensor.\n created at: 2020-10-06\n- model_name: pytorch/vision/deeplabv3_resnet50\n  description: DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: pytorch/vision/deeplabv3_mobilenet_v3_large\n  description: DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: pytorch/vision/deeplabv3_resnet101\n  description: DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: mateuszbuda/brain-segmentation-pytorch/unet\n  description: U-Net with batch normalization for biomedical image segmentation with pretrained weights for abnormality segmentation in brain MRI. The model comprises four levels of blocks containing two convolutional layers with batch normalization and ReLU activation function, and one max pooling layer in the encoding part and up-convolutional layers instead in the decoding part. The number of convolutional filters in each block is 32, 64, 128, and 256. The bottleneck layer has 512 convolutional filters. From the encoding layers, skip connections are used to the corresponding layers in the decoding part. Input image is a 3-channel brain MRI slice from pre-contrast, FLAIR, and post-contrast sequences, respectively. Output is a one-channel probability map of abnormality regions with the same size as the input image.\n created at: 2019-05-26\n- model_name: pytorch/vision/fcn_resnet101\n  description: FCN-ResNet is a Fully-Convolutional Network model using a ResNet-50 or a ResNet-101 backbone. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: pytorch/fairseq\n  description: Transformer (NMT) is a powerful sequence-to-sequence modeling architecture that produces state-of-the-art neural machine translation systems. It is based on the paper 'Attention Is All You Need' and has been improved using techniques such as large-scale semi-supervised training, back-translation, and noisy-channel reranking. It supports English-French and English-German translation as well as round-trip translation for paraphrasing.\n created at: 2017-08-29\n- model_name: pytorch/vision/fcn_resnet50\n  description: FCN-ResNet is a Fully-Convolutional Network model using a ResNet-50 or a ResNet-101 backbone. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: facebook/mask2former-swin-large-ade-semantic\n  description: Mask2Former model trained on ADE20k semantic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-large-ade-panoptic\n  description: Mask2Former model trained on ADE20k panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-large-mapillary-vistas-panoptic\n  description: Mask2Former model trained on Mapillary Vistas panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-large-mapillary-vistas-semantic\n  description: Mask2Former model trained on Mapillary Vistas semantic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-large-cityscapes-panoptic\n  description: Mask2Former model trained on Cityscapes panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-03\n- model_name: shi-labs/oneformer_cityscapes_swin_large\n  description: OneFormer model trained on the Cityscapes dataset (large-sized version, Swin backbone). It was introduced in the paper [OneFormer: One Transformer to Rule Universal Image Segmentation](https://arxiv.org/abs/2211.06220) by Jain et al. and first released in [this repository](https://github.com/SHI-Labs/OneFormer).\n\n![model image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/oneformer_teaser.png)\n created at: 2022-11-15\n- model_name: google/deeplabv3_mobilenet_v2_1.0_513\n  description: MobileNet V2 model pre-trained on PASCAL VOC at resolution 513x513. It was introduced in [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381) by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen. It was first released in [this repository](https://github.com/tensorflow/models/tree/master/research/deeplab).\n\nDisclaimer: The team releasing MobileNet V2 did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2022-11-10\n- model_name: apple/deeplabv3-mobilevit-small\n  description: MobileViT model pre-trained on PASCAL VOC at resolution 512x512. It was introduced in [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer](https://arxiv.org/abs/2110.02178) by Sachin Mehta and Mohammad Rastegari, and first released in [this repository](https://github.com/apple/ml-cvnets). The license used is [Apple sample code license](https://github.com/apple/ml-cvnets/blob/main/LICENSE).\n\nDisclaimer: The team releasing MobileViT did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2022-05-30\n- model_name: eugenesiow/edsr-base\n  description: The goal of image super resolution is to restore a high resolution (HR) image from a single low resolution (LR) image. The image below shows the ground truth (HR), the bicubic upscaling x2 and EDSR upscaling x2.\n\n![Comparing Bicubic upscaling against EDSR x2 upscaling on Set5 Image 4](images/Set5_4_compare.png \"Comparing Bicubic upscaling against EDSR x2 upscaling on Set5 Image 4\")\n## Model description\nEDSR is a model that uses both deeper and wider architecture (32 ResBlocks and 256 channels) to improve performance. It uses both global and local skip connections, and up-scaling is done at the end of the network. It doesn't use batch normalization layers (input and output have similar distributions, normalizing intermediate features may not be desirable) instead it uses constant scaling layers to ensure stable training. An L1 loss function (absolute error) is used instead of L2 (MSE), the authors showed better performance empirically and it requires less computation.\n\nThis is a base model (~5mb vs ~100mb) that includes just 16 ResBlocks and 64 channels.\n## Intended uses & limitations\nYou can use the pre-trained models for upscaling your images 2x, 3x and 4x. You can also use the trainer to train a model on your own dataset.\n### How to use\nThe model can be used with the [super_image](https://github.com/eugenesiow/super-image) library:\n```bash\npip install super-image\n```\nHere is how to use a pre-trained model to upscale your image:\n```python\nfrom super_image import EdsrModel, ImageLoader\nfrom PIL import Image\nimport requests\n\nurl = 'https://paperswithcode.com/media/datasets/Set5-0000002728-07a9793f_zA3bDjj.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel = EdsrModel.from_pretrained('eugenesiow/edsr-base', scale=2)      # scale 2, 3 and 4 models available\ninputs = ImageLoader.load_image(image)\npreds = model(inputs)\n\nImageLoader.save_image(preds, './scaled_2x.png')                        # save the output 2x scaled image to `./scaled_2x.png`\nImageLoader.save_compare(inputs, preds, './scaled_2x_compare.png')      # save an output comparing the super-image with a bicubic scaling\n```\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Upscale_Images_with_Pretrained_super_image_Models.ipynb \"Open in Colab\")\n## Training data\nThe models for 2x, 3x and 4x image super resolution were pretrained on [DIV2K](https://huggingface.co/datasets/eugenesiow/Div2k), a dataset of 800 high-quality (2K resolution) images for training, augmented to 4000 images and uses a dev set of  100 validation images (images numbered 801 to 900). \n## Training procedure\n### Preprocessing\nWe follow the pre-processing and training method of [Wang et al.](https://arxiv.org/abs/2104.07566).\nLow Resolution (LR) images are created by using bicubic interpolation as the resizing method to reduce the size of the High Resolution (HR) images by x2, x3 and x4 times.\nDuring training, RGB patches with size of 64√ó64 from the LR input are used together with their corresponding HR patches. \nData augmentation is applied to the training set in the pre-processing stage where five images are created from the four corners and center of the original image. \n\nWe need the huggingface [datasets](https://huggingface.co/datasets?filter=task_ids:other-other-image-super-resolution) library to download the data:\n```bash\npip install datasets\n```\nThe following code gets the data and preprocesses/augments the data.\n\n```python\nfrom datasets import load_dataset\nfrom super_image.data import EvalDataset, TrainDataset, augment_five_crop\n\naugmented_dataset = load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='train')\\\n    .map(augment_five_crop, batched=True, desc=\"Augmenting Dataset\")                                # download and augment the data with the five_crop method\ntrain_dataset = TrainDataset(augmented_dataset)                                                     # prepare the train dataset for loading PyTorch DataLoader\neval_dataset = EvalDataset(load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='validation'))      # prepare the eval dataset for the PyTorch DataLoader\n```\n### Pretraining\nThe model was trained on GPU. The training code is provided below:\n```python\nfrom super_image import Trainer, TrainingArguments, EdsrModel, EdsrConfig\n\ntraining_args = TrainingArguments(\n    output_dir='./results',                 # output directory\n    num_train_epochs=1000,                  # total number of training epochs\n)\n\nconfig = EdsrConfig(\n    scale=4,                                # train a model to upscale 4x\n)\nmodel = EdsrModel(config)\n\ntrainer = Trainer(\n    model=model,                         # the instantiated model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=eval_dataset            # evaluation dataset\n)\n\ntrainer.train()\n```\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Train_super_image_Models.ipynb \"Open in Colab\")\n## Evaluation results\nThe evaluation metrics include [PSNR](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio#Quality_estimation_with_PSNR) and [SSIM](https://en.wikipedia.org/wiki/Structural_similarity#Algorithm). \n\nEvaluation datasets include:\n- Set5 - [Bevilacqua et al. (2012)](https://huggingface.co/datasets/eugenesiow/Set5)\n- Set14 - [Zeyde et al. (2010)](https://huggingface.co/datasets/eugenesiow/Set14)\n- BSD100 - [Martin et al. (2001)](https://huggingface.co/datasets/eugenesiow/BSD100)\n- Urban100 - [Huang et al. (2015)](https://huggingface.co/datasets/eugenesiow/Urban100)\n\nThe results columns below are represented below as `PSNR/SSIM`. They are compared against a Bicubic baseline.\n\n|Dataset  \t    |Scale      |Bicubic  \t        |edsr-base  \t        |\n|---\t        |---\t    |---\t            |---\t                |\n|Set5  \t        |2x         |33.64/0.9292       |**38.02/0.9607**       |\n|Set5  \t        |3x  \t    |30.39/0.8678  \t    |**35.04/0.9403**  \t    |\n|Set5  \t        |4x  \t    |28.42/0.8101  \t    |**32.12/0.8947**       |\n|Set14  \t    |2x         |30.22/0.8683  \t    |**33.57/0.9172**  \t    |\n|Set14  \t    |3x         |27.53/0.7737  \t    |**30.93/0.8567**  \t    |\n|Set14  \t    |4x         |25.99/0.7023  \t    |**28.60/0.7815**  \t    |\n|BSD100  \t    |2x  \t    |29.55/0.8425  \t    |**32.21/0.8999**  \t    |\n|BSD100  \t    |3x  \t    |27.20/0.7382  \t    |**29.65/0.8204**  \t    |\n|BSD100  \t    |4x  \t    |25.96/0.6672  \t    |**27.61/0.7363**  \t    |\n|Urban100  \t    |2x  \t    |26.66/0.8408  \t    |**32.04/0.9276**  \t    |\n|Urban100  \t    |3x  \t    |  \t                |**29.23/0.8723**  \t    |\n|Urban100  \t    |4x  \t    |23.14/0.6573  \t    |**26.02/0.7832**  \t    |\n\n![Comparing Bicubic upscaling against x2 upscaling on Set5 Image 2](images/Set5_2_compare.png \"Comparing Bicubic upscaling against x2 upscaling on Set5 Image 2\")\n\nYou can find a notebook to easily run evaluation on pretrained models below:\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Evaluate_Pretrained_super_image_Models.ipynb \"Open in Colab\")\n created at: 2022-03-02\n- model_name: eugenesiow/msrn-bam\n  description: The goal of image super resolution is to restore a high resolution (HR) image from a single low resolution (LR) image. The image below shows the ground truth (HR), the bicubic upscaling x2 and model upscaling x2.\n\n![Comparing Bicubic upscaling against the models x4 upscaling on Set5 Image 4](images/msrn_4_4_compare.png \"Comparing Bicubic upscaling against the models x4 upscaling on Set5 Image 4\")\n## Model description\nThe MSRN model proposes a feature extraction structure called the multi-scale residual block. This module can \"adaptively detect image features at different scales\" and \"exploit the potential features of the image\".\n\nThis model also applies the balanced attention (BAM) method invented by [Wang et al. (2021)](https://arxiv.org/abs/2104.07566) to further improve the results.\n## Intended uses & limitations\nYou can use the pre-trained models for upscaling your images 2x, 3x and 4x. You can also use the trainer to train a model on your own dataset.\n### How to use\nThe model can be used with the [super_image](https://github.com/eugenesiow/super-image) library:\n```bash\npip install super-image\n```\nHere is how to use a pre-trained model to upscale your image:\n```python\nfrom super_image import MsrnModel, ImageLoader\nfrom PIL import Image\nimport requests\n\nurl = 'https://paperswithcode.com/media/datasets/Set5-0000002728-07a9793f_zA3bDjj.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel = MsrnModel.from_pretrained('eugenesiow/msrn-bam', scale=2)      # scale 2, 3 and 4 models available\ninputs = ImageLoader.load_image(image)\npreds = model(inputs)\n\nImageLoader.save_image(preds, './scaled_2x.png')                        # save the output 2x scaled image to `./scaled_2x.png`\nImageLoader.save_compare(inputs, preds, './scaled_2x_compare.png')      # save an output comparing the super-image with a bicubic scaling\n```\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Upscale_Images_with_Pretrained_super_image_Models.ipynb \"Open in Colab\")\n## Training data\nThe models for 2x, 3x and 4x image super resolution were pretrained on [DIV2K](https://huggingface.co/datasets/eugenesiow/Div2k), a dataset of 800 high-quality (2K resolution) images for training, augmented to 4000 images and uses a dev set of  100 validation images (images numbered 801 to 900). \n## Training procedure\n### Preprocessing\nWe follow the pre-processing and training method of [Wang et al.](https://arxiv.org/abs/2104.07566).\nLow Resolution (LR) images are created by using bicubic interpolation as the resizing method to reduce the size of the High Resolution (HR) images by x2, x3 and x4 times.\nDuring training, RGB patches with size of 64√ó64 from the LR input are used together with their corresponding HR patches. \nData augmentation is applied to the training set in the pre-processing stage where five images are created from the four corners and center of the original image. \n\nWe need the huggingface [datasets](https://huggingface.co/datasets?filter=task_ids:other-other-image-super-resolution) library to download the data:\n```bash\npip install datasets\n```\nThe following code gets the data and preprocesses/augments the data.\n\n```python\nfrom datasets import load_dataset\nfrom super_image.data import EvalDataset, TrainDataset, augment_five_crop\n\naugmented_dataset = load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='train')\\\n    .map(augment_five_crop, batched=True, desc=\"Augmenting Dataset\")                                # download and augment the data with the five_crop method\ntrain_dataset = TrainDataset(augmented_dataset)                                                     # prepare the train dataset for loading PyTorch DataLoader\neval_dataset = EvalDataset(load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='validation'))      # prepare the eval dataset for the PyTorch DataLoader\n```\n### Pretraining\nThe model was trained on GPU. The training code is provided below:\n```python\nfrom super_image import Trainer, TrainingArguments, MsrnModel, MsrnConfig\n\ntraining_args = TrainingArguments(\n    output_dir='./results',                 # output directory\n    num_train_epochs=1000,                  # total number of training epochs\n)\n\nconfig = MsrnConfig(\n    scale=4,                                # train a model to upscale 4x\n    bam=True,                               # apply balanced attention to the network\n)\nmodel = MsrnModel(config)\n\ntrainer = Trainer(\n    model=model,                         # the instantiated model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=eval_dataset            # evaluation dataset\n)\n\ntrainer.train()\n```\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Train_super_image_Models.ipynb \"Open in Colab\")\n## Evaluation results\nThe evaluation metrics include [PSNR](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio#Quality_estimation_with_PSNR) and [SSIM](https://en.wikipedia.org/wiki/Structural_similarity#Algorithm). \n\nEvaluation datasets include:\n- Set5 - [Bevilacqua et al. (2012)](https://huggingface.co/datasets/eugenesiow/Set5)\n- Set14 - [Zeyde et al. (2010)](https://huggingface.co/datasets/eugenesiow/Set14)\n- BSD100 - [Martin et al. (2001)](https://huggingface.co/datasets/eugenesiow/BSD100)\n- Urban100 - [Huang et al. (2015)](https://huggingface.co/datasets/eugenesiow/Urban100)\n\nThe results columns below are represented below as `PSNR/SSIM`. They are compared against a Bicubic baseline.\n\n|Dataset  \t    |Scale      |Bicubic  \t        |msrn-bam  \t                    |\n|---\t        |---\t    |---\t            |---\t                        |\n|Set5  \t        |2x         |33.64/0.9292       |**38.02/0.9608**       |\n|Set5  \t        |3x  \t    |30.39/0.8678  \t    |**35.13/0.9408**  \t    |\n|Set5  \t        |4x  \t    |28.42/0.8101  \t    |**32.26/0.8955**       |\n|Set14  \t    |2x         |30.22/0.8683  \t    |**33.73/0.9186**  \t    |\n|Set14  \t    |3x         |27.53/0.7737  \t    |**31.06/0.8588**  \t    |\n|Set14  \t    |4x         |25.99/0.7023  \t    |**28.78/0.7859**  \t    |\n|BSD100  \t    |2x  \t    |29.55/0.8425  \t    |**33.78/0.9253**  \t    |\n|BSD100  \t    |3x  \t    |27.20/0.7382  \t    |**29.65/0.8196**  \t    |\n|BSD100  \t    |4x  \t    |25.96/0.6672  \t    |**28.51/0.7651**  \t    |\n|Urban100  \t    |2x  \t    |26.66/0.8408  \t    |**32.08/0.9276**  \t    |\n|Urban100  \t    |3x  \t    |  \t                |**29.26/0.8736**  \t    |\n|Urban100  \t    |4x  \t    |23.14/0.6573  \t    |**26.10/0.7857**  \t    |\n\n![Comparing Bicubic upscaling against the models x4 upscaling on Set5 Image 2](images/msrn_2_4_compare.png \"Comparing Bicubic upscaling against the models x4 upscaling on Set5 Image 2\")\n\nYou can find a notebook to easily run evaluation on pretrained models below:\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Evaluate_Pretrained_super_image_Models.ipynb \"Open in Colab\")\n created at: 2022-03-02\n- model_name: ZhengPeng7/BiRefNet_HR-matting\n  description: this birefnet was trained with images in `2048x2048` for higher resolution image matting with transparency. all tested in fp16 mode. dataset method resolution maxfm wfmeasure mae smeasure meanem hce maxem meanfm adpem adpfm mba maxbiou meanbiou\n created at: 2025-02-12T16:19:23+00:00\n- model_name: PramaLLC/BEN2\n  description: ben2 background erase network introduces a novel approach to foreground segmentation through its innovative confidence guided matting cgm pipeline. the architecture employs a refiner network that targets and processes pixels where the base model exhibits lower confidence levels, resulting in more precise and reliable matting results. this model is built on ben: ben2 was trained on the dis5k and our 22k proprietary segmentation dataset. our enhanced model delivers superior performance in hair matting, 4k processing, object segmentation, and edge refinement. our base model is open source. to try the full model through our free web demo or integrate ben2 into your project with our api: - üåê\n created at: 2025-01-22T14:39:05+00:00\n- model_name: ZhengPeng7/BiRefNet_HR\n  description: this birefnet was trained with images in `2048x2048` for higher resolution inference. all tested in fp16 mode. dataset method resolution maxfm wfmeasure mae smeasure meanem hce maxem meanfm adpem adpfm mba maxbiou meanbiou\n created at: 2025-02-01T17:52:14+00:00\n- model_name: briaai/RMBG-2.0\n  description: rmbg v2.0 is our new state-of-the-art background removal model significantly improves rmbg v1.4. the model is designed to effectively separate foreground from background in a range of categories and image types. this model has been trained on a carefully selected dataset, which includes: general stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale. - developed by:  - model type: background removal - license:  - the model is released under a cc by-nc 4.0 license for non-commercial use. - commercial use is subject to a commercial agreement with bria. available  purchase: to purchase a commercial license simply click . - model description: bria rmbg-2.0 is a dichotomous image segmentation model trained exclusively on a professional-grade dataset. the model output includes a single-channel 8-bit grayscale alpha matte, where each pixel value indicates the opacity level of the corresponding pixel in the original image. this non-binary output approach offers developers the flexibility to define custom thresholds for foreground-background separation, catering to varied use cases requirements and enhancing integration into complex pipelines. - bria: resources for more information:\n created at: 2024-10-29T12:14:36+00:00\n- model_name: ZhengPeng7/BiRefNet_dynamic\n  description: bilateral reference for high-resolution dichotomous image segmentation an arbitrary shape adaptable birefnet for general segmentation. this model was trained on arbitrary shapes 256x256 2304x2304 and shows great robustness on inputs with any shape.\n created at: 2025-03-31T03:37:03+00:00\n- model_name: Ricky06662/Seg-Zero-7B\n  description: this model is based on the paper . it uses a decoupled architecture with a reasoning model and a segmentation model. it's trained via reinforcement learning using grpo without explicit reasoning data, leading to robust zero-shot generalization and emergent test-time reasoning. code: this is a seg-zero-7b model. it introduces a decoupled architecture consisting of a reasoning model and a segmentation model. the reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate pixel-level masks. this is a seg-zero-7b model. it introduces a decoupled architecture consisting of a reasoning model and a segmentation model. the reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate pixel-level masks.\n created at: 2025-03-07T15:08:24+00:00\n- model_name: cocktailpeanut/rm\n  description: rmbg v2.0 is our new state-of-the-art background removal model significantly improves rmbg v1.4. the model is designed to effectively separate foreground from background in a range of categories and image types. this model has been trained on a carefully selected dataset, which includes: general stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale. - developed by:  - model type: background removal - license:  - the model is released under a cc by-nc 4.0 license for non-commercial use. - commercial use is subject to a commercial agreement with bria. available  purchase: to purchase a commercial license simply click . - model description: bria rmbg-2.0 is a dichotomous image segmentation model trained exclusively on a professional-grade dataset. - bria: resources for more information:\n created at: 2025-04-12T06:16:07+00:00\n- model_name: yuvraj108c/RMBG-2.0\n  description: rmbg v2.0 is our new state-of-the-art background removal model significantly improves rmbg v1.4. the model is designed to effectively separate foreground from background in a range of categories and image types. this model has been trained on a carefully selected dataset, which includes: general stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale. - developed by:  - model type: background removal - license:  - the model is released under a cc by-nc 4.0 license for non-commercial use. - commercial use is subject to a commercial agreement with bria. available  purchase: to purchase a commercial license simply click . - model description: bria rmbg-2.0 is a dichotomous image segmentation model trained exclusively on a professional-grade dataset. the model output includes a single-channel 8-bit grayscale alpha matte, where each pixel value indicates the opacity level of the corresponding pixel in the original image. this non-binary output approach offers developers the flexibility to define custom thresholds for foreground-background separation, catering to varied use cases requirements and enhancing integration into complex pipelines. - bria: resources for more information:\n created at: 2025-07-28T10:18:49+00:00\n- model_name: Ricky06662/VisionReasoner-7B\n  description: this repository contains the visionreasoner-7b model, developed as part of the novel seg-zero framework, presented in the paper . this model is also associated with the paper . code: project page:  seg-zero is a novel framework that demonstrates remarkable generalizability and derives explicit chain-of-thought reasoning through cognitive reinforcement for reasoning segmentation. this visionreasoner-7b model employs a decoupled architecture consisting of a reasoning model and a segmentation model. the reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate precise pixel-level masks. trained exclusively via reinforcement learning with grpo and without explicit reasoning data, seg-zero achieves robust zero-shot generalization and exhibits emergent test-time reasoning capabilities. experiments show that seg-zero-7b achieves a zero-shot performance of 57.5 on the reasonseg benchmark, surpassing the prior lisa-7b by 18%. this significant improvement highlights seg-zero's ability to generalize across domains while presenting an explicit reasoning process.\n created at: 2025-05-18T05:46:01+00:00\n- model_name: tue-mps/coco_panoptic_eomt_large_640\n  description: eomt encoder-only mask transformer is a vision transformer vit architecture designed for high-quality and efficient image segmentation. it was introduced in the cvpr 2025 highlight paper: by tommie kerssies, niccol√≤ cavagnero, alexander hermans, narges norouzi, giuseppe averta, bastian leibe, gijs dubbelman, and daan de geus.\n created at: 2025-03-26T17:28:05+00:00\n- model_name: tue-mps/ade20k_semantic_eomt_large_512\n  description: eomt encoder-only mask transformer is a vision transformer vit architecture designed for high-quality and efficient image segmentation. it was introduced in the cvpr 2025 highlight paper: by tommie kerssies, niccol√≤ cavagnero, alexander hermans, narges norouzi, giuseppe averta, bastian leibe, gijs dubbelman, and daan de geus.\n created at: 2025-03-26T19:29:47+00:00\n- model_name: bwittmann/vesselFM\n  description: tl;dr : vesselfm is a foundation model for universal 3d blood vessel segmentation in arbitrary imaging domains. for details, please refer to our preprint and our github repo\n created at: 2024-12-02T12:45:08+00:00\n- model_name: Ricky06662/Seg-Zero-7B-Best-on-ReasonSegTest\n  description: this model is based on the paper . code: seg-zero introduces a novel framework for reasoning segmentation that addresses the limitations of traditional supervised fine-tuning methods, which often struggle with out-of-domain generalization and lack explicit reasoning processes. the framework features a decoupled architecture consisting of a reasoning model and a segmentation model. the reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate precise pixel-level masks.\n created at: 2025-04-09T16:12:22+00:00\n- model_name: tue-mps/coco_instance_eomt_large_640\n  description: eomt encoder-only mask transformer is a vision transformer vit architecture designed for high-quality and efficient image segmentation. it was introduced in the cvpr 2025 highlight paper: by tommie kerssies, niccol√≤ cavagnero, alexander hermans, narges norouzi, giuseppe averta, bastian leibe, gijs dubbelman, and daan de geus.\n created at: 2025-03-26T17:16:21+00:00\n- model_name: facebook/sapiens-seg-1b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T20:26:24+00:00\n- model_name: tue-mps/coco_instance_eomt_large_1280\n  description: eomt encoder-only mask transformer is a vision transformer vit architecture designed for high-quality and efficient image segmentation. it was introduced in the cvpr 2025 highlight paper: by tommie kerssies, niccol√≤ cavagnero, alexander hermans, narges norouzi, giuseppe averta, bastian leibe, gijs dubbelman, and daan de geus.\n created at: 2025-03-26T19:34:47+00:00\n- model_name: nevernever69/dit-doclaynet-segmentation\n  description: this model is a fine-tuned version of for document layout semantic segmentation on the dataset small subset: `nevernever69/small-doclaynet-v1.1`. it segments scanned document images into 11 layout categories such as title, paragraph, table, and footer. - segment document images into structured layout elements - assist in downstream tasks like document ocr, archiving, and automatic annotation\n created at: 2025-03-12T11:45:04+00:00\n- model_name: Pushti/BirefNetHR\n  description: this birefnet was trained with images in `2048x2048` for higher resolution inference. all tested in fp16 mode. dataset method resolution maxfm wfmeasure mae smeasure meanem hce maxem meanfm adpem adpfm mba maxbiou meanbiou\n created at: 2025-04-18T07:28:32+00:00\n- model_name: ZQL9711/RMBG-2-Matting\n  description: rmbg v2.0 is our new state-of-the-art background removal model significantly improves rmbg v1.4. the model is designed to effectively separate foreground from background in a range of categories and image types. this model has been trained on a carefully selected dataset, which includes: general stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale. - developed by:  - model type: background removal - license:  - the model is released under a cc by-nc 4.0 license for non-commercial use. - commercial use is subject to a commercial agreement with bria. available  purchase: to purchase a commercial license simply click . - model description: bria rmbg-2.0 is a dichotomous image segmentation model trained exclusively on a professional-grade dataset. - bria: resources for more information:\n created at: 2025-07-05T08:31:53+00:00\n- model_name: Acly/BiRefNet-GGUF\n  description: birefnet is a model for dichotomous image segmentation background removal. the weights in this repository are converted for lightweight inference on consumer hardware with .\n created at: 2025-07-20T10:33:31+00:00\n\nExamples:\n- example user query: Our company is building a software to analyze satellite images for land use. We need to segment different regions in the images.\n  model_name: nvidia/segformer-b0-finetuned-ade-512-512\n- example user query: Identify an API to perform semantic segmentation of the street labeled with road, sidewalk, and building for a given picture.\n  model_name: pytorch/vision/deeplabv3_resnet50\n- example user query: A drone is flying over the neighborhood and is recording video. How can I process the video and perform semantic segmentation on each frame?\n  model_name: pytorch/vision/deeplabv3_resnet101\n- example user query: To identify and separate different objects in an image, create an image segmentation model.\n  model_name: openmmlab/upernet-convnext-small\n- example user query: The company is developing a fashion application which needs to identify the clothing items in pictures.\n  model_name: mattmdjaga/segformer_b2_clothes\n- example user query: An urban development company wants to analyze city images to inform infrastructure plans. Extract features using a semantic segmentation model.\n  model_name: nvidia/segformer-b5-finetuned-cityscapes-1024-1024\n\nUser prompt:\nI fervently desire you to strip everything within this illustration.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_seg>\n\nCandidate Models:\n- model_name: CIDAS/clipseg-rd64-refined\n  description: CLIPSeg model with reduce dimension 64, refined (using a more complex convolution). It was introduced in the paper Image Segmentation Using Text and Image Prompts by L√ºddecke et al. and first released in this repository. This model is intended for zero-shot and one-shot image segmentation.\n created at: 2022-11-01\n- model_name: openmmlab/upernet-convnext-small\n  description: UperNet framework for semantic segmentation, leveraging a ConvNeXt backbone. UperNet was introduced in the paper Unified Perceptual Parsing for Scene Understanding by Xiao et al. Combining UperNet with a ConvNeXt backbone was introduced in the paper A ConvNet for the 2020s.\n created at: 2023-01-13\n- model_name: nvidia/segformer-b0-finetuned-ade-512-512\n  description: SegFormer model fine-tuned on ADE20k at resolution 512x512. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: nvidia/segformer-b5-finetuned-ade-640-640\n  description: SegFormer model fine-tuned on ADE20k at resolution 640x640. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: nvidia/segformer-b2-finetuned-cityscapes-1024-1024\n  description: SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: nvidia/segformer-b0-finetuned-cityscapes-1024-1024\n  description: SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: facebook/maskformer-swin-base-coco\n  description: MaskFormer model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.\n created at: 2022-03-02\n- model_name: facebook/mask2former-swin-large-cityscapes-semantic\n  description: Mask2Former model trained on Cityscapes semantic segmentation (large-sized version, Swin backbone). It addresses instance, semantic and panoptic segmentation by predicting a set of masks and corresponding labels. The model outperforms the previous SOTA, MaskFormer, in terms of performance and efficiency.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-base-coco-panoptic\n  description: Mask2Former model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.\n created at: 2023-01-02\n- model_name: mattmdjaga/segformer_b2_clothes\n  description: SegFormer model fine-tuned on ATR dataset for clothes segmentation.\n created at: 2022-11-24\n- model_name: shi-labs/oneformer_coco_swin_large\n  description: OneFormer model trained on the COCO dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.\n created at: 2022-11-15\n- model_name: facebook/mask2former-swin-large-coco-panoptic\n  description: Mask2Former model trained on COCO panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.\n created at: 2023-01-02\n- model_name: shi-labs/oneformer_ade20k_swin_large\n  description: OneFormer model trained on the ADE20k dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.\n created at: 2022-11-15\n- model_name: facebook/mask2former-swin-small-coco-instance\n  description: Mask2Former model trained on COCO instance segmentation (small-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.\n created at: 2022-12-26\n- model_name: nvidia/segformer-b5-finetuned-cityscapes-1024-1024\n  description: SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: shi-labs/oneformer_ade20k_swin_tiny\n  description: OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.\n created at: 2022-11-16\n- model_name: keremberke/yolov8m-building-segmentation\n  description: A YOLOv8 model for building segmentation in satellite images. It can detect and segment buildings in the input images.\n created at: 2023-01-27\n- model_name: facebook/maskformer-swin-base-ade\n  description: MaskFormer model trained on ADE20k semantic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.\n created at: 2022-03-02\n- model_name: facebook/mask2former-swin-tiny-coco-instance\n  description: Mask2Former model trained on COCO instance segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. You can use this particular checkpoint for instance segmentation.\n created at: 2022-12-23\n- model_name: keremberke/yolov8m-pcb-defect-segmentation\n  description: A YOLOv8 model for PCB defect segmentation trained on the pcb-defect-segmentation dataset. The model can detect and segment defects in PCB images, such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.\n created at: 2023-01-28\n- model_name: facebook/maskformer-swin-tiny-coco\n  description: MaskFormer model trained on COCO panoptic segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.\n created at: 2022-03-02\n- model_name: keremberke/yolov8m-pothole-segmentation\n  description: A YOLOv8 model for pothole segmentation trained on keremberke/pothole-segmentation dataset. It can detect potholes in images and provide segmentation masks for the detected potholes.\n created at: 2023-01-26\n- model_name: keremberke/yolov8s-building-segmentation\n  description: A YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can detect and segment buildings with high accuracy.\n created at: 2023-01-26\n- model_name: keremberke/yolov8s-pothole-segmentation\n  description: A YOLOv8 model for pothole segmentation. This model detects potholes in images and outputs bounding boxes and masks for the detected potholes.\n created at: 2023-01-26\n- model_name: keremberke/yolov8s-pcb-defect-segmentation\n  description: YOLOv8s model for PCB defect segmentation. The model is trained to detect and segment PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.\n created at: 2023-01-28\n- model_name: keremberke/yolov8n-pcb-defect-segmentation\n  description: A YOLOv8 model for detecting and segmenting PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.\n created at: 2023-01-28\n- model_name: keremberke/yolov8n-pothole-segmentation\n  description: A YOLOv8 model for pothole segmentation in images. The model is trained on the pothole-segmentation dataset and achieves high accuracy in detecting potholes.\n created at: 2023-01-15\n- model_name: facebook/maskformer-swin-large-ade\n  description: MaskFormer model trained on ADE20k semantic segmentation (large-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.\n created at: 2022-03-02\n- model_name: facebook/detr-resnet-50-panoptic\n  description: DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 panoptic (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: tensorflow/lite-model/deeplabv3/1/default\n  description: TensorFlow Hub provides pre-trained image segmentation models that can be loaded and used for various tasks such as semantic segmentation. The example code demonstrates how to load a DeepLabV3 model and perform segmentation on an input image tensor.\n created at: 2020-10-06\n- model_name: pytorch/vision/deeplabv3_resnet50\n  description: DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: pytorch/vision/deeplabv3_mobilenet_v3_large\n  description: DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: pytorch/vision/deeplabv3_resnet101\n  description: DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: mateuszbuda/brain-segmentation-pytorch/unet\n  description: U-Net with batch normalization for biomedical image segmentation with pretrained weights for abnormality segmentation in brain MRI. The model comprises four levels of blocks containing two convolutional layers with batch normalization and ReLU activation function, and one max pooling layer in the encoding part and up-convolutional layers instead in the decoding part. The number of convolutional filters in each block is 32, 64, 128, and 256. The bottleneck layer has 512 convolutional filters. From the encoding layers, skip connections are used to the corresponding layers in the decoding part. Input image is a 3-channel brain MRI slice from pre-contrast, FLAIR, and post-contrast sequences, respectively. Output is a one-channel probability map of abnormality regions with the same size as the input image.\n created at: 2019-05-26\n- model_name: pytorch/vision/fcn_resnet101\n  description: FCN-ResNet is a Fully-Convolutional Network model using a ResNet-50 or a ResNet-101 backbone. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: pytorch/fairseq\n  description: Transformer (NMT) is a powerful sequence-to-sequence modeling architecture that produces state-of-the-art neural machine translation systems. It is based on the paper 'Attention Is All You Need' and has been improved using techniques such as large-scale semi-supervised training, back-translation, and noisy-channel reranking. It supports English-French and English-German translation as well as round-trip translation for paraphrasing.\n created at: 2017-08-29\n- model_name: pytorch/vision/fcn_resnet50\n  description: FCN-ResNet is a Fully-Convolutional Network model using a ResNet-50 or a ResNet-101 backbone. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: facebook/mask2former-swin-large-ade-semantic\n  description: Mask2Former model trained on ADE20k semantic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-large-ade-panoptic\n  description: Mask2Former model trained on ADE20k panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-large-mapillary-vistas-panoptic\n  description: Mask2Former model trained on Mapillary Vistas panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-large-mapillary-vistas-semantic\n  description: Mask2Former model trained on Mapillary Vistas semantic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-large-cityscapes-panoptic\n  description: Mask2Former model trained on Cityscapes panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-03\n- model_name: shi-labs/oneformer_cityscapes_swin_large\n  description: OneFormer model trained on the Cityscapes dataset (large-sized version, Swin backbone). It was introduced in the paper [OneFormer: One Transformer to Rule Universal Image Segmentation](https://arxiv.org/abs/2211.06220) by Jain et al. and first released in [this repository](https://github.com/SHI-Labs/OneFormer).\n\n![model image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/oneformer_teaser.png)\n created at: 2022-11-15\n- model_name: google/deeplabv3_mobilenet_v2_1.0_513\n  description: MobileNet V2 model pre-trained on PASCAL VOC at resolution 513x513. It was introduced in [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381) by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen. It was first released in [this repository](https://github.com/tensorflow/models/tree/master/research/deeplab).\n\nDisclaimer: The team releasing MobileNet V2 did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2022-11-10\n- model_name: apple/deeplabv3-mobilevit-small\n  description: MobileViT model pre-trained on PASCAL VOC at resolution 512x512. It was introduced in [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer](https://arxiv.org/abs/2110.02178) by Sachin Mehta and Mohammad Rastegari, and first released in [this repository](https://github.com/apple/ml-cvnets). The license used is [Apple sample code license](https://github.com/apple/ml-cvnets/blob/main/LICENSE).\n\nDisclaimer: The team releasing MobileViT did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2022-05-30\n- model_name: eugenesiow/edsr-base\n  description: The goal of image super resolution is to restore a high resolution (HR) image from a single low resolution (LR) image. The image below shows the ground truth (HR), the bicubic upscaling x2 and EDSR upscaling x2.\n\n![Comparing Bicubic upscaling against EDSR x2 upscaling on Set5 Image 4](images/Set5_4_compare.png \"Comparing Bicubic upscaling against EDSR x2 upscaling on Set5 Image 4\")\n## Model description\nEDSR is a model that uses both deeper and wider architecture (32 ResBlocks and 256 channels) to improve performance. It uses both global and local skip connections, and up-scaling is done at the end of the network. It doesn't use batch normalization layers (input and output have similar distributions, normalizing intermediate features may not be desirable) instead it uses constant scaling layers to ensure stable training. An L1 loss function (absolute error) is used instead of L2 (MSE), the authors showed better performance empirically and it requires less computation.\n\nThis is a base model (~5mb vs ~100mb) that includes just 16 ResBlocks and 64 channels.\n## Intended uses & limitations\nYou can use the pre-trained models for upscaling your images 2x, 3x and 4x. You can also use the trainer to train a model on your own dataset.\n### How to use\nThe model can be used with the [super_image](https://github.com/eugenesiow/super-image) library:\n```bash\npip install super-image\n```\nHere is how to use a pre-trained model to upscale your image:\n```python\nfrom super_image import EdsrModel, ImageLoader\nfrom PIL import Image\nimport requests\n\nurl = 'https://paperswithcode.com/media/datasets/Set5-0000002728-07a9793f_zA3bDjj.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel = EdsrModel.from_pretrained('eugenesiow/edsr-base', scale=2)      # scale 2, 3 and 4 models available\ninputs = ImageLoader.load_image(image)\npreds = model(inputs)\n\nImageLoader.save_image(preds, './scaled_2x.png')                        # save the output 2x scaled image to `./scaled_2x.png`\nImageLoader.save_compare(inputs, preds, './scaled_2x_compare.png')      # save an output comparing the super-image with a bicubic scaling\n```\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Upscale_Images_with_Pretrained_super_image_Models.ipynb \"Open in Colab\")\n## Training data\nThe models for 2x, 3x and 4x image super resolution were pretrained on [DIV2K](https://huggingface.co/datasets/eugenesiow/Div2k), a dataset of 800 high-quality (2K resolution) images for training, augmented to 4000 images and uses a dev set of  100 validation images (images numbered 801 to 900). \n## Training procedure\n### Preprocessing\nWe follow the pre-processing and training method of [Wang et al.](https://arxiv.org/abs/2104.07566).\nLow Resolution (LR) images are created by using bicubic interpolation as the resizing method to reduce the size of the High Resolution (HR) images by x2, x3 and x4 times.\nDuring training, RGB patches with size of 64√ó64 from the LR input are used together with their corresponding HR patches. \nData augmentation is applied to the training set in the pre-processing stage where five images are created from the four corners and center of the original image. \n\nWe need the huggingface [datasets](https://huggingface.co/datasets?filter=task_ids:other-other-image-super-resolution) library to download the data:\n```bash\npip install datasets\n```\nThe following code gets the data and preprocesses/augments the data.\n\n```python\nfrom datasets import load_dataset\nfrom super_image.data import EvalDataset, TrainDataset, augment_five_crop\n\naugmented_dataset = load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='train')\\\n    .map(augment_five_crop, batched=True, desc=\"Augmenting Dataset\")                                # download and augment the data with the five_crop method\ntrain_dataset = TrainDataset(augmented_dataset)                                                     # prepare the train dataset for loading PyTorch DataLoader\neval_dataset = EvalDataset(load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='validation'))      # prepare the eval dataset for the PyTorch DataLoader\n```\n### Pretraining\nThe model was trained on GPU. The training code is provided below:\n```python\nfrom super_image import Trainer, TrainingArguments, EdsrModel, EdsrConfig\n\ntraining_args = TrainingArguments(\n    output_dir='./results',                 # output directory\n    num_train_epochs=1000,                  # total number of training epochs\n)\n\nconfig = EdsrConfig(\n    scale=4,                                # train a model to upscale 4x\n)\nmodel = EdsrModel(config)\n\ntrainer = Trainer(\n    model=model,                         # the instantiated model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=eval_dataset            # evaluation dataset\n)\n\ntrainer.train()\n```\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Train_super_image_Models.ipynb \"Open in Colab\")\n## Evaluation results\nThe evaluation metrics include [PSNR](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio#Quality_estimation_with_PSNR) and [SSIM](https://en.wikipedia.org/wiki/Structural_similarity#Algorithm). \n\nEvaluation datasets include:\n- Set5 - [Bevilacqua et al. (2012)](https://huggingface.co/datasets/eugenesiow/Set5)\n- Set14 - [Zeyde et al. (2010)](https://huggingface.co/datasets/eugenesiow/Set14)\n- BSD100 - [Martin et al. (2001)](https://huggingface.co/datasets/eugenesiow/BSD100)\n- Urban100 - [Huang et al. (2015)](https://huggingface.co/datasets/eugenesiow/Urban100)\n\nThe results columns below are represented below as `PSNR/SSIM`. They are compared against a Bicubic baseline.\n\n|Dataset  \t    |Scale      |Bicubic  \t        |edsr-base  \t        |\n|---\t        |---\t    |---\t            |---\t                |\n|Set5  \t        |2x         |33.64/0.9292       |**38.02/0.9607**       |\n|Set5  \t        |3x  \t    |30.39/0.8678  \t    |**35.04/0.9403**  \t    |\n|Set5  \t        |4x  \t    |28.42/0.8101  \t    |**32.12/0.8947**       |\n|Set14  \t    |2x         |30.22/0.8683  \t    |**33.57/0.9172**  \t    |\n|Set14  \t    |3x         |27.53/0.7737  \t    |**30.93/0.8567**  \t    |\n|Set14  \t    |4x         |25.99/0.7023  \t    |**28.60/0.7815**  \t    |\n|BSD100  \t    |2x  \t    |29.55/0.8425  \t    |**32.21/0.8999**  \t    |\n|BSD100  \t    |3x  \t    |27.20/0.7382  \t    |**29.65/0.8204**  \t    |\n|BSD100  \t    |4x  \t    |25.96/0.6672  \t    |**27.61/0.7363**  \t    |\n|Urban100  \t    |2x  \t    |26.66/0.8408  \t    |**32.04/0.9276**  \t    |\n|Urban100  \t    |3x  \t    |  \t                |**29.23/0.8723**  \t    |\n|Urban100  \t    |4x  \t    |23.14/0.6573  \t    |**26.02/0.7832**  \t    |\n\n![Comparing Bicubic upscaling against x2 upscaling on Set5 Image 2](images/Set5_2_compare.png \"Comparing Bicubic upscaling against x2 upscaling on Set5 Image 2\")\n\nYou can find a notebook to easily run evaluation on pretrained models below:\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Evaluate_Pretrained_super_image_Models.ipynb \"Open in Colab\")\n created at: 2022-03-02\n- model_name: eugenesiow/msrn-bam\n  description: The goal of image super resolution is to restore a high resolution (HR) image from a single low resolution (LR) image. The image below shows the ground truth (HR), the bicubic upscaling x2 and model upscaling x2.\n\n![Comparing Bicubic upscaling against the models x4 upscaling on Set5 Image 4](images/msrn_4_4_compare.png \"Comparing Bicubic upscaling against the models x4 upscaling on Set5 Image 4\")\n## Model description\nThe MSRN model proposes a feature extraction structure called the multi-scale residual block. This module can \"adaptively detect image features at different scales\" and \"exploit the potential features of the image\".\n\nThis model also applies the balanced attention (BAM) method invented by [Wang et al. (2021)](https://arxiv.org/abs/2104.07566) to further improve the results.\n## Intended uses & limitations\nYou can use the pre-trained models for upscaling your images 2x, 3x and 4x. You can also use the trainer to train a model on your own dataset.\n### How to use\nThe model can be used with the [super_image](https://github.com/eugenesiow/super-image) library:\n```bash\npip install super-image\n```\nHere is how to use a pre-trained model to upscale your image:\n```python\nfrom super_image import MsrnModel, ImageLoader\nfrom PIL import Image\nimport requests\n\nurl = 'https://paperswithcode.com/media/datasets/Set5-0000002728-07a9793f_zA3bDjj.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel = MsrnModel.from_pretrained('eugenesiow/msrn-bam', scale=2)      # scale 2, 3 and 4 models available\ninputs = ImageLoader.load_image(image)\npreds = model(inputs)\n\nImageLoader.save_image(preds, './scaled_2x.png')                        # save the output 2x scaled image to `./scaled_2x.png`\nImageLoader.save_compare(inputs, preds, './scaled_2x_compare.png')      # save an output comparing the super-image with a bicubic scaling\n```\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Upscale_Images_with_Pretrained_super_image_Models.ipynb \"Open in Colab\")\n## Training data\nThe models for 2x, 3x and 4x image super resolution were pretrained on [DIV2K](https://huggingface.co/datasets/eugenesiow/Div2k), a dataset of 800 high-quality (2K resolution) images for training, augmented to 4000 images and uses a dev set of  100 validation images (images numbered 801 to 900). \n## Training procedure\n### Preprocessing\nWe follow the pre-processing and training method of [Wang et al.](https://arxiv.org/abs/2104.07566).\nLow Resolution (LR) images are created by using bicubic interpolation as the resizing method to reduce the size of the High Resolution (HR) images by x2, x3 and x4 times.\nDuring training, RGB patches with size of 64√ó64 from the LR input are used together with their corresponding HR patches. \nData augmentation is applied to the training set in the pre-processing stage where five images are created from the four corners and center of the original image. \n\nWe need the huggingface [datasets](https://huggingface.co/datasets?filter=task_ids:other-other-image-super-resolution) library to download the data:\n```bash\npip install datasets\n```\nThe following code gets the data and preprocesses/augments the data.\n\n```python\nfrom datasets import load_dataset\nfrom super_image.data import EvalDataset, TrainDataset, augment_five_crop\n\naugmented_dataset = load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='train')\\\n    .map(augment_five_crop, batched=True, desc=\"Augmenting Dataset\")                                # download and augment the data with the five_crop method\ntrain_dataset = TrainDataset(augmented_dataset)                                                     # prepare the train dataset for loading PyTorch DataLoader\neval_dataset = EvalDataset(load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='validation'))      # prepare the eval dataset for the PyTorch DataLoader\n```\n### Pretraining\nThe model was trained on GPU. The training code is provided below:\n```python\nfrom super_image import Trainer, TrainingArguments, MsrnModel, MsrnConfig\n\ntraining_args = TrainingArguments(\n    output_dir='./results',                 # output directory\n    num_train_epochs=1000,                  # total number of training epochs\n)\n\nconfig = MsrnConfig(\n    scale=4,                                # train a model to upscale 4x\n    bam=True,                               # apply balanced attention to the network\n)\nmodel = MsrnModel(config)\n\ntrainer = Trainer(\n    model=model,                         # the instantiated model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=eval_dataset            # evaluation dataset\n)\n\ntrainer.train()\n```\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Train_super_image_Models.ipynb \"Open in Colab\")\n## Evaluation results\nThe evaluation metrics include [PSNR](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio#Quality_estimation_with_PSNR) and [SSIM](https://en.wikipedia.org/wiki/Structural_similarity#Algorithm). \n\nEvaluation datasets include:\n- Set5 - [Bevilacqua et al. (2012)](https://huggingface.co/datasets/eugenesiow/Set5)\n- Set14 - [Zeyde et al. (2010)](https://huggingface.co/datasets/eugenesiow/Set14)\n- BSD100 - [Martin et al. (2001)](https://huggingface.co/datasets/eugenesiow/BSD100)\n- Urban100 - [Huang et al. (2015)](https://huggingface.co/datasets/eugenesiow/Urban100)\n\nThe results columns below are represented below as `PSNR/SSIM`. They are compared against a Bicubic baseline.\n\n|Dataset  \t    |Scale      |Bicubic  \t        |msrn-bam  \t                    |\n|---\t        |---\t    |---\t            |---\t                        |\n|Set5  \t        |2x         |33.64/0.9292       |**38.02/0.9608**       |\n|Set5  \t        |3x  \t    |30.39/0.8678  \t    |**35.13/0.9408**  \t    |\n|Set5  \t        |4x  \t    |28.42/0.8101  \t    |**32.26/0.8955**       |\n|Set14  \t    |2x         |30.22/0.8683  \t    |**33.73/0.9186**  \t    |\n|Set14  \t    |3x         |27.53/0.7737  \t    |**31.06/0.8588**  \t    |\n|Set14  \t    |4x         |25.99/0.7023  \t    |**28.78/0.7859**  \t    |\n|BSD100  \t    |2x  \t    |29.55/0.8425  \t    |**33.78/0.9253**  \t    |\n|BSD100  \t    |3x  \t    |27.20/0.7382  \t    |**29.65/0.8196**  \t    |\n|BSD100  \t    |4x  \t    |25.96/0.6672  \t    |**28.51/0.7651**  \t    |\n|Urban100  \t    |2x  \t    |26.66/0.8408  \t    |**32.08/0.9276**  \t    |\n|Urban100  \t    |3x  \t    |  \t                |**29.26/0.8736**  \t    |\n|Urban100  \t    |4x  \t    |23.14/0.6573  \t    |**26.10/0.7857**  \t    |\n\n![Comparing Bicubic upscaling against the models x4 upscaling on Set5 Image 2](images/msrn_2_4_compare.png \"Comparing Bicubic upscaling against the models x4 upscaling on Set5 Image 2\")\n\nYou can find a notebook to easily run evaluation on pretrained models below:\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Evaluate_Pretrained_super_image_Models.ipynb \"Open in Colab\")\n created at: 2022-03-02\n- model_name: ZhengPeng7/BiRefNet_HR-matting\n  description: this birefnet was trained with images in `2048x2048` for higher resolution image matting with transparency. all tested in fp16 mode. dataset method resolution maxfm wfmeasure mae smeasure meanem hce maxem meanfm adpem adpfm mba maxbiou meanbiou\n created at: 2025-02-12T16:19:23+00:00\n- model_name: PramaLLC/BEN2\n  description: ben2 background erase network introduces a novel approach to foreground segmentation through its innovative confidence guided matting cgm pipeline. the architecture employs a refiner network that targets and processes pixels where the base model exhibits lower confidence levels, resulting in more precise and reliable matting results. this model is built on ben: ben2 was trained on the dis5k and our 22k proprietary segmentation dataset. our enhanced model delivers superior performance in hair matting, 4k processing, object segmentation, and edge refinement. our base model is open source. to try the full model through our free web demo or integrate ben2 into your project with our api: - üåê\n created at: 2025-01-22T14:39:05+00:00\n- model_name: ZhengPeng7/BiRefNet_HR\n  description: this birefnet was trained with images in `2048x2048` for higher resolution inference. all tested in fp16 mode. dataset method resolution maxfm wfmeasure mae smeasure meanem hce maxem meanfm adpem adpfm mba maxbiou meanbiou\n created at: 2025-02-01T17:52:14+00:00\n- model_name: briaai/RMBG-2.0\n  description: rmbg v2.0 is our new state-of-the-art background removal model significantly improves rmbg v1.4. the model is designed to effectively separate foreground from background in a range of categories and image types. this model has been trained on a carefully selected dataset, which includes: general stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale. - developed by:  - model type: background removal - license:  - the model is released under a cc by-nc 4.0 license for non-commercial use. - commercial use is subject to a commercial agreement with bria. available  purchase: to purchase a commercial license simply click . - model description: bria rmbg-2.0 is a dichotomous image segmentation model trained exclusively on a professional-grade dataset. the model output includes a single-channel 8-bit grayscale alpha matte, where each pixel value indicates the opacity level of the corresponding pixel in the original image. this non-binary output approach offers developers the flexibility to define custom thresholds for foreground-background separation, catering to varied use cases requirements and enhancing integration into complex pipelines. - bria: resources for more information:\n created at: 2024-10-29T12:14:36+00:00\n- model_name: ZhengPeng7/BiRefNet_dynamic\n  description: bilateral reference for high-resolution dichotomous image segmentation an arbitrary shape adaptable birefnet for general segmentation. this model was trained on arbitrary shapes 256x256 2304x2304 and shows great robustness on inputs with any shape.\n created at: 2025-03-31T03:37:03+00:00\n- model_name: Ricky06662/Seg-Zero-7B\n  description: this model is based on the paper . it uses a decoupled architecture with a reasoning model and a segmentation model. it's trained via reinforcement learning using grpo without explicit reasoning data, leading to robust zero-shot generalization and emergent test-time reasoning. code: this is a seg-zero-7b model. it introduces a decoupled architecture consisting of a reasoning model and a segmentation model. the reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate pixel-level masks. this is a seg-zero-7b model. it introduces a decoupled architecture consisting of a reasoning model and a segmentation model. the reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate pixel-level masks.\n created at: 2025-03-07T15:08:24+00:00\n- model_name: cocktailpeanut/rm\n  description: rmbg v2.0 is our new state-of-the-art background removal model significantly improves rmbg v1.4. the model is designed to effectively separate foreground from background in a range of categories and image types. this model has been trained on a carefully selected dataset, which includes: general stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale. - developed by:  - model type: background removal - license:  - the model is released under a cc by-nc 4.0 license for non-commercial use. - commercial use is subject to a commercial agreement with bria. available  purchase: to purchase a commercial license simply click . - model description: bria rmbg-2.0 is a dichotomous image segmentation model trained exclusively on a professional-grade dataset. - bria: resources for more information:\n created at: 2025-04-12T06:16:07+00:00\n- model_name: yuvraj108c/RMBG-2.0\n  description: rmbg v2.0 is our new state-of-the-art background removal model significantly improves rmbg v1.4. the model is designed to effectively separate foreground from background in a range of categories and image types. this model has been trained on a carefully selected dataset, which includes: general stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale. - developed by:  - model type: background removal - license:  - the model is released under a cc by-nc 4.0 license for non-commercial use. - commercial use is subject to a commercial agreement with bria. available  purchase: to purchase a commercial license simply click . - model description: bria rmbg-2.0 is a dichotomous image segmentation model trained exclusively on a professional-grade dataset. the model output includes a single-channel 8-bit grayscale alpha matte, where each pixel value indicates the opacity level of the corresponding pixel in the original image. this non-binary output approach offers developers the flexibility to define custom thresholds for foreground-background separation, catering to varied use cases requirements and enhancing integration into complex pipelines. - bria: resources for more information:\n created at: 2025-07-28T10:18:49+00:00\n- model_name: Ricky06662/VisionReasoner-7B\n  description: this repository contains the visionreasoner-7b model, developed as part of the novel seg-zero framework, presented in the paper . this model is also associated with the paper . code: project page:  seg-zero is a novel framework that demonstrates remarkable generalizability and derives explicit chain-of-thought reasoning through cognitive reinforcement for reasoning segmentation. this visionreasoner-7b model employs a decoupled architecture consisting of a reasoning model and a segmentation model. the reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate precise pixel-level masks. trained exclusively via reinforcement learning with grpo and without explicit reasoning data, seg-zero achieves robust zero-shot generalization and exhibits emergent test-time reasoning capabilities. experiments show that seg-zero-7b achieves a zero-shot performance of 57.5 on the reasonseg benchmark, surpassing the prior lisa-7b by 18%. this significant improvement highlights seg-zero's ability to generalize across domains while presenting an explicit reasoning process.\n created at: 2025-05-18T05:46:01+00:00\n- model_name: tue-mps/coco_panoptic_eomt_large_640\n  description: eomt encoder-only mask transformer is a vision transformer vit architecture designed for high-quality and efficient image segmentation. it was introduced in the cvpr 2025 highlight paper: by tommie kerssies, niccol√≤ cavagnero, alexander hermans, narges norouzi, giuseppe averta, bastian leibe, gijs dubbelman, and daan de geus.\n created at: 2025-03-26T17:28:05+00:00\n- model_name: tue-mps/ade20k_semantic_eomt_large_512\n  description: eomt encoder-only mask transformer is a vision transformer vit architecture designed for high-quality and efficient image segmentation. it was introduced in the cvpr 2025 highlight paper: by tommie kerssies, niccol√≤ cavagnero, alexander hermans, narges norouzi, giuseppe averta, bastian leibe, gijs dubbelman, and daan de geus.\n created at: 2025-03-26T19:29:47+00:00\n- model_name: bwittmann/vesselFM\n  description: tl;dr : vesselfm is a foundation model for universal 3d blood vessel segmentation in arbitrary imaging domains. for details, please refer to our preprint and our github repo\n created at: 2024-12-02T12:45:08+00:00\n- model_name: Ricky06662/Seg-Zero-7B-Best-on-ReasonSegTest\n  description: this model is based on the paper . code: seg-zero introduces a novel framework for reasoning segmentation that addresses the limitations of traditional supervised fine-tuning methods, which often struggle with out-of-domain generalization and lack explicit reasoning processes. the framework features a decoupled architecture consisting of a reasoning model and a segmentation model. the reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate precise pixel-level masks.\n created at: 2025-04-09T16:12:22+00:00\n- model_name: tue-mps/coco_instance_eomt_large_640\n  description: eomt encoder-only mask transformer is a vision transformer vit architecture designed for high-quality and efficient image segmentation. it was introduced in the cvpr 2025 highlight paper: by tommie kerssies, niccol√≤ cavagnero, alexander hermans, narges norouzi, giuseppe averta, bastian leibe, gijs dubbelman, and daan de geus.\n created at: 2025-03-26T17:16:21+00:00\n- model_name: facebook/sapiens-seg-1b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T20:26:24+00:00\n- model_name: tue-mps/coco_instance_eomt_large_1280\n  description: eomt encoder-only mask transformer is a vision transformer vit architecture designed for high-quality and efficient image segmentation. it was introduced in the cvpr 2025 highlight paper: by tommie kerssies, niccol√≤ cavagnero, alexander hermans, narges norouzi, giuseppe averta, bastian leibe, gijs dubbelman, and daan de geus.\n created at: 2025-03-26T19:34:47+00:00\n- model_name: nevernever69/dit-doclaynet-segmentation\n  description: this model is a fine-tuned version of for document layout semantic segmentation on the dataset small subset: `nevernever69/small-doclaynet-v1.1`. it segments scanned document images into 11 layout categories such as title, paragraph, table, and footer. - segment document images into structured layout elements - assist in downstream tasks like document ocr, archiving, and automatic annotation\n created at: 2025-03-12T11:45:04+00:00\n- model_name: Pushti/BirefNetHR\n  description: this birefnet was trained with images in `2048x2048` for higher resolution inference. all tested in fp16 mode. dataset method resolution maxfm wfmeasure mae smeasure meanem hce maxem meanfm adpem adpfm mba maxbiou meanbiou\n created at: 2025-04-18T07:28:32+00:00\n- model_name: ZQL9711/RMBG-2-Matting\n  description: rmbg v2.0 is our new state-of-the-art background removal model significantly improves rmbg v1.4. the model is designed to effectively separate foreground from background in a range of categories and image types. this model has been trained on a carefully selected dataset, which includes: general stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale. - developed by:  - model type: background removal - license:  - the model is released under a cc by-nc 4.0 license for non-commercial use. - commercial use is subject to a commercial agreement with bria. available  purchase: to purchase a commercial license simply click . - model description: bria rmbg-2.0 is a dichotomous image segmentation model trained exclusively on a professional-grade dataset. - bria: resources for more information:\n created at: 2025-07-05T08:31:53+00:00\n- model_name: Acly/BiRefNet-GGUF\n  description: birefnet is a model for dichotomous image segmentation background removal. the weights in this repository are converted for lightweight inference on consumer hardware with .\n created at: 2025-07-20T10:33:31+00:00\n\nExamples:\n- example user query: Our company is building a software to analyze satellite images for land use. We need to segment different regions in the images.\n  model_name: nvidia/segformer-b0-finetuned-ade-512-512\n- example user query: Identify an API to perform semantic segmentation of the street labeled with road, sidewalk, and building for a given picture.\n  model_name: pytorch/vision/deeplabv3_resnet50\n- example user query: A drone is flying over the neighborhood and is recording video. How can I process the video and perform semantic segmentation on each frame?\n  model_name: pytorch/vision/deeplabv3_resnet101\n- example user query: To identify and separate different objects in an image, create an image segmentation model.\n  model_name: openmmlab/upernet-convnext-small\n- example user query: The company is developing a fashion application which needs to identify the clothing items in pictures.\n  model_name: mattmdjaga/segformer_b2_clothes\n- example user query: An urban development company wants to analyze city images to inform infrastructure plans. Extract features using a semantic segmentation model.\n  model_name: nvidia/segformer-b5-finetuned-cityscapes-1024-1024\n\nUser prompt:\nPartition all items in the representation.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_seg>\n\nCandidate Models:\n- model_name: CIDAS/clipseg-rd64-refined\n  description: CLIPSeg model with reduce dimension 64, refined (using a more complex convolution). It was introduced in the paper Image Segmentation Using Text and Image Prompts by L√ºddecke et al. and first released in this repository. This model is intended for zero-shot and one-shot image segmentation.\n created at: 2022-11-01\n- model_name: openmmlab/upernet-convnext-small\n  description: UperNet framework for semantic segmentation, leveraging a ConvNeXt backbone. UperNet was introduced in the paper Unified Perceptual Parsing for Scene Understanding by Xiao et al. Combining UperNet with a ConvNeXt backbone was introduced in the paper A ConvNet for the 2020s.\n created at: 2023-01-13\n- model_name: nvidia/segformer-b0-finetuned-ade-512-512\n  description: SegFormer model fine-tuned on ADE20k at resolution 512x512. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: nvidia/segformer-b5-finetuned-ade-640-640\n  description: SegFormer model fine-tuned on ADE20k at resolution 640x640. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: nvidia/segformer-b2-finetuned-cityscapes-1024-1024\n  description: SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: nvidia/segformer-b0-finetuned-cityscapes-1024-1024\n  description: SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: facebook/maskformer-swin-base-coco\n  description: MaskFormer model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.\n created at: 2022-03-02\n- model_name: facebook/mask2former-swin-large-cityscapes-semantic\n  description: Mask2Former model trained on Cityscapes semantic segmentation (large-sized version, Swin backbone). It addresses instance, semantic and panoptic segmentation by predicting a set of masks and corresponding labels. The model outperforms the previous SOTA, MaskFormer, in terms of performance and efficiency.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-base-coco-panoptic\n  description: Mask2Former model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.\n created at: 2023-01-02\n- model_name: mattmdjaga/segformer_b2_clothes\n  description: SegFormer model fine-tuned on ATR dataset for clothes segmentation.\n created at: 2022-11-24\n- model_name: shi-labs/oneformer_coco_swin_large\n  description: OneFormer model trained on the COCO dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.\n created at: 2022-11-15\n- model_name: facebook/mask2former-swin-large-coco-panoptic\n  description: Mask2Former model trained on COCO panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.\n created at: 2023-01-02\n- model_name: shi-labs/oneformer_ade20k_swin_large\n  description: OneFormer model trained on the ADE20k dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.\n created at: 2022-11-15\n- model_name: facebook/mask2former-swin-small-coco-instance\n  description: Mask2Former model trained on COCO instance segmentation (small-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.\n created at: 2022-12-26\n- model_name: nvidia/segformer-b5-finetuned-cityscapes-1024-1024\n  description: SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: shi-labs/oneformer_ade20k_swin_tiny\n  description: OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.\n created at: 2022-11-16\n- model_name: keremberke/yolov8m-building-segmentation\n  description: A YOLOv8 model for building segmentation in satellite images. It can detect and segment buildings in the input images.\n created at: 2023-01-27\n- model_name: facebook/maskformer-swin-base-ade\n  description: MaskFormer model trained on ADE20k semantic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.\n created at: 2022-03-02\n- model_name: facebook/mask2former-swin-tiny-coco-instance\n  description: Mask2Former model trained on COCO instance segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. You can use this particular checkpoint for instance segmentation.\n created at: 2022-12-23\n- model_name: keremberke/yolov8m-pcb-defect-segmentation\n  description: A YOLOv8 model for PCB defect segmentation trained on the pcb-defect-segmentation dataset. The model can detect and segment defects in PCB images, such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.\n created at: 2023-01-28\n- model_name: facebook/maskformer-swin-tiny-coco\n  description: MaskFormer model trained on COCO panoptic segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.\n created at: 2022-03-02\n- model_name: keremberke/yolov8m-pothole-segmentation\n  description: A YOLOv8 model for pothole segmentation trained on keremberke/pothole-segmentation dataset. It can detect potholes in images and provide segmentation masks for the detected potholes.\n created at: 2023-01-26\n- model_name: keremberke/yolov8s-building-segmentation\n  description: A YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can detect and segment buildings with high accuracy.\n created at: 2023-01-26\n- model_name: keremberke/yolov8s-pothole-segmentation\n  description: A YOLOv8 model for pothole segmentation. This model detects potholes in images and outputs bounding boxes and masks for the detected potholes.\n created at: 2023-01-26\n- model_name: keremberke/yolov8s-pcb-defect-segmentation\n  description: YOLOv8s model for PCB defect segmentation. The model is trained to detect and segment PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.\n created at: 2023-01-28\n- model_name: keremberke/yolov8n-pcb-defect-segmentation\n  description: A YOLOv8 model for detecting and segmenting PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.\n created at: 2023-01-28\n- model_name: keremberke/yolov8n-pothole-segmentation\n  description: A YOLOv8 model for pothole segmentation in images. The model is trained on the pothole-segmentation dataset and achieves high accuracy in detecting potholes.\n created at: 2023-01-15\n- model_name: facebook/maskformer-swin-large-ade\n  description: MaskFormer model trained on ADE20k semantic segmentation (large-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.\n created at: 2022-03-02\n- model_name: facebook/detr-resnet-50-panoptic\n  description: DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 panoptic (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: tensorflow/lite-model/deeplabv3/1/default\n  description: TensorFlow Hub provides pre-trained image segmentation models that can be loaded and used for various tasks such as semantic segmentation. The example code demonstrates how to load a DeepLabV3 model and perform segmentation on an input image tensor.\n created at: 2020-10-06\n- model_name: pytorch/vision/deeplabv3_resnet50\n  description: DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: pytorch/vision/deeplabv3_mobilenet_v3_large\n  description: DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: pytorch/vision/deeplabv3_resnet101\n  description: DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: mateuszbuda/brain-segmentation-pytorch/unet\n  description: U-Net with batch normalization for biomedical image segmentation with pretrained weights for abnormality segmentation in brain MRI. The model comprises four levels of blocks containing two convolutional layers with batch normalization and ReLU activation function, and one max pooling layer in the encoding part and up-convolutional layers instead in the decoding part. The number of convolutional filters in each block is 32, 64, 128, and 256. The bottleneck layer has 512 convolutional filters. From the encoding layers, skip connections are used to the corresponding layers in the decoding part. Input image is a 3-channel brain MRI slice from pre-contrast, FLAIR, and post-contrast sequences, respectively. Output is a one-channel probability map of abnormality regions with the same size as the input image.\n created at: 2019-05-26\n- model_name: pytorch/vision/fcn_resnet101\n  description: FCN-ResNet is a Fully-Convolutional Network model using a ResNet-50 or a ResNet-101 backbone. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: pytorch/fairseq\n  description: Transformer (NMT) is a powerful sequence-to-sequence modeling architecture that produces state-of-the-art neural machine translation systems. It is based on the paper 'Attention Is All You Need' and has been improved using techniques such as large-scale semi-supervised training, back-translation, and noisy-channel reranking. It supports English-French and English-German translation as well as round-trip translation for paraphrasing.\n created at: 2017-08-29\n- model_name: pytorch/vision/fcn_resnet50\n  description: FCN-ResNet is a Fully-Convolutional Network model using a ResNet-50 or a ResNet-101 backbone. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: facebook/mask2former-swin-large-ade-semantic\n  description: Mask2Former model trained on ADE20k semantic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-large-ade-panoptic\n  description: Mask2Former model trained on ADE20k panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-large-mapillary-vistas-panoptic\n  description: Mask2Former model trained on Mapillary Vistas panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-large-mapillary-vistas-semantic\n  description: Mask2Former model trained on Mapillary Vistas semantic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-large-cityscapes-panoptic\n  description: Mask2Former model trained on Cityscapes panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-03\n- model_name: shi-labs/oneformer_cityscapes_swin_large\n  description: OneFormer model trained on the Cityscapes dataset (large-sized version, Swin backbone). It was introduced in the paper [OneFormer: One Transformer to Rule Universal Image Segmentation](https://arxiv.org/abs/2211.06220) by Jain et al. and first released in [this repository](https://github.com/SHI-Labs/OneFormer).\n\n![model image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/oneformer_teaser.png)\n created at: 2022-11-15\n- model_name: google/deeplabv3_mobilenet_v2_1.0_513\n  description: MobileNet V2 model pre-trained on PASCAL VOC at resolution 513x513. It was introduced in [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381) by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen. It was first released in [this repository](https://github.com/tensorflow/models/tree/master/research/deeplab).\n\nDisclaimer: The team releasing MobileNet V2 did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2022-11-10\n- model_name: apple/deeplabv3-mobilevit-small\n  description: MobileViT model pre-trained on PASCAL VOC at resolution 512x512. It was introduced in [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer](https://arxiv.org/abs/2110.02178) by Sachin Mehta and Mohammad Rastegari, and first released in [this repository](https://github.com/apple/ml-cvnets). The license used is [Apple sample code license](https://github.com/apple/ml-cvnets/blob/main/LICENSE).\n\nDisclaimer: The team releasing MobileViT did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2022-05-30\n- model_name: eugenesiow/edsr-base\n  description: The goal of image super resolution is to restore a high resolution (HR) image from a single low resolution (LR) image. The image below shows the ground truth (HR), the bicubic upscaling x2 and EDSR upscaling x2.\n\n![Comparing Bicubic upscaling against EDSR x2 upscaling on Set5 Image 4](images/Set5_4_compare.png \"Comparing Bicubic upscaling against EDSR x2 upscaling on Set5 Image 4\")\n## Model description\nEDSR is a model that uses both deeper and wider architecture (32 ResBlocks and 256 channels) to improve performance. It uses both global and local skip connections, and up-scaling is done at the end of the network. It doesn't use batch normalization layers (input and output have similar distributions, normalizing intermediate features may not be desirable) instead it uses constant scaling layers to ensure stable training. An L1 loss function (absolute error) is used instead of L2 (MSE), the authors showed better performance empirically and it requires less computation.\n\nThis is a base model (~5mb vs ~100mb) that includes just 16 ResBlocks and 64 channels.\n## Intended uses & limitations\nYou can use the pre-trained models for upscaling your images 2x, 3x and 4x. You can also use the trainer to train a model on your own dataset.\n### How to use\nThe model can be used with the [super_image](https://github.com/eugenesiow/super-image) library:\n```bash\npip install super-image\n```\nHere is how to use a pre-trained model to upscale your image:\n```python\nfrom super_image import EdsrModel, ImageLoader\nfrom PIL import Image\nimport requests\n\nurl = 'https://paperswithcode.com/media/datasets/Set5-0000002728-07a9793f_zA3bDjj.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel = EdsrModel.from_pretrained('eugenesiow/edsr-base', scale=2)      # scale 2, 3 and 4 models available\ninputs = ImageLoader.load_image(image)\npreds = model(inputs)\n\nImageLoader.save_image(preds, './scaled_2x.png')                        # save the output 2x scaled image to `./scaled_2x.png`\nImageLoader.save_compare(inputs, preds, './scaled_2x_compare.png')      # save an output comparing the super-image with a bicubic scaling\n```\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Upscale_Images_with_Pretrained_super_image_Models.ipynb \"Open in Colab\")\n## Training data\nThe models for 2x, 3x and 4x image super resolution were pretrained on [DIV2K](https://huggingface.co/datasets/eugenesiow/Div2k), a dataset of 800 high-quality (2K resolution) images for training, augmented to 4000 images and uses a dev set of  100 validation images (images numbered 801 to 900). \n## Training procedure\n### Preprocessing\nWe follow the pre-processing and training method of [Wang et al.](https://arxiv.org/abs/2104.07566).\nLow Resolution (LR) images are created by using bicubic interpolation as the resizing method to reduce the size of the High Resolution (HR) images by x2, x3 and x4 times.\nDuring training, RGB patches with size of 64√ó64 from the LR input are used together with their corresponding HR patches. \nData augmentation is applied to the training set in the pre-processing stage where five images are created from the four corners and center of the original image. \n\nWe need the huggingface [datasets](https://huggingface.co/datasets?filter=task_ids:other-other-image-super-resolution) library to download the data:\n```bash\npip install datasets\n```\nThe following code gets the data and preprocesses/augments the data.\n\n```python\nfrom datasets import load_dataset\nfrom super_image.data import EvalDataset, TrainDataset, augment_five_crop\n\naugmented_dataset = load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='train')\\\n    .map(augment_five_crop, batched=True, desc=\"Augmenting Dataset\")                                # download and augment the data with the five_crop method\ntrain_dataset = TrainDataset(augmented_dataset)                                                     # prepare the train dataset for loading PyTorch DataLoader\neval_dataset = EvalDataset(load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='validation'))      # prepare the eval dataset for the PyTorch DataLoader\n```\n### Pretraining\nThe model was trained on GPU. The training code is provided below:\n```python\nfrom super_image import Trainer, TrainingArguments, EdsrModel, EdsrConfig\n\ntraining_args = TrainingArguments(\n    output_dir='./results',                 # output directory\n    num_train_epochs=1000,                  # total number of training epochs\n)\n\nconfig = EdsrConfig(\n    scale=4,                                # train a model to upscale 4x\n)\nmodel = EdsrModel(config)\n\ntrainer = Trainer(\n    model=model,                         # the instantiated model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=eval_dataset            # evaluation dataset\n)\n\ntrainer.train()\n```\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Train_super_image_Models.ipynb \"Open in Colab\")\n## Evaluation results\nThe evaluation metrics include [PSNR](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio#Quality_estimation_with_PSNR) and [SSIM](https://en.wikipedia.org/wiki/Structural_similarity#Algorithm). \n\nEvaluation datasets include:\n- Set5 - [Bevilacqua et al. (2012)](https://huggingface.co/datasets/eugenesiow/Set5)\n- Set14 - [Zeyde et al. (2010)](https://huggingface.co/datasets/eugenesiow/Set14)\n- BSD100 - [Martin et al. (2001)](https://huggingface.co/datasets/eugenesiow/BSD100)\n- Urban100 - [Huang et al. (2015)](https://huggingface.co/datasets/eugenesiow/Urban100)\n\nThe results columns below are represented below as `PSNR/SSIM`. They are compared against a Bicubic baseline.\n\n|Dataset  \t    |Scale      |Bicubic  \t        |edsr-base  \t        |\n|---\t        |---\t    |---\t            |---\t                |\n|Set5  \t        |2x         |33.64/0.9292       |**38.02/0.9607**       |\n|Set5  \t        |3x  \t    |30.39/0.8678  \t    |**35.04/0.9403**  \t    |\n|Set5  \t        |4x  \t    |28.42/0.8101  \t    |**32.12/0.8947**       |\n|Set14  \t    |2x         |30.22/0.8683  \t    |**33.57/0.9172**  \t    |\n|Set14  \t    |3x         |27.53/0.7737  \t    |**30.93/0.8567**  \t    |\n|Set14  \t    |4x         |25.99/0.7023  \t    |**28.60/0.7815**  \t    |\n|BSD100  \t    |2x  \t    |29.55/0.8425  \t    |**32.21/0.8999**  \t    |\n|BSD100  \t    |3x  \t    |27.20/0.7382  \t    |**29.65/0.8204**  \t    |\n|BSD100  \t    |4x  \t    |25.96/0.6672  \t    |**27.61/0.7363**  \t    |\n|Urban100  \t    |2x  \t    |26.66/0.8408  \t    |**32.04/0.9276**  \t    |\n|Urban100  \t    |3x  \t    |  \t                |**29.23/0.8723**  \t    |\n|Urban100  \t    |4x  \t    |23.14/0.6573  \t    |**26.02/0.7832**  \t    |\n\n![Comparing Bicubic upscaling against x2 upscaling on Set5 Image 2](images/Set5_2_compare.png \"Comparing Bicubic upscaling against x2 upscaling on Set5 Image 2\")\n\nYou can find a notebook to easily run evaluation on pretrained models below:\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Evaluate_Pretrained_super_image_Models.ipynb \"Open in Colab\")\n created at: 2022-03-02\n- model_name: eugenesiow/msrn-bam\n  description: The goal of image super resolution is to restore a high resolution (HR) image from a single low resolution (LR) image. The image below shows the ground truth (HR), the bicubic upscaling x2 and model upscaling x2.\n\n![Comparing Bicubic upscaling against the models x4 upscaling on Set5 Image 4](images/msrn_4_4_compare.png \"Comparing Bicubic upscaling against the models x4 upscaling on Set5 Image 4\")\n## Model description\nThe MSRN model proposes a feature extraction structure called the multi-scale residual block. This module can \"adaptively detect image features at different scales\" and \"exploit the potential features of the image\".\n\nThis model also applies the balanced attention (BAM) method invented by [Wang et al. (2021)](https://arxiv.org/abs/2104.07566) to further improve the results.\n## Intended uses & limitations\nYou can use the pre-trained models for upscaling your images 2x, 3x and 4x. You can also use the trainer to train a model on your own dataset.\n### How to use\nThe model can be used with the [super_image](https://github.com/eugenesiow/super-image) library:\n```bash\npip install super-image\n```\nHere is how to use a pre-trained model to upscale your image:\n```python\nfrom super_image import MsrnModel, ImageLoader\nfrom PIL import Image\nimport requests\n\nurl = 'https://paperswithcode.com/media/datasets/Set5-0000002728-07a9793f_zA3bDjj.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel = MsrnModel.from_pretrained('eugenesiow/msrn-bam', scale=2)      # scale 2, 3 and 4 models available\ninputs = ImageLoader.load_image(image)\npreds = model(inputs)\n\nImageLoader.save_image(preds, './scaled_2x.png')                        # save the output 2x scaled image to `./scaled_2x.png`\nImageLoader.save_compare(inputs, preds, './scaled_2x_compare.png')      # save an output comparing the super-image with a bicubic scaling\n```\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Upscale_Images_with_Pretrained_super_image_Models.ipynb \"Open in Colab\")\n## Training data\nThe models for 2x, 3x and 4x image super resolution were pretrained on [DIV2K](https://huggingface.co/datasets/eugenesiow/Div2k), a dataset of 800 high-quality (2K resolution) images for training, augmented to 4000 images and uses a dev set of  100 validation images (images numbered 801 to 900). \n## Training procedure\n### Preprocessing\nWe follow the pre-processing and training method of [Wang et al.](https://arxiv.org/abs/2104.07566).\nLow Resolution (LR) images are created by using bicubic interpolation as the resizing method to reduce the size of the High Resolution (HR) images by x2, x3 and x4 times.\nDuring training, RGB patches with size of 64√ó64 from the LR input are used together with their corresponding HR patches. \nData augmentation is applied to the training set in the pre-processing stage where five images are created from the four corners and center of the original image. \n\nWe need the huggingface [datasets](https://huggingface.co/datasets?filter=task_ids:other-other-image-super-resolution) library to download the data:\n```bash\npip install datasets\n```\nThe following code gets the data and preprocesses/augments the data.\n\n```python\nfrom datasets import load_dataset\nfrom super_image.data import EvalDataset, TrainDataset, augment_five_crop\n\naugmented_dataset = load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='train')\\\n    .map(augment_five_crop, batched=True, desc=\"Augmenting Dataset\")                                # download and augment the data with the five_crop method\ntrain_dataset = TrainDataset(augmented_dataset)                                                     # prepare the train dataset for loading PyTorch DataLoader\neval_dataset = EvalDataset(load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='validation'))      # prepare the eval dataset for the PyTorch DataLoader\n```\n### Pretraining\nThe model was trained on GPU. The training code is provided below:\n```python\nfrom super_image import Trainer, TrainingArguments, MsrnModel, MsrnConfig\n\ntraining_args = TrainingArguments(\n    output_dir='./results',                 # output directory\n    num_train_epochs=1000,                  # total number of training epochs\n)\n\nconfig = MsrnConfig(\n    scale=4,                                # train a model to upscale 4x\n    bam=True,                               # apply balanced attention to the network\n)\nmodel = MsrnModel(config)\n\ntrainer = Trainer(\n    model=model,                         # the instantiated model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=eval_dataset            # evaluation dataset\n)\n\ntrainer.train()\n```\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Train_super_image_Models.ipynb \"Open in Colab\")\n## Evaluation results\nThe evaluation metrics include [PSNR](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio#Quality_estimation_with_PSNR) and [SSIM](https://en.wikipedia.org/wiki/Structural_similarity#Algorithm). \n\nEvaluation datasets include:\n- Set5 - [Bevilacqua et al. (2012)](https://huggingface.co/datasets/eugenesiow/Set5)\n- Set14 - [Zeyde et al. (2010)](https://huggingface.co/datasets/eugenesiow/Set14)\n- BSD100 - [Martin et al. (2001)](https://huggingface.co/datasets/eugenesiow/BSD100)\n- Urban100 - [Huang et al. (2015)](https://huggingface.co/datasets/eugenesiow/Urban100)\n\nThe results columns below are represented below as `PSNR/SSIM`. They are compared against a Bicubic baseline.\n\n|Dataset  \t    |Scale      |Bicubic  \t        |msrn-bam  \t                    |\n|---\t        |---\t    |---\t            |---\t                        |\n|Set5  \t        |2x         |33.64/0.9292       |**38.02/0.9608**       |\n|Set5  \t        |3x  \t    |30.39/0.8678  \t    |**35.13/0.9408**  \t    |\n|Set5  \t        |4x  \t    |28.42/0.8101  \t    |**32.26/0.8955**       |\n|Set14  \t    |2x         |30.22/0.8683  \t    |**33.73/0.9186**  \t    |\n|Set14  \t    |3x         |27.53/0.7737  \t    |**31.06/0.8588**  \t    |\n|Set14  \t    |4x         |25.99/0.7023  \t    |**28.78/0.7859**  \t    |\n|BSD100  \t    |2x  \t    |29.55/0.8425  \t    |**33.78/0.9253**  \t    |\n|BSD100  \t    |3x  \t    |27.20/0.7382  \t    |**29.65/0.8196**  \t    |\n|BSD100  \t    |4x  \t    |25.96/0.6672  \t    |**28.51/0.7651**  \t    |\n|Urban100  \t    |2x  \t    |26.66/0.8408  \t    |**32.08/0.9276**  \t    |\n|Urban100  \t    |3x  \t    |  \t                |**29.26/0.8736**  \t    |\n|Urban100  \t    |4x  \t    |23.14/0.6573  \t    |**26.10/0.7857**  \t    |\n\n![Comparing Bicubic upscaling against the models x4 upscaling on Set5 Image 2](images/msrn_2_4_compare.png \"Comparing Bicubic upscaling against the models x4 upscaling on Set5 Image 2\")\n\nYou can find a notebook to easily run evaluation on pretrained models below:\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Evaluate_Pretrained_super_image_Models.ipynb \"Open in Colab\")\n created at: 2022-03-02\n- model_name: ZhengPeng7/BiRefNet_HR-matting\n  description: this birefnet was trained with images in `2048x2048` for higher resolution image matting with transparency. all tested in fp16 mode. dataset method resolution maxfm wfmeasure mae smeasure meanem hce maxem meanfm adpem adpfm mba maxbiou meanbiou\n created at: 2025-02-12T16:19:23+00:00\n- model_name: PramaLLC/BEN2\n  description: ben2 background erase network introduces a novel approach to foreground segmentation through its innovative confidence guided matting cgm pipeline. the architecture employs a refiner network that targets and processes pixels where the base model exhibits lower confidence levels, resulting in more precise and reliable matting results. this model is built on ben: ben2 was trained on the dis5k and our 22k proprietary segmentation dataset. our enhanced model delivers superior performance in hair matting, 4k processing, object segmentation, and edge refinement. our base model is open source. to try the full model through our free web demo or integrate ben2 into your project with our api: - üåê\n created at: 2025-01-22T14:39:05+00:00\n- model_name: ZhengPeng7/BiRefNet_HR\n  description: this birefnet was trained with images in `2048x2048` for higher resolution inference. all tested in fp16 mode. dataset method resolution maxfm wfmeasure mae smeasure meanem hce maxem meanfm adpem adpfm mba maxbiou meanbiou\n created at: 2025-02-01T17:52:14+00:00\n- model_name: briaai/RMBG-2.0\n  description: rmbg v2.0 is our new state-of-the-art background removal model significantly improves rmbg v1.4. the model is designed to effectively separate foreground from background in a range of categories and image types. this model has been trained on a carefully selected dataset, which includes: general stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale. - developed by:  - model type: background removal - license:  - the model is released under a cc by-nc 4.0 license for non-commercial use. - commercial use is subject to a commercial agreement with bria. available  purchase: to purchase a commercial license simply click . - model description: bria rmbg-2.0 is a dichotomous image segmentation model trained exclusively on a professional-grade dataset. the model output includes a single-channel 8-bit grayscale alpha matte, where each pixel value indicates the opacity level of the corresponding pixel in the original image. this non-binary output approach offers developers the flexibility to define custom thresholds for foreground-background separation, catering to varied use cases requirements and enhancing integration into complex pipelines. - bria: resources for more information:\n created at: 2024-10-29T12:14:36+00:00\n- model_name: ZhengPeng7/BiRefNet_dynamic\n  description: bilateral reference for high-resolution dichotomous image segmentation an arbitrary shape adaptable birefnet for general segmentation. this model was trained on arbitrary shapes 256x256 2304x2304 and shows great robustness on inputs with any shape.\n created at: 2025-03-31T03:37:03+00:00\n- model_name: Ricky06662/Seg-Zero-7B\n  description: this model is based on the paper . it uses a decoupled architecture with a reasoning model and a segmentation model. it's trained via reinforcement learning using grpo without explicit reasoning data, leading to robust zero-shot generalization and emergent test-time reasoning. code: this is a seg-zero-7b model. it introduces a decoupled architecture consisting of a reasoning model and a segmentation model. the reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate pixel-level masks. this is a seg-zero-7b model. it introduces a decoupled architecture consisting of a reasoning model and a segmentation model. the reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate pixel-level masks.\n created at: 2025-03-07T15:08:24+00:00\n- model_name: cocktailpeanut/rm\n  description: rmbg v2.0 is our new state-of-the-art background removal model significantly improves rmbg v1.4. the model is designed to effectively separate foreground from background in a range of categories and image types. this model has been trained on a carefully selected dataset, which includes: general stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale. - developed by:  - model type: background removal - license:  - the model is released under a cc by-nc 4.0 license for non-commercial use. - commercial use is subject to a commercial agreement with bria. available  purchase: to purchase a commercial license simply click . - model description: bria rmbg-2.0 is a dichotomous image segmentation model trained exclusively on a professional-grade dataset. - bria: resources for more information:\n created at: 2025-04-12T06:16:07+00:00\n- model_name: yuvraj108c/RMBG-2.0\n  description: rmbg v2.0 is our new state-of-the-art background removal model significantly improves rmbg v1.4. the model is designed to effectively separate foreground from background in a range of categories and image types. this model has been trained on a carefully selected dataset, which includes: general stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale. - developed by:  - model type: background removal - license:  - the model is released under a cc by-nc 4.0 license for non-commercial use. - commercial use is subject to a commercial agreement with bria. available  purchase: to purchase a commercial license simply click . - model description: bria rmbg-2.0 is a dichotomous image segmentation model trained exclusively on a professional-grade dataset. the model output includes a single-channel 8-bit grayscale alpha matte, where each pixel value indicates the opacity level of the corresponding pixel in the original image. this non-binary output approach offers developers the flexibility to define custom thresholds for foreground-background separation, catering to varied use cases requirements and enhancing integration into complex pipelines. - bria: resources for more information:\n created at: 2025-07-28T10:18:49+00:00\n- model_name: Ricky06662/VisionReasoner-7B\n  description: this repository contains the visionreasoner-7b model, developed as part of the novel seg-zero framework, presented in the paper . this model is also associated with the paper . code: project page:  seg-zero is a novel framework that demonstrates remarkable generalizability and derives explicit chain-of-thought reasoning through cognitive reinforcement for reasoning segmentation. this visionreasoner-7b model employs a decoupled architecture consisting of a reasoning model and a segmentation model. the reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate precise pixel-level masks. trained exclusively via reinforcement learning with grpo and without explicit reasoning data, seg-zero achieves robust zero-shot generalization and exhibits emergent test-time reasoning capabilities. experiments show that seg-zero-7b achieves a zero-shot performance of 57.5 on the reasonseg benchmark, surpassing the prior lisa-7b by 18%. this significant improvement highlights seg-zero's ability to generalize across domains while presenting an explicit reasoning process.\n created at: 2025-05-18T05:46:01+00:00\n- model_name: tue-mps/coco_panoptic_eomt_large_640\n  description: eomt encoder-only mask transformer is a vision transformer vit architecture designed for high-quality and efficient image segmentation. it was introduced in the cvpr 2025 highlight paper: by tommie kerssies, niccol√≤ cavagnero, alexander hermans, narges norouzi, giuseppe averta, bastian leibe, gijs dubbelman, and daan de geus.\n created at: 2025-03-26T17:28:05+00:00\n- model_name: tue-mps/ade20k_semantic_eomt_large_512\n  description: eomt encoder-only mask transformer is a vision transformer vit architecture designed for high-quality and efficient image segmentation. it was introduced in the cvpr 2025 highlight paper: by tommie kerssies, niccol√≤ cavagnero, alexander hermans, narges norouzi, giuseppe averta, bastian leibe, gijs dubbelman, and daan de geus.\n created at: 2025-03-26T19:29:47+00:00\n- model_name: bwittmann/vesselFM\n  description: tl;dr : vesselfm is a foundation model for universal 3d blood vessel segmentation in arbitrary imaging domains. for details, please refer to our preprint and our github repo\n created at: 2024-12-02T12:45:08+00:00\n- model_name: Ricky06662/Seg-Zero-7B-Best-on-ReasonSegTest\n  description: this model is based on the paper . code: seg-zero introduces a novel framework for reasoning segmentation that addresses the limitations of traditional supervised fine-tuning methods, which often struggle with out-of-domain generalization and lack explicit reasoning processes. the framework features a decoupled architecture consisting of a reasoning model and a segmentation model. the reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate precise pixel-level masks.\n created at: 2025-04-09T16:12:22+00:00\n- model_name: tue-mps/coco_instance_eomt_large_640\n  description: eomt encoder-only mask transformer is a vision transformer vit architecture designed for high-quality and efficient image segmentation. it was introduced in the cvpr 2025 highlight paper: by tommie kerssies, niccol√≤ cavagnero, alexander hermans, narges norouzi, giuseppe averta, bastian leibe, gijs dubbelman, and daan de geus.\n created at: 2025-03-26T17:16:21+00:00\n- model_name: facebook/sapiens-seg-1b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T20:26:24+00:00\n- model_name: tue-mps/coco_instance_eomt_large_1280\n  description: eomt encoder-only mask transformer is a vision transformer vit architecture designed for high-quality and efficient image segmentation. it was introduced in the cvpr 2025 highlight paper: by tommie kerssies, niccol√≤ cavagnero, alexander hermans, narges norouzi, giuseppe averta, bastian leibe, gijs dubbelman, and daan de geus.\n created at: 2025-03-26T19:34:47+00:00\n- model_name: nevernever69/dit-doclaynet-segmentation\n  description: this model is a fine-tuned version of for document layout semantic segmentation on the dataset small subset: `nevernever69/small-doclaynet-v1.1`. it segments scanned document images into 11 layout categories such as title, paragraph, table, and footer. - segment document images into structured layout elements - assist in downstream tasks like document ocr, archiving, and automatic annotation\n created at: 2025-03-12T11:45:04+00:00\n- model_name: Pushti/BirefNetHR\n  description: this birefnet was trained with images in `2048x2048` for higher resolution inference. all tested in fp16 mode. dataset method resolution maxfm wfmeasure mae smeasure meanem hce maxem meanfm adpem adpfm mba maxbiou meanbiou\n created at: 2025-04-18T07:28:32+00:00\n- model_name: ZQL9711/RMBG-2-Matting\n  description: rmbg v2.0 is our new state-of-the-art background removal model significantly improves rmbg v1.4. the model is designed to effectively separate foreground from background in a range of categories and image types. this model has been trained on a carefully selected dataset, which includes: general stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale. - developed by:  - model type: background removal - license:  - the model is released under a cc by-nc 4.0 license for non-commercial use. - commercial use is subject to a commercial agreement with bria. available  purchase: to purchase a commercial license simply click . - model description: bria rmbg-2.0 is a dichotomous image segmentation model trained exclusively on a professional-grade dataset. - bria: resources for more information:\n created at: 2025-07-05T08:31:53+00:00\n- model_name: Acly/BiRefNet-GGUF\n  description: birefnet is a model for dichotomous image segmentation background removal. the weights in this repository are converted for lightweight inference on consumer hardware with .\n created at: 2025-07-20T10:33:31+00:00\n\nExamples:\n- example user query: Our company is building a software to analyze satellite images for land use. We need to segment different regions in the images.\n  model_name: nvidia/segformer-b0-finetuned-ade-512-512\n- example user query: Identify an API to perform semantic segmentation of the street labeled with road, sidewalk, and building for a given picture.\n  model_name: pytorch/vision/deeplabv3_resnet50\n- example user query: A drone is flying over the neighborhood and is recording video. How can I process the video and perform semantic segmentation on each frame?\n  model_name: pytorch/vision/deeplabv3_resnet101\n- example user query: To identify and separate different objects in an image, create an image segmentation model.\n  model_name: openmmlab/upernet-convnext-small\n- example user query: The company is developing a fashion application which needs to identify the clothing items in pictures.\n  model_name: mattmdjaga/segformer_b2_clothes\n- example user query: An urban development company wants to analyze city images to inform infrastructure plans. Extract features using a semantic segmentation model.\n  model_name: nvidia/segformer-b5-finetuned-cityscapes-1024-1024\n\nUser prompt:\nDivide each object in the visual.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_seg>\n\nCandidate Models:\n- model_name: CIDAS/clipseg-rd64-refined\n  description: CLIPSeg model with reduce dimension 64, refined (using a more complex convolution). It was introduced in the paper Image Segmentation Using Text and Image Prompts by L√ºddecke et al. and first released in this repository. This model is intended for zero-shot and one-shot image segmentation.\n created at: 2022-11-01\n- model_name: openmmlab/upernet-convnext-small\n  description: UperNet framework for semantic segmentation, leveraging a ConvNeXt backbone. UperNet was introduced in the paper Unified Perceptual Parsing for Scene Understanding by Xiao et al. Combining UperNet with a ConvNeXt backbone was introduced in the paper A ConvNet for the 2020s.\n created at: 2023-01-13\n- model_name: nvidia/segformer-b0-finetuned-ade-512-512\n  description: SegFormer model fine-tuned on ADE20k at resolution 512x512. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: nvidia/segformer-b5-finetuned-ade-640-640\n  description: SegFormer model fine-tuned on ADE20k at resolution 640x640. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: nvidia/segformer-b2-finetuned-cityscapes-1024-1024\n  description: SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: nvidia/segformer-b0-finetuned-cityscapes-1024-1024\n  description: SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: facebook/maskformer-swin-base-coco\n  description: MaskFormer model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.\n created at: 2022-03-02\n- model_name: facebook/mask2former-swin-large-cityscapes-semantic\n  description: Mask2Former model trained on Cityscapes semantic segmentation (large-sized version, Swin backbone). It addresses instance, semantic and panoptic segmentation by predicting a set of masks and corresponding labels. The model outperforms the previous SOTA, MaskFormer, in terms of performance and efficiency.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-base-coco-panoptic\n  description: Mask2Former model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.\n created at: 2023-01-02\n- model_name: mattmdjaga/segformer_b2_clothes\n  description: SegFormer model fine-tuned on ATR dataset for clothes segmentation.\n created at: 2022-11-24\n- model_name: shi-labs/oneformer_coco_swin_large\n  description: OneFormer model trained on the COCO dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.\n created at: 2022-11-15\n- model_name: facebook/mask2former-swin-large-coco-panoptic\n  description: Mask2Former model trained on COCO panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.\n created at: 2023-01-02\n- model_name: shi-labs/oneformer_ade20k_swin_large\n  description: OneFormer model trained on the ADE20k dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.\n created at: 2022-11-15\n- model_name: facebook/mask2former-swin-small-coco-instance\n  description: Mask2Former model trained on COCO instance segmentation (small-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.\n created at: 2022-12-26\n- model_name: nvidia/segformer-b5-finetuned-cityscapes-1024-1024\n  description: SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: shi-labs/oneformer_ade20k_swin_tiny\n  description: OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.\n created at: 2022-11-16\n- model_name: keremberke/yolov8m-building-segmentation\n  description: A YOLOv8 model for building segmentation in satellite images. It can detect and segment buildings in the input images.\n created at: 2023-01-27\n- model_name: facebook/maskformer-swin-base-ade\n  description: MaskFormer model trained on ADE20k semantic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.\n created at: 2022-03-02\n- model_name: facebook/mask2former-swin-tiny-coco-instance\n  description: Mask2Former model trained on COCO instance segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. You can use this particular checkpoint for instance segmentation.\n created at: 2022-12-23\n- model_name: keremberke/yolov8m-pcb-defect-segmentation\n  description: A YOLOv8 model for PCB defect segmentation trained on the pcb-defect-segmentation dataset. The model can detect and segment defects in PCB images, such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.\n created at: 2023-01-28\n- model_name: facebook/maskformer-swin-tiny-coco\n  description: MaskFormer model trained on COCO panoptic segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.\n created at: 2022-03-02\n- model_name: keremberke/yolov8m-pothole-segmentation\n  description: A YOLOv8 model for pothole segmentation trained on keremberke/pothole-segmentation dataset. It can detect potholes in images and provide segmentation masks for the detected potholes.\n created at: 2023-01-26\n- model_name: keremberke/yolov8s-building-segmentation\n  description: A YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can detect and segment buildings with high accuracy.\n created at: 2023-01-26\n- model_name: keremberke/yolov8s-pothole-segmentation\n  description: A YOLOv8 model for pothole segmentation. This model detects potholes in images and outputs bounding boxes and masks for the detected potholes.\n created at: 2023-01-26\n- model_name: keremberke/yolov8s-pcb-defect-segmentation\n  description: YOLOv8s model for PCB defect segmentation. The model is trained to detect and segment PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.\n created at: 2023-01-28\n- model_name: keremberke/yolov8n-pcb-defect-segmentation\n  description: A YOLOv8 model for detecting and segmenting PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.\n created at: 2023-01-28\n- model_name: keremberke/yolov8n-pothole-segmentation\n  description: A YOLOv8 model for pothole segmentation in images. The model is trained on the pothole-segmentation dataset and achieves high accuracy in detecting potholes.\n created at: 2023-01-15\n- model_name: facebook/maskformer-swin-large-ade\n  description: MaskFormer model trained on ADE20k semantic segmentation (large-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.\n created at: 2022-03-02\n- model_name: facebook/detr-resnet-50-panoptic\n  description: DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 panoptic (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: tensorflow/lite-model/deeplabv3/1/default\n  description: TensorFlow Hub provides pre-trained image segmentation models that can be loaded and used for various tasks such as semantic segmentation. The example code demonstrates how to load a DeepLabV3 model and perform segmentation on an input image tensor.\n created at: 2020-10-06\n- model_name: pytorch/vision/deeplabv3_resnet50\n  description: DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: pytorch/vision/deeplabv3_mobilenet_v3_large\n  description: DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: pytorch/vision/deeplabv3_resnet101\n  description: DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: mateuszbuda/brain-segmentation-pytorch/unet\n  description: U-Net with batch normalization for biomedical image segmentation with pretrained weights for abnormality segmentation in brain MRI. The model comprises four levels of blocks containing two convolutional layers with batch normalization and ReLU activation function, and one max pooling layer in the encoding part and up-convolutional layers instead in the decoding part. The number of convolutional filters in each block is 32, 64, 128, and 256. The bottleneck layer has 512 convolutional filters. From the encoding layers, skip connections are used to the corresponding layers in the decoding part. Input image is a 3-channel brain MRI slice from pre-contrast, FLAIR, and post-contrast sequences, respectively. Output is a one-channel probability map of abnormality regions with the same size as the input image.\n created at: 2019-05-26\n- model_name: pytorch/vision/fcn_resnet101\n  description: FCN-ResNet is a Fully-Convolutional Network model using a ResNet-50 or a ResNet-101 backbone. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: pytorch/fairseq\n  description: Transformer (NMT) is a powerful sequence-to-sequence modeling architecture that produces state-of-the-art neural machine translation systems. It is based on the paper 'Attention Is All You Need' and has been improved using techniques such as large-scale semi-supervised training, back-translation, and noisy-channel reranking. It supports English-French and English-German translation as well as round-trip translation for paraphrasing.\n created at: 2017-08-29\n- model_name: pytorch/vision/fcn_resnet50\n  description: FCN-ResNet is a Fully-Convolutional Network model using a ResNet-50 or a ResNet-101 backbone. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: facebook/mask2former-swin-large-ade-semantic\n  description: Mask2Former model trained on ADE20k semantic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-large-ade-panoptic\n  description: Mask2Former model trained on ADE20k panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-large-mapillary-vistas-panoptic\n  description: Mask2Former model trained on Mapillary Vistas panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-large-mapillary-vistas-semantic\n  description: Mask2Former model trained on Mapillary Vistas semantic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-large-cityscapes-panoptic\n  description: Mask2Former model trained on Cityscapes panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-03\n- model_name: shi-labs/oneformer_cityscapes_swin_large\n  description: OneFormer model trained on the Cityscapes dataset (large-sized version, Swin backbone). It was introduced in the paper [OneFormer: One Transformer to Rule Universal Image Segmentation](https://arxiv.org/abs/2211.06220) by Jain et al. and first released in [this repository](https://github.com/SHI-Labs/OneFormer).\n\n![model image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/oneformer_teaser.png)\n created at: 2022-11-15\n- model_name: google/deeplabv3_mobilenet_v2_1.0_513\n  description: MobileNet V2 model pre-trained on PASCAL VOC at resolution 513x513. It was introduced in [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381) by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen. It was first released in [this repository](https://github.com/tensorflow/models/tree/master/research/deeplab).\n\nDisclaimer: The team releasing MobileNet V2 did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2022-11-10\n- model_name: apple/deeplabv3-mobilevit-small\n  description: MobileViT model pre-trained on PASCAL VOC at resolution 512x512. It was introduced in [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer](https://arxiv.org/abs/2110.02178) by Sachin Mehta and Mohammad Rastegari, and first released in [this repository](https://github.com/apple/ml-cvnets). The license used is [Apple sample code license](https://github.com/apple/ml-cvnets/blob/main/LICENSE).\n\nDisclaimer: The team releasing MobileViT did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2022-05-30\n- model_name: eugenesiow/edsr-base\n  description: The goal of image super resolution is to restore a high resolution (HR) image from a single low resolution (LR) image. The image below shows the ground truth (HR), the bicubic upscaling x2 and EDSR upscaling x2.\n\n![Comparing Bicubic upscaling against EDSR x2 upscaling on Set5 Image 4](images/Set5_4_compare.png \"Comparing Bicubic upscaling against EDSR x2 upscaling on Set5 Image 4\")\n## Model description\nEDSR is a model that uses both deeper and wider architecture (32 ResBlocks and 256 channels) to improve performance. It uses both global and local skip connections, and up-scaling is done at the end of the network. It doesn't use batch normalization layers (input and output have similar distributions, normalizing intermediate features may not be desirable) instead it uses constant scaling layers to ensure stable training. An L1 loss function (absolute error) is used instead of L2 (MSE), the authors showed better performance empirically and it requires less computation.\n\nThis is a base model (~5mb vs ~100mb) that includes just 16 ResBlocks and 64 channels.\n## Intended uses & limitations\nYou can use the pre-trained models for upscaling your images 2x, 3x and 4x. You can also use the trainer to train a model on your own dataset.\n### How to use\nThe model can be used with the [super_image](https://github.com/eugenesiow/super-image) library:\n```bash\npip install super-image\n```\nHere is how to use a pre-trained model to upscale your image:\n```python\nfrom super_image import EdsrModel, ImageLoader\nfrom PIL import Image\nimport requests\n\nurl = 'https://paperswithcode.com/media/datasets/Set5-0000002728-07a9793f_zA3bDjj.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel = EdsrModel.from_pretrained('eugenesiow/edsr-base', scale=2)      # scale 2, 3 and 4 models available\ninputs = ImageLoader.load_image(image)\npreds = model(inputs)\n\nImageLoader.save_image(preds, './scaled_2x.png')                        # save the output 2x scaled image to `./scaled_2x.png`\nImageLoader.save_compare(inputs, preds, './scaled_2x_compare.png')      # save an output comparing the super-image with a bicubic scaling\n```\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Upscale_Images_with_Pretrained_super_image_Models.ipynb \"Open in Colab\")\n## Training data\nThe models for 2x, 3x and 4x image super resolution were pretrained on [DIV2K](https://huggingface.co/datasets/eugenesiow/Div2k), a dataset of 800 high-quality (2K resolution) images for training, augmented to 4000 images and uses a dev set of  100 validation images (images numbered 801 to 900). \n## Training procedure\n### Preprocessing\nWe follow the pre-processing and training method of [Wang et al.](https://arxiv.org/abs/2104.07566).\nLow Resolution (LR) images are created by using bicubic interpolation as the resizing method to reduce the size of the High Resolution (HR) images by x2, x3 and x4 times.\nDuring training, RGB patches with size of 64√ó64 from the LR input are used together with their corresponding HR patches. \nData augmentation is applied to the training set in the pre-processing stage where five images are created from the four corners and center of the original image. \n\nWe need the huggingface [datasets](https://huggingface.co/datasets?filter=task_ids:other-other-image-super-resolution) library to download the data:\n```bash\npip install datasets\n```\nThe following code gets the data and preprocesses/augments the data.\n\n```python\nfrom datasets import load_dataset\nfrom super_image.data import EvalDataset, TrainDataset, augment_five_crop\n\naugmented_dataset = load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='train')\\\n    .map(augment_five_crop, batched=True, desc=\"Augmenting Dataset\")                                # download and augment the data with the five_crop method\ntrain_dataset = TrainDataset(augmented_dataset)                                                     # prepare the train dataset for loading PyTorch DataLoader\neval_dataset = EvalDataset(load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='validation'))      # prepare the eval dataset for the PyTorch DataLoader\n```\n### Pretraining\nThe model was trained on GPU. The training code is provided below:\n```python\nfrom super_image import Trainer, TrainingArguments, EdsrModel, EdsrConfig\n\ntraining_args = TrainingArguments(\n    output_dir='./results',                 # output directory\n    num_train_epochs=1000,                  # total number of training epochs\n)\n\nconfig = EdsrConfig(\n    scale=4,                                # train a model to upscale 4x\n)\nmodel = EdsrModel(config)\n\ntrainer = Trainer(\n    model=model,                         # the instantiated model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=eval_dataset            # evaluation dataset\n)\n\ntrainer.train()\n```\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Train_super_image_Models.ipynb \"Open in Colab\")\n## Evaluation results\nThe evaluation metrics include [PSNR](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio#Quality_estimation_with_PSNR) and [SSIM](https://en.wikipedia.org/wiki/Structural_similarity#Algorithm). \n\nEvaluation datasets include:\n- Set5 - [Bevilacqua et al. (2012)](https://huggingface.co/datasets/eugenesiow/Set5)\n- Set14 - [Zeyde et al. (2010)](https://huggingface.co/datasets/eugenesiow/Set14)\n- BSD100 - [Martin et al. (2001)](https://huggingface.co/datasets/eugenesiow/BSD100)\n- Urban100 - [Huang et al. (2015)](https://huggingface.co/datasets/eugenesiow/Urban100)\n\nThe results columns below are represented below as `PSNR/SSIM`. They are compared against a Bicubic baseline.\n\n|Dataset  \t    |Scale      |Bicubic  \t        |edsr-base  \t        |\n|---\t        |---\t    |---\t            |---\t                |\n|Set5  \t        |2x         |33.64/0.9292       |**38.02/0.9607**       |\n|Set5  \t        |3x  \t    |30.39/0.8678  \t    |**35.04/0.9403**  \t    |\n|Set5  \t        |4x  \t    |28.42/0.8101  \t    |**32.12/0.8947**       |\n|Set14  \t    |2x         |30.22/0.8683  \t    |**33.57/0.9172**  \t    |\n|Set14  \t    |3x         |27.53/0.7737  \t    |**30.93/0.8567**  \t    |\n|Set14  \t    |4x         |25.99/0.7023  \t    |**28.60/0.7815**  \t    |\n|BSD100  \t    |2x  \t    |29.55/0.8425  \t    |**32.21/0.8999**  \t    |\n|BSD100  \t    |3x  \t    |27.20/0.7382  \t    |**29.65/0.8204**  \t    |\n|BSD100  \t    |4x  \t    |25.96/0.6672  \t    |**27.61/0.7363**  \t    |\n|Urban100  \t    |2x  \t    |26.66/0.8408  \t    |**32.04/0.9276**  \t    |\n|Urban100  \t    |3x  \t    |  \t                |**29.23/0.8723**  \t    |\n|Urban100  \t    |4x  \t    |23.14/0.6573  \t    |**26.02/0.7832**  \t    |\n\n![Comparing Bicubic upscaling against x2 upscaling on Set5 Image 2](images/Set5_2_compare.png \"Comparing Bicubic upscaling against x2 upscaling on Set5 Image 2\")\n\nYou can find a notebook to easily run evaluation on pretrained models below:\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Evaluate_Pretrained_super_image_Models.ipynb \"Open in Colab\")\n created at: 2022-03-02\n- model_name: eugenesiow/msrn-bam\n  description: The goal of image super resolution is to restore a high resolution (HR) image from a single low resolution (LR) image. The image below shows the ground truth (HR), the bicubic upscaling x2 and model upscaling x2.\n\n![Comparing Bicubic upscaling against the models x4 upscaling on Set5 Image 4](images/msrn_4_4_compare.png \"Comparing Bicubic upscaling against the models x4 upscaling on Set5 Image 4\")\n## Model description\nThe MSRN model proposes a feature extraction structure called the multi-scale residual block. This module can \"adaptively detect image features at different scales\" and \"exploit the potential features of the image\".\n\nThis model also applies the balanced attention (BAM) method invented by [Wang et al. (2021)](https://arxiv.org/abs/2104.07566) to further improve the results.\n## Intended uses & limitations\nYou can use the pre-trained models for upscaling your images 2x, 3x and 4x. You can also use the trainer to train a model on your own dataset.\n### How to use\nThe model can be used with the [super_image](https://github.com/eugenesiow/super-image) library:\n```bash\npip install super-image\n```\nHere is how to use a pre-trained model to upscale your image:\n```python\nfrom super_image import MsrnModel, ImageLoader\nfrom PIL import Image\nimport requests\n\nurl = 'https://paperswithcode.com/media/datasets/Set5-0000002728-07a9793f_zA3bDjj.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel = MsrnModel.from_pretrained('eugenesiow/msrn-bam', scale=2)      # scale 2, 3 and 4 models available\ninputs = ImageLoader.load_image(image)\npreds = model(inputs)\n\nImageLoader.save_image(preds, './scaled_2x.png')                        # save the output 2x scaled image to `./scaled_2x.png`\nImageLoader.save_compare(inputs, preds, './scaled_2x_compare.png')      # save an output comparing the super-image with a bicubic scaling\n```\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Upscale_Images_with_Pretrained_super_image_Models.ipynb \"Open in Colab\")\n## Training data\nThe models for 2x, 3x and 4x image super resolution were pretrained on [DIV2K](https://huggingface.co/datasets/eugenesiow/Div2k), a dataset of 800 high-quality (2K resolution) images for training, augmented to 4000 images and uses a dev set of  100 validation images (images numbered 801 to 900). \n## Training procedure\n### Preprocessing\nWe follow the pre-processing and training method of [Wang et al.](https://arxiv.org/abs/2104.07566).\nLow Resolution (LR) images are created by using bicubic interpolation as the resizing method to reduce the size of the High Resolution (HR) images by x2, x3 and x4 times.\nDuring training, RGB patches with size of 64√ó64 from the LR input are used together with their corresponding HR patches. \nData augmentation is applied to the training set in the pre-processing stage where five images are created from the four corners and center of the original image. \n\nWe need the huggingface [datasets](https://huggingface.co/datasets?filter=task_ids:other-other-image-super-resolution) library to download the data:\n```bash\npip install datasets\n```\nThe following code gets the data and preprocesses/augments the data.\n\n```python\nfrom datasets import load_dataset\nfrom super_image.data import EvalDataset, TrainDataset, augment_five_crop\n\naugmented_dataset = load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='train')\\\n    .map(augment_five_crop, batched=True, desc=\"Augmenting Dataset\")                                # download and augment the data with the five_crop method\ntrain_dataset = TrainDataset(augmented_dataset)                                                     # prepare the train dataset for loading PyTorch DataLoader\neval_dataset = EvalDataset(load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='validation'))      # prepare the eval dataset for the PyTorch DataLoader\n```\n### Pretraining\nThe model was trained on GPU. The training code is provided below:\n```python\nfrom super_image import Trainer, TrainingArguments, MsrnModel, MsrnConfig\n\ntraining_args = TrainingArguments(\n    output_dir='./results',                 # output directory\n    num_train_epochs=1000,                  # total number of training epochs\n)\n\nconfig = MsrnConfig(\n    scale=4,                                # train a model to upscale 4x\n    bam=True,                               # apply balanced attention to the network\n)\nmodel = MsrnModel(config)\n\ntrainer = Trainer(\n    model=model,                         # the instantiated model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=eval_dataset            # evaluation dataset\n)\n\ntrainer.train()\n```\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Train_super_image_Models.ipynb \"Open in Colab\")\n## Evaluation results\nThe evaluation metrics include [PSNR](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio#Quality_estimation_with_PSNR) and [SSIM](https://en.wikipedia.org/wiki/Structural_similarity#Algorithm). \n\nEvaluation datasets include:\n- Set5 - [Bevilacqua et al. (2012)](https://huggingface.co/datasets/eugenesiow/Set5)\n- Set14 - [Zeyde et al. (2010)](https://huggingface.co/datasets/eugenesiow/Set14)\n- BSD100 - [Martin et al. (2001)](https://huggingface.co/datasets/eugenesiow/BSD100)\n- Urban100 - [Huang et al. (2015)](https://huggingface.co/datasets/eugenesiow/Urban100)\n\nThe results columns below are represented below as `PSNR/SSIM`. They are compared against a Bicubic baseline.\n\n|Dataset  \t    |Scale      |Bicubic  \t        |msrn-bam  \t                    |\n|---\t        |---\t    |---\t            |---\t                        |\n|Set5  \t        |2x         |33.64/0.9292       |**38.02/0.9608**       |\n|Set5  \t        |3x  \t    |30.39/0.8678  \t    |**35.13/0.9408**  \t    |\n|Set5  \t        |4x  \t    |28.42/0.8101  \t    |**32.26/0.8955**       |\n|Set14  \t    |2x         |30.22/0.8683  \t    |**33.73/0.9186**  \t    |\n|Set14  \t    |3x         |27.53/0.7737  \t    |**31.06/0.8588**  \t    |\n|Set14  \t    |4x         |25.99/0.7023  \t    |**28.78/0.7859**  \t    |\n|BSD100  \t    |2x  \t    |29.55/0.8425  \t    |**33.78/0.9253**  \t    |\n|BSD100  \t    |3x  \t    |27.20/0.7382  \t    |**29.65/0.8196**  \t    |\n|BSD100  \t    |4x  \t    |25.96/0.6672  \t    |**28.51/0.7651**  \t    |\n|Urban100  \t    |2x  \t    |26.66/0.8408  \t    |**32.08/0.9276**  \t    |\n|Urban100  \t    |3x  \t    |  \t                |**29.26/0.8736**  \t    |\n|Urban100  \t    |4x  \t    |23.14/0.6573  \t    |**26.10/0.7857**  \t    |\n\n![Comparing Bicubic upscaling against the models x4 upscaling on Set5 Image 2](images/msrn_2_4_compare.png \"Comparing Bicubic upscaling against the models x4 upscaling on Set5 Image 2\")\n\nYou can find a notebook to easily run evaluation on pretrained models below:\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Evaluate_Pretrained_super_image_Models.ipynb \"Open in Colab\")\n created at: 2022-03-02\n- model_name: ZhengPeng7/BiRefNet_HR-matting\n  description: this birefnet was trained with images in `2048x2048` for higher resolution image matting with transparency. all tested in fp16 mode. dataset method resolution maxfm wfmeasure mae smeasure meanem hce maxem meanfm adpem adpfm mba maxbiou meanbiou\n created at: 2025-02-12T16:19:23+00:00\n- model_name: PramaLLC/BEN2\n  description: ben2 background erase network introduces a novel approach to foreground segmentation through its innovative confidence guided matting cgm pipeline. the architecture employs a refiner network that targets and processes pixels where the base model exhibits lower confidence levels, resulting in more precise and reliable matting results. this model is built on ben: ben2 was trained on the dis5k and our 22k proprietary segmentation dataset. our enhanced model delivers superior performance in hair matting, 4k processing, object segmentation, and edge refinement. our base model is open source. to try the full model through our free web demo or integrate ben2 into your project with our api: - üåê\n created at: 2025-01-22T14:39:05+00:00\n- model_name: ZhengPeng7/BiRefNet_HR\n  description: this birefnet was trained with images in `2048x2048` for higher resolution inference. all tested in fp16 mode. dataset method resolution maxfm wfmeasure mae smeasure meanem hce maxem meanfm adpem adpfm mba maxbiou meanbiou\n created at: 2025-02-01T17:52:14+00:00\n- model_name: briaai/RMBG-2.0\n  description: rmbg v2.0 is our new state-of-the-art background removal model significantly improves rmbg v1.4. the model is designed to effectively separate foreground from background in a range of categories and image types. this model has been trained on a carefully selected dataset, which includes: general stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale. - developed by:  - model type: background removal - license:  - the model is released under a cc by-nc 4.0 license for non-commercial use. - commercial use is subject to a commercial agreement with bria. available  purchase: to purchase a commercial license simply click . - model description: bria rmbg-2.0 is a dichotomous image segmentation model trained exclusively on a professional-grade dataset. the model output includes a single-channel 8-bit grayscale alpha matte, where each pixel value indicates the opacity level of the corresponding pixel in the original image. this non-binary output approach offers developers the flexibility to define custom thresholds for foreground-background separation, catering to varied use cases requirements and enhancing integration into complex pipelines. - bria: resources for more information:\n created at: 2024-10-29T12:14:36+00:00\n- model_name: ZhengPeng7/BiRefNet_dynamic\n  description: bilateral reference for high-resolution dichotomous image segmentation an arbitrary shape adaptable birefnet for general segmentation. this model was trained on arbitrary shapes 256x256 2304x2304 and shows great robustness on inputs with any shape.\n created at: 2025-03-31T03:37:03+00:00\n- model_name: Ricky06662/Seg-Zero-7B\n  description: this model is based on the paper . it uses a decoupled architecture with a reasoning model and a segmentation model. it's trained via reinforcement learning using grpo without explicit reasoning data, leading to robust zero-shot generalization and emergent test-time reasoning. code: this is a seg-zero-7b model. it introduces a decoupled architecture consisting of a reasoning model and a segmentation model. the reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate pixel-level masks. this is a seg-zero-7b model. it introduces a decoupled architecture consisting of a reasoning model and a segmentation model. the reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate pixel-level masks.\n created at: 2025-03-07T15:08:24+00:00\n- model_name: cocktailpeanut/rm\n  description: rmbg v2.0 is our new state-of-the-art background removal model significantly improves rmbg v1.4. the model is designed to effectively separate foreground from background in a range of categories and image types. this model has been trained on a carefully selected dataset, which includes: general stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale. - developed by:  - model type: background removal - license:  - the model is released under a cc by-nc 4.0 license for non-commercial use. - commercial use is subject to a commercial agreement with bria. available  purchase: to purchase a commercial license simply click . - model description: bria rmbg-2.0 is a dichotomous image segmentation model trained exclusively on a professional-grade dataset. - bria: resources for more information:\n created at: 2025-04-12T06:16:07+00:00\n- model_name: yuvraj108c/RMBG-2.0\n  description: rmbg v2.0 is our new state-of-the-art background removal model significantly improves rmbg v1.4. the model is designed to effectively separate foreground from background in a range of categories and image types. this model has been trained on a carefully selected dataset, which includes: general stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale. - developed by:  - model type: background removal - license:  - the model is released under a cc by-nc 4.0 license for non-commercial use. - commercial use is subject to a commercial agreement with bria. available  purchase: to purchase a commercial license simply click . - model description: bria rmbg-2.0 is a dichotomous image segmentation model trained exclusively on a professional-grade dataset. the model output includes a single-channel 8-bit grayscale alpha matte, where each pixel value indicates the opacity level of the corresponding pixel in the original image. this non-binary output approach offers developers the flexibility to define custom thresholds for foreground-background separation, catering to varied use cases requirements and enhancing integration into complex pipelines. - bria: resources for more information:\n created at: 2025-07-28T10:18:49+00:00\n- model_name: Ricky06662/VisionReasoner-7B\n  description: this repository contains the visionreasoner-7b model, developed as part of the novel seg-zero framework, presented in the paper . this model is also associated with the paper . code: project page:  seg-zero is a novel framework that demonstrates remarkable generalizability and derives explicit chain-of-thought reasoning through cognitive reinforcement for reasoning segmentation. this visionreasoner-7b model employs a decoupled architecture consisting of a reasoning model and a segmentation model. the reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate precise pixel-level masks. trained exclusively via reinforcement learning with grpo and without explicit reasoning data, seg-zero achieves robust zero-shot generalization and exhibits emergent test-time reasoning capabilities. experiments show that seg-zero-7b achieves a zero-shot performance of 57.5 on the reasonseg benchmark, surpassing the prior lisa-7b by 18%. this significant improvement highlights seg-zero's ability to generalize across domains while presenting an explicit reasoning process.\n created at: 2025-05-18T05:46:01+00:00\n- model_name: tue-mps/coco_panoptic_eomt_large_640\n  description: eomt encoder-only mask transformer is a vision transformer vit architecture designed for high-quality and efficient image segmentation. it was introduced in the cvpr 2025 highlight paper: by tommie kerssies, niccol√≤ cavagnero, alexander hermans, narges norouzi, giuseppe averta, bastian leibe, gijs dubbelman, and daan de geus.\n created at: 2025-03-26T17:28:05+00:00\n- model_name: tue-mps/ade20k_semantic_eomt_large_512\n  description: eomt encoder-only mask transformer is a vision transformer vit architecture designed for high-quality and efficient image segmentation. it was introduced in the cvpr 2025 highlight paper: by tommie kerssies, niccol√≤ cavagnero, alexander hermans, narges norouzi, giuseppe averta, bastian leibe, gijs dubbelman, and daan de geus.\n created at: 2025-03-26T19:29:47+00:00\n- model_name: bwittmann/vesselFM\n  description: tl;dr : vesselfm is a foundation model for universal 3d blood vessel segmentation in arbitrary imaging domains. for details, please refer to our preprint and our github repo\n created at: 2024-12-02T12:45:08+00:00\n- model_name: Ricky06662/Seg-Zero-7B-Best-on-ReasonSegTest\n  description: this model is based on the paper . code: seg-zero introduces a novel framework for reasoning segmentation that addresses the limitations of traditional supervised fine-tuning methods, which often struggle with out-of-domain generalization and lack explicit reasoning processes. the framework features a decoupled architecture consisting of a reasoning model and a segmentation model. the reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate precise pixel-level masks.\n created at: 2025-04-09T16:12:22+00:00\n- model_name: tue-mps/coco_instance_eomt_large_640\n  description: eomt encoder-only mask transformer is a vision transformer vit architecture designed for high-quality and efficient image segmentation. it was introduced in the cvpr 2025 highlight paper: by tommie kerssies, niccol√≤ cavagnero, alexander hermans, narges norouzi, giuseppe averta, bastian leibe, gijs dubbelman, and daan de geus.\n created at: 2025-03-26T17:16:21+00:00\n- model_name: facebook/sapiens-seg-1b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T20:26:24+00:00\n- model_name: tue-mps/coco_instance_eomt_large_1280\n  description: eomt encoder-only mask transformer is a vision transformer vit architecture designed for high-quality and efficient image segmentation. it was introduced in the cvpr 2025 highlight paper: by tommie kerssies, niccol√≤ cavagnero, alexander hermans, narges norouzi, giuseppe averta, bastian leibe, gijs dubbelman, and daan de geus.\n created at: 2025-03-26T19:34:47+00:00\n- model_name: nevernever69/dit-doclaynet-segmentation\n  description: this model is a fine-tuned version of for document layout semantic segmentation on the dataset small subset: `nevernever69/small-doclaynet-v1.1`. it segments scanned document images into 11 layout categories such as title, paragraph, table, and footer. - segment document images into structured layout elements - assist in downstream tasks like document ocr, archiving, and automatic annotation\n created at: 2025-03-12T11:45:04+00:00\n- model_name: Pushti/BirefNetHR\n  description: this birefnet was trained with images in `2048x2048` for higher resolution inference. all tested in fp16 mode. dataset method resolution maxfm wfmeasure mae smeasure meanem hce maxem meanfm adpem adpfm mba maxbiou meanbiou\n created at: 2025-04-18T07:28:32+00:00\n- model_name: ZQL9711/RMBG-2-Matting\n  description: rmbg v2.0 is our new state-of-the-art background removal model significantly improves rmbg v1.4. the model is designed to effectively separate foreground from background in a range of categories and image types. this model has been trained on a carefully selected dataset, which includes: general stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale. - developed by:  - model type: background removal - license:  - the model is released under a cc by-nc 4.0 license for non-commercial use. - commercial use is subject to a commercial agreement with bria. available  purchase: to purchase a commercial license simply click . - model description: bria rmbg-2.0 is a dichotomous image segmentation model trained exclusively on a professional-grade dataset. - bria: resources for more information:\n created at: 2025-07-05T08:31:53+00:00\n- model_name: Acly/BiRefNet-GGUF\n  description: birefnet is a model for dichotomous image segmentation background removal. the weights in this repository are converted for lightweight inference on consumer hardware with .\n created at: 2025-07-20T10:33:31+00:00\n\nExamples:\n- example user query: Our company is building a software to analyze satellite images for land use. We need to segment different regions in the images.\n  model_name: nvidia/segformer-b0-finetuned-ade-512-512\n- example user query: Identify an API to perform semantic segmentation of the street labeled with road, sidewalk, and building for a given picture.\n  model_name: pytorch/vision/deeplabv3_resnet50\n- example user query: A drone is flying over the neighborhood and is recording video. How can I process the video and perform semantic segmentation on each frame?\n  model_name: pytorch/vision/deeplabv3_resnet101\n- example user query: To identify and separate different objects in an image, create an image segmentation model.\n  model_name: openmmlab/upernet-convnext-small\n- example user query: The company is developing a fashion application which needs to identify the clothing items in pictures.\n  model_name: mattmdjaga/segformer_b2_clothes\n- example user query: An urban development company wants to analyze city images to inform infrastructure plans. Extract features using a semantic segmentation model.\n  model_name: nvidia/segformer-b5-finetuned-cityscapes-1024-1024\n\nUser prompt:\nI desire you to delineate all objects in the rendering.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_seg>\n\nCandidate Models:\n- model_name: CIDAS/clipseg-rd64-refined\n  description: CLIPSeg model with reduce dimension 64, refined (using a more complex convolution). It was introduced in the paper Image Segmentation Using Text and Image Prompts by L√ºddecke et al. and first released in this repository. This model is intended for zero-shot and one-shot image segmentation.\n created at: 2022-11-01\n- model_name: openmmlab/upernet-convnext-small\n  description: UperNet framework for semantic segmentation, leveraging a ConvNeXt backbone. UperNet was introduced in the paper Unified Perceptual Parsing for Scene Understanding by Xiao et al. Combining UperNet with a ConvNeXt backbone was introduced in the paper A ConvNet for the 2020s.\n created at: 2023-01-13\n- model_name: nvidia/segformer-b0-finetuned-ade-512-512\n  description: SegFormer model fine-tuned on ADE20k at resolution 512x512. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: nvidia/segformer-b5-finetuned-ade-640-640\n  description: SegFormer model fine-tuned on ADE20k at resolution 640x640. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: nvidia/segformer-b2-finetuned-cityscapes-1024-1024\n  description: SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: nvidia/segformer-b0-finetuned-cityscapes-1024-1024\n  description: SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: facebook/maskformer-swin-base-coco\n  description: MaskFormer model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.\n created at: 2022-03-02\n- model_name: facebook/mask2former-swin-large-cityscapes-semantic\n  description: Mask2Former model trained on Cityscapes semantic segmentation (large-sized version, Swin backbone). It addresses instance, semantic and panoptic segmentation by predicting a set of masks and corresponding labels. The model outperforms the previous SOTA, MaskFormer, in terms of performance and efficiency.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-base-coco-panoptic\n  description: Mask2Former model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.\n created at: 2023-01-02\n- model_name: mattmdjaga/segformer_b2_clothes\n  description: SegFormer model fine-tuned on ATR dataset for clothes segmentation.\n created at: 2022-11-24\n- model_name: shi-labs/oneformer_coco_swin_large\n  description: OneFormer model trained on the COCO dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.\n created at: 2022-11-15\n- model_name: facebook/mask2former-swin-large-coco-panoptic\n  description: Mask2Former model trained on COCO panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.\n created at: 2023-01-02\n- model_name: shi-labs/oneformer_ade20k_swin_large\n  description: OneFormer model trained on the ADE20k dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.\n created at: 2022-11-15\n- model_name: facebook/mask2former-swin-small-coco-instance\n  description: Mask2Former model trained on COCO instance segmentation (small-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.\n created at: 2022-12-26\n- model_name: nvidia/segformer-b5-finetuned-cityscapes-1024-1024\n  description: SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: shi-labs/oneformer_ade20k_swin_tiny\n  description: OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.\n created at: 2022-11-16\n- model_name: keremberke/yolov8m-building-segmentation\n  description: A YOLOv8 model for building segmentation in satellite images. It can detect and segment buildings in the input images.\n created at: 2023-01-27\n- model_name: facebook/maskformer-swin-base-ade\n  description: MaskFormer model trained on ADE20k semantic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.\n created at: 2022-03-02\n- model_name: facebook/mask2former-swin-tiny-coco-instance\n  description: Mask2Former model trained on COCO instance segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. You can use this particular checkpoint for instance segmentation.\n created at: 2022-12-23\n- model_name: keremberke/yolov8m-pcb-defect-segmentation\n  description: A YOLOv8 model for PCB defect segmentation trained on the pcb-defect-segmentation dataset. The model can detect and segment defects in PCB images, such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.\n created at: 2023-01-28\n- model_name: facebook/maskformer-swin-tiny-coco\n  description: MaskFormer model trained on COCO panoptic segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.\n created at: 2022-03-02\n- model_name: keremberke/yolov8m-pothole-segmentation\n  description: A YOLOv8 model for pothole segmentation trained on keremberke/pothole-segmentation dataset. It can detect potholes in images and provide segmentation masks for the detected potholes.\n created at: 2023-01-26\n- model_name: keremberke/yolov8s-building-segmentation\n  description: A YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can detect and segment buildings with high accuracy.\n created at: 2023-01-26\n- model_name: keremberke/yolov8s-pothole-segmentation\n  description: A YOLOv8 model for pothole segmentation. This model detects potholes in images and outputs bounding boxes and masks for the detected potholes.\n created at: 2023-01-26\n- model_name: keremberke/yolov8s-pcb-defect-segmentation\n  description: YOLOv8s model for PCB defect segmentation. The model is trained to detect and segment PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.\n created at: 2023-01-28\n- model_name: keremberke/yolov8n-pcb-defect-segmentation\n  description: A YOLOv8 model for detecting and segmenting PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.\n created at: 2023-01-28\n- model_name: keremberke/yolov8n-pothole-segmentation\n  description: A YOLOv8 model for pothole segmentation in images. The model is trained on the pothole-segmentation dataset and achieves high accuracy in detecting potholes.\n created at: 2023-01-15\n- model_name: facebook/maskformer-swin-large-ade\n  description: MaskFormer model trained on ADE20k semantic segmentation (large-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.\n created at: 2022-03-02\n- model_name: facebook/detr-resnet-50-panoptic\n  description: DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 panoptic (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: tensorflow/lite-model/deeplabv3/1/default\n  description: TensorFlow Hub provides pre-trained image segmentation models that can be loaded and used for various tasks such as semantic segmentation. The example code demonstrates how to load a DeepLabV3 model and perform segmentation on an input image tensor.\n created at: 2020-10-06\n- model_name: pytorch/vision/deeplabv3_resnet50\n  description: DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: pytorch/vision/deeplabv3_mobilenet_v3_large\n  description: DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: pytorch/vision/deeplabv3_resnet101\n  description: DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: mateuszbuda/brain-segmentation-pytorch/unet\n  description: U-Net with batch normalization for biomedical image segmentation with pretrained weights for abnormality segmentation in brain MRI. The model comprises four levels of blocks containing two convolutional layers with batch normalization and ReLU activation function, and one max pooling layer in the encoding part and up-convolutional layers instead in the decoding part. The number of convolutional filters in each block is 32, 64, 128, and 256. The bottleneck layer has 512 convolutional filters. From the encoding layers, skip connections are used to the corresponding layers in the decoding part. Input image is a 3-channel brain MRI slice from pre-contrast, FLAIR, and post-contrast sequences, respectively. Output is a one-channel probability map of abnormality regions with the same size as the input image.\n created at: 2019-05-26\n- model_name: pytorch/vision/fcn_resnet101\n  description: FCN-ResNet is a Fully-Convolutional Network model using a ResNet-50 or a ResNet-101 backbone. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: pytorch/fairseq\n  description: Transformer (NMT) is a powerful sequence-to-sequence modeling architecture that produces state-of-the-art neural machine translation systems. It is based on the paper 'Attention Is All You Need' and has been improved using techniques such as large-scale semi-supervised training, back-translation, and noisy-channel reranking. It supports English-French and English-German translation as well as round-trip translation for paraphrasing.\n created at: 2017-08-29\n- model_name: pytorch/vision/fcn_resnet50\n  description: FCN-ResNet is a Fully-Convolutional Network model using a ResNet-50 or a ResNet-101 backbone. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: facebook/mask2former-swin-large-ade-semantic\n  description: Mask2Former model trained on ADE20k semantic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-large-ade-panoptic\n  description: Mask2Former model trained on ADE20k panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-large-mapillary-vistas-panoptic\n  description: Mask2Former model trained on Mapillary Vistas panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-large-mapillary-vistas-semantic\n  description: Mask2Former model trained on Mapillary Vistas semantic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-large-cityscapes-panoptic\n  description: Mask2Former model trained on Cityscapes panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-03\n- model_name: shi-labs/oneformer_cityscapes_swin_large\n  description: OneFormer model trained on the Cityscapes dataset (large-sized version, Swin backbone). It was introduced in the paper [OneFormer: One Transformer to Rule Universal Image Segmentation](https://arxiv.org/abs/2211.06220) by Jain et al. and first released in [this repository](https://github.com/SHI-Labs/OneFormer).\n\n![model image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/oneformer_teaser.png)\n created at: 2022-11-15\n- model_name: google/deeplabv3_mobilenet_v2_1.0_513\n  description: MobileNet V2 model pre-trained on PASCAL VOC at resolution 513x513. It was introduced in [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381) by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen. It was first released in [this repository](https://github.com/tensorflow/models/tree/master/research/deeplab).\n\nDisclaimer: The team releasing MobileNet V2 did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2022-11-10\n- model_name: apple/deeplabv3-mobilevit-small\n  description: MobileViT model pre-trained on PASCAL VOC at resolution 512x512. It was introduced in [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer](https://arxiv.org/abs/2110.02178) by Sachin Mehta and Mohammad Rastegari, and first released in [this repository](https://github.com/apple/ml-cvnets). The license used is [Apple sample code license](https://github.com/apple/ml-cvnets/blob/main/LICENSE).\n\nDisclaimer: The team releasing MobileViT did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2022-05-30\n- model_name: eugenesiow/edsr-base\n  description: The goal of image super resolution is to restore a high resolution (HR) image from a single low resolution (LR) image. The image below shows the ground truth (HR), the bicubic upscaling x2 and EDSR upscaling x2.\n\n![Comparing Bicubic upscaling against EDSR x2 upscaling on Set5 Image 4](images/Set5_4_compare.png \"Comparing Bicubic upscaling against EDSR x2 upscaling on Set5 Image 4\")\n## Model description\nEDSR is a model that uses both deeper and wider architecture (32 ResBlocks and 256 channels) to improve performance. It uses both global and local skip connections, and up-scaling is done at the end of the network. It doesn't use batch normalization layers (input and output have similar distributions, normalizing intermediate features may not be desirable) instead it uses constant scaling layers to ensure stable training. An L1 loss function (absolute error) is used instead of L2 (MSE), the authors showed better performance empirically and it requires less computation.\n\nThis is a base model (~5mb vs ~100mb) that includes just 16 ResBlocks and 64 channels.\n## Intended uses & limitations\nYou can use the pre-trained models for upscaling your images 2x, 3x and 4x. You can also use the trainer to train a model on your own dataset.\n### How to use\nThe model can be used with the [super_image](https://github.com/eugenesiow/super-image) library:\n```bash\npip install super-image\n```\nHere is how to use a pre-trained model to upscale your image:\n```python\nfrom super_image import EdsrModel, ImageLoader\nfrom PIL import Image\nimport requests\n\nurl = 'https://paperswithcode.com/media/datasets/Set5-0000002728-07a9793f_zA3bDjj.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel = EdsrModel.from_pretrained('eugenesiow/edsr-base', scale=2)      # scale 2, 3 and 4 models available\ninputs = ImageLoader.load_image(image)\npreds = model(inputs)\n\nImageLoader.save_image(preds, './scaled_2x.png')                        # save the output 2x scaled image to `./scaled_2x.png`\nImageLoader.save_compare(inputs, preds, './scaled_2x_compare.png')      # save an output comparing the super-image with a bicubic scaling\n```\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Upscale_Images_with_Pretrained_super_image_Models.ipynb \"Open in Colab\")\n## Training data\nThe models for 2x, 3x and 4x image super resolution were pretrained on [DIV2K](https://huggingface.co/datasets/eugenesiow/Div2k), a dataset of 800 high-quality (2K resolution) images for training, augmented to 4000 images and uses a dev set of  100 validation images (images numbered 801 to 900). \n## Training procedure\n### Preprocessing\nWe follow the pre-processing and training method of [Wang et al.](https://arxiv.org/abs/2104.07566).\nLow Resolution (LR) images are created by using bicubic interpolation as the resizing method to reduce the size of the High Resolution (HR) images by x2, x3 and x4 times.\nDuring training, RGB patches with size of 64√ó64 from the LR input are used together with their corresponding HR patches. \nData augmentation is applied to the training set in the pre-processing stage where five images are created from the four corners and center of the original image. \n\nWe need the huggingface [datasets](https://huggingface.co/datasets?filter=task_ids:other-other-image-super-resolution) library to download the data:\n```bash\npip install datasets\n```\nThe following code gets the data and preprocesses/augments the data.\n\n```python\nfrom datasets import load_dataset\nfrom super_image.data import EvalDataset, TrainDataset, augment_five_crop\n\naugmented_dataset = load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='train')\\\n    .map(augment_five_crop, batched=True, desc=\"Augmenting Dataset\")                                # download and augment the data with the five_crop method\ntrain_dataset = TrainDataset(augmented_dataset)                                                     # prepare the train dataset for loading PyTorch DataLoader\neval_dataset = EvalDataset(load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='validation'))      # prepare the eval dataset for the PyTorch DataLoader\n```\n### Pretraining\nThe model was trained on GPU. The training code is provided below:\n```python\nfrom super_image import Trainer, TrainingArguments, EdsrModel, EdsrConfig\n\ntraining_args = TrainingArguments(\n    output_dir='./results',                 # output directory\n    num_train_epochs=1000,                  # total number of training epochs\n)\n\nconfig = EdsrConfig(\n    scale=4,                                # train a model to upscale 4x\n)\nmodel = EdsrModel(config)\n\ntrainer = Trainer(\n    model=model,                         # the instantiated model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=eval_dataset            # evaluation dataset\n)\n\ntrainer.train()\n```\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Train_super_image_Models.ipynb \"Open in Colab\")\n## Evaluation results\nThe evaluation metrics include [PSNR](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio#Quality_estimation_with_PSNR) and [SSIM](https://en.wikipedia.org/wiki/Structural_similarity#Algorithm). \n\nEvaluation datasets include:\n- Set5 - [Bevilacqua et al. (2012)](https://huggingface.co/datasets/eugenesiow/Set5)\n- Set14 - [Zeyde et al. (2010)](https://huggingface.co/datasets/eugenesiow/Set14)\n- BSD100 - [Martin et al. (2001)](https://huggingface.co/datasets/eugenesiow/BSD100)\n- Urban100 - [Huang et al. (2015)](https://huggingface.co/datasets/eugenesiow/Urban100)\n\nThe results columns below are represented below as `PSNR/SSIM`. They are compared against a Bicubic baseline.\n\n|Dataset  \t    |Scale      |Bicubic  \t        |edsr-base  \t        |\n|---\t        |---\t    |---\t            |---\t                |\n|Set5  \t        |2x         |33.64/0.9292       |**38.02/0.9607**       |\n|Set5  \t        |3x  \t    |30.39/0.8678  \t    |**35.04/0.9403**  \t    |\n|Set5  \t        |4x  \t    |28.42/0.8101  \t    |**32.12/0.8947**       |\n|Set14  \t    |2x         |30.22/0.8683  \t    |**33.57/0.9172**  \t    |\n|Set14  \t    |3x         |27.53/0.7737  \t    |**30.93/0.8567**  \t    |\n|Set14  \t    |4x         |25.99/0.7023  \t    |**28.60/0.7815**  \t    |\n|BSD100  \t    |2x  \t    |29.55/0.8425  \t    |**32.21/0.8999**  \t    |\n|BSD100  \t    |3x  \t    |27.20/0.7382  \t    |**29.65/0.8204**  \t    |\n|BSD100  \t    |4x  \t    |25.96/0.6672  \t    |**27.61/0.7363**  \t    |\n|Urban100  \t    |2x  \t    |26.66/0.8408  \t    |**32.04/0.9276**  \t    |\n|Urban100  \t    |3x  \t    |  \t                |**29.23/0.8723**  \t    |\n|Urban100  \t    |4x  \t    |23.14/0.6573  \t    |**26.02/0.7832**  \t    |\n\n![Comparing Bicubic upscaling against x2 upscaling on Set5 Image 2](images/Set5_2_compare.png \"Comparing Bicubic upscaling against x2 upscaling on Set5 Image 2\")\n\nYou can find a notebook to easily run evaluation on pretrained models below:\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Evaluate_Pretrained_super_image_Models.ipynb \"Open in Colab\")\n created at: 2022-03-02\n- model_name: eugenesiow/msrn-bam\n  description: The goal of image super resolution is to restore a high resolution (HR) image from a single low resolution (LR) image. The image below shows the ground truth (HR), the bicubic upscaling x2 and model upscaling x2.\n\n![Comparing Bicubic upscaling against the models x4 upscaling on Set5 Image 4](images/msrn_4_4_compare.png \"Comparing Bicubic upscaling against the models x4 upscaling on Set5 Image 4\")\n## Model description\nThe MSRN model proposes a feature extraction structure called the multi-scale residual block. This module can \"adaptively detect image features at different scales\" and \"exploit the potential features of the image\".\n\nThis model also applies the balanced attention (BAM) method invented by [Wang et al. (2021)](https://arxiv.org/abs/2104.07566) to further improve the results.\n## Intended uses & limitations\nYou can use the pre-trained models for upscaling your images 2x, 3x and 4x. You can also use the trainer to train a model on your own dataset.\n### How to use\nThe model can be used with the [super_image](https://github.com/eugenesiow/super-image) library:\n```bash\npip install super-image\n```\nHere is how to use a pre-trained model to upscale your image:\n```python\nfrom super_image import MsrnModel, ImageLoader\nfrom PIL import Image\nimport requests\n\nurl = 'https://paperswithcode.com/media/datasets/Set5-0000002728-07a9793f_zA3bDjj.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel = MsrnModel.from_pretrained('eugenesiow/msrn-bam', scale=2)      # scale 2, 3 and 4 models available\ninputs = ImageLoader.load_image(image)\npreds = model(inputs)\n\nImageLoader.save_image(preds, './scaled_2x.png')                        # save the output 2x scaled image to `./scaled_2x.png`\nImageLoader.save_compare(inputs, preds, './scaled_2x_compare.png')      # save an output comparing the super-image with a bicubic scaling\n```\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Upscale_Images_with_Pretrained_super_image_Models.ipynb \"Open in Colab\")\n## Training data\nThe models for 2x, 3x and 4x image super resolution were pretrained on [DIV2K](https://huggingface.co/datasets/eugenesiow/Div2k), a dataset of 800 high-quality (2K resolution) images for training, augmented to 4000 images and uses a dev set of  100 validation images (images numbered 801 to 900). \n## Training procedure\n### Preprocessing\nWe follow the pre-processing and training method of [Wang et al.](https://arxiv.org/abs/2104.07566).\nLow Resolution (LR) images are created by using bicubic interpolation as the resizing method to reduce the size of the High Resolution (HR) images by x2, x3 and x4 times.\nDuring training, RGB patches with size of 64√ó64 from the LR input are used together with their corresponding HR patches. \nData augmentation is applied to the training set in the pre-processing stage where five images are created from the four corners and center of the original image. \n\nWe need the huggingface [datasets](https://huggingface.co/datasets?filter=task_ids:other-other-image-super-resolution) library to download the data:\n```bash\npip install datasets\n```\nThe following code gets the data and preprocesses/augments the data.\n\n```python\nfrom datasets import load_dataset\nfrom super_image.data import EvalDataset, TrainDataset, augment_five_crop\n\naugmented_dataset = load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='train')\\\n    .map(augment_five_crop, batched=True, desc=\"Augmenting Dataset\")                                # download and augment the data with the five_crop method\ntrain_dataset = TrainDataset(augmented_dataset)                                                     # prepare the train dataset for loading PyTorch DataLoader\neval_dataset = EvalDataset(load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='validation'))      # prepare the eval dataset for the PyTorch DataLoader\n```\n### Pretraining\nThe model was trained on GPU. The training code is provided below:\n```python\nfrom super_image import Trainer, TrainingArguments, MsrnModel, MsrnConfig\n\ntraining_args = TrainingArguments(\n    output_dir='./results',                 # output directory\n    num_train_epochs=1000,                  # total number of training epochs\n)\n\nconfig = MsrnConfig(\n    scale=4,                                # train a model to upscale 4x\n    bam=True,                               # apply balanced attention to the network\n)\nmodel = MsrnModel(config)\n\ntrainer = Trainer(\n    model=model,                         # the instantiated model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=eval_dataset            # evaluation dataset\n)\n\ntrainer.train()\n```\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Train_super_image_Models.ipynb \"Open in Colab\")\n## Evaluation results\nThe evaluation metrics include [PSNR](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio#Quality_estimation_with_PSNR) and [SSIM](https://en.wikipedia.org/wiki/Structural_similarity#Algorithm). \n\nEvaluation datasets include:\n- Set5 - [Bevilacqua et al. (2012)](https://huggingface.co/datasets/eugenesiow/Set5)\n- Set14 - [Zeyde et al. (2010)](https://huggingface.co/datasets/eugenesiow/Set14)\n- BSD100 - [Martin et al. (2001)](https://huggingface.co/datasets/eugenesiow/BSD100)\n- Urban100 - [Huang et al. (2015)](https://huggingface.co/datasets/eugenesiow/Urban100)\n\nThe results columns below are represented below as `PSNR/SSIM`. They are compared against a Bicubic baseline.\n\n|Dataset  \t    |Scale      |Bicubic  \t        |msrn-bam  \t                    |\n|---\t        |---\t    |---\t            |---\t                        |\n|Set5  \t        |2x         |33.64/0.9292       |**38.02/0.9608**       |\n|Set5  \t        |3x  \t    |30.39/0.8678  \t    |**35.13/0.9408**  \t    |\n|Set5  \t        |4x  \t    |28.42/0.8101  \t    |**32.26/0.8955**       |\n|Set14  \t    |2x         |30.22/0.8683  \t    |**33.73/0.9186**  \t    |\n|Set14  \t    |3x         |27.53/0.7737  \t    |**31.06/0.8588**  \t    |\n|Set14  \t    |4x         |25.99/0.7023  \t    |**28.78/0.7859**  \t    |\n|BSD100  \t    |2x  \t    |29.55/0.8425  \t    |**33.78/0.9253**  \t    |\n|BSD100  \t    |3x  \t    |27.20/0.7382  \t    |**29.65/0.8196**  \t    |\n|BSD100  \t    |4x  \t    |25.96/0.6672  \t    |**28.51/0.7651**  \t    |\n|Urban100  \t    |2x  \t    |26.66/0.8408  \t    |**32.08/0.9276**  \t    |\n|Urban100  \t    |3x  \t    |  \t                |**29.26/0.8736**  \t    |\n|Urban100  \t    |4x  \t    |23.14/0.6573  \t    |**26.10/0.7857**  \t    |\n\n![Comparing Bicubic upscaling against the models x4 upscaling on Set5 Image 2](images/msrn_2_4_compare.png \"Comparing Bicubic upscaling against the models x4 upscaling on Set5 Image 2\")\n\nYou can find a notebook to easily run evaluation on pretrained models below:\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Evaluate_Pretrained_super_image_Models.ipynb \"Open in Colab\")\n created at: 2022-03-02\n- model_name: ZhengPeng7/BiRefNet_HR-matting\n  description: this birefnet was trained with images in `2048x2048` for higher resolution image matting with transparency. all tested in fp16 mode. dataset method resolution maxfm wfmeasure mae smeasure meanem hce maxem meanfm adpem adpfm mba maxbiou meanbiou\n created at: 2025-02-12T16:19:23+00:00\n- model_name: PramaLLC/BEN2\n  description: ben2 background erase network introduces a novel approach to foreground segmentation through its innovative confidence guided matting cgm pipeline. the architecture employs a refiner network that targets and processes pixels where the base model exhibits lower confidence levels, resulting in more precise and reliable matting results. this model is built on ben: ben2 was trained on the dis5k and our 22k proprietary segmentation dataset. our enhanced model delivers superior performance in hair matting, 4k processing, object segmentation, and edge refinement. our base model is open source. to try the full model through our free web demo or integrate ben2 into your project with our api: - üåê\n created at: 2025-01-22T14:39:05+00:00\n- model_name: ZhengPeng7/BiRefNet_HR\n  description: this birefnet was trained with images in `2048x2048` for higher resolution inference. all tested in fp16 mode. dataset method resolution maxfm wfmeasure mae smeasure meanem hce maxem meanfm adpem adpfm mba maxbiou meanbiou\n created at: 2025-02-01T17:52:14+00:00\n- model_name: briaai/RMBG-2.0\n  description: rmbg v2.0 is our new state-of-the-art background removal model significantly improves rmbg v1.4. the model is designed to effectively separate foreground from background in a range of categories and image types. this model has been trained on a carefully selected dataset, which includes: general stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale. - developed by:  - model type: background removal - license:  - the model is released under a cc by-nc 4.0 license for non-commercial use. - commercial use is subject to a commercial agreement with bria. available  purchase: to purchase a commercial license simply click . - model description: bria rmbg-2.0 is a dichotomous image segmentation model trained exclusively on a professional-grade dataset. the model output includes a single-channel 8-bit grayscale alpha matte, where each pixel value indicates the opacity level of the corresponding pixel in the original image. this non-binary output approach offers developers the flexibility to define custom thresholds for foreground-background separation, catering to varied use cases requirements and enhancing integration into complex pipelines. - bria: resources for more information:\n created at: 2024-10-29T12:14:36+00:00\n- model_name: ZhengPeng7/BiRefNet_dynamic\n  description: bilateral reference for high-resolution dichotomous image segmentation an arbitrary shape adaptable birefnet for general segmentation. this model was trained on arbitrary shapes 256x256 2304x2304 and shows great robustness on inputs with any shape.\n created at: 2025-03-31T03:37:03+00:00\n- model_name: Ricky06662/Seg-Zero-7B\n  description: this model is based on the paper . it uses a decoupled architecture with a reasoning model and a segmentation model. it's trained via reinforcement learning using grpo without explicit reasoning data, leading to robust zero-shot generalization and emergent test-time reasoning. code: this is a seg-zero-7b model. it introduces a decoupled architecture consisting of a reasoning model and a segmentation model. the reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate pixel-level masks. this is a seg-zero-7b model. it introduces a decoupled architecture consisting of a reasoning model and a segmentation model. the reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate pixel-level masks.\n created at: 2025-03-07T15:08:24+00:00\n- model_name: cocktailpeanut/rm\n  description: rmbg v2.0 is our new state-of-the-art background removal model significantly improves rmbg v1.4. the model is designed to effectively separate foreground from background in a range of categories and image types. this model has been trained on a carefully selected dataset, which includes: general stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale. - developed by:  - model type: background removal - license:  - the model is released under a cc by-nc 4.0 license for non-commercial use. - commercial use is subject to a commercial agreement with bria. available  purchase: to purchase a commercial license simply click . - model description: bria rmbg-2.0 is a dichotomous image segmentation model trained exclusively on a professional-grade dataset. - bria: resources for more information:\n created at: 2025-04-12T06:16:07+00:00\n- model_name: yuvraj108c/RMBG-2.0\n  description: rmbg v2.0 is our new state-of-the-art background removal model significantly improves rmbg v1.4. the model is designed to effectively separate foreground from background in a range of categories and image types. this model has been trained on a carefully selected dataset, which includes: general stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale. - developed by:  - model type: background removal - license:  - the model is released under a cc by-nc 4.0 license for non-commercial use. - commercial use is subject to a commercial agreement with bria. available  purchase: to purchase a commercial license simply click . - model description: bria rmbg-2.0 is a dichotomous image segmentation model trained exclusively on a professional-grade dataset. the model output includes a single-channel 8-bit grayscale alpha matte, where each pixel value indicates the opacity level of the corresponding pixel in the original image. this non-binary output approach offers developers the flexibility to define custom thresholds for foreground-background separation, catering to varied use cases requirements and enhancing integration into complex pipelines. - bria: resources for more information:\n created at: 2025-07-28T10:18:49+00:00\n- model_name: Ricky06662/VisionReasoner-7B\n  description: this repository contains the visionreasoner-7b model, developed as part of the novel seg-zero framework, presented in the paper . this model is also associated with the paper . code: project page:  seg-zero is a novel framework that demonstrates remarkable generalizability and derives explicit chain-of-thought reasoning through cognitive reinforcement for reasoning segmentation. this visionreasoner-7b model employs a decoupled architecture consisting of a reasoning model and a segmentation model. the reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate precise pixel-level masks. trained exclusively via reinforcement learning with grpo and without explicit reasoning data, seg-zero achieves robust zero-shot generalization and exhibits emergent test-time reasoning capabilities. experiments show that seg-zero-7b achieves a zero-shot performance of 57.5 on the reasonseg benchmark, surpassing the prior lisa-7b by 18%. this significant improvement highlights seg-zero's ability to generalize across domains while presenting an explicit reasoning process.\n created at: 2025-05-18T05:46:01+00:00\n- model_name: tue-mps/coco_panoptic_eomt_large_640\n  description: eomt encoder-only mask transformer is a vision transformer vit architecture designed for high-quality and efficient image segmentation. it was introduced in the cvpr 2025 highlight paper: by tommie kerssies, niccol√≤ cavagnero, alexander hermans, narges norouzi, giuseppe averta, bastian leibe, gijs dubbelman, and daan de geus.\n created at: 2025-03-26T17:28:05+00:00\n- model_name: tue-mps/ade20k_semantic_eomt_large_512\n  description: eomt encoder-only mask transformer is a vision transformer vit architecture designed for high-quality and efficient image segmentation. it was introduced in the cvpr 2025 highlight paper: by tommie kerssies, niccol√≤ cavagnero, alexander hermans, narges norouzi, giuseppe averta, bastian leibe, gijs dubbelman, and daan de geus.\n created at: 2025-03-26T19:29:47+00:00\n- model_name: bwittmann/vesselFM\n  description: tl;dr : vesselfm is a foundation model for universal 3d blood vessel segmentation in arbitrary imaging domains. for details, please refer to our preprint and our github repo\n created at: 2024-12-02T12:45:08+00:00\n- model_name: Ricky06662/Seg-Zero-7B-Best-on-ReasonSegTest\n  description: this model is based on the paper . code: seg-zero introduces a novel framework for reasoning segmentation that addresses the limitations of traditional supervised fine-tuning methods, which often struggle with out-of-domain generalization and lack explicit reasoning processes. the framework features a decoupled architecture consisting of a reasoning model and a segmentation model. the reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate precise pixel-level masks.\n created at: 2025-04-09T16:12:22+00:00\n- model_name: tue-mps/coco_instance_eomt_large_640\n  description: eomt encoder-only mask transformer is a vision transformer vit architecture designed for high-quality and efficient image segmentation. it was introduced in the cvpr 2025 highlight paper: by tommie kerssies, niccol√≤ cavagnero, alexander hermans, narges norouzi, giuseppe averta, bastian leibe, gijs dubbelman, and daan de geus.\n created at: 2025-03-26T17:16:21+00:00\n- model_name: facebook/sapiens-seg-1b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T20:26:24+00:00\n- model_name: tue-mps/coco_instance_eomt_large_1280\n  description: eomt encoder-only mask transformer is a vision transformer vit architecture designed for high-quality and efficient image segmentation. it was introduced in the cvpr 2025 highlight paper: by tommie kerssies, niccol√≤ cavagnero, alexander hermans, narges norouzi, giuseppe averta, bastian leibe, gijs dubbelman, and daan de geus.\n created at: 2025-03-26T19:34:47+00:00\n- model_name: nevernever69/dit-doclaynet-segmentation\n  description: this model is a fine-tuned version of for document layout semantic segmentation on the dataset small subset: `nevernever69/small-doclaynet-v1.1`. it segments scanned document images into 11 layout categories such as title, paragraph, table, and footer. - segment document images into structured layout elements - assist in downstream tasks like document ocr, archiving, and automatic annotation\n created at: 2025-03-12T11:45:04+00:00\n- model_name: Pushti/BirefNetHR\n  description: this birefnet was trained with images in `2048x2048` for higher resolution inference. all tested in fp16 mode. dataset method resolution maxfm wfmeasure mae smeasure meanem hce maxem meanfm adpem adpfm mba maxbiou meanbiou\n created at: 2025-04-18T07:28:32+00:00\n- model_name: ZQL9711/RMBG-2-Matting\n  description: rmbg v2.0 is our new state-of-the-art background removal model significantly improves rmbg v1.4. the model is designed to effectively separate foreground from background in a range of categories and image types. this model has been trained on a carefully selected dataset, which includes: general stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale. - developed by:  - model type: background removal - license:  - the model is released under a cc by-nc 4.0 license for non-commercial use. - commercial use is subject to a commercial agreement with bria. available  purchase: to purchase a commercial license simply click . - model description: bria rmbg-2.0 is a dichotomous image segmentation model trained exclusively on a professional-grade dataset. - bria: resources for more information:\n created at: 2025-07-05T08:31:53+00:00\n- model_name: Acly/BiRefNet-GGUF\n  description: birefnet is a model for dichotomous image segmentation background removal. the weights in this repository are converted for lightweight inference on consumer hardware with .\n created at: 2025-07-20T10:33:31+00:00\n\nExamples:\n- example user query: Our company is building a software to analyze satellite images for land use. We need to segment different regions in the images.\n  model_name: nvidia/segformer-b0-finetuned-ade-512-512\n- example user query: Identify an API to perform semantic segmentation of the street labeled with road, sidewalk, and building for a given picture.\n  model_name: pytorch/vision/deeplabv3_resnet50\n- example user query: A drone is flying over the neighborhood and is recording video. How can I process the video and perform semantic segmentation on each frame?\n  model_name: pytorch/vision/deeplabv3_resnet101\n- example user query: To identify and separate different objects in an image, create an image segmentation model.\n  model_name: openmmlab/upernet-convnext-small\n- example user query: The company is developing a fashion application which needs to identify the clothing items in pictures.\n  model_name: mattmdjaga/segformer_b2_clothes\n- example user query: An urban development company wants to analyze city images to inform infrastructure plans. Extract features using a semantic segmentation model.\n  model_name: nvidia/segformer-b5-finetuned-cityscapes-1024-1024\n\nUser prompt:\nCategorize everything in the shot.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_seg>\n\nCandidate Models:\n- model_name: CIDAS/clipseg-rd64-refined\n  description: CLIPSeg model with reduce dimension 64, refined (using a more complex convolution). It was introduced in the paper Image Segmentation Using Text and Image Prompts by L√ºddecke et al. and first released in this repository. This model is intended for zero-shot and one-shot image segmentation.\n created at: 2022-11-01\n- model_name: openmmlab/upernet-convnext-small\n  description: UperNet framework for semantic segmentation, leveraging a ConvNeXt backbone. UperNet was introduced in the paper Unified Perceptual Parsing for Scene Understanding by Xiao et al. Combining UperNet with a ConvNeXt backbone was introduced in the paper A ConvNet for the 2020s.\n created at: 2023-01-13\n- model_name: nvidia/segformer-b0-finetuned-ade-512-512\n  description: SegFormer model fine-tuned on ADE20k at resolution 512x512. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: nvidia/segformer-b5-finetuned-ade-640-640\n  description: SegFormer model fine-tuned on ADE20k at resolution 640x640. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: nvidia/segformer-b2-finetuned-cityscapes-1024-1024\n  description: SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: nvidia/segformer-b0-finetuned-cityscapes-1024-1024\n  description: SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: facebook/maskformer-swin-base-coco\n  description: MaskFormer model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.\n created at: 2022-03-02\n- model_name: facebook/mask2former-swin-large-cityscapes-semantic\n  description: Mask2Former model trained on Cityscapes semantic segmentation (large-sized version, Swin backbone). It addresses instance, semantic and panoptic segmentation by predicting a set of masks and corresponding labels. The model outperforms the previous SOTA, MaskFormer, in terms of performance and efficiency.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-base-coco-panoptic\n  description: Mask2Former model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.\n created at: 2023-01-02\n- model_name: mattmdjaga/segformer_b2_clothes\n  description: SegFormer model fine-tuned on ATR dataset for clothes segmentation.\n created at: 2022-11-24\n- model_name: shi-labs/oneformer_coco_swin_large\n  description: OneFormer model trained on the COCO dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.\n created at: 2022-11-15\n- model_name: facebook/mask2former-swin-large-coco-panoptic\n  description: Mask2Former model trained on COCO panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.\n created at: 2023-01-02\n- model_name: shi-labs/oneformer_ade20k_swin_large\n  description: OneFormer model trained on the ADE20k dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.\n created at: 2022-11-15\n- model_name: facebook/mask2former-swin-small-coco-instance\n  description: Mask2Former model trained on COCO instance segmentation (small-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.\n created at: 2022-12-26\n- model_name: nvidia/segformer-b5-finetuned-cityscapes-1024-1024\n  description: SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: shi-labs/oneformer_ade20k_swin_tiny\n  description: OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.\n created at: 2022-11-16\n- model_name: keremberke/yolov8m-building-segmentation\n  description: A YOLOv8 model for building segmentation in satellite images. It can detect and segment buildings in the input images.\n created at: 2023-01-27\n- model_name: facebook/maskformer-swin-base-ade\n  description: MaskFormer model trained on ADE20k semantic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.\n created at: 2022-03-02\n- model_name: facebook/mask2former-swin-tiny-coco-instance\n  description: Mask2Former model trained on COCO instance segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. You can use this particular checkpoint for instance segmentation.\n created at: 2022-12-23\n- model_name: keremberke/yolov8m-pcb-defect-segmentation\n  description: A YOLOv8 model for PCB defect segmentation trained on the pcb-defect-segmentation dataset. The model can detect and segment defects in PCB images, such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.\n created at: 2023-01-28\n- model_name: facebook/maskformer-swin-tiny-coco\n  description: MaskFormer model trained on COCO panoptic segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.\n created at: 2022-03-02\n- model_name: keremberke/yolov8m-pothole-segmentation\n  description: A YOLOv8 model for pothole segmentation trained on keremberke/pothole-segmentation dataset. It can detect potholes in images and provide segmentation masks for the detected potholes.\n created at: 2023-01-26\n- model_name: keremberke/yolov8s-building-segmentation\n  description: A YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can detect and segment buildings with high accuracy.\n created at: 2023-01-26\n- model_name: keremberke/yolov8s-pothole-segmentation\n  description: A YOLOv8 model for pothole segmentation. This model detects potholes in images and outputs bounding boxes and masks for the detected potholes.\n created at: 2023-01-26\n- model_name: keremberke/yolov8s-pcb-defect-segmentation\n  description: YOLOv8s model for PCB defect segmentation. The model is trained to detect and segment PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.\n created at: 2023-01-28\n- model_name: keremberke/yolov8n-pcb-defect-segmentation\n  description: A YOLOv8 model for detecting and segmenting PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.\n created at: 2023-01-28\n- model_name: keremberke/yolov8n-pothole-segmentation\n  description: A YOLOv8 model for pothole segmentation in images. The model is trained on the pothole-segmentation dataset and achieves high accuracy in detecting potholes.\n created at: 2023-01-15\n- model_name: facebook/maskformer-swin-large-ade\n  description: MaskFormer model trained on ADE20k semantic segmentation (large-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.\n created at: 2022-03-02\n- model_name: facebook/detr-resnet-50-panoptic\n  description: DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 panoptic (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: tensorflow/lite-model/deeplabv3/1/default\n  description: TensorFlow Hub provides pre-trained image segmentation models that can be loaded and used for various tasks such as semantic segmentation. The example code demonstrates how to load a DeepLabV3 model and perform segmentation on an input image tensor.\n created at: 2020-10-06\n- model_name: pytorch/vision/deeplabv3_resnet50\n  description: DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: pytorch/vision/deeplabv3_mobilenet_v3_large\n  description: DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: pytorch/vision/deeplabv3_resnet101\n  description: DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: mateuszbuda/brain-segmentation-pytorch/unet\n  description: U-Net with batch normalization for biomedical image segmentation with pretrained weights for abnormality segmentation in brain MRI. The model comprises four levels of blocks containing two convolutional layers with batch normalization and ReLU activation function, and one max pooling layer in the encoding part and up-convolutional layers instead in the decoding part. The number of convolutional filters in each block is 32, 64, 128, and 256. The bottleneck layer has 512 convolutional filters. From the encoding layers, skip connections are used to the corresponding layers in the decoding part. Input image is a 3-channel brain MRI slice from pre-contrast, FLAIR, and post-contrast sequences, respectively. Output is a one-channel probability map of abnormality regions with the same size as the input image.\n created at: 2019-05-26\n- model_name: pytorch/vision/fcn_resnet101\n  description: FCN-ResNet is a Fully-Convolutional Network model using a ResNet-50 or a ResNet-101 backbone. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: pytorch/fairseq\n  description: Transformer (NMT) is a powerful sequence-to-sequence modeling architecture that produces state-of-the-art neural machine translation systems. It is based on the paper 'Attention Is All You Need' and has been improved using techniques such as large-scale semi-supervised training, back-translation, and noisy-channel reranking. It supports English-French and English-German translation as well as round-trip translation for paraphrasing.\n created at: 2017-08-29\n- model_name: pytorch/vision/fcn_resnet50\n  description: FCN-ResNet is a Fully-Convolutional Network model using a ResNet-50 or a ResNet-101 backbone. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: facebook/mask2former-swin-large-ade-semantic\n  description: Mask2Former model trained on ADE20k semantic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-large-ade-panoptic\n  description: Mask2Former model trained on ADE20k panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-large-mapillary-vistas-panoptic\n  description: Mask2Former model trained on Mapillary Vistas panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-large-mapillary-vistas-semantic\n  description: Mask2Former model trained on Mapillary Vistas semantic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-large-cityscapes-panoptic\n  description: Mask2Former model trained on Cityscapes panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-03\n- model_name: shi-labs/oneformer_cityscapes_swin_large\n  description: OneFormer model trained on the Cityscapes dataset (large-sized version, Swin backbone). It was introduced in the paper [OneFormer: One Transformer to Rule Universal Image Segmentation](https://arxiv.org/abs/2211.06220) by Jain et al. and first released in [this repository](https://github.com/SHI-Labs/OneFormer).\n\n![model image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/oneformer_teaser.png)\n created at: 2022-11-15\n- model_name: google/deeplabv3_mobilenet_v2_1.0_513\n  description: MobileNet V2 model pre-trained on PASCAL VOC at resolution 513x513. It was introduced in [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381) by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen. It was first released in [this repository](https://github.com/tensorflow/models/tree/master/research/deeplab).\n\nDisclaimer: The team releasing MobileNet V2 did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2022-11-10\n- model_name: apple/deeplabv3-mobilevit-small\n  description: MobileViT model pre-trained on PASCAL VOC at resolution 512x512. It was introduced in [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer](https://arxiv.org/abs/2110.02178) by Sachin Mehta and Mohammad Rastegari, and first released in [this repository](https://github.com/apple/ml-cvnets). The license used is [Apple sample code license](https://github.com/apple/ml-cvnets/blob/main/LICENSE).\n\nDisclaimer: The team releasing MobileViT did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2022-05-30\n- model_name: eugenesiow/edsr-base\n  description: The goal of image super resolution is to restore a high resolution (HR) image from a single low resolution (LR) image. The image below shows the ground truth (HR), the bicubic upscaling x2 and EDSR upscaling x2.\n\n![Comparing Bicubic upscaling against EDSR x2 upscaling on Set5 Image 4](images/Set5_4_compare.png \"Comparing Bicubic upscaling against EDSR x2 upscaling on Set5 Image 4\")\n## Model description\nEDSR is a model that uses both deeper and wider architecture (32 ResBlocks and 256 channels) to improve performance. It uses both global and local skip connections, and up-scaling is done at the end of the network. It doesn't use batch normalization layers (input and output have similar distributions, normalizing intermediate features may not be desirable) instead it uses constant scaling layers to ensure stable training. An L1 loss function (absolute error) is used instead of L2 (MSE), the authors showed better performance empirically and it requires less computation.\n\nThis is a base model (~5mb vs ~100mb) that includes just 16 ResBlocks and 64 channels.\n## Intended uses & limitations\nYou can use the pre-trained models for upscaling your images 2x, 3x and 4x. You can also use the trainer to train a model on your own dataset.\n### How to use\nThe model can be used with the [super_image](https://github.com/eugenesiow/super-image) library:\n```bash\npip install super-image\n```\nHere is how to use a pre-trained model to upscale your image:\n```python\nfrom super_image import EdsrModel, ImageLoader\nfrom PIL import Image\nimport requests\n\nurl = 'https://paperswithcode.com/media/datasets/Set5-0000002728-07a9793f_zA3bDjj.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel = EdsrModel.from_pretrained('eugenesiow/edsr-base', scale=2)      # scale 2, 3 and 4 models available\ninputs = ImageLoader.load_image(image)\npreds = model(inputs)\n\nImageLoader.save_image(preds, './scaled_2x.png')                        # save the output 2x scaled image to `./scaled_2x.png`\nImageLoader.save_compare(inputs, preds, './scaled_2x_compare.png')      # save an output comparing the super-image with a bicubic scaling\n```\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Upscale_Images_with_Pretrained_super_image_Models.ipynb \"Open in Colab\")\n## Training data\nThe models for 2x, 3x and 4x image super resolution were pretrained on [DIV2K](https://huggingface.co/datasets/eugenesiow/Div2k), a dataset of 800 high-quality (2K resolution) images for training, augmented to 4000 images and uses a dev set of  100 validation images (images numbered 801 to 900). \n## Training procedure\n### Preprocessing\nWe follow the pre-processing and training method of [Wang et al.](https://arxiv.org/abs/2104.07566).\nLow Resolution (LR) images are created by using bicubic interpolation as the resizing method to reduce the size of the High Resolution (HR) images by x2, x3 and x4 times.\nDuring training, RGB patches with size of 64√ó64 from the LR input are used together with their corresponding HR patches. \nData augmentation is applied to the training set in the pre-processing stage where five images are created from the four corners and center of the original image. \n\nWe need the huggingface [datasets](https://huggingface.co/datasets?filter=task_ids:other-other-image-super-resolution) library to download the data:\n```bash\npip install datasets\n```\nThe following code gets the data and preprocesses/augments the data.\n\n```python\nfrom datasets import load_dataset\nfrom super_image.data import EvalDataset, TrainDataset, augment_five_crop\n\naugmented_dataset = load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='train')\\\n    .map(augment_five_crop, batched=True, desc=\"Augmenting Dataset\")                                # download and augment the data with the five_crop method\ntrain_dataset = TrainDataset(augmented_dataset)                                                     # prepare the train dataset for loading PyTorch DataLoader\neval_dataset = EvalDataset(load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='validation'))      # prepare the eval dataset for the PyTorch DataLoader\n```\n### Pretraining\nThe model was trained on GPU. The training code is provided below:\n```python\nfrom super_image import Trainer, TrainingArguments, EdsrModel, EdsrConfig\n\ntraining_args = TrainingArguments(\n    output_dir='./results',                 # output directory\n    num_train_epochs=1000,                  # total number of training epochs\n)\n\nconfig = EdsrConfig(\n    scale=4,                                # train a model to upscale 4x\n)\nmodel = EdsrModel(config)\n\ntrainer = Trainer(\n    model=model,                         # the instantiated model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=eval_dataset            # evaluation dataset\n)\n\ntrainer.train()\n```\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Train_super_image_Models.ipynb \"Open in Colab\")\n## Evaluation results\nThe evaluation metrics include [PSNR](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio#Quality_estimation_with_PSNR) and [SSIM](https://en.wikipedia.org/wiki/Structural_similarity#Algorithm). \n\nEvaluation datasets include:\n- Set5 - [Bevilacqua et al. (2012)](https://huggingface.co/datasets/eugenesiow/Set5)\n- Set14 - [Zeyde et al. (2010)](https://huggingface.co/datasets/eugenesiow/Set14)\n- BSD100 - [Martin et al. (2001)](https://huggingface.co/datasets/eugenesiow/BSD100)\n- Urban100 - [Huang et al. (2015)](https://huggingface.co/datasets/eugenesiow/Urban100)\n\nThe results columns below are represented below as `PSNR/SSIM`. They are compared against a Bicubic baseline.\n\n|Dataset  \t    |Scale      |Bicubic  \t        |edsr-base  \t        |\n|---\t        |---\t    |---\t            |---\t                |\n|Set5  \t        |2x         |33.64/0.9292       |**38.02/0.9607**       |\n|Set5  \t        |3x  \t    |30.39/0.8678  \t    |**35.04/0.9403**  \t    |\n|Set5  \t        |4x  \t    |28.42/0.8101  \t    |**32.12/0.8947**       |\n|Set14  \t    |2x         |30.22/0.8683  \t    |**33.57/0.9172**  \t    |\n|Set14  \t    |3x         |27.53/0.7737  \t    |**30.93/0.8567**  \t    |\n|Set14  \t    |4x         |25.99/0.7023  \t    |**28.60/0.7815**  \t    |\n|BSD100  \t    |2x  \t    |29.55/0.8425  \t    |**32.21/0.8999**  \t    |\n|BSD100  \t    |3x  \t    |27.20/0.7382  \t    |**29.65/0.8204**  \t    |\n|BSD100  \t    |4x  \t    |25.96/0.6672  \t    |**27.61/0.7363**  \t    |\n|Urban100  \t    |2x  \t    |26.66/0.8408  \t    |**32.04/0.9276**  \t    |\n|Urban100  \t    |3x  \t    |  \t                |**29.23/0.8723**  \t    |\n|Urban100  \t    |4x  \t    |23.14/0.6573  \t    |**26.02/0.7832**  \t    |\n\n![Comparing Bicubic upscaling against x2 upscaling on Set5 Image 2](images/Set5_2_compare.png \"Comparing Bicubic upscaling against x2 upscaling on Set5 Image 2\")\n\nYou can find a notebook to easily run evaluation on pretrained models below:\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Evaluate_Pretrained_super_image_Models.ipynb \"Open in Colab\")\n created at: 2022-03-02\n- model_name: eugenesiow/msrn-bam\n  description: The goal of image super resolution is to restore a high resolution (HR) image from a single low resolution (LR) image. The image below shows the ground truth (HR), the bicubic upscaling x2 and model upscaling x2.\n\n![Comparing Bicubic upscaling against the models x4 upscaling on Set5 Image 4](images/msrn_4_4_compare.png \"Comparing Bicubic upscaling against the models x4 upscaling on Set5 Image 4\")\n## Model description\nThe MSRN model proposes a feature extraction structure called the multi-scale residual block. This module can \"adaptively detect image features at different scales\" and \"exploit the potential features of the image\".\n\nThis model also applies the balanced attention (BAM) method invented by [Wang et al. (2021)](https://arxiv.org/abs/2104.07566) to further improve the results.\n## Intended uses & limitations\nYou can use the pre-trained models for upscaling your images 2x, 3x and 4x. You can also use the trainer to train a model on your own dataset.\n### How to use\nThe model can be used with the [super_image](https://github.com/eugenesiow/super-image) library:\n```bash\npip install super-image\n```\nHere is how to use a pre-trained model to upscale your image:\n```python\nfrom super_image import MsrnModel, ImageLoader\nfrom PIL import Image\nimport requests\n\nurl = 'https://paperswithcode.com/media/datasets/Set5-0000002728-07a9793f_zA3bDjj.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel = MsrnModel.from_pretrained('eugenesiow/msrn-bam', scale=2)      # scale 2, 3 and 4 models available\ninputs = ImageLoader.load_image(image)\npreds = model(inputs)\n\nImageLoader.save_image(preds, './scaled_2x.png')                        # save the output 2x scaled image to `./scaled_2x.png`\nImageLoader.save_compare(inputs, preds, './scaled_2x_compare.png')      # save an output comparing the super-image with a bicubic scaling\n```\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Upscale_Images_with_Pretrained_super_image_Models.ipynb \"Open in Colab\")\n## Training data\nThe models for 2x, 3x and 4x image super resolution were pretrained on [DIV2K](https://huggingface.co/datasets/eugenesiow/Div2k), a dataset of 800 high-quality (2K resolution) images for training, augmented to 4000 images and uses a dev set of  100 validation images (images numbered 801 to 900). \n## Training procedure\n### Preprocessing\nWe follow the pre-processing and training method of [Wang et al.](https://arxiv.org/abs/2104.07566).\nLow Resolution (LR) images are created by using bicubic interpolation as the resizing method to reduce the size of the High Resolution (HR) images by x2, x3 and x4 times.\nDuring training, RGB patches with size of 64√ó64 from the LR input are used together with their corresponding HR patches. \nData augmentation is applied to the training set in the pre-processing stage where five images are created from the four corners and center of the original image. \n\nWe need the huggingface [datasets](https://huggingface.co/datasets?filter=task_ids:other-other-image-super-resolution) library to download the data:\n```bash\npip install datasets\n```\nThe following code gets the data and preprocesses/augments the data.\n\n```python\nfrom datasets import load_dataset\nfrom super_image.data import EvalDataset, TrainDataset, augment_five_crop\n\naugmented_dataset = load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='train')\\\n    .map(augment_five_crop, batched=True, desc=\"Augmenting Dataset\")                                # download and augment the data with the five_crop method\ntrain_dataset = TrainDataset(augmented_dataset)                                                     # prepare the train dataset for loading PyTorch DataLoader\neval_dataset = EvalDataset(load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='validation'))      # prepare the eval dataset for the PyTorch DataLoader\n```\n### Pretraining\nThe model was trained on GPU. The training code is provided below:\n```python\nfrom super_image import Trainer, TrainingArguments, MsrnModel, MsrnConfig\n\ntraining_args = TrainingArguments(\n    output_dir='./results',                 # output directory\n    num_train_epochs=1000,                  # total number of training epochs\n)\n\nconfig = MsrnConfig(\n    scale=4,                                # train a model to upscale 4x\n    bam=True,                               # apply balanced attention to the network\n)\nmodel = MsrnModel(config)\n\ntrainer = Trainer(\n    model=model,                         # the instantiated model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=eval_dataset            # evaluation dataset\n)\n\ntrainer.train()\n```\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Train_super_image_Models.ipynb \"Open in Colab\")\n## Evaluation results\nThe evaluation metrics include [PSNR](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio#Quality_estimation_with_PSNR) and [SSIM](https://en.wikipedia.org/wiki/Structural_similarity#Algorithm). \n\nEvaluation datasets include:\n- Set5 - [Bevilacqua et al. (2012)](https://huggingface.co/datasets/eugenesiow/Set5)\n- Set14 - [Zeyde et al. (2010)](https://huggingface.co/datasets/eugenesiow/Set14)\n- BSD100 - [Martin et al. (2001)](https://huggingface.co/datasets/eugenesiow/BSD100)\n- Urban100 - [Huang et al. (2015)](https://huggingface.co/datasets/eugenesiow/Urban100)\n\nThe results columns below are represented below as `PSNR/SSIM`. They are compared against a Bicubic baseline.\n\n|Dataset  \t    |Scale      |Bicubic  \t        |msrn-bam  \t                    |\n|---\t        |---\t    |---\t            |---\t                        |\n|Set5  \t        |2x         |33.64/0.9292       |**38.02/0.9608**       |\n|Set5  \t        |3x  \t    |30.39/0.8678  \t    |**35.13/0.9408**  \t    |\n|Set5  \t        |4x  \t    |28.42/0.8101  \t    |**32.26/0.8955**       |\n|Set14  \t    |2x         |30.22/0.8683  \t    |**33.73/0.9186**  \t    |\n|Set14  \t    |3x         |27.53/0.7737  \t    |**31.06/0.8588**  \t    |\n|Set14  \t    |4x         |25.99/0.7023  \t    |**28.78/0.7859**  \t    |\n|BSD100  \t    |2x  \t    |29.55/0.8425  \t    |**33.78/0.9253**  \t    |\n|BSD100  \t    |3x  \t    |27.20/0.7382  \t    |**29.65/0.8196**  \t    |\n|BSD100  \t    |4x  \t    |25.96/0.6672  \t    |**28.51/0.7651**  \t    |\n|Urban100  \t    |2x  \t    |26.66/0.8408  \t    |**32.08/0.9276**  \t    |\n|Urban100  \t    |3x  \t    |  \t                |**29.26/0.8736**  \t    |\n|Urban100  \t    |4x  \t    |23.14/0.6573  \t    |**26.10/0.7857**  \t    |\n\n![Comparing Bicubic upscaling against the models x4 upscaling on Set5 Image 2](images/msrn_2_4_compare.png \"Comparing Bicubic upscaling against the models x4 upscaling on Set5 Image 2\")\n\nYou can find a notebook to easily run evaluation on pretrained models below:\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Evaluate_Pretrained_super_image_Models.ipynb \"Open in Colab\")\n created at: 2022-03-02\n- model_name: ZhengPeng7/BiRefNet_HR-matting\n  description: this birefnet was trained with images in `2048x2048` for higher resolution image matting with transparency. all tested in fp16 mode. dataset method resolution maxfm wfmeasure mae smeasure meanem hce maxem meanfm adpem adpfm mba maxbiou meanbiou\n created at: 2025-02-12T16:19:23+00:00\n- model_name: PramaLLC/BEN2\n  description: ben2 background erase network introduces a novel approach to foreground segmentation through its innovative confidence guided matting cgm pipeline. the architecture employs a refiner network that targets and processes pixels where the base model exhibits lower confidence levels, resulting in more precise and reliable matting results. this model is built on ben: ben2 was trained on the dis5k and our 22k proprietary segmentation dataset. our enhanced model delivers superior performance in hair matting, 4k processing, object segmentation, and edge refinement. our base model is open source. to try the full model through our free web demo or integrate ben2 into your project with our api: - üåê\n created at: 2025-01-22T14:39:05+00:00\n- model_name: ZhengPeng7/BiRefNet_HR\n  description: this birefnet was trained with images in `2048x2048` for higher resolution inference. all tested in fp16 mode. dataset method resolution maxfm wfmeasure mae smeasure meanem hce maxem meanfm adpem adpfm mba maxbiou meanbiou\n created at: 2025-02-01T17:52:14+00:00\n- model_name: briaai/RMBG-2.0\n  description: rmbg v2.0 is our new state-of-the-art background removal model significantly improves rmbg v1.4. the model is designed to effectively separate foreground from background in a range of categories and image types. this model has been trained on a carefully selected dataset, which includes: general stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale. - developed by:  - model type: background removal - license:  - the model is released under a cc by-nc 4.0 license for non-commercial use. - commercial use is subject to a commercial agreement with bria. available  purchase: to purchase a commercial license simply click . - model description: bria rmbg-2.0 is a dichotomous image segmentation model trained exclusively on a professional-grade dataset. the model output includes a single-channel 8-bit grayscale alpha matte, where each pixel value indicates the opacity level of the corresponding pixel in the original image. this non-binary output approach offers developers the flexibility to define custom thresholds for foreground-background separation, catering to varied use cases requirements and enhancing integration into complex pipelines. - bria: resources for more information:\n created at: 2024-10-29T12:14:36+00:00\n- model_name: ZhengPeng7/BiRefNet_dynamic\n  description: bilateral reference for high-resolution dichotomous image segmentation an arbitrary shape adaptable birefnet for general segmentation. this model was trained on arbitrary shapes 256x256 2304x2304 and shows great robustness on inputs with any shape.\n created at: 2025-03-31T03:37:03+00:00\n- model_name: Ricky06662/Seg-Zero-7B\n  description: this model is based on the paper . it uses a decoupled architecture with a reasoning model and a segmentation model. it's trained via reinforcement learning using grpo without explicit reasoning data, leading to robust zero-shot generalization and emergent test-time reasoning. code: this is a seg-zero-7b model. it introduces a decoupled architecture consisting of a reasoning model and a segmentation model. the reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate pixel-level masks. this is a seg-zero-7b model. it introduces a decoupled architecture consisting of a reasoning model and a segmentation model. the reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate pixel-level masks.\n created at: 2025-03-07T15:08:24+00:00\n- model_name: cocktailpeanut/rm\n  description: rmbg v2.0 is our new state-of-the-art background removal model significantly improves rmbg v1.4. the model is designed to effectively separate foreground from background in a range of categories and image types. this model has been trained on a carefully selected dataset, which includes: general stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale. - developed by:  - model type: background removal - license:  - the model is released under a cc by-nc 4.0 license for non-commercial use. - commercial use is subject to a commercial agreement with bria. available  purchase: to purchase a commercial license simply click . - model description: bria rmbg-2.0 is a dichotomous image segmentation model trained exclusively on a professional-grade dataset. - bria: resources for more information:\n created at: 2025-04-12T06:16:07+00:00\n- model_name: yuvraj108c/RMBG-2.0\n  description: rmbg v2.0 is our new state-of-the-art background removal model significantly improves rmbg v1.4. the model is designed to effectively separate foreground from background in a range of categories and image types. this model has been trained on a carefully selected dataset, which includes: general stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale. - developed by:  - model type: background removal - license:  - the model is released under a cc by-nc 4.0 license for non-commercial use. - commercial use is subject to a commercial agreement with bria. available  purchase: to purchase a commercial license simply click . - model description: bria rmbg-2.0 is a dichotomous image segmentation model trained exclusively on a professional-grade dataset. the model output includes a single-channel 8-bit grayscale alpha matte, where each pixel value indicates the opacity level of the corresponding pixel in the original image. this non-binary output approach offers developers the flexibility to define custom thresholds for foreground-background separation, catering to varied use cases requirements and enhancing integration into complex pipelines. - bria: resources for more information:\n created at: 2025-07-28T10:18:49+00:00\n- model_name: Ricky06662/VisionReasoner-7B\n  description: this repository contains the visionreasoner-7b model, developed as part of the novel seg-zero framework, presented in the paper . this model is also associated with the paper . code: project page:  seg-zero is a novel framework that demonstrates remarkable generalizability and derives explicit chain-of-thought reasoning through cognitive reinforcement for reasoning segmentation. this visionreasoner-7b model employs a decoupled architecture consisting of a reasoning model and a segmentation model. the reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate precise pixel-level masks. trained exclusively via reinforcement learning with grpo and without explicit reasoning data, seg-zero achieves robust zero-shot generalization and exhibits emergent test-time reasoning capabilities. experiments show that seg-zero-7b achieves a zero-shot performance of 57.5 on the reasonseg benchmark, surpassing the prior lisa-7b by 18%. this significant improvement highlights seg-zero's ability to generalize across domains while presenting an explicit reasoning process.\n created at: 2025-05-18T05:46:01+00:00\n- model_name: tue-mps/coco_panoptic_eomt_large_640\n  description: eomt encoder-only mask transformer is a vision transformer vit architecture designed for high-quality and efficient image segmentation. it was introduced in the cvpr 2025 highlight paper: by tommie kerssies, niccol√≤ cavagnero, alexander hermans, narges norouzi, giuseppe averta, bastian leibe, gijs dubbelman, and daan de geus.\n created at: 2025-03-26T17:28:05+00:00\n- model_name: tue-mps/ade20k_semantic_eomt_large_512\n  description: eomt encoder-only mask transformer is a vision transformer vit architecture designed for high-quality and efficient image segmentation. it was introduced in the cvpr 2025 highlight paper: by tommie kerssies, niccol√≤ cavagnero, alexander hermans, narges norouzi, giuseppe averta, bastian leibe, gijs dubbelman, and daan de geus.\n created at: 2025-03-26T19:29:47+00:00\n- model_name: bwittmann/vesselFM\n  description: tl;dr : vesselfm is a foundation model for universal 3d blood vessel segmentation in arbitrary imaging domains. for details, please refer to our preprint and our github repo\n created at: 2024-12-02T12:45:08+00:00\n- model_name: Ricky06662/Seg-Zero-7B-Best-on-ReasonSegTest\n  description: this model is based on the paper . code: seg-zero introduces a novel framework for reasoning segmentation that addresses the limitations of traditional supervised fine-tuning methods, which often struggle with out-of-domain generalization and lack explicit reasoning processes. the framework features a decoupled architecture consisting of a reasoning model and a segmentation model. the reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate precise pixel-level masks.\n created at: 2025-04-09T16:12:22+00:00\n- model_name: tue-mps/coco_instance_eomt_large_640\n  description: eomt encoder-only mask transformer is a vision transformer vit architecture designed for high-quality and efficient image segmentation. it was introduced in the cvpr 2025 highlight paper: by tommie kerssies, niccol√≤ cavagnero, alexander hermans, narges norouzi, giuseppe averta, bastian leibe, gijs dubbelman, and daan de geus.\n created at: 2025-03-26T17:16:21+00:00\n- model_name: facebook/sapiens-seg-1b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T20:26:24+00:00\n- model_name: tue-mps/coco_instance_eomt_large_1280\n  description: eomt encoder-only mask transformer is a vision transformer vit architecture designed for high-quality and efficient image segmentation. it was introduced in the cvpr 2025 highlight paper: by tommie kerssies, niccol√≤ cavagnero, alexander hermans, narges norouzi, giuseppe averta, bastian leibe, gijs dubbelman, and daan de geus.\n created at: 2025-03-26T19:34:47+00:00\n- model_name: nevernever69/dit-doclaynet-segmentation\n  description: this model is a fine-tuned version of for document layout semantic segmentation on the dataset small subset: `nevernever69/small-doclaynet-v1.1`. it segments scanned document images into 11 layout categories such as title, paragraph, table, and footer. - segment document images into structured layout elements - assist in downstream tasks like document ocr, archiving, and automatic annotation\n created at: 2025-03-12T11:45:04+00:00\n- model_name: Pushti/BirefNetHR\n  description: this birefnet was trained with images in `2048x2048` for higher resolution inference. all tested in fp16 mode. dataset method resolution maxfm wfmeasure mae smeasure meanem hce maxem meanfm adpem adpfm mba maxbiou meanbiou\n created at: 2025-04-18T07:28:32+00:00\n- model_name: ZQL9711/RMBG-2-Matting\n  description: rmbg v2.0 is our new state-of-the-art background removal model significantly improves rmbg v1.4. the model is designed to effectively separate foreground from background in a range of categories and image types. this model has been trained on a carefully selected dataset, which includes: general stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale. - developed by:  - model type: background removal - license:  - the model is released under a cc by-nc 4.0 license for non-commercial use. - commercial use is subject to a commercial agreement with bria. available  purchase: to purchase a commercial license simply click . - model description: bria rmbg-2.0 is a dichotomous image segmentation model trained exclusively on a professional-grade dataset. - bria: resources for more information:\n created at: 2025-07-05T08:31:53+00:00\n- model_name: Acly/BiRefNet-GGUF\n  description: birefnet is a model for dichotomous image segmentation background removal. the weights in this repository are converted for lightweight inference on consumer hardware with .\n created at: 2025-07-20T10:33:31+00:00\n\nExamples:\n- example user query: Our company is building a software to analyze satellite images for land use. We need to segment different regions in the images.\n  model_name: nvidia/segformer-b0-finetuned-ade-512-512\n- example user query: Identify an API to perform semantic segmentation of the street labeled with road, sidewalk, and building for a given picture.\n  model_name: pytorch/vision/deeplabv3_resnet50\n- example user query: A drone is flying over the neighborhood and is recording video. How can I process the video and perform semantic segmentation on each frame?\n  model_name: pytorch/vision/deeplabv3_resnet101\n- example user query: To identify and separate different objects in an image, create an image segmentation model.\n  model_name: openmmlab/upernet-convnext-small\n- example user query: The company is developing a fashion application which needs to identify the clothing items in pictures.\n  model_name: mattmdjaga/segformer_b2_clothes\n- example user query: An urban development company wants to analyze city images to inform infrastructure plans. Extract features using a semantic segmentation model.\n  model_name: nvidia/segformer-b5-finetuned-cityscapes-1024-1024\n\nUser prompt:\nClassify each element in the scene.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_seg>\n\nCandidate Models:\n- model_name: CIDAS/clipseg-rd64-refined\n  description: CLIPSeg model with reduce dimension 64, refined (using a more complex convolution). It was introduced in the paper Image Segmentation Using Text and Image Prompts by L√ºddecke et al. and first released in this repository. This model is intended for zero-shot and one-shot image segmentation.\n created at: 2022-11-01\n- model_name: openmmlab/upernet-convnext-small\n  description: UperNet framework for semantic segmentation, leveraging a ConvNeXt backbone. UperNet was introduced in the paper Unified Perceptual Parsing for Scene Understanding by Xiao et al. Combining UperNet with a ConvNeXt backbone was introduced in the paper A ConvNet for the 2020s.\n created at: 2023-01-13\n- model_name: nvidia/segformer-b0-finetuned-ade-512-512\n  description: SegFormer model fine-tuned on ADE20k at resolution 512x512. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: nvidia/segformer-b5-finetuned-ade-640-640\n  description: SegFormer model fine-tuned on ADE20k at resolution 640x640. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: nvidia/segformer-b2-finetuned-cityscapes-1024-1024\n  description: SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: nvidia/segformer-b0-finetuned-cityscapes-1024-1024\n  description: SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: facebook/maskformer-swin-base-coco\n  description: MaskFormer model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.\n created at: 2022-03-02\n- model_name: facebook/mask2former-swin-large-cityscapes-semantic\n  description: Mask2Former model trained on Cityscapes semantic segmentation (large-sized version, Swin backbone). It addresses instance, semantic and panoptic segmentation by predicting a set of masks and corresponding labels. The model outperforms the previous SOTA, MaskFormer, in terms of performance and efficiency.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-base-coco-panoptic\n  description: Mask2Former model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.\n created at: 2023-01-02\n- model_name: mattmdjaga/segformer_b2_clothes\n  description: SegFormer model fine-tuned on ATR dataset for clothes segmentation.\n created at: 2022-11-24\n- model_name: shi-labs/oneformer_coco_swin_large\n  description: OneFormer model trained on the COCO dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.\n created at: 2022-11-15\n- model_name: facebook/mask2former-swin-large-coco-panoptic\n  description: Mask2Former model trained on COCO panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.\n created at: 2023-01-02\n- model_name: shi-labs/oneformer_ade20k_swin_large\n  description: OneFormer model trained on the ADE20k dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.\n created at: 2022-11-15\n- model_name: facebook/mask2former-swin-small-coco-instance\n  description: Mask2Former model trained on COCO instance segmentation (small-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.\n created at: 2022-12-26\n- model_name: nvidia/segformer-b5-finetuned-cityscapes-1024-1024\n  description: SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: shi-labs/oneformer_ade20k_swin_tiny\n  description: OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.\n created at: 2022-11-16\n- model_name: keremberke/yolov8m-building-segmentation\n  description: A YOLOv8 model for building segmentation in satellite images. It can detect and segment buildings in the input images.\n created at: 2023-01-27\n- model_name: facebook/maskformer-swin-base-ade\n  description: MaskFormer model trained on ADE20k semantic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.\n created at: 2022-03-02\n- model_name: facebook/mask2former-swin-tiny-coco-instance\n  description: Mask2Former model trained on COCO instance segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. You can use this particular checkpoint for instance segmentation.\n created at: 2022-12-23\n- model_name: keremberke/yolov8m-pcb-defect-segmentation\n  description: A YOLOv8 model for PCB defect segmentation trained on the pcb-defect-segmentation dataset. The model can detect and segment defects in PCB images, such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.\n created at: 2023-01-28\n- model_name: facebook/maskformer-swin-tiny-coco\n  description: MaskFormer model trained on COCO panoptic segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.\n created at: 2022-03-02\n- model_name: keremberke/yolov8m-pothole-segmentation\n  description: A YOLOv8 model for pothole segmentation trained on keremberke/pothole-segmentation dataset. It can detect potholes in images and provide segmentation masks for the detected potholes.\n created at: 2023-01-26\n- model_name: keremberke/yolov8s-building-segmentation\n  description: A YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can detect and segment buildings with high accuracy.\n created at: 2023-01-26\n- model_name: keremberke/yolov8s-pothole-segmentation\n  description: A YOLOv8 model for pothole segmentation. This model detects potholes in images and outputs bounding boxes and masks for the detected potholes.\n created at: 2023-01-26\n- model_name: keremberke/yolov8s-pcb-defect-segmentation\n  description: YOLOv8s model for PCB defect segmentation. The model is trained to detect and segment PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.\n created at: 2023-01-28\n- model_name: keremberke/yolov8n-pcb-defect-segmentation\n  description: A YOLOv8 model for detecting and segmenting PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.\n created at: 2023-01-28\n- model_name: keremberke/yolov8n-pothole-segmentation\n  description: A YOLOv8 model for pothole segmentation in images. The model is trained on the pothole-segmentation dataset and achieves high accuracy in detecting potholes.\n created at: 2023-01-15\n- model_name: facebook/maskformer-swin-large-ade\n  description: MaskFormer model trained on ADE20k semantic segmentation (large-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.\n created at: 2022-03-02\n- model_name: facebook/detr-resnet-50-panoptic\n  description: DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 panoptic (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: tensorflow/lite-model/deeplabv3/1/default\n  description: TensorFlow Hub provides pre-trained image segmentation models that can be loaded and used for various tasks such as semantic segmentation. The example code demonstrates how to load a DeepLabV3 model and perform segmentation on an input image tensor.\n created at: 2020-10-06\n- model_name: pytorch/vision/deeplabv3_resnet50\n  description: DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: pytorch/vision/deeplabv3_mobilenet_v3_large\n  description: DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: pytorch/vision/deeplabv3_resnet101\n  description: DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: mateuszbuda/brain-segmentation-pytorch/unet\n  description: U-Net with batch normalization for biomedical image segmentation with pretrained weights for abnormality segmentation in brain MRI. The model comprises four levels of blocks containing two convolutional layers with batch normalization and ReLU activation function, and one max pooling layer in the encoding part and up-convolutional layers instead in the decoding part. The number of convolutional filters in each block is 32, 64, 128, and 256. The bottleneck layer has 512 convolutional filters. From the encoding layers, skip connections are used to the corresponding layers in the decoding part. Input image is a 3-channel brain MRI slice from pre-contrast, FLAIR, and post-contrast sequences, respectively. Output is a one-channel probability map of abnormality regions with the same size as the input image.\n created at: 2019-05-26\n- model_name: pytorch/vision/fcn_resnet101\n  description: FCN-ResNet is a Fully-Convolutional Network model using a ResNet-50 or a ResNet-101 backbone. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: pytorch/fairseq\n  description: Transformer (NMT) is a powerful sequence-to-sequence modeling architecture that produces state-of-the-art neural machine translation systems. It is based on the paper 'Attention Is All You Need' and has been improved using techniques such as large-scale semi-supervised training, back-translation, and noisy-channel reranking. It supports English-French and English-German translation as well as round-trip translation for paraphrasing.\n created at: 2017-08-29\n- model_name: pytorch/vision/fcn_resnet50\n  description: FCN-ResNet is a Fully-Convolutional Network model using a ResNet-50 or a ResNet-101 backbone. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: facebook/mask2former-swin-large-ade-semantic\n  description: Mask2Former model trained on ADE20k semantic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-large-ade-panoptic\n  description: Mask2Former model trained on ADE20k panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-large-mapillary-vistas-panoptic\n  description: Mask2Former model trained on Mapillary Vistas panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-large-mapillary-vistas-semantic\n  description: Mask2Former model trained on Mapillary Vistas semantic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-large-cityscapes-panoptic\n  description: Mask2Former model trained on Cityscapes panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-03\n- model_name: shi-labs/oneformer_cityscapes_swin_large\n  description: OneFormer model trained on the Cityscapes dataset (large-sized version, Swin backbone). It was introduced in the paper [OneFormer: One Transformer to Rule Universal Image Segmentation](https://arxiv.org/abs/2211.06220) by Jain et al. and first released in [this repository](https://github.com/SHI-Labs/OneFormer).\n\n![model image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/oneformer_teaser.png)\n created at: 2022-11-15\n- model_name: google/deeplabv3_mobilenet_v2_1.0_513\n  description: MobileNet V2 model pre-trained on PASCAL VOC at resolution 513x513. It was introduced in [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381) by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen. It was first released in [this repository](https://github.com/tensorflow/models/tree/master/research/deeplab).\n\nDisclaimer: The team releasing MobileNet V2 did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2022-11-10\n- model_name: apple/deeplabv3-mobilevit-small\n  description: MobileViT model pre-trained on PASCAL VOC at resolution 512x512. It was introduced in [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer](https://arxiv.org/abs/2110.02178) by Sachin Mehta and Mohammad Rastegari, and first released in [this repository](https://github.com/apple/ml-cvnets). The license used is [Apple sample code license](https://github.com/apple/ml-cvnets/blob/main/LICENSE).\n\nDisclaimer: The team releasing MobileViT did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2022-05-30\n- model_name: eugenesiow/edsr-base\n  description: The goal of image super resolution is to restore a high resolution (HR) image from a single low resolution (LR) image. The image below shows the ground truth (HR), the bicubic upscaling x2 and EDSR upscaling x2.\n\n![Comparing Bicubic upscaling against EDSR x2 upscaling on Set5 Image 4](images/Set5_4_compare.png \"Comparing Bicubic upscaling against EDSR x2 upscaling on Set5 Image 4\")\n## Model description\nEDSR is a model that uses both deeper and wider architecture (32 ResBlocks and 256 channels) to improve performance. It uses both global and local skip connections, and up-scaling is done at the end of the network. It doesn't use batch normalization layers (input and output have similar distributions, normalizing intermediate features may not be desirable) instead it uses constant scaling layers to ensure stable training. An L1 loss function (absolute error) is used instead of L2 (MSE), the authors showed better performance empirically and it requires less computation.\n\nThis is a base model (~5mb vs ~100mb) that includes just 16 ResBlocks and 64 channels.\n## Intended uses & limitations\nYou can use the pre-trained models for upscaling your images 2x, 3x and 4x. You can also use the trainer to train a model on your own dataset.\n### How to use\nThe model can be used with the [super_image](https://github.com/eugenesiow/super-image) library:\n```bash\npip install super-image\n```\nHere is how to use a pre-trained model to upscale your image:\n```python\nfrom super_image import EdsrModel, ImageLoader\nfrom PIL import Image\nimport requests\n\nurl = 'https://paperswithcode.com/media/datasets/Set5-0000002728-07a9793f_zA3bDjj.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel = EdsrModel.from_pretrained('eugenesiow/edsr-base', scale=2)      # scale 2, 3 and 4 models available\ninputs = ImageLoader.load_image(image)\npreds = model(inputs)\n\nImageLoader.save_image(preds, './scaled_2x.png')                        # save the output 2x scaled image to `./scaled_2x.png`\nImageLoader.save_compare(inputs, preds, './scaled_2x_compare.png')      # save an output comparing the super-image with a bicubic scaling\n```\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Upscale_Images_with_Pretrained_super_image_Models.ipynb \"Open in Colab\")\n## Training data\nThe models for 2x, 3x and 4x image super resolution were pretrained on [DIV2K](https://huggingface.co/datasets/eugenesiow/Div2k), a dataset of 800 high-quality (2K resolution) images for training, augmented to 4000 images and uses a dev set of  100 validation images (images numbered 801 to 900). \n## Training procedure\n### Preprocessing\nWe follow the pre-processing and training method of [Wang et al.](https://arxiv.org/abs/2104.07566).\nLow Resolution (LR) images are created by using bicubic interpolation as the resizing method to reduce the size of the High Resolution (HR) images by x2, x3 and x4 times.\nDuring training, RGB patches with size of 64√ó64 from the LR input are used together with their corresponding HR patches. \nData augmentation is applied to the training set in the pre-processing stage where five images are created from the four corners and center of the original image. \n\nWe need the huggingface [datasets](https://huggingface.co/datasets?filter=task_ids:other-other-image-super-resolution) library to download the data:\n```bash\npip install datasets\n```\nThe following code gets the data and preprocesses/augments the data.\n\n```python\nfrom datasets import load_dataset\nfrom super_image.data import EvalDataset, TrainDataset, augment_five_crop\n\naugmented_dataset = load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='train')\\\n    .map(augment_five_crop, batched=True, desc=\"Augmenting Dataset\")                                # download and augment the data with the five_crop method\ntrain_dataset = TrainDataset(augmented_dataset)                                                     # prepare the train dataset for loading PyTorch DataLoader\neval_dataset = EvalDataset(load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='validation'))      # prepare the eval dataset for the PyTorch DataLoader\n```\n### Pretraining\nThe model was trained on GPU. The training code is provided below:\n```python\nfrom super_image import Trainer, TrainingArguments, EdsrModel, EdsrConfig\n\ntraining_args = TrainingArguments(\n    output_dir='./results',                 # output directory\n    num_train_epochs=1000,                  # total number of training epochs\n)\n\nconfig = EdsrConfig(\n    scale=4,                                # train a model to upscale 4x\n)\nmodel = EdsrModel(config)\n\ntrainer = Trainer(\n    model=model,                         # the instantiated model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=eval_dataset            # evaluation dataset\n)\n\ntrainer.train()\n```\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Train_super_image_Models.ipynb \"Open in Colab\")\n## Evaluation results\nThe evaluation metrics include [PSNR](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio#Quality_estimation_with_PSNR) and [SSIM](https://en.wikipedia.org/wiki/Structural_similarity#Algorithm). \n\nEvaluation datasets include:\n- Set5 - [Bevilacqua et al. (2012)](https://huggingface.co/datasets/eugenesiow/Set5)\n- Set14 - [Zeyde et al. (2010)](https://huggingface.co/datasets/eugenesiow/Set14)\n- BSD100 - [Martin et al. (2001)](https://huggingface.co/datasets/eugenesiow/BSD100)\n- Urban100 - [Huang et al. (2015)](https://huggingface.co/datasets/eugenesiow/Urban100)\n\nThe results columns below are represented below as `PSNR/SSIM`. They are compared against a Bicubic baseline.\n\n|Dataset  \t    |Scale      |Bicubic  \t        |edsr-base  \t        |\n|---\t        |---\t    |---\t            |---\t                |\n|Set5  \t        |2x         |33.64/0.9292       |**38.02/0.9607**       |\n|Set5  \t        |3x  \t    |30.39/0.8678  \t    |**35.04/0.9403**  \t    |\n|Set5  \t        |4x  \t    |28.42/0.8101  \t    |**32.12/0.8947**       |\n|Set14  \t    |2x         |30.22/0.8683  \t    |**33.57/0.9172**  \t    |\n|Set14  \t    |3x         |27.53/0.7737  \t    |**30.93/0.8567**  \t    |\n|Set14  \t    |4x         |25.99/0.7023  \t    |**28.60/0.7815**  \t    |\n|BSD100  \t    |2x  \t    |29.55/0.8425  \t    |**32.21/0.8999**  \t    |\n|BSD100  \t    |3x  \t    |27.20/0.7382  \t    |**29.65/0.8204**  \t    |\n|BSD100  \t    |4x  \t    |25.96/0.6672  \t    |**27.61/0.7363**  \t    |\n|Urban100  \t    |2x  \t    |26.66/0.8408  \t    |**32.04/0.9276**  \t    |\n|Urban100  \t    |3x  \t    |  \t                |**29.23/0.8723**  \t    |\n|Urban100  \t    |4x  \t    |23.14/0.6573  \t    |**26.02/0.7832**  \t    |\n\n![Comparing Bicubic upscaling against x2 upscaling on Set5 Image 2](images/Set5_2_compare.png \"Comparing Bicubic upscaling against x2 upscaling on Set5 Image 2\")\n\nYou can find a notebook to easily run evaluation on pretrained models below:\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Evaluate_Pretrained_super_image_Models.ipynb \"Open in Colab\")\n created at: 2022-03-02\n- model_name: eugenesiow/msrn-bam\n  description: The goal of image super resolution is to restore a high resolution (HR) image from a single low resolution (LR) image. The image below shows the ground truth (HR), the bicubic upscaling x2 and model upscaling x2.\n\n![Comparing Bicubic upscaling against the models x4 upscaling on Set5 Image 4](images/msrn_4_4_compare.png \"Comparing Bicubic upscaling against the models x4 upscaling on Set5 Image 4\")\n## Model description\nThe MSRN model proposes a feature extraction structure called the multi-scale residual block. This module can \"adaptively detect image features at different scales\" and \"exploit the potential features of the image\".\n\nThis model also applies the balanced attention (BAM) method invented by [Wang et al. (2021)](https://arxiv.org/abs/2104.07566) to further improve the results.\n## Intended uses & limitations\nYou can use the pre-trained models for upscaling your images 2x, 3x and 4x. You can also use the trainer to train a model on your own dataset.\n### How to use\nThe model can be used with the [super_image](https://github.com/eugenesiow/super-image) library:\n```bash\npip install super-image\n```\nHere is how to use a pre-trained model to upscale your image:\n```python\nfrom super_image import MsrnModel, ImageLoader\nfrom PIL import Image\nimport requests\n\nurl = 'https://paperswithcode.com/media/datasets/Set5-0000002728-07a9793f_zA3bDjj.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel = MsrnModel.from_pretrained('eugenesiow/msrn-bam', scale=2)      # scale 2, 3 and 4 models available\ninputs = ImageLoader.load_image(image)\npreds = model(inputs)\n\nImageLoader.save_image(preds, './scaled_2x.png')                        # save the output 2x scaled image to `./scaled_2x.png`\nImageLoader.save_compare(inputs, preds, './scaled_2x_compare.png')      # save an output comparing the super-image with a bicubic scaling\n```\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Upscale_Images_with_Pretrained_super_image_Models.ipynb \"Open in Colab\")\n## Training data\nThe models for 2x, 3x and 4x image super resolution were pretrained on [DIV2K](https://huggingface.co/datasets/eugenesiow/Div2k), a dataset of 800 high-quality (2K resolution) images for training, augmented to 4000 images and uses a dev set of  100 validation images (images numbered 801 to 900). \n## Training procedure\n### Preprocessing\nWe follow the pre-processing and training method of [Wang et al.](https://arxiv.org/abs/2104.07566).\nLow Resolution (LR) images are created by using bicubic interpolation as the resizing method to reduce the size of the High Resolution (HR) images by x2, x3 and x4 times.\nDuring training, RGB patches with size of 64√ó64 from the LR input are used together with their corresponding HR patches. \nData augmentation is applied to the training set in the pre-processing stage where five images are created from the four corners and center of the original image. \n\nWe need the huggingface [datasets](https://huggingface.co/datasets?filter=task_ids:other-other-image-super-resolution) library to download the data:\n```bash\npip install datasets\n```\nThe following code gets the data and preprocesses/augments the data.\n\n```python\nfrom datasets import load_dataset\nfrom super_image.data import EvalDataset, TrainDataset, augment_five_crop\n\naugmented_dataset = load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='train')\\\n    .map(augment_five_crop, batched=True, desc=\"Augmenting Dataset\")                                # download and augment the data with the five_crop method\ntrain_dataset = TrainDataset(augmented_dataset)                                                     # prepare the train dataset for loading PyTorch DataLoader\neval_dataset = EvalDataset(load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='validation'))      # prepare the eval dataset for the PyTorch DataLoader\n```\n### Pretraining\nThe model was trained on GPU. The training code is provided below:\n```python\nfrom super_image import Trainer, TrainingArguments, MsrnModel, MsrnConfig\n\ntraining_args = TrainingArguments(\n    output_dir='./results',                 # output directory\n    num_train_epochs=1000,                  # total number of training epochs\n)\n\nconfig = MsrnConfig(\n    scale=4,                                # train a model to upscale 4x\n    bam=True,                               # apply balanced attention to the network\n)\nmodel = MsrnModel(config)\n\ntrainer = Trainer(\n    model=model,                         # the instantiated model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=eval_dataset            # evaluation dataset\n)\n\ntrainer.train()\n```\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Train_super_image_Models.ipynb \"Open in Colab\")\n## Evaluation results\nThe evaluation metrics include [PSNR](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio#Quality_estimation_with_PSNR) and [SSIM](https://en.wikipedia.org/wiki/Structural_similarity#Algorithm). \n\nEvaluation datasets include:\n- Set5 - [Bevilacqua et al. (2012)](https://huggingface.co/datasets/eugenesiow/Set5)\n- Set14 - [Zeyde et al. (2010)](https://huggingface.co/datasets/eugenesiow/Set14)\n- BSD100 - [Martin et al. (2001)](https://huggingface.co/datasets/eugenesiow/BSD100)\n- Urban100 - [Huang et al. (2015)](https://huggingface.co/datasets/eugenesiow/Urban100)\n\nThe results columns below are represented below as `PSNR/SSIM`. They are compared against a Bicubic baseline.\n\n|Dataset  \t    |Scale      |Bicubic  \t        |msrn-bam  \t                    |\n|---\t        |---\t    |---\t            |---\t                        |\n|Set5  \t        |2x         |33.64/0.9292       |**38.02/0.9608**       |\n|Set5  \t        |3x  \t    |30.39/0.8678  \t    |**35.13/0.9408**  \t    |\n|Set5  \t        |4x  \t    |28.42/0.8101  \t    |**32.26/0.8955**       |\n|Set14  \t    |2x         |30.22/0.8683  \t    |**33.73/0.9186**  \t    |\n|Set14  \t    |3x         |27.53/0.7737  \t    |**31.06/0.8588**  \t    |\n|Set14  \t    |4x         |25.99/0.7023  \t    |**28.78/0.7859**  \t    |\n|BSD100  \t    |2x  \t    |29.55/0.8425  \t    |**33.78/0.9253**  \t    |\n|BSD100  \t    |3x  \t    |27.20/0.7382  \t    |**29.65/0.8196**  \t    |\n|BSD100  \t    |4x  \t    |25.96/0.6672  \t    |**28.51/0.7651**  \t    |\n|Urban100  \t    |2x  \t    |26.66/0.8408  \t    |**32.08/0.9276**  \t    |\n|Urban100  \t    |3x  \t    |  \t                |**29.26/0.8736**  \t    |\n|Urban100  \t    |4x  \t    |23.14/0.6573  \t    |**26.10/0.7857**  \t    |\n\n![Comparing Bicubic upscaling against the models x4 upscaling on Set5 Image 2](images/msrn_2_4_compare.png \"Comparing Bicubic upscaling against the models x4 upscaling on Set5 Image 2\")\n\nYou can find a notebook to easily run evaluation on pretrained models below:\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Evaluate_Pretrained_super_image_Models.ipynb \"Open in Colab\")\n created at: 2022-03-02\n- model_name: ZhengPeng7/BiRefNet_HR-matting\n  description: this birefnet was trained with images in `2048x2048` for higher resolution image matting with transparency. all tested in fp16 mode. dataset method resolution maxfm wfmeasure mae smeasure meanem hce maxem meanfm adpem adpfm mba maxbiou meanbiou\n created at: 2025-02-12T16:19:23+00:00\n- model_name: PramaLLC/BEN2\n  description: ben2 background erase network introduces a novel approach to foreground segmentation through its innovative confidence guided matting cgm pipeline. the architecture employs a refiner network that targets and processes pixels where the base model exhibits lower confidence levels, resulting in more precise and reliable matting results. this model is built on ben: ben2 was trained on the dis5k and our 22k proprietary segmentation dataset. our enhanced model delivers superior performance in hair matting, 4k processing, object segmentation, and edge refinement. our base model is open source. to try the full model through our free web demo or integrate ben2 into your project with our api: - üåê\n created at: 2025-01-22T14:39:05+00:00\n- model_name: ZhengPeng7/BiRefNet_HR\n  description: this birefnet was trained with images in `2048x2048` for higher resolution inference. all tested in fp16 mode. dataset method resolution maxfm wfmeasure mae smeasure meanem hce maxem meanfm adpem adpfm mba maxbiou meanbiou\n created at: 2025-02-01T17:52:14+00:00\n- model_name: briaai/RMBG-2.0\n  description: rmbg v2.0 is our new state-of-the-art background removal model significantly improves rmbg v1.4. the model is designed to effectively separate foreground from background in a range of categories and image types. this model has been trained on a carefully selected dataset, which includes: general stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale. - developed by:  - model type: background removal - license:  - the model is released under a cc by-nc 4.0 license for non-commercial use. - commercial use is subject to a commercial agreement with bria. available  purchase: to purchase a commercial license simply click . - model description: bria rmbg-2.0 is a dichotomous image segmentation model trained exclusively on a professional-grade dataset. the model output includes a single-channel 8-bit grayscale alpha matte, where each pixel value indicates the opacity level of the corresponding pixel in the original image. this non-binary output approach offers developers the flexibility to define custom thresholds for foreground-background separation, catering to varied use cases requirements and enhancing integration into complex pipelines. - bria: resources for more information:\n created at: 2024-10-29T12:14:36+00:00\n- model_name: ZhengPeng7/BiRefNet_dynamic\n  description: bilateral reference for high-resolution dichotomous image segmentation an arbitrary shape adaptable birefnet for general segmentation. this model was trained on arbitrary shapes 256x256 2304x2304 and shows great robustness on inputs with any shape.\n created at: 2025-03-31T03:37:03+00:00\n- model_name: Ricky06662/Seg-Zero-7B\n  description: this model is based on the paper . it uses a decoupled architecture with a reasoning model and a segmentation model. it's trained via reinforcement learning using grpo without explicit reasoning data, leading to robust zero-shot generalization and emergent test-time reasoning. code: this is a seg-zero-7b model. it introduces a decoupled architecture consisting of a reasoning model and a segmentation model. the reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate pixel-level masks. this is a seg-zero-7b model. it introduces a decoupled architecture consisting of a reasoning model and a segmentation model. the reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate pixel-level masks.\n created at: 2025-03-07T15:08:24+00:00\n- model_name: cocktailpeanut/rm\n  description: rmbg v2.0 is our new state-of-the-art background removal model significantly improves rmbg v1.4. the model is designed to effectively separate foreground from background in a range of categories and image types. this model has been trained on a carefully selected dataset, which includes: general stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale. - developed by:  - model type: background removal - license:  - the model is released under a cc by-nc 4.0 license for non-commercial use. - commercial use is subject to a commercial agreement with bria. available  purchase: to purchase a commercial license simply click . - model description: bria rmbg-2.0 is a dichotomous image segmentation model trained exclusively on a professional-grade dataset. - bria: resources for more information:\n created at: 2025-04-12T06:16:07+00:00\n- model_name: yuvraj108c/RMBG-2.0\n  description: rmbg v2.0 is our new state-of-the-art background removal model significantly improves rmbg v1.4. the model is designed to effectively separate foreground from background in a range of categories and image types. this model has been trained on a carefully selected dataset, which includes: general stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale. - developed by:  - model type: background removal - license:  - the model is released under a cc by-nc 4.0 license for non-commercial use. - commercial use is subject to a commercial agreement with bria. available  purchase: to purchase a commercial license simply click . - model description: bria rmbg-2.0 is a dichotomous image segmentation model trained exclusively on a professional-grade dataset. the model output includes a single-channel 8-bit grayscale alpha matte, where each pixel value indicates the opacity level of the corresponding pixel in the original image. this non-binary output approach offers developers the flexibility to define custom thresholds for foreground-background separation, catering to varied use cases requirements and enhancing integration into complex pipelines. - bria: resources for more information:\n created at: 2025-07-28T10:18:49+00:00\n- model_name: Ricky06662/VisionReasoner-7B\n  description: this repository contains the visionreasoner-7b model, developed as part of the novel seg-zero framework, presented in the paper . this model is also associated with the paper . code: project page:  seg-zero is a novel framework that demonstrates remarkable generalizability and derives explicit chain-of-thought reasoning through cognitive reinforcement for reasoning segmentation. this visionreasoner-7b model employs a decoupled architecture consisting of a reasoning model and a segmentation model. the reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate precise pixel-level masks. trained exclusively via reinforcement learning with grpo and without explicit reasoning data, seg-zero achieves robust zero-shot generalization and exhibits emergent test-time reasoning capabilities. experiments show that seg-zero-7b achieves a zero-shot performance of 57.5 on the reasonseg benchmark, surpassing the prior lisa-7b by 18%. this significant improvement highlights seg-zero's ability to generalize across domains while presenting an explicit reasoning process.\n created at: 2025-05-18T05:46:01+00:00\n- model_name: tue-mps/coco_panoptic_eomt_large_640\n  description: eomt encoder-only mask transformer is a vision transformer vit architecture designed for high-quality and efficient image segmentation. it was introduced in the cvpr 2025 highlight paper: by tommie kerssies, niccol√≤ cavagnero, alexander hermans, narges norouzi, giuseppe averta, bastian leibe, gijs dubbelman, and daan de geus.\n created at: 2025-03-26T17:28:05+00:00\n- model_name: tue-mps/ade20k_semantic_eomt_large_512\n  description: eomt encoder-only mask transformer is a vision transformer vit architecture designed for high-quality and efficient image segmentation. it was introduced in the cvpr 2025 highlight paper: by tommie kerssies, niccol√≤ cavagnero, alexander hermans, narges norouzi, giuseppe averta, bastian leibe, gijs dubbelman, and daan de geus.\n created at: 2025-03-26T19:29:47+00:00\n- model_name: bwittmann/vesselFM\n  description: tl;dr : vesselfm is a foundation model for universal 3d blood vessel segmentation in arbitrary imaging domains. for details, please refer to our preprint and our github repo\n created at: 2024-12-02T12:45:08+00:00\n- model_name: Ricky06662/Seg-Zero-7B-Best-on-ReasonSegTest\n  description: this model is based on the paper . code: seg-zero introduces a novel framework for reasoning segmentation that addresses the limitations of traditional supervised fine-tuning methods, which often struggle with out-of-domain generalization and lack explicit reasoning processes. the framework features a decoupled architecture consisting of a reasoning model and a segmentation model. the reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate precise pixel-level masks.\n created at: 2025-04-09T16:12:22+00:00\n- model_name: tue-mps/coco_instance_eomt_large_640\n  description: eomt encoder-only mask transformer is a vision transformer vit architecture designed for high-quality and efficient image segmentation. it was introduced in the cvpr 2025 highlight paper: by tommie kerssies, niccol√≤ cavagnero, alexander hermans, narges norouzi, giuseppe averta, bastian leibe, gijs dubbelman, and daan de geus.\n created at: 2025-03-26T17:16:21+00:00\n- model_name: facebook/sapiens-seg-1b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T20:26:24+00:00\n- model_name: tue-mps/coco_instance_eomt_large_1280\n  description: eomt encoder-only mask transformer is a vision transformer vit architecture designed for high-quality and efficient image segmentation. it was introduced in the cvpr 2025 highlight paper: by tommie kerssies, niccol√≤ cavagnero, alexander hermans, narges norouzi, giuseppe averta, bastian leibe, gijs dubbelman, and daan de geus.\n created at: 2025-03-26T19:34:47+00:00\n- model_name: nevernever69/dit-doclaynet-segmentation\n  description: this model is a fine-tuned version of for document layout semantic segmentation on the dataset small subset: `nevernever69/small-doclaynet-v1.1`. it segments scanned document images into 11 layout categories such as title, paragraph, table, and footer. - segment document images into structured layout elements - assist in downstream tasks like document ocr, archiving, and automatic annotation\n created at: 2025-03-12T11:45:04+00:00\n- model_name: Pushti/BirefNetHR\n  description: this birefnet was trained with images in `2048x2048` for higher resolution inference. all tested in fp16 mode. dataset method resolution maxfm wfmeasure mae smeasure meanem hce maxem meanfm adpem adpfm mba maxbiou meanbiou\n created at: 2025-04-18T07:28:32+00:00\n- model_name: ZQL9711/RMBG-2-Matting\n  description: rmbg v2.0 is our new state-of-the-art background removal model significantly improves rmbg v1.4. the model is designed to effectively separate foreground from background in a range of categories and image types. this model has been trained on a carefully selected dataset, which includes: general stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale. - developed by:  - model type: background removal - license:  - the model is released under a cc by-nc 4.0 license for non-commercial use. - commercial use is subject to a commercial agreement with bria. available  purchase: to purchase a commercial license simply click . - model description: bria rmbg-2.0 is a dichotomous image segmentation model trained exclusively on a professional-grade dataset. - bria: resources for more information:\n created at: 2025-07-05T08:31:53+00:00\n- model_name: Acly/BiRefNet-GGUF\n  description: birefnet is a model for dichotomous image segmentation background removal. the weights in this repository are converted for lightweight inference on consumer hardware with .\n created at: 2025-07-20T10:33:31+00:00\n\nExamples:\n- example user query: Our company is building a software to analyze satellite images for land use. We need to segment different regions in the images.\n  model_name: nvidia/segformer-b0-finetuned-ade-512-512\n- example user query: Identify an API to perform semantic segmentation of the street labeled with road, sidewalk, and building for a given picture.\n  model_name: pytorch/vision/deeplabv3_resnet50\n- example user query: A drone is flying over the neighborhood and is recording video. How can I process the video and perform semantic segmentation on each frame?\n  model_name: pytorch/vision/deeplabv3_resnet101\n- example user query: To identify and separate different objects in an image, create an image segmentation model.\n  model_name: openmmlab/upernet-convnext-small\n- example user query: The company is developing a fashion application which needs to identify the clothing items in pictures.\n  model_name: mattmdjaga/segformer_b2_clothes\n- example user query: An urban development company wants to analyze city images to inform infrastructure plans. Extract features using a semantic segmentation model.\n  model_name: nvidia/segformer-b5-finetuned-cityscapes-1024-1024\n\nUser prompt:\nSegment all components in the frame.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_seg>\n\nCandidate Models:\n- model_name: CIDAS/clipseg-rd64-refined\n  description: CLIPSeg model with reduce dimension 64, refined (using a more complex convolution). It was introduced in the paper Image Segmentation Using Text and Image Prompts by L√ºddecke et al. and first released in this repository. This model is intended for zero-shot and one-shot image segmentation.\n created at: 2022-11-01\n- model_name: openmmlab/upernet-convnext-small\n  description: UperNet framework for semantic segmentation, leveraging a ConvNeXt backbone. UperNet was introduced in the paper Unified Perceptual Parsing for Scene Understanding by Xiao et al. Combining UperNet with a ConvNeXt backbone was introduced in the paper A ConvNet for the 2020s.\n created at: 2023-01-13\n- model_name: nvidia/segformer-b0-finetuned-ade-512-512\n  description: SegFormer model fine-tuned on ADE20k at resolution 512x512. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: nvidia/segformer-b5-finetuned-ade-640-640\n  description: SegFormer model fine-tuned on ADE20k at resolution 640x640. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: nvidia/segformer-b2-finetuned-cityscapes-1024-1024\n  description: SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: nvidia/segformer-b0-finetuned-cityscapes-1024-1024\n  description: SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: facebook/maskformer-swin-base-coco\n  description: MaskFormer model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.\n created at: 2022-03-02\n- model_name: facebook/mask2former-swin-large-cityscapes-semantic\n  description: Mask2Former model trained on Cityscapes semantic segmentation (large-sized version, Swin backbone). It addresses instance, semantic and panoptic segmentation by predicting a set of masks and corresponding labels. The model outperforms the previous SOTA, MaskFormer, in terms of performance and efficiency.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-base-coco-panoptic\n  description: Mask2Former model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.\n created at: 2023-01-02\n- model_name: mattmdjaga/segformer_b2_clothes\n  description: SegFormer model fine-tuned on ATR dataset for clothes segmentation.\n created at: 2022-11-24\n- model_name: shi-labs/oneformer_coco_swin_large\n  description: OneFormer model trained on the COCO dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.\n created at: 2022-11-15\n- model_name: facebook/mask2former-swin-large-coco-panoptic\n  description: Mask2Former model trained on COCO panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.\n created at: 2023-01-02\n- model_name: shi-labs/oneformer_ade20k_swin_large\n  description: OneFormer model trained on the ADE20k dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.\n created at: 2022-11-15\n- model_name: facebook/mask2former-swin-small-coco-instance\n  description: Mask2Former model trained on COCO instance segmentation (small-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.\n created at: 2022-12-26\n- model_name: nvidia/segformer-b5-finetuned-cityscapes-1024-1024\n  description: SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: shi-labs/oneformer_ade20k_swin_tiny\n  description: OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.\n created at: 2022-11-16\n- model_name: keremberke/yolov8m-building-segmentation\n  description: A YOLOv8 model for building segmentation in satellite images. It can detect and segment buildings in the input images.\n created at: 2023-01-27\n- model_name: facebook/maskformer-swin-base-ade\n  description: MaskFormer model trained on ADE20k semantic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.\n created at: 2022-03-02\n- model_name: facebook/mask2former-swin-tiny-coco-instance\n  description: Mask2Former model trained on COCO instance segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. You can use this particular checkpoint for instance segmentation.\n created at: 2022-12-23\n- model_name: keremberke/yolov8m-pcb-defect-segmentation\n  description: A YOLOv8 model for PCB defect segmentation trained on the pcb-defect-segmentation dataset. The model can detect and segment defects in PCB images, such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.\n created at: 2023-01-28\n- model_name: facebook/maskformer-swin-tiny-coco\n  description: MaskFormer model trained on COCO panoptic segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.\n created at: 2022-03-02\n- model_name: keremberke/yolov8m-pothole-segmentation\n  description: A YOLOv8 model for pothole segmentation trained on keremberke/pothole-segmentation dataset. It can detect potholes in images and provide segmentation masks for the detected potholes.\n created at: 2023-01-26\n- model_name: keremberke/yolov8s-building-segmentation\n  description: A YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can detect and segment buildings with high accuracy.\n created at: 2023-01-26\n- model_name: keremberke/yolov8s-pothole-segmentation\n  description: A YOLOv8 model for pothole segmentation. This model detects potholes in images and outputs bounding boxes and masks for the detected potholes.\n created at: 2023-01-26\n- model_name: keremberke/yolov8s-pcb-defect-segmentation\n  description: YOLOv8s model for PCB defect segmentation. The model is trained to detect and segment PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.\n created at: 2023-01-28\n- model_name: keremberke/yolov8n-pcb-defect-segmentation\n  description: A YOLOv8 model for detecting and segmenting PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.\n created at: 2023-01-28\n- model_name: keremberke/yolov8n-pothole-segmentation\n  description: A YOLOv8 model for pothole segmentation in images. The model is trained on the pothole-segmentation dataset and achieves high accuracy in detecting potholes.\n created at: 2023-01-15\n- model_name: facebook/maskformer-swin-large-ade\n  description: MaskFormer model trained on ADE20k semantic segmentation (large-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.\n created at: 2022-03-02\n- model_name: facebook/detr-resnet-50-panoptic\n  description: DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 panoptic (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.\n created at: 2022-03-02\n- model_name: tensorflow/lite-model/deeplabv3/1/default\n  description: TensorFlow Hub provides pre-trained image segmentation models that can be loaded and used for various tasks such as semantic segmentation. The example code demonstrates how to load a DeepLabV3 model and perform segmentation on an input image tensor.\n created at: 2020-10-06\n- model_name: pytorch/vision/deeplabv3_resnet50\n  description: DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: pytorch/vision/deeplabv3_mobilenet_v3_large\n  description: DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: pytorch/vision/deeplabv3_resnet101\n  description: DeepLabV3 models with ResNet-50, ResNet-101 and MobileNet-V3 backbones for semantic segmentation. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: mateuszbuda/brain-segmentation-pytorch/unet\n  description: U-Net with batch normalization for biomedical image segmentation with pretrained weights for abnormality segmentation in brain MRI. The model comprises four levels of blocks containing two convolutional layers with batch normalization and ReLU activation function, and one max pooling layer in the encoding part and up-convolutional layers instead in the decoding part. The number of convolutional filters in each block is 32, 64, 128, and 256. The bottleneck layer has 512 convolutional filters. From the encoding layers, skip connections are used to the corresponding layers in the decoding part. Input image is a 3-channel brain MRI slice from pre-contrast, FLAIR, and post-contrast sequences, respectively. Output is a one-channel probability map of abnormality regions with the same size as the input image.\n created at: 2019-05-26\n- model_name: pytorch/vision/fcn_resnet101\n  description: FCN-ResNet is a Fully-Convolutional Network model using a ResNet-50 or a ResNet-101 backbone. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: pytorch/fairseq\n  description: Transformer (NMT) is a powerful sequence-to-sequence modeling architecture that produces state-of-the-art neural machine translation systems. It is based on the paper 'Attention Is All You Need' and has been improved using techniques such as large-scale semi-supervised training, back-translation, and noisy-channel reranking. It supports English-French and English-German translation as well as round-trip translation for paraphrasing.\n created at: 2017-08-29\n- model_name: pytorch/vision/fcn_resnet50\n  description: FCN-ResNet is a Fully-Convolutional Network model using a ResNet-50 or a ResNet-101 backbone. The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n created at: 2016-11-09\n- model_name: facebook/mask2former-swin-large-ade-semantic\n  description: Mask2Former model trained on ADE20k semantic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-large-ade-panoptic\n  description: Mask2Former model trained on ADE20k panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-large-mapillary-vistas-panoptic\n  description: Mask2Former model trained on Mapillary Vistas panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-large-mapillary-vistas-semantic\n  description: Mask2Former model trained on Mapillary Vistas semantic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-05\n- model_name: facebook/mask2former-swin-large-cityscapes-panoptic\n  description: Mask2Former model trained on Cityscapes panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper [Masked-attention Mask Transformer for Universal Image Segmentation\n](https://arxiv.org/abs/2112.01527) and first released in [this repository](https://github.com/facebookresearch/Mask2Former/). \n\nDisclaimer: The team releasing Mask2Former did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2023-01-03\n- model_name: shi-labs/oneformer_cityscapes_swin_large\n  description: OneFormer model trained on the Cityscapes dataset (large-sized version, Swin backbone). It was introduced in the paper [OneFormer: One Transformer to Rule Universal Image Segmentation](https://arxiv.org/abs/2211.06220) by Jain et al. and first released in [this repository](https://github.com/SHI-Labs/OneFormer).\n\n![model image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/oneformer_teaser.png)\n created at: 2022-11-15\n- model_name: google/deeplabv3_mobilenet_v2_1.0_513\n  description: MobileNet V2 model pre-trained on PASCAL VOC at resolution 513x513. It was introduced in [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381) by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen. It was first released in [this repository](https://github.com/tensorflow/models/tree/master/research/deeplab).\n\nDisclaimer: The team releasing MobileNet V2 did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2022-11-10\n- model_name: apple/deeplabv3-mobilevit-small\n  description: MobileViT model pre-trained on PASCAL VOC at resolution 512x512. It was introduced in [MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer](https://arxiv.org/abs/2110.02178) by Sachin Mehta and Mohammad Rastegari, and first released in [this repository](https://github.com/apple/ml-cvnets). The license used is [Apple sample code license](https://github.com/apple/ml-cvnets/blob/main/LICENSE).\n\nDisclaimer: The team releasing MobileViT did not write a model card for this model so this model card has been written by the Hugging Face team.\n created at: 2022-05-30\n- model_name: eugenesiow/edsr-base\n  description: The goal of image super resolution is to restore a high resolution (HR) image from a single low resolution (LR) image. The image below shows the ground truth (HR), the bicubic upscaling x2 and EDSR upscaling x2.\n\n![Comparing Bicubic upscaling against EDSR x2 upscaling on Set5 Image 4](images/Set5_4_compare.png \"Comparing Bicubic upscaling against EDSR x2 upscaling on Set5 Image 4\")\n## Model description\nEDSR is a model that uses both deeper and wider architecture (32 ResBlocks and 256 channels) to improve performance. It uses both global and local skip connections, and up-scaling is done at the end of the network. It doesn't use batch normalization layers (input and output have similar distributions, normalizing intermediate features may not be desirable) instead it uses constant scaling layers to ensure stable training. An L1 loss function (absolute error) is used instead of L2 (MSE), the authors showed better performance empirically and it requires less computation.\n\nThis is a base model (~5mb vs ~100mb) that includes just 16 ResBlocks and 64 channels.\n## Intended uses & limitations\nYou can use the pre-trained models for upscaling your images 2x, 3x and 4x. You can also use the trainer to train a model on your own dataset.\n### How to use\nThe model can be used with the [super_image](https://github.com/eugenesiow/super-image) library:\n```bash\npip install super-image\n```\nHere is how to use a pre-trained model to upscale your image:\n```python\nfrom super_image import EdsrModel, ImageLoader\nfrom PIL import Image\nimport requests\n\nurl = 'https://paperswithcode.com/media/datasets/Set5-0000002728-07a9793f_zA3bDjj.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel = EdsrModel.from_pretrained('eugenesiow/edsr-base', scale=2)      # scale 2, 3 and 4 models available\ninputs = ImageLoader.load_image(image)\npreds = model(inputs)\n\nImageLoader.save_image(preds, './scaled_2x.png')                        # save the output 2x scaled image to `./scaled_2x.png`\nImageLoader.save_compare(inputs, preds, './scaled_2x_compare.png')      # save an output comparing the super-image with a bicubic scaling\n```\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Upscale_Images_with_Pretrained_super_image_Models.ipynb \"Open in Colab\")\n## Training data\nThe models for 2x, 3x and 4x image super resolution were pretrained on [DIV2K](https://huggingface.co/datasets/eugenesiow/Div2k), a dataset of 800 high-quality (2K resolution) images for training, augmented to 4000 images and uses a dev set of  100 validation images (images numbered 801 to 900). \n## Training procedure\n### Preprocessing\nWe follow the pre-processing and training method of [Wang et al.](https://arxiv.org/abs/2104.07566).\nLow Resolution (LR) images are created by using bicubic interpolation as the resizing method to reduce the size of the High Resolution (HR) images by x2, x3 and x4 times.\nDuring training, RGB patches with size of 64√ó64 from the LR input are used together with their corresponding HR patches. \nData augmentation is applied to the training set in the pre-processing stage where five images are created from the four corners and center of the original image. \n\nWe need the huggingface [datasets](https://huggingface.co/datasets?filter=task_ids:other-other-image-super-resolution) library to download the data:\n```bash\npip install datasets\n```\nThe following code gets the data and preprocesses/augments the data.\n\n```python\nfrom datasets import load_dataset\nfrom super_image.data import EvalDataset, TrainDataset, augment_five_crop\n\naugmented_dataset = load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='train')\\\n    .map(augment_five_crop, batched=True, desc=\"Augmenting Dataset\")                                # download and augment the data with the five_crop method\ntrain_dataset = TrainDataset(augmented_dataset)                                                     # prepare the train dataset for loading PyTorch DataLoader\neval_dataset = EvalDataset(load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='validation'))      # prepare the eval dataset for the PyTorch DataLoader\n```\n### Pretraining\nThe model was trained on GPU. The training code is provided below:\n```python\nfrom super_image import Trainer, TrainingArguments, EdsrModel, EdsrConfig\n\ntraining_args = TrainingArguments(\n    output_dir='./results',                 # output directory\n    num_train_epochs=1000,                  # total number of training epochs\n)\n\nconfig = EdsrConfig(\n    scale=4,                                # train a model to upscale 4x\n)\nmodel = EdsrModel(config)\n\ntrainer = Trainer(\n    model=model,                         # the instantiated model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=eval_dataset            # evaluation dataset\n)\n\ntrainer.train()\n```\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Train_super_image_Models.ipynb \"Open in Colab\")\n## Evaluation results\nThe evaluation metrics include [PSNR](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio#Quality_estimation_with_PSNR) and [SSIM](https://en.wikipedia.org/wiki/Structural_similarity#Algorithm). \n\nEvaluation datasets include:\n- Set5 - [Bevilacqua et al. (2012)](https://huggingface.co/datasets/eugenesiow/Set5)\n- Set14 - [Zeyde et al. (2010)](https://huggingface.co/datasets/eugenesiow/Set14)\n- BSD100 - [Martin et al. (2001)](https://huggingface.co/datasets/eugenesiow/BSD100)\n- Urban100 - [Huang et al. (2015)](https://huggingface.co/datasets/eugenesiow/Urban100)\n\nThe results columns below are represented below as `PSNR/SSIM`. They are compared against a Bicubic baseline.\n\n|Dataset  \t    |Scale      |Bicubic  \t        |edsr-base  \t        |\n|---\t        |---\t    |---\t            |---\t                |\n|Set5  \t        |2x         |33.64/0.9292       |**38.02/0.9607**       |\n|Set5  \t        |3x  \t    |30.39/0.8678  \t    |**35.04/0.9403**  \t    |\n|Set5  \t        |4x  \t    |28.42/0.8101  \t    |**32.12/0.8947**       |\n|Set14  \t    |2x         |30.22/0.8683  \t    |**33.57/0.9172**  \t    |\n|Set14  \t    |3x         |27.53/0.7737  \t    |**30.93/0.8567**  \t    |\n|Set14  \t    |4x         |25.99/0.7023  \t    |**28.60/0.7815**  \t    |\n|BSD100  \t    |2x  \t    |29.55/0.8425  \t    |**32.21/0.8999**  \t    |\n|BSD100  \t    |3x  \t    |27.20/0.7382  \t    |**29.65/0.8204**  \t    |\n|BSD100  \t    |4x  \t    |25.96/0.6672  \t    |**27.61/0.7363**  \t    |\n|Urban100  \t    |2x  \t    |26.66/0.8408  \t    |**32.04/0.9276**  \t    |\n|Urban100  \t    |3x  \t    |  \t                |**29.23/0.8723**  \t    |\n|Urban100  \t    |4x  \t    |23.14/0.6573  \t    |**26.02/0.7832**  \t    |\n\n![Comparing Bicubic upscaling against x2 upscaling on Set5 Image 2](images/Set5_2_compare.png \"Comparing Bicubic upscaling against x2 upscaling on Set5 Image 2\")\n\nYou can find a notebook to easily run evaluation on pretrained models below:\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Evaluate_Pretrained_super_image_Models.ipynb \"Open in Colab\")\n created at: 2022-03-02\n- model_name: eugenesiow/msrn-bam\n  description: The goal of image super resolution is to restore a high resolution (HR) image from a single low resolution (LR) image. The image below shows the ground truth (HR), the bicubic upscaling x2 and model upscaling x2.\n\n![Comparing Bicubic upscaling against the models x4 upscaling on Set5 Image 4](images/msrn_4_4_compare.png \"Comparing Bicubic upscaling against the models x4 upscaling on Set5 Image 4\")\n## Model description\nThe MSRN model proposes a feature extraction structure called the multi-scale residual block. This module can \"adaptively detect image features at different scales\" and \"exploit the potential features of the image\".\n\nThis model also applies the balanced attention (BAM) method invented by [Wang et al. (2021)](https://arxiv.org/abs/2104.07566) to further improve the results.\n## Intended uses & limitations\nYou can use the pre-trained models for upscaling your images 2x, 3x and 4x. You can also use the trainer to train a model on your own dataset.\n### How to use\nThe model can be used with the [super_image](https://github.com/eugenesiow/super-image) library:\n```bash\npip install super-image\n```\nHere is how to use a pre-trained model to upscale your image:\n```python\nfrom super_image import MsrnModel, ImageLoader\nfrom PIL import Image\nimport requests\n\nurl = 'https://paperswithcode.com/media/datasets/Set5-0000002728-07a9793f_zA3bDjj.jpg'\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel = MsrnModel.from_pretrained('eugenesiow/msrn-bam', scale=2)      # scale 2, 3 and 4 models available\ninputs = ImageLoader.load_image(image)\npreds = model(inputs)\n\nImageLoader.save_image(preds, './scaled_2x.png')                        # save the output 2x scaled image to `./scaled_2x.png`\nImageLoader.save_compare(inputs, preds, './scaled_2x_compare.png')      # save an output comparing the super-image with a bicubic scaling\n```\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Upscale_Images_with_Pretrained_super_image_Models.ipynb \"Open in Colab\")\n## Training data\nThe models for 2x, 3x and 4x image super resolution were pretrained on [DIV2K](https://huggingface.co/datasets/eugenesiow/Div2k), a dataset of 800 high-quality (2K resolution) images for training, augmented to 4000 images and uses a dev set of  100 validation images (images numbered 801 to 900). \n## Training procedure\n### Preprocessing\nWe follow the pre-processing and training method of [Wang et al.](https://arxiv.org/abs/2104.07566).\nLow Resolution (LR) images are created by using bicubic interpolation as the resizing method to reduce the size of the High Resolution (HR) images by x2, x3 and x4 times.\nDuring training, RGB patches with size of 64√ó64 from the LR input are used together with their corresponding HR patches. \nData augmentation is applied to the training set in the pre-processing stage where five images are created from the four corners and center of the original image. \n\nWe need the huggingface [datasets](https://huggingface.co/datasets?filter=task_ids:other-other-image-super-resolution) library to download the data:\n```bash\npip install datasets\n```\nThe following code gets the data and preprocesses/augments the data.\n\n```python\nfrom datasets import load_dataset\nfrom super_image.data import EvalDataset, TrainDataset, augment_five_crop\n\naugmented_dataset = load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='train')\\\n    .map(augment_five_crop, batched=True, desc=\"Augmenting Dataset\")                                # download and augment the data with the five_crop method\ntrain_dataset = TrainDataset(augmented_dataset)                                                     # prepare the train dataset for loading PyTorch DataLoader\neval_dataset = EvalDataset(load_dataset('eugenesiow/Div2k', 'bicubic_x4', split='validation'))      # prepare the eval dataset for the PyTorch DataLoader\n```\n### Pretraining\nThe model was trained on GPU. The training code is provided below:\n```python\nfrom super_image import Trainer, TrainingArguments, MsrnModel, MsrnConfig\n\ntraining_args = TrainingArguments(\n    output_dir='./results',                 # output directory\n    num_train_epochs=1000,                  # total number of training epochs\n)\n\nconfig = MsrnConfig(\n    scale=4,                                # train a model to upscale 4x\n    bam=True,                               # apply balanced attention to the network\n)\nmodel = MsrnModel(config)\n\ntrainer = Trainer(\n    model=model,                         # the instantiated model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=eval_dataset            # evaluation dataset\n)\n\ntrainer.train()\n```\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Train_super_image_Models.ipynb \"Open in Colab\")\n## Evaluation results\nThe evaluation metrics include [PSNR](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio#Quality_estimation_with_PSNR) and [SSIM](https://en.wikipedia.org/wiki/Structural_similarity#Algorithm). \n\nEvaluation datasets include:\n- Set5 - [Bevilacqua et al. (2012)](https://huggingface.co/datasets/eugenesiow/Set5)\n- Set14 - [Zeyde et al. (2010)](https://huggingface.co/datasets/eugenesiow/Set14)\n- BSD100 - [Martin et al. (2001)](https://huggingface.co/datasets/eugenesiow/BSD100)\n- Urban100 - [Huang et al. (2015)](https://huggingface.co/datasets/eugenesiow/Urban100)\n\nThe results columns below are represented below as `PSNR/SSIM`. They are compared against a Bicubic baseline.\n\n|Dataset  \t    |Scale      |Bicubic  \t        |msrn-bam  \t                    |\n|---\t        |---\t    |---\t            |---\t                        |\n|Set5  \t        |2x         |33.64/0.9292       |**38.02/0.9608**       |\n|Set5  \t        |3x  \t    |30.39/0.8678  \t    |**35.13/0.9408**  \t    |\n|Set5  \t        |4x  \t    |28.42/0.8101  \t    |**32.26/0.8955**       |\n|Set14  \t    |2x         |30.22/0.8683  \t    |**33.73/0.9186**  \t    |\n|Set14  \t    |3x         |27.53/0.7737  \t    |**31.06/0.8588**  \t    |\n|Set14  \t    |4x         |25.99/0.7023  \t    |**28.78/0.7859**  \t    |\n|BSD100  \t    |2x  \t    |29.55/0.8425  \t    |**33.78/0.9253**  \t    |\n|BSD100  \t    |3x  \t    |27.20/0.7382  \t    |**29.65/0.8196**  \t    |\n|BSD100  \t    |4x  \t    |25.96/0.6672  \t    |**28.51/0.7651**  \t    |\n|Urban100  \t    |2x  \t    |26.66/0.8408  \t    |**32.08/0.9276**  \t    |\n|Urban100  \t    |3x  \t    |  \t                |**29.26/0.8736**  \t    |\n|Urban100  \t    |4x  \t    |23.14/0.6573  \t    |**26.10/0.7857**  \t    |\n\n![Comparing Bicubic upscaling against the models x4 upscaling on Set5 Image 2](images/msrn_2_4_compare.png \"Comparing Bicubic upscaling against the models x4 upscaling on Set5 Image 2\")\n\nYou can find a notebook to easily run evaluation on pretrained models below:\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eugenesiow/super-image-notebooks/blob/master/notebooks/Evaluate_Pretrained_super_image_Models.ipynb \"Open in Colab\")\n created at: 2022-03-02\n- model_name: ZhengPeng7/BiRefNet_HR-matting\n  description: this birefnet was trained with images in `2048x2048` for higher resolution image matting with transparency. all tested in fp16 mode. dataset method resolution maxfm wfmeasure mae smeasure meanem hce maxem meanfm adpem adpfm mba maxbiou meanbiou\n created at: 2025-02-12T16:19:23+00:00\n- model_name: PramaLLC/BEN2\n  description: ben2 background erase network introduces a novel approach to foreground segmentation through its innovative confidence guided matting cgm pipeline. the architecture employs a refiner network that targets and processes pixels where the base model exhibits lower confidence levels, resulting in more precise and reliable matting results. this model is built on ben: ben2 was trained on the dis5k and our 22k proprietary segmentation dataset. our enhanced model delivers superior performance in hair matting, 4k processing, object segmentation, and edge refinement. our base model is open source. to try the full model through our free web demo or integrate ben2 into your project with our api: - üåê\n created at: 2025-01-22T14:39:05+00:00\n- model_name: ZhengPeng7/BiRefNet_HR\n  description: this birefnet was trained with images in `2048x2048` for higher resolution inference. all tested in fp16 mode. dataset method resolution maxfm wfmeasure mae smeasure meanem hce maxem meanfm adpem adpfm mba maxbiou meanbiou\n created at: 2025-02-01T17:52:14+00:00\n- model_name: briaai/RMBG-2.0\n  description: rmbg v2.0 is our new state-of-the-art background removal model significantly improves rmbg v1.4. the model is designed to effectively separate foreground from background in a range of categories and image types. this model has been trained on a carefully selected dataset, which includes: general stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale. - developed by:  - model type: background removal - license:  - the model is released under a cc by-nc 4.0 license for non-commercial use. - commercial use is subject to a commercial agreement with bria. available  purchase: to purchase a commercial license simply click . - model description: bria rmbg-2.0 is a dichotomous image segmentation model trained exclusively on a professional-grade dataset. the model output includes a single-channel 8-bit grayscale alpha matte, where each pixel value indicates the opacity level of the corresponding pixel in the original image. this non-binary output approach offers developers the flexibility to define custom thresholds for foreground-background separation, catering to varied use cases requirements and enhancing integration into complex pipelines. - bria: resources for more information:\n created at: 2024-10-29T12:14:36+00:00\n- model_name: ZhengPeng7/BiRefNet_dynamic\n  description: bilateral reference for high-resolution dichotomous image segmentation an arbitrary shape adaptable birefnet for general segmentation. this model was trained on arbitrary shapes 256x256 2304x2304 and shows great robustness on inputs with any shape.\n created at: 2025-03-31T03:37:03+00:00\n- model_name: Ricky06662/Seg-Zero-7B\n  description: this model is based on the paper . it uses a decoupled architecture with a reasoning model and a segmentation model. it's trained via reinforcement learning using grpo without explicit reasoning data, leading to robust zero-shot generalization and emergent test-time reasoning. code: this is a seg-zero-7b model. it introduces a decoupled architecture consisting of a reasoning model and a segmentation model. the reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate pixel-level masks. this is a seg-zero-7b model. it introduces a decoupled architecture consisting of a reasoning model and a segmentation model. the reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate pixel-level masks.\n created at: 2025-03-07T15:08:24+00:00\n- model_name: cocktailpeanut/rm\n  description: rmbg v2.0 is our new state-of-the-art background removal model significantly improves rmbg v1.4. the model is designed to effectively separate foreground from background in a range of categories and image types. this model has been trained on a carefully selected dataset, which includes: general stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale. - developed by:  - model type: background removal - license:  - the model is released under a cc by-nc 4.0 license for non-commercial use. - commercial use is subject to a commercial agreement with bria. available  purchase: to purchase a commercial license simply click . - model description: bria rmbg-2.0 is a dichotomous image segmentation model trained exclusively on a professional-grade dataset. - bria: resources for more information:\n created at: 2025-04-12T06:16:07+00:00\n- model_name: yuvraj108c/RMBG-2.0\n  description: rmbg v2.0 is our new state-of-the-art background removal model significantly improves rmbg v1.4. the model is designed to effectively separate foreground from background in a range of categories and image types. this model has been trained on a carefully selected dataset, which includes: general stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale. - developed by:  - model type: background removal - license:  - the model is released under a cc by-nc 4.0 license for non-commercial use. - commercial use is subject to a commercial agreement with bria. available  purchase: to purchase a commercial license simply click . - model description: bria rmbg-2.0 is a dichotomous image segmentation model trained exclusively on a professional-grade dataset. the model output includes a single-channel 8-bit grayscale alpha matte, where each pixel value indicates the opacity level of the corresponding pixel in the original image. this non-binary output approach offers developers the flexibility to define custom thresholds for foreground-background separation, catering to varied use cases requirements and enhancing integration into complex pipelines. - bria: resources for more information:\n created at: 2025-07-28T10:18:49+00:00\n- model_name: Ricky06662/VisionReasoner-7B\n  description: this repository contains the visionreasoner-7b model, developed as part of the novel seg-zero framework, presented in the paper . this model is also associated with the paper . code: project page:  seg-zero is a novel framework that demonstrates remarkable generalizability and derives explicit chain-of-thought reasoning through cognitive reinforcement for reasoning segmentation. this visionreasoner-7b model employs a decoupled architecture consisting of a reasoning model and a segmentation model. the reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate precise pixel-level masks. trained exclusively via reinforcement learning with grpo and without explicit reasoning data, seg-zero achieves robust zero-shot generalization and exhibits emergent test-time reasoning capabilities. experiments show that seg-zero-7b achieves a zero-shot performance of 57.5 on the reasonseg benchmark, surpassing the prior lisa-7b by 18%. this significant improvement highlights seg-zero's ability to generalize across domains while presenting an explicit reasoning process.\n created at: 2025-05-18T05:46:01+00:00\n- model_name: tue-mps/coco_panoptic_eomt_large_640\n  description: eomt encoder-only mask transformer is a vision transformer vit architecture designed for high-quality and efficient image segmentation. it was introduced in the cvpr 2025 highlight paper: by tommie kerssies, niccol√≤ cavagnero, alexander hermans, narges norouzi, giuseppe averta, bastian leibe, gijs dubbelman, and daan de geus.\n created at: 2025-03-26T17:28:05+00:00\n- model_name: tue-mps/ade20k_semantic_eomt_large_512\n  description: eomt encoder-only mask transformer is a vision transformer vit architecture designed for high-quality and efficient image segmentation. it was introduced in the cvpr 2025 highlight paper: by tommie kerssies, niccol√≤ cavagnero, alexander hermans, narges norouzi, giuseppe averta, bastian leibe, gijs dubbelman, and daan de geus.\n created at: 2025-03-26T19:29:47+00:00\n- model_name: bwittmann/vesselFM\n  description: tl;dr : vesselfm is a foundation model for universal 3d blood vessel segmentation in arbitrary imaging domains. for details, please refer to our preprint and our github repo\n created at: 2024-12-02T12:45:08+00:00\n- model_name: Ricky06662/Seg-Zero-7B-Best-on-ReasonSegTest\n  description: this model is based on the paper . code: seg-zero introduces a novel framework for reasoning segmentation that addresses the limitations of traditional supervised fine-tuning methods, which often struggle with out-of-domain generalization and lack explicit reasoning processes. the framework features a decoupled architecture consisting of a reasoning model and a segmentation model. the reasoning model interprets user intentions, generates explicit reasoning chains, and produces positional prompts, which are subsequently used by the segmentation model to generate precise pixel-level masks.\n created at: 2025-04-09T16:12:22+00:00\n- model_name: tue-mps/coco_instance_eomt_large_640\n  description: eomt encoder-only mask transformer is a vision transformer vit architecture designed for high-quality and efficient image segmentation. it was introduced in the cvpr 2025 highlight paper: by tommie kerssies, niccol√≤ cavagnero, alexander hermans, narges norouzi, giuseppe averta, bastian leibe, gijs dubbelman, and daan de geus.\n created at: 2025-03-26T17:16:21+00:00\n- model_name: facebook/sapiens-seg-1b-torchscript\n  description: sapiens is a family of vision transformers pretrained on 300 million human images at 1024 x 1024 image resolution. the pretrained models, when finetuned for human-centric vision tasks, generalize to in-the-wild conditions. sapiens-1b natively support 1k high-resolution inference. the resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. - developed by: meta\n created at: 2024-09-09T20:26:24+00:00\n- model_name: tue-mps/coco_instance_eomt_large_1280\n  description: eomt encoder-only mask transformer is a vision transformer vit architecture designed for high-quality and efficient image segmentation. it was introduced in the cvpr 2025 highlight paper: by tommie kerssies, niccol√≤ cavagnero, alexander hermans, narges norouzi, giuseppe averta, bastian leibe, gijs dubbelman, and daan de geus.\n created at: 2025-03-26T19:34:47+00:00\n- model_name: nevernever69/dit-doclaynet-segmentation\n  description: this model is a fine-tuned version of for document layout semantic segmentation on the dataset small subset: `nevernever69/small-doclaynet-v1.1`. it segments scanned document images into 11 layout categories such as title, paragraph, table, and footer. - segment document images into structured layout elements - assist in downstream tasks like document ocr, archiving, and automatic annotation\n created at: 2025-03-12T11:45:04+00:00\n- model_name: Pushti/BirefNetHR\n  description: this birefnet was trained with images in `2048x2048` for higher resolution inference. all tested in fp16 mode. dataset method resolution maxfm wfmeasure mae smeasure meanem hce maxem meanfm adpem adpfm mba maxbiou meanbiou\n created at: 2025-04-18T07:28:32+00:00\n- model_name: ZQL9711/RMBG-2-Matting\n  description: rmbg v2.0 is our new state-of-the-art background removal model significantly improves rmbg v1.4. the model is designed to effectively separate foreground from background in a range of categories and image types. this model has been trained on a carefully selected dataset, which includes: general stock images, e-commerce, gaming, and advertising content, making it suitable for commercial use cases powering enterprise content creation at scale. - developed by:  - model type: background removal - license:  - the model is released under a cc by-nc 4.0 license for non-commercial use. - commercial use is subject to a commercial agreement with bria. available  purchase: to purchase a commercial license simply click . - model description: bria rmbg-2.0 is a dichotomous image segmentation model trained exclusively on a professional-grade dataset. - bria: resources for more information:\n created at: 2025-07-05T08:31:53+00:00\n- model_name: Acly/BiRefNet-GGUF\n  description: birefnet is a model for dichotomous image segmentation background removal. the weights in this repository are converted for lightweight inference on consumer hardware with .\n created at: 2025-07-20T10:33:31+00:00\n\nExamples:\n- example user query: Our company is building a software to analyze satellite images for land use. We need to segment different regions in the images.\n  model_name: nvidia/segformer-b0-finetuned-ade-512-512\n- example user query: Identify an API to perform semantic segmentation of the street labeled with road, sidewalk, and building for a given picture.\n  model_name: pytorch/vision/deeplabv3_resnet50\n- example user query: A drone is flying over the neighborhood and is recording video. How can I process the video and perform semantic segmentation on each frame?\n  model_name: pytorch/vision/deeplabv3_resnet101\n- example user query: To identify and separate different objects in an image, create an image segmentation model.\n  model_name: openmmlab/upernet-convnext-small\n- example user query: The company is developing a fashion application which needs to identify the clothing items in pictures.\n  model_name: mattmdjaga/segformer_b2_clothes\n- example user query: An urban development company wants to analyze city images to inform infrastructure plans. Extract features using a semantic segmentation model.\n  model_name: nvidia/segformer-b5-finetuned-cityscapes-1024-1024\n\nUser prompt:\nClassify every object in the portrayal.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n"
  ],
  "<image_sr>": [
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_sr>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: AlekseyCalvin/Flux_Kontext_Dev_fp8_scaled_diffusers\n  description: 'flux.1 kontext dev' is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. it was developed by . for more info, read bfl's and !. 1. change existing images based on an edit instruction. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-27T00:38:23+00:00\n\nExamples:\n- example user query: Our client wants to upscale images in a coffee shop application.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n- example user query: Our client is a photographer who needs a tool to upscale a low resolution photo they took.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n- example user query: A company is developing an app for enhancing the quality of low-resolution images. For this purpose, they need to upscale the input images by a factor of 4.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n- example user query: A user wants to upscale a low-resolution photo to a 4x higher resolution. Suggest a model to achieve this task.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n- example user query: I am a graphic designer, I want to upscale a low resolution image to have better quality and higher resolution.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n\nUser prompt:\nAmplify the image details of the layered elements of the artwork.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_sr>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: AlekseyCalvin/Flux_Kontext_Dev_fp8_scaled_diffusers\n  description: 'flux.1 kontext dev' is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. it was developed by . for more info, read bfl's and !. 1. change existing images based on an edit instruction. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-27T00:38:23+00:00\n\nExamples:\n- example user query: Our client wants to upscale images in a coffee shop application.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n- example user query: Our client is a photographer who needs a tool to upscale a low resolution photo they took.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n- example user query: A company is developing an app for enhancing the quality of low-resolution images. For this purpose, they need to upscale the input images by a factor of 4.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n- example user query: A user wants to upscale a low-resolution photo to a 4x higher resolution. Suggest a model to achieve this task.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n- example user query: I am a graphic designer, I want to upscale a low resolution image to have better quality and higher resolution.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n\nUser prompt:\nI uphold you can cultivate quality of the conceptual artwork.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_sr>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: AlekseyCalvin/Flux_Kontext_Dev_fp8_scaled_diffusers\n  description: 'flux.1 kontext dev' is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. it was developed by . for more info, read bfl's and !. 1. change existing images based on an edit instruction. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-27T00:38:23+00:00\n\nExamples:\n- example user query: Our client wants to upscale images in a coffee shop application.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n- example user query: Our client is a photographer who needs a tool to upscale a low resolution photo they took.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n- example user query: A company is developing an app for enhancing the quality of low-resolution images. For this purpose, they need to upscale the input images by a factor of 4.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n- example user query: A user wants to upscale a low-resolution photo to a 4x higher resolution. Suggest a model to achieve this task.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n- example user query: I am a graphic designer, I want to upscale a low resolution image to have better quality and higher resolution.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n\nUser prompt:\nBoost the image resolution of the overlapping features of the capture.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_sr>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: AlekseyCalvin/Flux_Kontext_Dev_fp8_scaled_diffusers\n  description: 'flux.1 kontext dev' is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. it was developed by . for more info, read bfl's and !. 1. change existing images based on an edit instruction. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-27T00:38:23+00:00\n\nExamples:\n- example user query: Our client wants to upscale images in a coffee shop application.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n- example user query: Our client is a photographer who needs a tool to upscale a low resolution photo they took.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n- example user query: A company is developing an app for enhancing the quality of low-resolution images. For this purpose, they need to upscale the input images by a factor of 4.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n- example user query: A user wants to upscale a low-resolution photo to a 4x higher resolution. Suggest a model to achieve this task.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n- example user query: I am a graphic designer, I want to upscale a low resolution image to have better quality and higher resolution.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n\nUser prompt:\nProgress the image resolution of the prominent features of the visualization.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_sr>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: AlekseyCalvin/Flux_Kontext_Dev_fp8_scaled_diffusers\n  description: 'flux.1 kontext dev' is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. it was developed by . for more info, read bfl's and !. 1. change existing images based on an edit instruction. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-27T00:38:23+00:00\n\nExamples:\n- example user query: Our client wants to upscale images in a coffee shop application.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n- example user query: Our client is a photographer who needs a tool to upscale a low resolution photo they took.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n- example user query: A company is developing an app for enhancing the quality of low-resolution images. For this purpose, they need to upscale the input images by a factor of 4.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n- example user query: A user wants to upscale a low-resolution photo to a 4x higher resolution. Suggest a model to achieve this task.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n- example user query: I am a graphic designer, I want to upscale a low resolution image to have better quality and higher resolution.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n\nUser prompt:\nI implore you to intensify the image details of the intricate details of the photo.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_sr>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: AlekseyCalvin/Flux_Kontext_Dev_fp8_scaled_diffusers\n  description: 'flux.1 kontext dev' is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. it was developed by . for more info, read bfl's and !. 1. change existing images based on an edit instruction. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-27T00:38:23+00:00\n\nExamples:\n- example user query: Our client wants to upscale images in a coffee shop application.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n- example user query: Our client is a photographer who needs a tool to upscale a low resolution photo they took.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n- example user query: A company is developing an app for enhancing the quality of low-resolution images. For this purpose, they need to upscale the input images by a factor of 4.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n- example user query: A user wants to upscale a low-resolution photo to a 4x higher resolution. Suggest a model to achieve this task.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n- example user query: I am a graphic designer, I want to upscale a low resolution image to have better quality and higher resolution.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n\nUser prompt:\nI implore you to progress quality of the candid scene.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_sr>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: AlekseyCalvin/Flux_Kontext_Dev_fp8_scaled_diffusers\n  description: 'flux.1 kontext dev' is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. it was developed by . for more info, read bfl's and !. 1. change existing images based on an edit instruction. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-27T00:38:23+00:00\n\nExamples:\n- example user query: Our client wants to upscale images in a coffee shop application.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n- example user query: Our client is a photographer who needs a tool to upscale a low resolution photo they took.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n- example user query: A company is developing an app for enhancing the quality of low-resolution images. For this purpose, they need to upscale the input images by a factor of 4.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n- example user query: A user wants to upscale a low-resolution photo to a 4x higher resolution. Suggest a model to achieve this task.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n- example user query: I am a graphic designer, I want to upscale a low resolution image to have better quality and higher resolution.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n\nUser prompt:\nImprove the image definition of the complex textures of the visual.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_sr>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: AlekseyCalvin/Flux_Kontext_Dev_fp8_scaled_diffusers\n  description: 'flux.1 kontext dev' is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. it was developed by . for more info, read bfl's and !. 1. change existing images based on an edit instruction. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-27T00:38:23+00:00\n\nExamples:\n- example user query: Our client wants to upscale images in a coffee shop application.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n- example user query: Our client is a photographer who needs a tool to upscale a low resolution photo they took.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n- example user query: A company is developing an app for enhancing the quality of low-resolution images. For this purpose, they need to upscale the input images by a factor of 4.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n- example user query: A user wants to upscale a low-resolution photo to a 4x higher resolution. Suggest a model to achieve this task.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n- example user query: I am a graphic designer, I want to upscale a low resolution image to have better quality and higher resolution.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n\nUser prompt:\nProgress quality of the prominent features.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_sr>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: AlekseyCalvin/Flux_Kontext_Dev_fp8_scaled_diffusers\n  description: 'flux.1 kontext dev' is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. it was developed by . for more info, read bfl's and !. 1. change existing images based on an edit instruction. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-27T00:38:23+00:00\n\nExamples:\n- example user query: Our client wants to upscale images in a coffee shop application.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n- example user query: Our client is a photographer who needs a tool to upscale a low resolution photo they took.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n- example user query: A company is developing an app for enhancing the quality of low-resolution images. For this purpose, they need to upscale the input images by a factor of 4.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n- example user query: A user wants to upscale a low-resolution photo to a 4x higher resolution. Suggest a model to achieve this task.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n- example user query: I am a graphic designer, I want to upscale a low resolution image to have better quality and higher resolution.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n\nUser prompt:\nBoost quality of the outdoor scene.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <image_sr>\n\nCandidate Models:\n- model_name: lambdalabs/sd-image-variations-diffusers\n  description: This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.\n created at: 2022-09-09\n- model_name: lllyasviel/sd-controlnet-canny\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-seg\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-hed\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-depth\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/sd-controlnet-mlsd\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on M-LSD straight line detection. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_canny\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-scribble\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_lineart\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.\n created at: 2023-04-14\n- model_name: lllyasviel/sd-controlnet-normal\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.\n created at: 2023-02-24\n- model_name: lllyasviel/control_v11p_sd15_scribble\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: caidas/swin2sr-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository.\n created at: 2022-12-16\n- model_name: lllyasviel/control_v11p_sd15_openpose\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11e_sd15_ip2p\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.\n created at: 2023-04-14\n- model_name: caidas/swin2SR-classical-sr-x4-64\n  description: Swin2SR model that upscales images x4. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for image super resolution.\n created at: 2022-12-16\n- model_name: GreeneryScenery/SheepsControlV3\n  description: GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.\n created at: 2023-04-07\n- model_name: lllyasviel/control_v11p_sd15_seg\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_mlsd\n  description: Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_normalbae\n  description: ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15_softedge\n  description: Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.\n created at: 2023-04-14\n- model_name: GreeneryScenery/SheepsControlV5\n  description: SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.\n created at: 2023-04-14\n- model_name: google/maxim-s3-deblurring-gopro\n  description: MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.\n created at: 2022-10-18\n- model_name: lllyasviel/control_v11p_sd15_inpaint\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.\n created at: 2023-04-14\n- model_name: lllyasviel/control_v11p_sd15s2_lineart_anime\n  description: ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.\n created at: 2023-04-14\n- model_name: openai/shap-e-img2img\n  description: Shap-E introduces a diffusion process that can generate a 3D image from a text prompt. It was introduced in [Shap-E: Generating Conditional 3D Implicit Functions](https://arxiv.org/abs/2305.02463) by Heewoo Jun and Alex Nichol from OpenAI. \n\nOriginal repository of Shap-E can be found here: https://github.com/openai/shap-e. \n\n_The authors of Shap-E didn't author this model card. They provide a separate model card [here](https://github.com/openai/shap-e/blob/main/model-card.md)._\n created at: 2023-07-04\n- model_name: DionTimmer/controlnet_qrcode-control_v1p_sd15\n  description: ![1](https://www.dropbox.com/s/fxyuqpot2z2ftty/5.png?raw=1)\n created at: 2023-06-15\n- model_name: lllyasviel/control_v11f1e_sd15_tile\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1e_sd15_tile.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **tiled image**. Conceptually, it is similar to a super-resolution model, but its usage is not limited to that. It is also possible to generate details at the same size as the input (conditione) image.\n\n**This model was contributed by [*takuma104*](https://huggingface.co/takuma104)**\n created at: 2023-05-04\n- model_name: mfidabel/controlnet-segment-anything\n  description: These are controlnet weights trained on runwayml/stable-diffusion-v1-5 with a new type of conditioning. You can find some example images in the following. \n\n**prompt**: contemporary living room of a house\n\n**negative prompt**: low quality\n![images_0)](./images_0.png)\n\n**prompt**: new york buildings,  Vincent Van Gogh starry night \n\n**negative prompt**: low quality, monochrome\n![images_1)](./images_1.png)\n\n**prompt**: contemporary living room,  high quality, 4k, realistic\n\n**negative prompt**: low quality, monochrome, low res\n![images_2)](./images_2.png)\n created at: 2023-05-01\n- model_name: ioclab/control_v1p_sd15_brightness\n  description: This model brings brightness control to Stable Diffusion, allowing users to colorize grayscale images or recolor generated images.\n created at: 2023-04-19\n- model_name: lllyasviel/control_v11f1p_sd15_depth\n  description: **Controlnet v1.1** is the successor model of [Controlnet v1.0](https://huggingface.co/lllyasviel/ControlNet)\nand was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11f1p_sd15_depth.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **depth images**.\n created at: 2023-04-16\n- model_name: lllyasviel/control_v11e_sd15_shuffle\n  description: **Controlnet v1.1** was released in [lllyasviel/ControlNet-v1-1](https://huggingface.co/lllyasviel/ControlNet-v1-1) by [Lvmin Zhang](https://huggingface.co/lllyasviel).\n\nThis checkpoint is a conversion of [the original checkpoint](https://huggingface.co/lllyasviel/ControlNet-v1-1/blob/main/control_v11e_sd15_shuffle.pth) into `diffusers` format.\nIt can be used in combination with **Stable Diffusion**, such as [runwayml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5).\n\n\nFor more details, please also have a look at the [üß® Diffusers docs](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/controlnet).\n\n\nControlNet is a neural network structure to control diffusion models by adding extra conditions. \n\n![img](./sd.png)\n\nThis checkpoint corresponds to the ControlNet conditioned on **shuffle images**.\n created at: 2023-04-14\n- model_name: CrucibleAI/ControlNetMediaPipeFace\n  description: ## Table of Contents:\n- Overview: Samples, Contents, and Construction\n- Usage: Downloading, Training, and Inference\n- License\n- Credits and Thanks\n\n# Overview:\n\nThis dataset is designed to train a ControlNet with human facial expressions.  It includes keypoints for pupils to allow gaze direction.  Training has been tested on Stable Diffusion v2.1 base (512) and Stable Diffusion v1.5.\n created at: 2023-03-30\n- model_name: instruction-tuning-sd/low-level-img-proc\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-21\n- model_name: instruction-tuning-sd/cartoonizer\n  description: This pipeline is an 'instruction-tuned' version of [Stable Diffusion (v1.5)](https://huggingface.co/runwayml/stable-diffusion-v1-5). It was\nfine-tuned from the existing [InstructPix2Pix checkpoints](https://huggingface.co/timbrooks/instruct-pix2pix).\n created at: 2023-03-18\n- model_name: timbrooks/instruct-pix2pix\n  description: \n created at: 2023-01-20\n- model_name: caidas/swin2SR-classical-sr-x2-64\n  description: Swin2SR model that upscales images x2. It was introduced in the paper [Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration](https://arxiv.org/abs/2209.11345)\nby Conde et al. and first released in [this repository](https://github.com/mv-lab/swin2sr). \n\n# Intended use cases\n\nThis model is intended for image super resolution.\n\n# Usage\n\nRefer to the [documentation](https://huggingface.co/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRForImageSuperResolution.forward.example).\n created at: 2022-12-16\n- model_name: AlekseyCalvin/Flux_Kontext_Dev_fp8_scaled_diffusers\n  description: 'flux.1 kontext dev' is a 12 billion parameter rectified flow transformer capable of editing images based on text instructions. it was developed by . for more info, read bfl's and !. 1. change existing images based on an edit instruction. 1. change existing images based on an edit instruction. 2. have character, style and object reference without any finetuning. 3. robust consistency allows users to refine an image through multiple successive edits with minimal visual drift. 4. trained using guidance distillation, making `flux.1 kontext dev` more efficient. 5. open weights to drive new scientific research, and empower artists to develop innovative workflows. 6. generated outputs can be used for personal, scientific, and commercial purposes, as described in the flux.1 \\dev\\ non-commercial license\n created at: 2025-06-27T00:38:23+00:00\n\nExamples:\n- example user query: Our client wants to upscale images in a coffee shop application.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n- example user query: Our client is a photographer who needs a tool to upscale a low resolution photo they took.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n- example user query: A company is developing an app for enhancing the quality of low-resolution images. For this purpose, they need to upscale the input images by a factor of 4.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n- example user query: A user wants to upscale a low-resolution photo to a 4x higher resolution. Suggest a model to achieve this task.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n- example user query: I am a graphic designer, I want to upscale a low resolution image to have better quality and higher resolution.\n  model_name: caidas/swin2SR-classical-sr-x4-64\n\nUser prompt:\nAdvance the image quality of the overlapping features of the photograph.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n"
  ],
  "<video_gen>": [
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <video_gen>\n\nCandidate Models:\n- model_name: camenduru/text2-video-zero\n  description: This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.\n created at: 2023-03-29\n- model_name: chavinlo/TempoFunk\n  description: A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text.\n created at: 2023-02-07\n- model_name: damo-vilab/text-to-video-ms-1.7b\n  description: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. The overall model parameters are about 1.7 billion. Currently, it only supports English input.\n created at: 2023-03-22\n- model_name: nitrosocke/redshift-diffusion\n  description: Tune-A-Video - Redshift is a text-to-video generation model based on the nitrosocke/redshift-diffusion model. It generates videos based on textual prompts, such as 'a man is skiing' or '(redshift style) spider man is skiing'.\n created at: 2022-11-06\n- model_name: nitrosocke/mo-di-diffusion\n  description: Tune-A-Video is a text-to-video generation model based on the Hugging Face framework. The model generates videos based on textual prompts in a modern Disney style.\n created at: 2022-10-27\n- model_name: ImRma/Brucelee\n  description: A Hugging Face model for converting Persian and English text into video.\n created at: 2023-04-23\n- model_name: damo-vilab/text-to-video-ms-1.7b-legacy\n  description: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.\n created at: 2023-03-22\n- model_name: duncan93/video\n  description: A text-to-video model trained on OpenAssistant/oasst1 dataset.\n created at: 2023-05-02\n- model_name: cerspense/zeroscope_v2_576w\n  description: zeroscope_v2_576w uses 7.9gb of vram when rendering 30 frames at 576x320\n\n### Using it with the 1111 text2video extension\n\n1. Download files in the zs2_576w folder.\n2. Replace the respective files in the 'stable-diffusion-webui\\models\\ModelScope\\t2v' directory.\n\n### Upscaling recommendations\n\nFor upscaling, it's recommended to use [zeroscope_v2_XL](https://huggingface.co/cerspense/zeroscope_v2_XL) via vid2vid in the 1111 extension. It works best at 1024x576 with a denoise strength between 0.66 and 0.85. Remember to use the same prompt that was used to generate the original clip. <br />\n\n### Usage in üß® Diffusers\n\nLet's first install the libraries required:\n\n```bash\n$ pip install diffusers transformers accelerate torch\n```\n\nNow, generate a video:\n\n```py\nimport torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_video\n\npipe = DiffusionPipeline.from_pretrained(\"cerspense/zeroscope_v2_576w\", torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n\nprompt = \"Darth Vader is surfing on waves\"\nvideo_frames = pipe(prompt, num_inference_steps=40, height=320, width=576, num_frames=24).frames\nvideo_path = export_to_video(video_frames)\n```\n\nHere are some results:\n\n<table>\n    <tr>\n        Darth vader is surfing on waves.\n        <br>\n        <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/darthvader_cerpense.gif\"\n            alt=\"Darth vader surfing in waves.\"\n            style=\"width: 576;\" />\n        </center></td>\n    </tr>\n</table>\n\n### Known issues\n\nLower resolutions or fewer frames could lead to suboptimal output. <br />\n\nThanks to [camenduru](https://github.com/camenduru), [kabachuha](https://github.com/kabachuha), [ExponentialML](https://github.com/ExponentialML), [dotsimulate](https://www.instagram.com/dotsimulate/), [VANYA](https://twitter.com/veryVANYA), [polyware](https://twitter.com/polyware_ai), [tin2tin](https://github.com/tin2tin)<br />\n created at: 2023-06-21\n- model_name: Searchium-ai/clip4clip-webvid150k\n  description: A CLIP4Clip video-text retrieval model trained on a subset of the WebVid dataset. \nThe model and training method are described in the paper [\"Clip4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval\"](https://arxiv.org/pdf/2104.08860.pdf) by Lou et el, and implemented in the accompanying [GitHub repository](https://github.com/ArrowLuo/CLIP4Clip).\n\nThe training process utilized the [WebVid Dataset](https://m-bain.github.io/webvid-dataset/), a comprehensive collection of short videos with corresponding textual descriptions sourced from the web. \nFor training purposes, a subset consisting of the first 150,000 video-text pairs from the dataset were used.\n\nThis HF model is based on the [clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32) architecture, with weights trained by Daphna Idelson at [Searchium](https://www.searchium.ai).\n\n\n# How to use\n### Extracting Text Embeddings:\n\n```python\nimport numpy as np\nimport torch\nfrom transformers import CLIPTokenizer, CLIPTextModelWithProjection\n\n\nsearch_sentence = \"a basketball player performing a slam dunk\"\n\nmodel = CLIPTextModelWithProjection.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\ntokenizer = CLIPTokenizer.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\n\ninputs = tokenizer(text=search_sentence , return_tensors=\"pt\")\noutputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n\n# Normalize embeddings for retrieval:\nfinal_output = outputs[0] / outputs[0].norm(dim=-1, keepdim=True)\nfinal_output = final_output.cpu().detach().numpy()\nprint(\"final output: \", final_output)\n```\n\n### Extracting Video Embeddings:\n\nAn additional notebook [\"GSI_VideoRetrieval_VideoEmbedding.ipynb\"](https://huggingface.co/Searchium-ai/clip4clip-webvid150k/blob/main/Notebooks/GSI_VideoRetrieval_VideoEmbedding.ipynb), provides instructions for extracting video embeddings and includes the necessary tools for preprocessing videos.\n created at: 2023-04-17\n- model_name: PAIR/text2video-zero-controlnet-canny-avatar\n  description: [Text2Video-Zero](https://arxiv.org/abs/2303.13439) is a zero-shot text to video generator. It can perform `zero-shot text-to-video generation`, `Video Instruct Pix2Pix` (instruction-guided video editing), \n`text and pose conditional video generation`, `text and canny-edge conditional video generation`, and \n`text, canny-edge and dreambooth conditional video generation`. For more information about this work, \nplease have a look at our [paper](https://arxiv.org/abs/2303.13439) and our demo: [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/PAIR/Text2Video-Zero)\nOur [code](https://github.com/Picsart-AI-Research/Text2Video-Zero) works with any StableDiffusion base model.\n\nThis model provides [DreamBooth](https://arxiv.org/abs/2208.12242) weights for the `Avatar style` to be used with edge guidance (using [ControlNet](https://arxiv.org/abs/2302.05543)) in text2video zero.\n created at: 2023-03-27\n- model_name: Wan-AI/Wan2.1-T2V-1.3B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-01T08:39:07+00:00\n- model_name: bullerwins/Wan2.2-T2V-A14B-GGUF\n  description: you need to download both a high-noise model and a low-noise model. high noise is used for the first steps and the low-noise for the details. place them on comfyui/models/unet you will need from  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T16:29:01+00:00\n- model_name: Wan-AI/Wan2.2-T2V-A14B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T09:04:28+00:00\n- model_name: genmo/mochi-1-preview\n  description: a state of the art video generation model by . mochi 1 preview is an open state-of-the-art video generation model with high-fidelity motion and strong prompt adherence in preliminary evaluation. this model dramatically closes the gap between closed and open video generation systems. we‚Äôre releasing the model under a permissive apache 2.0 license. try this model for free on .\n created at: 2024-10-22T07:36:03+00:00\n- model_name: Wan-AI/Wan2.1-T2V-14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-02-25T07:26:34+00:00\n- model_name: Wan-AI/Wan2.1-T2V-14B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-01T08:39:52+00:00\n- model_name: Wan-AI/Wan2.2-TI2V-5B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T09:03:29+00:00\n- model_name: Skywork/SkyReels-V2-DF-14B-720P\n  description: skyreels v2: infinite-length film generative model üìë technical report ¬∑ üëã playground ¬∑ üí¨ discord ¬∑ ü§ó hugging face ¬∑ ü§ñ modelscope ¬∑ üåê github\n created at: 2025-04-18T13:34:52+00:00\n- model_name: Wan-AI/Wan2.2-TI2V-5B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-18T09:35:44+00:00\n- model_name: Wan-AI/Wan2.2-T2V-A14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-24T15:07:15+00:00\n- model_name: magespace/Wan2.2-I2V-A14B-Lightning-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp üîó lightx2v distilled & optimized wan2.2 for fast, high-quality 480p / 720p image-to-video generation   wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-08-14T20:06:59+00:00\n- model_name: Wan-AI/Wan2.1-T2V-1.3B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-02-25T11:13:48+00:00\n- model_name: QuantStack/Wan2.1_T2V_14B_FusionX-GGUF\n  description: this is a gguf conversion of by . all quantized versions were created from the base t2v fp16 model using the conversion scripts provided by city96, available at the github repository. the model files can be used in with the custom node. place the required models in the following folders:\n created at: 2025-06-07T01:47:02+00:00\n- model_name: Lightricks/LTX-Video-ICLoRA-detailer-13b-0.9.8\n  description: this is a video detailer model on top of trained on custom data. ic lora is a method that enables adding video context into the video generation process. this approach allows for video-to-video control on top of the text-to-video model, providing more precise control over the generated content by conditioning the model on reference video frames during inference. comfy compatible model:\n created at: 2025-07-16T13:41:33+00:00\n- model_name: Lightricks/LTX-Video-0.9.7-distilled\n  description: this model card focuses on the model associated with the ltx-video model, codebase available . ltx-video is the first dit-based video generation model capable of generating high-quality videos in real-time. it produces 30 fps videos at a 1216√ó704 resolution faster than they can be watched. trained on a large-scale dataset of diverse videos, the model generates high-resolution videos with realistic and varied content. we provide a model for both text-to-video as well as image+text-to-video usecases\n created at: 2025-05-14T16:55:19+00:00\n- model_name: Lightricks/LTX-Video-0.9.5\n  description: this model card focuses on the model associated with the ltx-video model, codebase available . ltx-video is the first dit-based video generation model capable of generating high-quality videos in real-time. it produces 30 fps videos at a 1216√ó704 resolution faster than they can be watched. trained on a large-scale dataset of diverse videos, the model generates high-resolution videos with realistic and varied content. we provide a model for both text-to-video as well as image+text-to-video usecases\n created at: 2025-03-16T11:14:42+00:00\n- model_name: Lightricks/LTX-Video-0.9.7-dev\n  description: this model card focuses on the model associated with the ltx-video model, codebase available . ltx-video is the first dit-based video generation model capable of generating high-quality videos in real-time. it produces 30 fps videos at a 1216√ó704 resolution faster than they can be watched. trained on a large-scale dataset of diverse videos, the model generates high-resolution videos with realistic and varied content. we provide a model for both text-to-video as well as image+text-to-video usecases\n created at: 2025-05-13T12:02:56+00:00\n- model_name: tencent/HunyuanVideo\n  description: this repo contains pytorch model definitions, pre-trained weights and inference/sampling code for our paper exploring hunyuanvideo. you can find more visualizations on our .\n created at: 2024-12-01T06:00:34+00:00\n- model_name: Isi99999/Wan2.1-T2V-14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-02T17:37:57+00:00\n- model_name: Skywork/SkyReels-V2-T2V-14B-720P\n  description: skyreels v2: infinite-length film generative model üìë technical report ¬∑ üëã playground ¬∑ üí¨ discord ¬∑ ü§ó hugging face ¬∑ ü§ñ modelscope ¬∑ üåê github\n created at: 2025-04-18T13:55:51+00:00\n\nExamples:\n- example user query: I am directing a movie and I want to generate a video scene where Spiderman is surfing.\n  model_name: damo-vilab/text-to-video-ms-1.7b-legacy\n- example user query: Describe the process of generating a video clip with a scene where a dog is playing with a ball in a park.\n  model_name: camenduru/text2-video-zero\n- example user query: The marketing team needs a tool to transform textual descriptions of their products into promotional videos. Build a model that can create videos from text.\n  model_name: chavinlo/TempoFunk\n- example user query: I am writing a script for an educational video on biology. Can you generate a video using the text of my script?\n  model_name: duncan93/video\n- example user query: I want to convey the story of a bear enjoying a beautiful sunset in the forest through an AI-generated video.\n  model_name: nitrosocke/redshift-diffusion\n- example user query: We are presenting a new tool that can convert English text into videos which will have Bruce Lee showcase the given text.\n  model_name: ImRma/Brucelee\n\nUser prompt:\nBring to life a video depicting How Long Do Guinea Pigs Live? A Guide To Guinea Pig Lifespan.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <video_gen>\n\nCandidate Models:\n- model_name: camenduru/text2-video-zero\n  description: This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.\n created at: 2023-03-29\n- model_name: chavinlo/TempoFunk\n  description: A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text.\n created at: 2023-02-07\n- model_name: damo-vilab/text-to-video-ms-1.7b\n  description: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. The overall model parameters are about 1.7 billion. Currently, it only supports English input.\n created at: 2023-03-22\n- model_name: nitrosocke/redshift-diffusion\n  description: Tune-A-Video - Redshift is a text-to-video generation model based on the nitrosocke/redshift-diffusion model. It generates videos based on textual prompts, such as 'a man is skiing' or '(redshift style) spider man is skiing'.\n created at: 2022-11-06\n- model_name: nitrosocke/mo-di-diffusion\n  description: Tune-A-Video is a text-to-video generation model based on the Hugging Face framework. The model generates videos based on textual prompts in a modern Disney style.\n created at: 2022-10-27\n- model_name: ImRma/Brucelee\n  description: A Hugging Face model for converting Persian and English text into video.\n created at: 2023-04-23\n- model_name: damo-vilab/text-to-video-ms-1.7b-legacy\n  description: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.\n created at: 2023-03-22\n- model_name: duncan93/video\n  description: A text-to-video model trained on OpenAssistant/oasst1 dataset.\n created at: 2023-05-02\n- model_name: cerspense/zeroscope_v2_576w\n  description: zeroscope_v2_576w uses 7.9gb of vram when rendering 30 frames at 576x320\n\n### Using it with the 1111 text2video extension\n\n1. Download files in the zs2_576w folder.\n2. Replace the respective files in the 'stable-diffusion-webui\\models\\ModelScope\\t2v' directory.\n\n### Upscaling recommendations\n\nFor upscaling, it's recommended to use [zeroscope_v2_XL](https://huggingface.co/cerspense/zeroscope_v2_XL) via vid2vid in the 1111 extension. It works best at 1024x576 with a denoise strength between 0.66 and 0.85. Remember to use the same prompt that was used to generate the original clip. <br />\n\n### Usage in üß® Diffusers\n\nLet's first install the libraries required:\n\n```bash\n$ pip install diffusers transformers accelerate torch\n```\n\nNow, generate a video:\n\n```py\nimport torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_video\n\npipe = DiffusionPipeline.from_pretrained(\"cerspense/zeroscope_v2_576w\", torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n\nprompt = \"Darth Vader is surfing on waves\"\nvideo_frames = pipe(prompt, num_inference_steps=40, height=320, width=576, num_frames=24).frames\nvideo_path = export_to_video(video_frames)\n```\n\nHere are some results:\n\n<table>\n    <tr>\n        Darth vader is surfing on waves.\n        <br>\n        <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/darthvader_cerpense.gif\"\n            alt=\"Darth vader surfing in waves.\"\n            style=\"width: 576;\" />\n        </center></td>\n    </tr>\n</table>\n\n### Known issues\n\nLower resolutions or fewer frames could lead to suboptimal output. <br />\n\nThanks to [camenduru](https://github.com/camenduru), [kabachuha](https://github.com/kabachuha), [ExponentialML](https://github.com/ExponentialML), [dotsimulate](https://www.instagram.com/dotsimulate/), [VANYA](https://twitter.com/veryVANYA), [polyware](https://twitter.com/polyware_ai), [tin2tin](https://github.com/tin2tin)<br />\n created at: 2023-06-21\n- model_name: Searchium-ai/clip4clip-webvid150k\n  description: A CLIP4Clip video-text retrieval model trained on a subset of the WebVid dataset. \nThe model and training method are described in the paper [\"Clip4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval\"](https://arxiv.org/pdf/2104.08860.pdf) by Lou et el, and implemented in the accompanying [GitHub repository](https://github.com/ArrowLuo/CLIP4Clip).\n\nThe training process utilized the [WebVid Dataset](https://m-bain.github.io/webvid-dataset/), a comprehensive collection of short videos with corresponding textual descriptions sourced from the web. \nFor training purposes, a subset consisting of the first 150,000 video-text pairs from the dataset were used.\n\nThis HF model is based on the [clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32) architecture, with weights trained by Daphna Idelson at [Searchium](https://www.searchium.ai).\n\n\n# How to use\n### Extracting Text Embeddings:\n\n```python\nimport numpy as np\nimport torch\nfrom transformers import CLIPTokenizer, CLIPTextModelWithProjection\n\n\nsearch_sentence = \"a basketball player performing a slam dunk\"\n\nmodel = CLIPTextModelWithProjection.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\ntokenizer = CLIPTokenizer.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\n\ninputs = tokenizer(text=search_sentence , return_tensors=\"pt\")\noutputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n\n# Normalize embeddings for retrieval:\nfinal_output = outputs[0] / outputs[0].norm(dim=-1, keepdim=True)\nfinal_output = final_output.cpu().detach().numpy()\nprint(\"final output: \", final_output)\n```\n\n### Extracting Video Embeddings:\n\nAn additional notebook [\"GSI_VideoRetrieval_VideoEmbedding.ipynb\"](https://huggingface.co/Searchium-ai/clip4clip-webvid150k/blob/main/Notebooks/GSI_VideoRetrieval_VideoEmbedding.ipynb), provides instructions for extracting video embeddings and includes the necessary tools for preprocessing videos.\n created at: 2023-04-17\n- model_name: PAIR/text2video-zero-controlnet-canny-avatar\n  description: [Text2Video-Zero](https://arxiv.org/abs/2303.13439) is a zero-shot text to video generator. It can perform `zero-shot text-to-video generation`, `Video Instruct Pix2Pix` (instruction-guided video editing), \n`text and pose conditional video generation`, `text and canny-edge conditional video generation`, and \n`text, canny-edge and dreambooth conditional video generation`. For more information about this work, \nplease have a look at our [paper](https://arxiv.org/abs/2303.13439) and our demo: [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/PAIR/Text2Video-Zero)\nOur [code](https://github.com/Picsart-AI-Research/Text2Video-Zero) works with any StableDiffusion base model.\n\nThis model provides [DreamBooth](https://arxiv.org/abs/2208.12242) weights for the `Avatar style` to be used with edge guidance (using [ControlNet](https://arxiv.org/abs/2302.05543)) in text2video zero.\n created at: 2023-03-27\n- model_name: Wan-AI/Wan2.1-T2V-1.3B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-01T08:39:07+00:00\n- model_name: bullerwins/Wan2.2-T2V-A14B-GGUF\n  description: you need to download both a high-noise model and a low-noise model. high noise is used for the first steps and the low-noise for the details. place them on comfyui/models/unet you will need from  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T16:29:01+00:00\n- model_name: Wan-AI/Wan2.2-T2V-A14B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T09:04:28+00:00\n- model_name: genmo/mochi-1-preview\n  description: a state of the art video generation model by . mochi 1 preview is an open state-of-the-art video generation model with high-fidelity motion and strong prompt adherence in preliminary evaluation. this model dramatically closes the gap between closed and open video generation systems. we‚Äôre releasing the model under a permissive apache 2.0 license. try this model for free on .\n created at: 2024-10-22T07:36:03+00:00\n- model_name: Wan-AI/Wan2.1-T2V-14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-02-25T07:26:34+00:00\n- model_name: Wan-AI/Wan2.1-T2V-14B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-01T08:39:52+00:00\n- model_name: Wan-AI/Wan2.2-TI2V-5B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T09:03:29+00:00\n- model_name: Skywork/SkyReels-V2-DF-14B-720P\n  description: skyreels v2: infinite-length film generative model üìë technical report ¬∑ üëã playground ¬∑ üí¨ discord ¬∑ ü§ó hugging face ¬∑ ü§ñ modelscope ¬∑ üåê github\n created at: 2025-04-18T13:34:52+00:00\n- model_name: Wan-AI/Wan2.2-TI2V-5B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-18T09:35:44+00:00\n- model_name: Wan-AI/Wan2.2-T2V-A14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-24T15:07:15+00:00\n- model_name: magespace/Wan2.2-I2V-A14B-Lightning-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp üîó lightx2v distilled & optimized wan2.2 for fast, high-quality 480p / 720p image-to-video generation   wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-08-14T20:06:59+00:00\n- model_name: Wan-AI/Wan2.1-T2V-1.3B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-02-25T11:13:48+00:00\n- model_name: QuantStack/Wan2.1_T2V_14B_FusionX-GGUF\n  description: this is a gguf conversion of by . all quantized versions were created from the base t2v fp16 model using the conversion scripts provided by city96, available at the github repository. the model files can be used in with the custom node. place the required models in the following folders:\n created at: 2025-06-07T01:47:02+00:00\n- model_name: Lightricks/LTX-Video-ICLoRA-detailer-13b-0.9.8\n  description: this is a video detailer model on top of trained on custom data. ic lora is a method that enables adding video context into the video generation process. this approach allows for video-to-video control on top of the text-to-video model, providing more precise control over the generated content by conditioning the model on reference video frames during inference. comfy compatible model:\n created at: 2025-07-16T13:41:33+00:00\n- model_name: Lightricks/LTX-Video-0.9.7-distilled\n  description: this model card focuses on the model associated with the ltx-video model, codebase available . ltx-video is the first dit-based video generation model capable of generating high-quality videos in real-time. it produces 30 fps videos at a 1216√ó704 resolution faster than they can be watched. trained on a large-scale dataset of diverse videos, the model generates high-resolution videos with realistic and varied content. we provide a model for both text-to-video as well as image+text-to-video usecases\n created at: 2025-05-14T16:55:19+00:00\n- model_name: Lightricks/LTX-Video-0.9.5\n  description: this model card focuses on the model associated with the ltx-video model, codebase available . ltx-video is the first dit-based video generation model capable of generating high-quality videos in real-time. it produces 30 fps videos at a 1216√ó704 resolution faster than they can be watched. trained on a large-scale dataset of diverse videos, the model generates high-resolution videos with realistic and varied content. we provide a model for both text-to-video as well as image+text-to-video usecases\n created at: 2025-03-16T11:14:42+00:00\n- model_name: Lightricks/LTX-Video-0.9.7-dev\n  description: this model card focuses on the model associated with the ltx-video model, codebase available . ltx-video is the first dit-based video generation model capable of generating high-quality videos in real-time. it produces 30 fps videos at a 1216√ó704 resolution faster than they can be watched. trained on a large-scale dataset of diverse videos, the model generates high-resolution videos with realistic and varied content. we provide a model for both text-to-video as well as image+text-to-video usecases\n created at: 2025-05-13T12:02:56+00:00\n- model_name: tencent/HunyuanVideo\n  description: this repo contains pytorch model definitions, pre-trained weights and inference/sampling code for our paper exploring hunyuanvideo. you can find more visualizations on our .\n created at: 2024-12-01T06:00:34+00:00\n- model_name: Isi99999/Wan2.1-T2V-14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-02T17:37:57+00:00\n- model_name: Skywork/SkyReels-V2-T2V-14B-720P\n  description: skyreels v2: infinite-length film generative model üìë technical report ¬∑ üëã playground ¬∑ üí¨ discord ¬∑ ü§ó hugging face ¬∑ ü§ñ modelscope ¬∑ üåê github\n created at: 2025-04-18T13:55:51+00:00\n\nExamples:\n- example user query: I am directing a movie and I want to generate a video scene where Spiderman is surfing.\n  model_name: damo-vilab/text-to-video-ms-1.7b-legacy\n- example user query: Describe the process of generating a video clip with a scene where a dog is playing with a ball in a park.\n  model_name: camenduru/text2-video-zero\n- example user query: The marketing team needs a tool to transform textual descriptions of their products into promotional videos. Build a model that can create videos from text.\n  model_name: chavinlo/TempoFunk\n- example user query: I am writing a script for an educational video on biology. Can you generate a video using the text of my script?\n  model_name: duncan93/video\n- example user query: I want to convey the story of a bear enjoying a beautiful sunset in the forest through an AI-generated video.\n  model_name: nitrosocke/redshift-diffusion\n- example user query: We are presenting a new tool that can convert English text into videos which will have Bruce Lee showcase the given text.\n  model_name: ImRma/Brucelee\n\nUser prompt:\nDesign a video illustration presenting Meryl Streep appears in the 2009 film &#39;Julie and Julia.&#39; She was nominated for an Oscar in the Best Actress category for her role as Julia Child. <span class=meta>(Columbia Pictures)</span>.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <video_gen>\n\nCandidate Models:\n- model_name: camenduru/text2-video-zero\n  description: This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.\n created at: 2023-03-29\n- model_name: chavinlo/TempoFunk\n  description: A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text.\n created at: 2023-02-07\n- model_name: damo-vilab/text-to-video-ms-1.7b\n  description: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. The overall model parameters are about 1.7 billion. Currently, it only supports English input.\n created at: 2023-03-22\n- model_name: nitrosocke/redshift-diffusion\n  description: Tune-A-Video - Redshift is a text-to-video generation model based on the nitrosocke/redshift-diffusion model. It generates videos based on textual prompts, such as 'a man is skiing' or '(redshift style) spider man is skiing'.\n created at: 2022-11-06\n- model_name: nitrosocke/mo-di-diffusion\n  description: Tune-A-Video is a text-to-video generation model based on the Hugging Face framework. The model generates videos based on textual prompts in a modern Disney style.\n created at: 2022-10-27\n- model_name: ImRma/Brucelee\n  description: A Hugging Face model for converting Persian and English text into video.\n created at: 2023-04-23\n- model_name: damo-vilab/text-to-video-ms-1.7b-legacy\n  description: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.\n created at: 2023-03-22\n- model_name: duncan93/video\n  description: A text-to-video model trained on OpenAssistant/oasst1 dataset.\n created at: 2023-05-02\n- model_name: cerspense/zeroscope_v2_576w\n  description: zeroscope_v2_576w uses 7.9gb of vram when rendering 30 frames at 576x320\n\n### Using it with the 1111 text2video extension\n\n1. Download files in the zs2_576w folder.\n2. Replace the respective files in the 'stable-diffusion-webui\\models\\ModelScope\\t2v' directory.\n\n### Upscaling recommendations\n\nFor upscaling, it's recommended to use [zeroscope_v2_XL](https://huggingface.co/cerspense/zeroscope_v2_XL) via vid2vid in the 1111 extension. It works best at 1024x576 with a denoise strength between 0.66 and 0.85. Remember to use the same prompt that was used to generate the original clip. <br />\n\n### Usage in üß® Diffusers\n\nLet's first install the libraries required:\n\n```bash\n$ pip install diffusers transformers accelerate torch\n```\n\nNow, generate a video:\n\n```py\nimport torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_video\n\npipe = DiffusionPipeline.from_pretrained(\"cerspense/zeroscope_v2_576w\", torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n\nprompt = \"Darth Vader is surfing on waves\"\nvideo_frames = pipe(prompt, num_inference_steps=40, height=320, width=576, num_frames=24).frames\nvideo_path = export_to_video(video_frames)\n```\n\nHere are some results:\n\n<table>\n    <tr>\n        Darth vader is surfing on waves.\n        <br>\n        <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/darthvader_cerpense.gif\"\n            alt=\"Darth vader surfing in waves.\"\n            style=\"width: 576;\" />\n        </center></td>\n    </tr>\n</table>\n\n### Known issues\n\nLower resolutions or fewer frames could lead to suboptimal output. <br />\n\nThanks to [camenduru](https://github.com/camenduru), [kabachuha](https://github.com/kabachuha), [ExponentialML](https://github.com/ExponentialML), [dotsimulate](https://www.instagram.com/dotsimulate/), [VANYA](https://twitter.com/veryVANYA), [polyware](https://twitter.com/polyware_ai), [tin2tin](https://github.com/tin2tin)<br />\n created at: 2023-06-21\n- model_name: Searchium-ai/clip4clip-webvid150k\n  description: A CLIP4Clip video-text retrieval model trained on a subset of the WebVid dataset. \nThe model and training method are described in the paper [\"Clip4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval\"](https://arxiv.org/pdf/2104.08860.pdf) by Lou et el, and implemented in the accompanying [GitHub repository](https://github.com/ArrowLuo/CLIP4Clip).\n\nThe training process utilized the [WebVid Dataset](https://m-bain.github.io/webvid-dataset/), a comprehensive collection of short videos with corresponding textual descriptions sourced from the web. \nFor training purposes, a subset consisting of the first 150,000 video-text pairs from the dataset were used.\n\nThis HF model is based on the [clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32) architecture, with weights trained by Daphna Idelson at [Searchium](https://www.searchium.ai).\n\n\n# How to use\n### Extracting Text Embeddings:\n\n```python\nimport numpy as np\nimport torch\nfrom transformers import CLIPTokenizer, CLIPTextModelWithProjection\n\n\nsearch_sentence = \"a basketball player performing a slam dunk\"\n\nmodel = CLIPTextModelWithProjection.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\ntokenizer = CLIPTokenizer.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\n\ninputs = tokenizer(text=search_sentence , return_tensors=\"pt\")\noutputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n\n# Normalize embeddings for retrieval:\nfinal_output = outputs[0] / outputs[0].norm(dim=-1, keepdim=True)\nfinal_output = final_output.cpu().detach().numpy()\nprint(\"final output: \", final_output)\n```\n\n### Extracting Video Embeddings:\n\nAn additional notebook [\"GSI_VideoRetrieval_VideoEmbedding.ipynb\"](https://huggingface.co/Searchium-ai/clip4clip-webvid150k/blob/main/Notebooks/GSI_VideoRetrieval_VideoEmbedding.ipynb), provides instructions for extracting video embeddings and includes the necessary tools for preprocessing videos.\n created at: 2023-04-17\n- model_name: PAIR/text2video-zero-controlnet-canny-avatar\n  description: [Text2Video-Zero](https://arxiv.org/abs/2303.13439) is a zero-shot text to video generator. It can perform `zero-shot text-to-video generation`, `Video Instruct Pix2Pix` (instruction-guided video editing), \n`text and pose conditional video generation`, `text and canny-edge conditional video generation`, and \n`text, canny-edge and dreambooth conditional video generation`. For more information about this work, \nplease have a look at our [paper](https://arxiv.org/abs/2303.13439) and our demo: [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/PAIR/Text2Video-Zero)\nOur [code](https://github.com/Picsart-AI-Research/Text2Video-Zero) works with any StableDiffusion base model.\n\nThis model provides [DreamBooth](https://arxiv.org/abs/2208.12242) weights for the `Avatar style` to be used with edge guidance (using [ControlNet](https://arxiv.org/abs/2302.05543)) in text2video zero.\n created at: 2023-03-27\n- model_name: Wan-AI/Wan2.1-T2V-1.3B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-01T08:39:07+00:00\n- model_name: bullerwins/Wan2.2-T2V-A14B-GGUF\n  description: you need to download both a high-noise model and a low-noise model. high noise is used for the first steps and the low-noise for the details. place them on comfyui/models/unet you will need from  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T16:29:01+00:00\n- model_name: Wan-AI/Wan2.2-T2V-A14B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T09:04:28+00:00\n- model_name: genmo/mochi-1-preview\n  description: a state of the art video generation model by . mochi 1 preview is an open state-of-the-art video generation model with high-fidelity motion and strong prompt adherence in preliminary evaluation. this model dramatically closes the gap between closed and open video generation systems. we‚Äôre releasing the model under a permissive apache 2.0 license. try this model for free on .\n created at: 2024-10-22T07:36:03+00:00\n- model_name: Wan-AI/Wan2.1-T2V-14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-02-25T07:26:34+00:00\n- model_name: Wan-AI/Wan2.1-T2V-14B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-01T08:39:52+00:00\n- model_name: Wan-AI/Wan2.2-TI2V-5B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T09:03:29+00:00\n- model_name: Skywork/SkyReels-V2-DF-14B-720P\n  description: skyreels v2: infinite-length film generative model üìë technical report ¬∑ üëã playground ¬∑ üí¨ discord ¬∑ ü§ó hugging face ¬∑ ü§ñ modelscope ¬∑ üåê github\n created at: 2025-04-18T13:34:52+00:00\n- model_name: Wan-AI/Wan2.2-TI2V-5B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-18T09:35:44+00:00\n- model_name: Wan-AI/Wan2.2-T2V-A14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-24T15:07:15+00:00\n- model_name: magespace/Wan2.2-I2V-A14B-Lightning-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp üîó lightx2v distilled & optimized wan2.2 for fast, high-quality 480p / 720p image-to-video generation   wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-08-14T20:06:59+00:00\n- model_name: Wan-AI/Wan2.1-T2V-1.3B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-02-25T11:13:48+00:00\n- model_name: QuantStack/Wan2.1_T2V_14B_FusionX-GGUF\n  description: this is a gguf conversion of by . all quantized versions were created from the base t2v fp16 model using the conversion scripts provided by city96, available at the github repository. the model files can be used in with the custom node. place the required models in the following folders:\n created at: 2025-06-07T01:47:02+00:00\n- model_name: Lightricks/LTX-Video-ICLoRA-detailer-13b-0.9.8\n  description: this is a video detailer model on top of trained on custom data. ic lora is a method that enables adding video context into the video generation process. this approach allows for video-to-video control on top of the text-to-video model, providing more precise control over the generated content by conditioning the model on reference video frames during inference. comfy compatible model:\n created at: 2025-07-16T13:41:33+00:00\n- model_name: Lightricks/LTX-Video-0.9.7-distilled\n  description: this model card focuses on the model associated with the ltx-video model, codebase available . ltx-video is the first dit-based video generation model capable of generating high-quality videos in real-time. it produces 30 fps videos at a 1216√ó704 resolution faster than they can be watched. trained on a large-scale dataset of diverse videos, the model generates high-resolution videos with realistic and varied content. we provide a model for both text-to-video as well as image+text-to-video usecases\n created at: 2025-05-14T16:55:19+00:00\n- model_name: Lightricks/LTX-Video-0.9.5\n  description: this model card focuses on the model associated with the ltx-video model, codebase available . ltx-video is the first dit-based video generation model capable of generating high-quality videos in real-time. it produces 30 fps videos at a 1216√ó704 resolution faster than they can be watched. trained on a large-scale dataset of diverse videos, the model generates high-resolution videos with realistic and varied content. we provide a model for both text-to-video as well as image+text-to-video usecases\n created at: 2025-03-16T11:14:42+00:00\n- model_name: Lightricks/LTX-Video-0.9.7-dev\n  description: this model card focuses on the model associated with the ltx-video model, codebase available . ltx-video is the first dit-based video generation model capable of generating high-quality videos in real-time. it produces 30 fps videos at a 1216√ó704 resolution faster than they can be watched. trained on a large-scale dataset of diverse videos, the model generates high-resolution videos with realistic and varied content. we provide a model for both text-to-video as well as image+text-to-video usecases\n created at: 2025-05-13T12:02:56+00:00\n- model_name: tencent/HunyuanVideo\n  description: this repo contains pytorch model definitions, pre-trained weights and inference/sampling code for our paper exploring hunyuanvideo. you can find more visualizations on our .\n created at: 2024-12-01T06:00:34+00:00\n- model_name: Isi99999/Wan2.1-T2V-14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-02T17:37:57+00:00\n- model_name: Skywork/SkyReels-V2-T2V-14B-720P\n  description: skyreels v2: infinite-length film generative model üìë technical report ¬∑ üëã playground ¬∑ üí¨ discord ¬∑ ü§ó hugging face ¬∑ ü§ñ modelscope ¬∑ üåê github\n created at: 2025-04-18T13:55:51+00:00\n\nExamples:\n- example user query: I am directing a movie and I want to generate a video scene where Spiderman is surfing.\n  model_name: damo-vilab/text-to-video-ms-1.7b-legacy\n- example user query: Describe the process of generating a video clip with a scene where a dog is playing with a ball in a park.\n  model_name: camenduru/text2-video-zero\n- example user query: The marketing team needs a tool to transform textual descriptions of their products into promotional videos. Build a model that can create videos from text.\n  model_name: chavinlo/TempoFunk\n- example user query: I am writing a script for an educational video on biology. Can you generate a video using the text of my script?\n  model_name: duncan93/video\n- example user query: I want to convey the story of a bear enjoying a beautiful sunset in the forest through an AI-generated video.\n  model_name: nitrosocke/redshift-diffusion\n- example user query: We are presenting a new tool that can convert English text into videos which will have Bruce Lee showcase the given text.\n  model_name: ImRma/Brucelee\n\nUser prompt:\nI desire a video segment where gold glittered rice krispie treats.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <video_gen>\n\nCandidate Models:\n- model_name: camenduru/text2-video-zero\n  description: This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.\n created at: 2023-03-29\n- model_name: chavinlo/TempoFunk\n  description: A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text.\n created at: 2023-02-07\n- model_name: damo-vilab/text-to-video-ms-1.7b\n  description: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. The overall model parameters are about 1.7 billion. Currently, it only supports English input.\n created at: 2023-03-22\n- model_name: nitrosocke/redshift-diffusion\n  description: Tune-A-Video - Redshift is a text-to-video generation model based on the nitrosocke/redshift-diffusion model. It generates videos based on textual prompts, such as 'a man is skiing' or '(redshift style) spider man is skiing'.\n created at: 2022-11-06\n- model_name: nitrosocke/mo-di-diffusion\n  description: Tune-A-Video is a text-to-video generation model based on the Hugging Face framework. The model generates videos based on textual prompts in a modern Disney style.\n created at: 2022-10-27\n- model_name: ImRma/Brucelee\n  description: A Hugging Face model for converting Persian and English text into video.\n created at: 2023-04-23\n- model_name: damo-vilab/text-to-video-ms-1.7b-legacy\n  description: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.\n created at: 2023-03-22\n- model_name: duncan93/video\n  description: A text-to-video model trained on OpenAssistant/oasst1 dataset.\n created at: 2023-05-02\n- model_name: cerspense/zeroscope_v2_576w\n  description: zeroscope_v2_576w uses 7.9gb of vram when rendering 30 frames at 576x320\n\n### Using it with the 1111 text2video extension\n\n1. Download files in the zs2_576w folder.\n2. Replace the respective files in the 'stable-diffusion-webui\\models\\ModelScope\\t2v' directory.\n\n### Upscaling recommendations\n\nFor upscaling, it's recommended to use [zeroscope_v2_XL](https://huggingface.co/cerspense/zeroscope_v2_XL) via vid2vid in the 1111 extension. It works best at 1024x576 with a denoise strength between 0.66 and 0.85. Remember to use the same prompt that was used to generate the original clip. <br />\n\n### Usage in üß® Diffusers\n\nLet's first install the libraries required:\n\n```bash\n$ pip install diffusers transformers accelerate torch\n```\n\nNow, generate a video:\n\n```py\nimport torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_video\n\npipe = DiffusionPipeline.from_pretrained(\"cerspense/zeroscope_v2_576w\", torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n\nprompt = \"Darth Vader is surfing on waves\"\nvideo_frames = pipe(prompt, num_inference_steps=40, height=320, width=576, num_frames=24).frames\nvideo_path = export_to_video(video_frames)\n```\n\nHere are some results:\n\n<table>\n    <tr>\n        Darth vader is surfing on waves.\n        <br>\n        <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/darthvader_cerpense.gif\"\n            alt=\"Darth vader surfing in waves.\"\n            style=\"width: 576;\" />\n        </center></td>\n    </tr>\n</table>\n\n### Known issues\n\nLower resolutions or fewer frames could lead to suboptimal output. <br />\n\nThanks to [camenduru](https://github.com/camenduru), [kabachuha](https://github.com/kabachuha), [ExponentialML](https://github.com/ExponentialML), [dotsimulate](https://www.instagram.com/dotsimulate/), [VANYA](https://twitter.com/veryVANYA), [polyware](https://twitter.com/polyware_ai), [tin2tin](https://github.com/tin2tin)<br />\n created at: 2023-06-21\n- model_name: Searchium-ai/clip4clip-webvid150k\n  description: A CLIP4Clip video-text retrieval model trained on a subset of the WebVid dataset. \nThe model and training method are described in the paper [\"Clip4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval\"](https://arxiv.org/pdf/2104.08860.pdf) by Lou et el, and implemented in the accompanying [GitHub repository](https://github.com/ArrowLuo/CLIP4Clip).\n\nThe training process utilized the [WebVid Dataset](https://m-bain.github.io/webvid-dataset/), a comprehensive collection of short videos with corresponding textual descriptions sourced from the web. \nFor training purposes, a subset consisting of the first 150,000 video-text pairs from the dataset were used.\n\nThis HF model is based on the [clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32) architecture, with weights trained by Daphna Idelson at [Searchium](https://www.searchium.ai).\n\n\n# How to use\n### Extracting Text Embeddings:\n\n```python\nimport numpy as np\nimport torch\nfrom transformers import CLIPTokenizer, CLIPTextModelWithProjection\n\n\nsearch_sentence = \"a basketball player performing a slam dunk\"\n\nmodel = CLIPTextModelWithProjection.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\ntokenizer = CLIPTokenizer.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\n\ninputs = tokenizer(text=search_sentence , return_tensors=\"pt\")\noutputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n\n# Normalize embeddings for retrieval:\nfinal_output = outputs[0] / outputs[0].norm(dim=-1, keepdim=True)\nfinal_output = final_output.cpu().detach().numpy()\nprint(\"final output: \", final_output)\n```\n\n### Extracting Video Embeddings:\n\nAn additional notebook [\"GSI_VideoRetrieval_VideoEmbedding.ipynb\"](https://huggingface.co/Searchium-ai/clip4clip-webvid150k/blob/main/Notebooks/GSI_VideoRetrieval_VideoEmbedding.ipynb), provides instructions for extracting video embeddings and includes the necessary tools for preprocessing videos.\n created at: 2023-04-17\n- model_name: PAIR/text2video-zero-controlnet-canny-avatar\n  description: [Text2Video-Zero](https://arxiv.org/abs/2303.13439) is a zero-shot text to video generator. It can perform `zero-shot text-to-video generation`, `Video Instruct Pix2Pix` (instruction-guided video editing), \n`text and pose conditional video generation`, `text and canny-edge conditional video generation`, and \n`text, canny-edge and dreambooth conditional video generation`. For more information about this work, \nplease have a look at our [paper](https://arxiv.org/abs/2303.13439) and our demo: [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/PAIR/Text2Video-Zero)\nOur [code](https://github.com/Picsart-AI-Research/Text2Video-Zero) works with any StableDiffusion base model.\n\nThis model provides [DreamBooth](https://arxiv.org/abs/2208.12242) weights for the `Avatar style` to be used with edge guidance (using [ControlNet](https://arxiv.org/abs/2302.05543)) in text2video zero.\n created at: 2023-03-27\n- model_name: Wan-AI/Wan2.1-T2V-1.3B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-01T08:39:07+00:00\n- model_name: bullerwins/Wan2.2-T2V-A14B-GGUF\n  description: you need to download both a high-noise model and a low-noise model. high noise is used for the first steps and the low-noise for the details. place them on comfyui/models/unet you will need from  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T16:29:01+00:00\n- model_name: Wan-AI/Wan2.2-T2V-A14B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T09:04:28+00:00\n- model_name: genmo/mochi-1-preview\n  description: a state of the art video generation model by . mochi 1 preview is an open state-of-the-art video generation model with high-fidelity motion and strong prompt adherence in preliminary evaluation. this model dramatically closes the gap between closed and open video generation systems. we‚Äôre releasing the model under a permissive apache 2.0 license. try this model for free on .\n created at: 2024-10-22T07:36:03+00:00\n- model_name: Wan-AI/Wan2.1-T2V-14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-02-25T07:26:34+00:00\n- model_name: Wan-AI/Wan2.1-T2V-14B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-01T08:39:52+00:00\n- model_name: Wan-AI/Wan2.2-TI2V-5B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T09:03:29+00:00\n- model_name: Skywork/SkyReels-V2-DF-14B-720P\n  description: skyreels v2: infinite-length film generative model üìë technical report ¬∑ üëã playground ¬∑ üí¨ discord ¬∑ ü§ó hugging face ¬∑ ü§ñ modelscope ¬∑ üåê github\n created at: 2025-04-18T13:34:52+00:00\n- model_name: Wan-AI/Wan2.2-TI2V-5B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-18T09:35:44+00:00\n- model_name: Wan-AI/Wan2.2-T2V-A14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-24T15:07:15+00:00\n- model_name: magespace/Wan2.2-I2V-A14B-Lightning-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp üîó lightx2v distilled & optimized wan2.2 for fast, high-quality 480p / 720p image-to-video generation   wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-08-14T20:06:59+00:00\n- model_name: Wan-AI/Wan2.1-T2V-1.3B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-02-25T11:13:48+00:00\n- model_name: QuantStack/Wan2.1_T2V_14B_FusionX-GGUF\n  description: this is a gguf conversion of by . all quantized versions were created from the base t2v fp16 model using the conversion scripts provided by city96, available at the github repository. the model files can be used in with the custom node. place the required models in the following folders:\n created at: 2025-06-07T01:47:02+00:00\n- model_name: Lightricks/LTX-Video-ICLoRA-detailer-13b-0.9.8\n  description: this is a video detailer model on top of trained on custom data. ic lora is a method that enables adding video context into the video generation process. this approach allows for video-to-video control on top of the text-to-video model, providing more precise control over the generated content by conditioning the model on reference video frames during inference. comfy compatible model:\n created at: 2025-07-16T13:41:33+00:00\n- model_name: Lightricks/LTX-Video-0.9.7-distilled\n  description: this model card focuses on the model associated with the ltx-video model, codebase available . ltx-video is the first dit-based video generation model capable of generating high-quality videos in real-time. it produces 30 fps videos at a 1216√ó704 resolution faster than they can be watched. trained on a large-scale dataset of diverse videos, the model generates high-resolution videos with realistic and varied content. we provide a model for both text-to-video as well as image+text-to-video usecases\n created at: 2025-05-14T16:55:19+00:00\n- model_name: Lightricks/LTX-Video-0.9.5\n  description: this model card focuses on the model associated with the ltx-video model, codebase available . ltx-video is the first dit-based video generation model capable of generating high-quality videos in real-time. it produces 30 fps videos at a 1216√ó704 resolution faster than they can be watched. trained on a large-scale dataset of diverse videos, the model generates high-resolution videos with realistic and varied content. we provide a model for both text-to-video as well as image+text-to-video usecases\n created at: 2025-03-16T11:14:42+00:00\n- model_name: Lightricks/LTX-Video-0.9.7-dev\n  description: this model card focuses on the model associated with the ltx-video model, codebase available . ltx-video is the first dit-based video generation model capable of generating high-quality videos in real-time. it produces 30 fps videos at a 1216√ó704 resolution faster than they can be watched. trained on a large-scale dataset of diverse videos, the model generates high-resolution videos with realistic and varied content. we provide a model for both text-to-video as well as image+text-to-video usecases\n created at: 2025-05-13T12:02:56+00:00\n- model_name: tencent/HunyuanVideo\n  description: this repo contains pytorch model definitions, pre-trained weights and inference/sampling code for our paper exploring hunyuanvideo. you can find more visualizations on our .\n created at: 2024-12-01T06:00:34+00:00\n- model_name: Isi99999/Wan2.1-T2V-14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-02T17:37:57+00:00\n- model_name: Skywork/SkyReels-V2-T2V-14B-720P\n  description: skyreels v2: infinite-length film generative model üìë technical report ¬∑ üëã playground ¬∑ üí¨ discord ¬∑ ü§ó hugging face ¬∑ ü§ñ modelscope ¬∑ üåê github\n created at: 2025-04-18T13:55:51+00:00\n\nExamples:\n- example user query: I am directing a movie and I want to generate a video scene where Spiderman is surfing.\n  model_name: damo-vilab/text-to-video-ms-1.7b-legacy\n- example user query: Describe the process of generating a video clip with a scene where a dog is playing with a ball in a park.\n  model_name: camenduru/text2-video-zero\n- example user query: The marketing team needs a tool to transform textual descriptions of their products into promotional videos. Build a model that can create videos from text.\n  model_name: chavinlo/TempoFunk\n- example user query: I am writing a script for an educational video on biology. Can you generate a video using the text of my script?\n  model_name: duncan93/video\n- example user query: I want to convey the story of a bear enjoying a beautiful sunset in the forest through an AI-generated video.\n  model_name: nitrosocke/redshift-diffusion\n- example user query: We are presenting a new tool that can convert English text into videos which will have Bruce Lee showcase the given text.\n  model_name: ImRma/Brucelee\n\nUser prompt:\nPlease produce a science experiment video explaining Stone columns and recessed lighting for the traditional, rustic deck [From: North Georgia Virtual Home Tours].\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <video_gen>\n\nCandidate Models:\n- model_name: camenduru/text2-video-zero\n  description: This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.\n created at: 2023-03-29\n- model_name: chavinlo/TempoFunk\n  description: A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text.\n created at: 2023-02-07\n- model_name: damo-vilab/text-to-video-ms-1.7b\n  description: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. The overall model parameters are about 1.7 billion. Currently, it only supports English input.\n created at: 2023-03-22\n- model_name: nitrosocke/redshift-diffusion\n  description: Tune-A-Video - Redshift is a text-to-video generation model based on the nitrosocke/redshift-diffusion model. It generates videos based on textual prompts, such as 'a man is skiing' or '(redshift style) spider man is skiing'.\n created at: 2022-11-06\n- model_name: nitrosocke/mo-di-diffusion\n  description: Tune-A-Video is a text-to-video generation model based on the Hugging Face framework. The model generates videos based on textual prompts in a modern Disney style.\n created at: 2022-10-27\n- model_name: ImRma/Brucelee\n  description: A Hugging Face model for converting Persian and English text into video.\n created at: 2023-04-23\n- model_name: damo-vilab/text-to-video-ms-1.7b-legacy\n  description: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.\n created at: 2023-03-22\n- model_name: duncan93/video\n  description: A text-to-video model trained on OpenAssistant/oasst1 dataset.\n created at: 2023-05-02\n- model_name: cerspense/zeroscope_v2_576w\n  description: zeroscope_v2_576w uses 7.9gb of vram when rendering 30 frames at 576x320\n\n### Using it with the 1111 text2video extension\n\n1. Download files in the zs2_576w folder.\n2. Replace the respective files in the 'stable-diffusion-webui\\models\\ModelScope\\t2v' directory.\n\n### Upscaling recommendations\n\nFor upscaling, it's recommended to use [zeroscope_v2_XL](https://huggingface.co/cerspense/zeroscope_v2_XL) via vid2vid in the 1111 extension. It works best at 1024x576 with a denoise strength between 0.66 and 0.85. Remember to use the same prompt that was used to generate the original clip. <br />\n\n### Usage in üß® Diffusers\n\nLet's first install the libraries required:\n\n```bash\n$ pip install diffusers transformers accelerate torch\n```\n\nNow, generate a video:\n\n```py\nimport torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_video\n\npipe = DiffusionPipeline.from_pretrained(\"cerspense/zeroscope_v2_576w\", torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n\nprompt = \"Darth Vader is surfing on waves\"\nvideo_frames = pipe(prompt, num_inference_steps=40, height=320, width=576, num_frames=24).frames\nvideo_path = export_to_video(video_frames)\n```\n\nHere are some results:\n\n<table>\n    <tr>\n        Darth vader is surfing on waves.\n        <br>\n        <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/darthvader_cerpense.gif\"\n            alt=\"Darth vader surfing in waves.\"\n            style=\"width: 576;\" />\n        </center></td>\n    </tr>\n</table>\n\n### Known issues\n\nLower resolutions or fewer frames could lead to suboptimal output. <br />\n\nThanks to [camenduru](https://github.com/camenduru), [kabachuha](https://github.com/kabachuha), [ExponentialML](https://github.com/ExponentialML), [dotsimulate](https://www.instagram.com/dotsimulate/), [VANYA](https://twitter.com/veryVANYA), [polyware](https://twitter.com/polyware_ai), [tin2tin](https://github.com/tin2tin)<br />\n created at: 2023-06-21\n- model_name: Searchium-ai/clip4clip-webvid150k\n  description: A CLIP4Clip video-text retrieval model trained on a subset of the WebVid dataset. \nThe model and training method are described in the paper [\"Clip4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval\"](https://arxiv.org/pdf/2104.08860.pdf) by Lou et el, and implemented in the accompanying [GitHub repository](https://github.com/ArrowLuo/CLIP4Clip).\n\nThe training process utilized the [WebVid Dataset](https://m-bain.github.io/webvid-dataset/), a comprehensive collection of short videos with corresponding textual descriptions sourced from the web. \nFor training purposes, a subset consisting of the first 150,000 video-text pairs from the dataset were used.\n\nThis HF model is based on the [clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32) architecture, with weights trained by Daphna Idelson at [Searchium](https://www.searchium.ai).\n\n\n# How to use\n### Extracting Text Embeddings:\n\n```python\nimport numpy as np\nimport torch\nfrom transformers import CLIPTokenizer, CLIPTextModelWithProjection\n\n\nsearch_sentence = \"a basketball player performing a slam dunk\"\n\nmodel = CLIPTextModelWithProjection.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\ntokenizer = CLIPTokenizer.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\n\ninputs = tokenizer(text=search_sentence , return_tensors=\"pt\")\noutputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n\n# Normalize embeddings for retrieval:\nfinal_output = outputs[0] / outputs[0].norm(dim=-1, keepdim=True)\nfinal_output = final_output.cpu().detach().numpy()\nprint(\"final output: \", final_output)\n```\n\n### Extracting Video Embeddings:\n\nAn additional notebook [\"GSI_VideoRetrieval_VideoEmbedding.ipynb\"](https://huggingface.co/Searchium-ai/clip4clip-webvid150k/blob/main/Notebooks/GSI_VideoRetrieval_VideoEmbedding.ipynb), provides instructions for extracting video embeddings and includes the necessary tools for preprocessing videos.\n created at: 2023-04-17\n- model_name: PAIR/text2video-zero-controlnet-canny-avatar\n  description: [Text2Video-Zero](https://arxiv.org/abs/2303.13439) is a zero-shot text to video generator. It can perform `zero-shot text-to-video generation`, `Video Instruct Pix2Pix` (instruction-guided video editing), \n`text and pose conditional video generation`, `text and canny-edge conditional video generation`, and \n`text, canny-edge and dreambooth conditional video generation`. For more information about this work, \nplease have a look at our [paper](https://arxiv.org/abs/2303.13439) and our demo: [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/PAIR/Text2Video-Zero)\nOur [code](https://github.com/Picsart-AI-Research/Text2Video-Zero) works with any StableDiffusion base model.\n\nThis model provides [DreamBooth](https://arxiv.org/abs/2208.12242) weights for the `Avatar style` to be used with edge guidance (using [ControlNet](https://arxiv.org/abs/2302.05543)) in text2video zero.\n created at: 2023-03-27\n- model_name: Wan-AI/Wan2.1-T2V-1.3B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-01T08:39:07+00:00\n- model_name: bullerwins/Wan2.2-T2V-A14B-GGUF\n  description: you need to download both a high-noise model and a low-noise model. high noise is used for the first steps and the low-noise for the details. place them on comfyui/models/unet you will need from  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T16:29:01+00:00\n- model_name: Wan-AI/Wan2.2-T2V-A14B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T09:04:28+00:00\n- model_name: genmo/mochi-1-preview\n  description: a state of the art video generation model by . mochi 1 preview is an open state-of-the-art video generation model with high-fidelity motion and strong prompt adherence in preliminary evaluation. this model dramatically closes the gap between closed and open video generation systems. we‚Äôre releasing the model under a permissive apache 2.0 license. try this model for free on .\n created at: 2024-10-22T07:36:03+00:00\n- model_name: Wan-AI/Wan2.1-T2V-14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-02-25T07:26:34+00:00\n- model_name: Wan-AI/Wan2.1-T2V-14B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-01T08:39:52+00:00\n- model_name: Wan-AI/Wan2.2-TI2V-5B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T09:03:29+00:00\n- model_name: Skywork/SkyReels-V2-DF-14B-720P\n  description: skyreels v2: infinite-length film generative model üìë technical report ¬∑ üëã playground ¬∑ üí¨ discord ¬∑ ü§ó hugging face ¬∑ ü§ñ modelscope ¬∑ üåê github\n created at: 2025-04-18T13:34:52+00:00\n- model_name: Wan-AI/Wan2.2-TI2V-5B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-18T09:35:44+00:00\n- model_name: Wan-AI/Wan2.2-T2V-A14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-24T15:07:15+00:00\n- model_name: magespace/Wan2.2-I2V-A14B-Lightning-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp üîó lightx2v distilled & optimized wan2.2 for fast, high-quality 480p / 720p image-to-video generation   wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-08-14T20:06:59+00:00\n- model_name: Wan-AI/Wan2.1-T2V-1.3B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-02-25T11:13:48+00:00\n- model_name: QuantStack/Wan2.1_T2V_14B_FusionX-GGUF\n  description: this is a gguf conversion of by . all quantized versions were created from the base t2v fp16 model using the conversion scripts provided by city96, available at the github repository. the model files can be used in with the custom node. place the required models in the following folders:\n created at: 2025-06-07T01:47:02+00:00\n- model_name: Lightricks/LTX-Video-ICLoRA-detailer-13b-0.9.8\n  description: this is a video detailer model on top of trained on custom data. ic lora is a method that enables adding video context into the video generation process. this approach allows for video-to-video control on top of the text-to-video model, providing more precise control over the generated content by conditioning the model on reference video frames during inference. comfy compatible model:\n created at: 2025-07-16T13:41:33+00:00\n- model_name: Lightricks/LTX-Video-0.9.7-distilled\n  description: this model card focuses on the model associated with the ltx-video model, codebase available . ltx-video is the first dit-based video generation model capable of generating high-quality videos in real-time. it produces 30 fps videos at a 1216√ó704 resolution faster than they can be watched. trained on a large-scale dataset of diverse videos, the model generates high-resolution videos with realistic and varied content. we provide a model for both text-to-video as well as image+text-to-video usecases\n created at: 2025-05-14T16:55:19+00:00\n- model_name: Lightricks/LTX-Video-0.9.5\n  description: this model card focuses on the model associated with the ltx-video model, codebase available . ltx-video is the first dit-based video generation model capable of generating high-quality videos in real-time. it produces 30 fps videos at a 1216√ó704 resolution faster than they can be watched. trained on a large-scale dataset of diverse videos, the model generates high-resolution videos with realistic and varied content. we provide a model for both text-to-video as well as image+text-to-video usecases\n created at: 2025-03-16T11:14:42+00:00\n- model_name: Lightricks/LTX-Video-0.9.7-dev\n  description: this model card focuses on the model associated with the ltx-video model, codebase available . ltx-video is the first dit-based video generation model capable of generating high-quality videos in real-time. it produces 30 fps videos at a 1216√ó704 resolution faster than they can be watched. trained on a large-scale dataset of diverse videos, the model generates high-resolution videos with realistic and varied content. we provide a model for both text-to-video as well as image+text-to-video usecases\n created at: 2025-05-13T12:02:56+00:00\n- model_name: tencent/HunyuanVideo\n  description: this repo contains pytorch model definitions, pre-trained weights and inference/sampling code for our paper exploring hunyuanvideo. you can find more visualizations on our .\n created at: 2024-12-01T06:00:34+00:00\n- model_name: Isi99999/Wan2.1-T2V-14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-02T17:37:57+00:00\n- model_name: Skywork/SkyReels-V2-T2V-14B-720P\n  description: skyreels v2: infinite-length film generative model üìë technical report ¬∑ üëã playground ¬∑ üí¨ discord ¬∑ ü§ó hugging face ¬∑ ü§ñ modelscope ¬∑ üåê github\n created at: 2025-04-18T13:55:51+00:00\n\nExamples:\n- example user query: I am directing a movie and I want to generate a video scene where Spiderman is surfing.\n  model_name: damo-vilab/text-to-video-ms-1.7b-legacy\n- example user query: Describe the process of generating a video clip with a scene where a dog is playing with a ball in a park.\n  model_name: camenduru/text2-video-zero\n- example user query: The marketing team needs a tool to transform textual descriptions of their products into promotional videos. Build a model that can create videos from text.\n  model_name: chavinlo/TempoFunk\n- example user query: I am writing a script for an educational video on biology. Can you generate a video using the text of my script?\n  model_name: duncan93/video\n- example user query: I want to convey the story of a bear enjoying a beautiful sunset in the forest through an AI-generated video.\n  model_name: nitrosocke/redshift-diffusion\n- example user query: We are presenting a new tool that can convert English text into videos which will have Bruce Lee showcase the given text.\n  model_name: ImRma/Brucelee\n\nUser prompt:\nCan you design a cinematic clip about Large Picture of '68 Chevrolet Camaro located in Florida Offered by Bullet Motorsports Inc - O7VM.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <video_gen>\n\nCandidate Models:\n- model_name: camenduru/text2-video-zero\n  description: This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.\n created at: 2023-03-29\n- model_name: chavinlo/TempoFunk\n  description: A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text.\n created at: 2023-02-07\n- model_name: damo-vilab/text-to-video-ms-1.7b\n  description: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. The overall model parameters are about 1.7 billion. Currently, it only supports English input.\n created at: 2023-03-22\n- model_name: nitrosocke/redshift-diffusion\n  description: Tune-A-Video - Redshift is a text-to-video generation model based on the nitrosocke/redshift-diffusion model. It generates videos based on textual prompts, such as 'a man is skiing' or '(redshift style) spider man is skiing'.\n created at: 2022-11-06\n- model_name: nitrosocke/mo-di-diffusion\n  description: Tune-A-Video is a text-to-video generation model based on the Hugging Face framework. The model generates videos based on textual prompts in a modern Disney style.\n created at: 2022-10-27\n- model_name: ImRma/Brucelee\n  description: A Hugging Face model for converting Persian and English text into video.\n created at: 2023-04-23\n- model_name: damo-vilab/text-to-video-ms-1.7b-legacy\n  description: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.\n created at: 2023-03-22\n- model_name: duncan93/video\n  description: A text-to-video model trained on OpenAssistant/oasst1 dataset.\n created at: 2023-05-02\n- model_name: cerspense/zeroscope_v2_576w\n  description: zeroscope_v2_576w uses 7.9gb of vram when rendering 30 frames at 576x320\n\n### Using it with the 1111 text2video extension\n\n1. Download files in the zs2_576w folder.\n2. Replace the respective files in the 'stable-diffusion-webui\\models\\ModelScope\\t2v' directory.\n\n### Upscaling recommendations\n\nFor upscaling, it's recommended to use [zeroscope_v2_XL](https://huggingface.co/cerspense/zeroscope_v2_XL) via vid2vid in the 1111 extension. It works best at 1024x576 with a denoise strength between 0.66 and 0.85. Remember to use the same prompt that was used to generate the original clip. <br />\n\n### Usage in üß® Diffusers\n\nLet's first install the libraries required:\n\n```bash\n$ pip install diffusers transformers accelerate torch\n```\n\nNow, generate a video:\n\n```py\nimport torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_video\n\npipe = DiffusionPipeline.from_pretrained(\"cerspense/zeroscope_v2_576w\", torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n\nprompt = \"Darth Vader is surfing on waves\"\nvideo_frames = pipe(prompt, num_inference_steps=40, height=320, width=576, num_frames=24).frames\nvideo_path = export_to_video(video_frames)\n```\n\nHere are some results:\n\n<table>\n    <tr>\n        Darth vader is surfing on waves.\n        <br>\n        <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/darthvader_cerpense.gif\"\n            alt=\"Darth vader surfing in waves.\"\n            style=\"width: 576;\" />\n        </center></td>\n    </tr>\n</table>\n\n### Known issues\n\nLower resolutions or fewer frames could lead to suboptimal output. <br />\n\nThanks to [camenduru](https://github.com/camenduru), [kabachuha](https://github.com/kabachuha), [ExponentialML](https://github.com/ExponentialML), [dotsimulate](https://www.instagram.com/dotsimulate/), [VANYA](https://twitter.com/veryVANYA), [polyware](https://twitter.com/polyware_ai), [tin2tin](https://github.com/tin2tin)<br />\n created at: 2023-06-21\n- model_name: Searchium-ai/clip4clip-webvid150k\n  description: A CLIP4Clip video-text retrieval model trained on a subset of the WebVid dataset. \nThe model and training method are described in the paper [\"Clip4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval\"](https://arxiv.org/pdf/2104.08860.pdf) by Lou et el, and implemented in the accompanying [GitHub repository](https://github.com/ArrowLuo/CLIP4Clip).\n\nThe training process utilized the [WebVid Dataset](https://m-bain.github.io/webvid-dataset/), a comprehensive collection of short videos with corresponding textual descriptions sourced from the web. \nFor training purposes, a subset consisting of the first 150,000 video-text pairs from the dataset were used.\n\nThis HF model is based on the [clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32) architecture, with weights trained by Daphna Idelson at [Searchium](https://www.searchium.ai).\n\n\n# How to use\n### Extracting Text Embeddings:\n\n```python\nimport numpy as np\nimport torch\nfrom transformers import CLIPTokenizer, CLIPTextModelWithProjection\n\n\nsearch_sentence = \"a basketball player performing a slam dunk\"\n\nmodel = CLIPTextModelWithProjection.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\ntokenizer = CLIPTokenizer.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\n\ninputs = tokenizer(text=search_sentence , return_tensors=\"pt\")\noutputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n\n# Normalize embeddings for retrieval:\nfinal_output = outputs[0] / outputs[0].norm(dim=-1, keepdim=True)\nfinal_output = final_output.cpu().detach().numpy()\nprint(\"final output: \", final_output)\n```\n\n### Extracting Video Embeddings:\n\nAn additional notebook [\"GSI_VideoRetrieval_VideoEmbedding.ipynb\"](https://huggingface.co/Searchium-ai/clip4clip-webvid150k/blob/main/Notebooks/GSI_VideoRetrieval_VideoEmbedding.ipynb), provides instructions for extracting video embeddings and includes the necessary tools for preprocessing videos.\n created at: 2023-04-17\n- model_name: PAIR/text2video-zero-controlnet-canny-avatar\n  description: [Text2Video-Zero](https://arxiv.org/abs/2303.13439) is a zero-shot text to video generator. It can perform `zero-shot text-to-video generation`, `Video Instruct Pix2Pix` (instruction-guided video editing), \n`text and pose conditional video generation`, `text and canny-edge conditional video generation`, and \n`text, canny-edge and dreambooth conditional video generation`. For more information about this work, \nplease have a look at our [paper](https://arxiv.org/abs/2303.13439) and our demo: [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/PAIR/Text2Video-Zero)\nOur [code](https://github.com/Picsart-AI-Research/Text2Video-Zero) works with any StableDiffusion base model.\n\nThis model provides [DreamBooth](https://arxiv.org/abs/2208.12242) weights for the `Avatar style` to be used with edge guidance (using [ControlNet](https://arxiv.org/abs/2302.05543)) in text2video zero.\n created at: 2023-03-27\n- model_name: Wan-AI/Wan2.1-T2V-1.3B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-01T08:39:07+00:00\n- model_name: bullerwins/Wan2.2-T2V-A14B-GGUF\n  description: you need to download both a high-noise model and a low-noise model. high noise is used for the first steps and the low-noise for the details. place them on comfyui/models/unet you will need from  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T16:29:01+00:00\n- model_name: Wan-AI/Wan2.2-T2V-A14B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T09:04:28+00:00\n- model_name: genmo/mochi-1-preview\n  description: a state of the art video generation model by . mochi 1 preview is an open state-of-the-art video generation model with high-fidelity motion and strong prompt adherence in preliminary evaluation. this model dramatically closes the gap between closed and open video generation systems. we‚Äôre releasing the model under a permissive apache 2.0 license. try this model for free on .\n created at: 2024-10-22T07:36:03+00:00\n- model_name: Wan-AI/Wan2.1-T2V-14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-02-25T07:26:34+00:00\n- model_name: Wan-AI/Wan2.1-T2V-14B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-01T08:39:52+00:00\n- model_name: Wan-AI/Wan2.2-TI2V-5B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T09:03:29+00:00\n- model_name: Skywork/SkyReels-V2-DF-14B-720P\n  description: skyreels v2: infinite-length film generative model üìë technical report ¬∑ üëã playground ¬∑ üí¨ discord ¬∑ ü§ó hugging face ¬∑ ü§ñ modelscope ¬∑ üåê github\n created at: 2025-04-18T13:34:52+00:00\n- model_name: Wan-AI/Wan2.2-TI2V-5B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-18T09:35:44+00:00\n- model_name: Wan-AI/Wan2.2-T2V-A14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-24T15:07:15+00:00\n- model_name: magespace/Wan2.2-I2V-A14B-Lightning-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp üîó lightx2v distilled & optimized wan2.2 for fast, high-quality 480p / 720p image-to-video generation   wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-08-14T20:06:59+00:00\n- model_name: Wan-AI/Wan2.1-T2V-1.3B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-02-25T11:13:48+00:00\n- model_name: QuantStack/Wan2.1_T2V_14B_FusionX-GGUF\n  description: this is a gguf conversion of by . all quantized versions were created from the base t2v fp16 model using the conversion scripts provided by city96, available at the github repository. the model files can be used in with the custom node. place the required models in the following folders:\n created at: 2025-06-07T01:47:02+00:00\n- model_name: Lightricks/LTX-Video-ICLoRA-detailer-13b-0.9.8\n  description: this is a video detailer model on top of trained on custom data. ic lora is a method that enables adding video context into the video generation process. this approach allows for video-to-video control on top of the text-to-video model, providing more precise control over the generated content by conditioning the model on reference video frames during inference. comfy compatible model:\n created at: 2025-07-16T13:41:33+00:00\n- model_name: Lightricks/LTX-Video-0.9.7-distilled\n  description: this model card focuses on the model associated with the ltx-video model, codebase available . ltx-video is the first dit-based video generation model capable of generating high-quality videos in real-time. it produces 30 fps videos at a 1216√ó704 resolution faster than they can be watched. trained on a large-scale dataset of diverse videos, the model generates high-resolution videos with realistic and varied content. we provide a model for both text-to-video as well as image+text-to-video usecases\n created at: 2025-05-14T16:55:19+00:00\n- model_name: Lightricks/LTX-Video-0.9.5\n  description: this model card focuses on the model associated with the ltx-video model, codebase available . ltx-video is the first dit-based video generation model capable of generating high-quality videos in real-time. it produces 30 fps videos at a 1216√ó704 resolution faster than they can be watched. trained on a large-scale dataset of diverse videos, the model generates high-resolution videos with realistic and varied content. we provide a model for both text-to-video as well as image+text-to-video usecases\n created at: 2025-03-16T11:14:42+00:00\n- model_name: Lightricks/LTX-Video-0.9.7-dev\n  description: this model card focuses on the model associated with the ltx-video model, codebase available . ltx-video is the first dit-based video generation model capable of generating high-quality videos in real-time. it produces 30 fps videos at a 1216√ó704 resolution faster than they can be watched. trained on a large-scale dataset of diverse videos, the model generates high-resolution videos with realistic and varied content. we provide a model for both text-to-video as well as image+text-to-video usecases\n created at: 2025-05-13T12:02:56+00:00\n- model_name: tencent/HunyuanVideo\n  description: this repo contains pytorch model definitions, pre-trained weights and inference/sampling code for our paper exploring hunyuanvideo. you can find more visualizations on our .\n created at: 2024-12-01T06:00:34+00:00\n- model_name: Isi99999/Wan2.1-T2V-14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-02T17:37:57+00:00\n- model_name: Skywork/SkyReels-V2-T2V-14B-720P\n  description: skyreels v2: infinite-length film generative model üìë technical report ¬∑ üëã playground ¬∑ üí¨ discord ¬∑ ü§ó hugging face ¬∑ ü§ñ modelscope ¬∑ üåê github\n created at: 2025-04-18T13:55:51+00:00\n\nExamples:\n- example user query: I am directing a movie and I want to generate a video scene where Spiderman is surfing.\n  model_name: damo-vilab/text-to-video-ms-1.7b-legacy\n- example user query: Describe the process of generating a video clip with a scene where a dog is playing with a ball in a park.\n  model_name: camenduru/text2-video-zero\n- example user query: The marketing team needs a tool to transform textual descriptions of their products into promotional videos. Build a model that can create videos from text.\n  model_name: chavinlo/TempoFunk\n- example user query: I am writing a script for an educational video on biology. Can you generate a video using the text of my script?\n  model_name: duncan93/video\n- example user query: I want to convey the story of a bear enjoying a beautiful sunset in the forest through an AI-generated video.\n  model_name: nitrosocke/redshift-diffusion\n- example user query: We are presenting a new tool that can convert English text into videos which will have Bruce Lee showcase the given text.\n  model_name: ImRma/Brucelee\n\nUser prompt:\nCan you develop a dynamic sequence capturing Styled Shoot | Love in Wartime.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <video_gen>\n\nCandidate Models:\n- model_name: camenduru/text2-video-zero\n  description: This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.\n created at: 2023-03-29\n- model_name: chavinlo/TempoFunk\n  description: A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text.\n created at: 2023-02-07\n- model_name: damo-vilab/text-to-video-ms-1.7b\n  description: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. The overall model parameters are about 1.7 billion. Currently, it only supports English input.\n created at: 2023-03-22\n- model_name: nitrosocke/redshift-diffusion\n  description: Tune-A-Video - Redshift is a text-to-video generation model based on the nitrosocke/redshift-diffusion model. It generates videos based on textual prompts, such as 'a man is skiing' or '(redshift style) spider man is skiing'.\n created at: 2022-11-06\n- model_name: nitrosocke/mo-di-diffusion\n  description: Tune-A-Video is a text-to-video generation model based on the Hugging Face framework. The model generates videos based on textual prompts in a modern Disney style.\n created at: 2022-10-27\n- model_name: ImRma/Brucelee\n  description: A Hugging Face model for converting Persian and English text into video.\n created at: 2023-04-23\n- model_name: damo-vilab/text-to-video-ms-1.7b-legacy\n  description: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.\n created at: 2023-03-22\n- model_name: duncan93/video\n  description: A text-to-video model trained on OpenAssistant/oasst1 dataset.\n created at: 2023-05-02\n- model_name: cerspense/zeroscope_v2_576w\n  description: zeroscope_v2_576w uses 7.9gb of vram when rendering 30 frames at 576x320\n\n### Using it with the 1111 text2video extension\n\n1. Download files in the zs2_576w folder.\n2. Replace the respective files in the 'stable-diffusion-webui\\models\\ModelScope\\t2v' directory.\n\n### Upscaling recommendations\n\nFor upscaling, it's recommended to use [zeroscope_v2_XL](https://huggingface.co/cerspense/zeroscope_v2_XL) via vid2vid in the 1111 extension. It works best at 1024x576 with a denoise strength between 0.66 and 0.85. Remember to use the same prompt that was used to generate the original clip. <br />\n\n### Usage in üß® Diffusers\n\nLet's first install the libraries required:\n\n```bash\n$ pip install diffusers transformers accelerate torch\n```\n\nNow, generate a video:\n\n```py\nimport torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_video\n\npipe = DiffusionPipeline.from_pretrained(\"cerspense/zeroscope_v2_576w\", torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n\nprompt = \"Darth Vader is surfing on waves\"\nvideo_frames = pipe(prompt, num_inference_steps=40, height=320, width=576, num_frames=24).frames\nvideo_path = export_to_video(video_frames)\n```\n\nHere are some results:\n\n<table>\n    <tr>\n        Darth vader is surfing on waves.\n        <br>\n        <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/darthvader_cerpense.gif\"\n            alt=\"Darth vader surfing in waves.\"\n            style=\"width: 576;\" />\n        </center></td>\n    </tr>\n</table>\n\n### Known issues\n\nLower resolutions or fewer frames could lead to suboptimal output. <br />\n\nThanks to [camenduru](https://github.com/camenduru), [kabachuha](https://github.com/kabachuha), [ExponentialML](https://github.com/ExponentialML), [dotsimulate](https://www.instagram.com/dotsimulate/), [VANYA](https://twitter.com/veryVANYA), [polyware](https://twitter.com/polyware_ai), [tin2tin](https://github.com/tin2tin)<br />\n created at: 2023-06-21\n- model_name: Searchium-ai/clip4clip-webvid150k\n  description: A CLIP4Clip video-text retrieval model trained on a subset of the WebVid dataset. \nThe model and training method are described in the paper [\"Clip4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval\"](https://arxiv.org/pdf/2104.08860.pdf) by Lou et el, and implemented in the accompanying [GitHub repository](https://github.com/ArrowLuo/CLIP4Clip).\n\nThe training process utilized the [WebVid Dataset](https://m-bain.github.io/webvid-dataset/), a comprehensive collection of short videos with corresponding textual descriptions sourced from the web. \nFor training purposes, a subset consisting of the first 150,000 video-text pairs from the dataset were used.\n\nThis HF model is based on the [clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32) architecture, with weights trained by Daphna Idelson at [Searchium](https://www.searchium.ai).\n\n\n# How to use\n### Extracting Text Embeddings:\n\n```python\nimport numpy as np\nimport torch\nfrom transformers import CLIPTokenizer, CLIPTextModelWithProjection\n\n\nsearch_sentence = \"a basketball player performing a slam dunk\"\n\nmodel = CLIPTextModelWithProjection.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\ntokenizer = CLIPTokenizer.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\n\ninputs = tokenizer(text=search_sentence , return_tensors=\"pt\")\noutputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n\n# Normalize embeddings for retrieval:\nfinal_output = outputs[0] / outputs[0].norm(dim=-1, keepdim=True)\nfinal_output = final_output.cpu().detach().numpy()\nprint(\"final output: \", final_output)\n```\n\n### Extracting Video Embeddings:\n\nAn additional notebook [\"GSI_VideoRetrieval_VideoEmbedding.ipynb\"](https://huggingface.co/Searchium-ai/clip4clip-webvid150k/blob/main/Notebooks/GSI_VideoRetrieval_VideoEmbedding.ipynb), provides instructions for extracting video embeddings and includes the necessary tools for preprocessing videos.\n created at: 2023-04-17\n- model_name: PAIR/text2video-zero-controlnet-canny-avatar\n  description: [Text2Video-Zero](https://arxiv.org/abs/2303.13439) is a zero-shot text to video generator. It can perform `zero-shot text-to-video generation`, `Video Instruct Pix2Pix` (instruction-guided video editing), \n`text and pose conditional video generation`, `text and canny-edge conditional video generation`, and \n`text, canny-edge and dreambooth conditional video generation`. For more information about this work, \nplease have a look at our [paper](https://arxiv.org/abs/2303.13439) and our demo: [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/PAIR/Text2Video-Zero)\nOur [code](https://github.com/Picsart-AI-Research/Text2Video-Zero) works with any StableDiffusion base model.\n\nThis model provides [DreamBooth](https://arxiv.org/abs/2208.12242) weights for the `Avatar style` to be used with edge guidance (using [ControlNet](https://arxiv.org/abs/2302.05543)) in text2video zero.\n created at: 2023-03-27\n- model_name: Wan-AI/Wan2.1-T2V-1.3B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-01T08:39:07+00:00\n- model_name: bullerwins/Wan2.2-T2V-A14B-GGUF\n  description: you need to download both a high-noise model and a low-noise model. high noise is used for the first steps and the low-noise for the details. place them on comfyui/models/unet you will need from  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T16:29:01+00:00\n- model_name: Wan-AI/Wan2.2-T2V-A14B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T09:04:28+00:00\n- model_name: genmo/mochi-1-preview\n  description: a state of the art video generation model by . mochi 1 preview is an open state-of-the-art video generation model with high-fidelity motion and strong prompt adherence in preliminary evaluation. this model dramatically closes the gap between closed and open video generation systems. we‚Äôre releasing the model under a permissive apache 2.0 license. try this model for free on .\n created at: 2024-10-22T07:36:03+00:00\n- model_name: Wan-AI/Wan2.1-T2V-14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-02-25T07:26:34+00:00\n- model_name: Wan-AI/Wan2.1-T2V-14B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-01T08:39:52+00:00\n- model_name: Wan-AI/Wan2.2-TI2V-5B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T09:03:29+00:00\n- model_name: Skywork/SkyReels-V2-DF-14B-720P\n  description: skyreels v2: infinite-length film generative model üìë technical report ¬∑ üëã playground ¬∑ üí¨ discord ¬∑ ü§ó hugging face ¬∑ ü§ñ modelscope ¬∑ üåê github\n created at: 2025-04-18T13:34:52+00:00\n- model_name: Wan-AI/Wan2.2-TI2V-5B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-18T09:35:44+00:00\n- model_name: Wan-AI/Wan2.2-T2V-A14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-24T15:07:15+00:00\n- model_name: magespace/Wan2.2-I2V-A14B-Lightning-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp üîó lightx2v distilled & optimized wan2.2 for fast, high-quality 480p / 720p image-to-video generation   wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-08-14T20:06:59+00:00\n- model_name: Wan-AI/Wan2.1-T2V-1.3B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-02-25T11:13:48+00:00\n- model_name: QuantStack/Wan2.1_T2V_14B_FusionX-GGUF\n  description: this is a gguf conversion of by . all quantized versions were created from the base t2v fp16 model using the conversion scripts provided by city96, available at the github repository. the model files can be used in with the custom node. place the required models in the following folders:\n created at: 2025-06-07T01:47:02+00:00\n- model_name: Lightricks/LTX-Video-ICLoRA-detailer-13b-0.9.8\n  description: this is a video detailer model on top of trained on custom data. ic lora is a method that enables adding video context into the video generation process. this approach allows for video-to-video control on top of the text-to-video model, providing more precise control over the generated content by conditioning the model on reference video frames during inference. comfy compatible model:\n created at: 2025-07-16T13:41:33+00:00\n- model_name: Lightricks/LTX-Video-0.9.7-distilled\n  description: this model card focuses on the model associated with the ltx-video model, codebase available . ltx-video is the first dit-based video generation model capable of generating high-quality videos in real-time. it produces 30 fps videos at a 1216√ó704 resolution faster than they can be watched. trained on a large-scale dataset of diverse videos, the model generates high-resolution videos with realistic and varied content. we provide a model for both text-to-video as well as image+text-to-video usecases\n created at: 2025-05-14T16:55:19+00:00\n- model_name: Lightricks/LTX-Video-0.9.5\n  description: this model card focuses on the model associated with the ltx-video model, codebase available . ltx-video is the first dit-based video generation model capable of generating high-quality videos in real-time. it produces 30 fps videos at a 1216√ó704 resolution faster than they can be watched. trained on a large-scale dataset of diverse videos, the model generates high-resolution videos with realistic and varied content. we provide a model for both text-to-video as well as image+text-to-video usecases\n created at: 2025-03-16T11:14:42+00:00\n- model_name: Lightricks/LTX-Video-0.9.7-dev\n  description: this model card focuses on the model associated with the ltx-video model, codebase available . ltx-video is the first dit-based video generation model capable of generating high-quality videos in real-time. it produces 30 fps videos at a 1216√ó704 resolution faster than they can be watched. trained on a large-scale dataset of diverse videos, the model generates high-resolution videos with realistic and varied content. we provide a model for both text-to-video as well as image+text-to-video usecases\n created at: 2025-05-13T12:02:56+00:00\n- model_name: tencent/HunyuanVideo\n  description: this repo contains pytorch model definitions, pre-trained weights and inference/sampling code for our paper exploring hunyuanvideo. you can find more visualizations on our .\n created at: 2024-12-01T06:00:34+00:00\n- model_name: Isi99999/Wan2.1-T2V-14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-02T17:37:57+00:00\n- model_name: Skywork/SkyReels-V2-T2V-14B-720P\n  description: skyreels v2: infinite-length film generative model üìë technical report ¬∑ üëã playground ¬∑ üí¨ discord ¬∑ ü§ó hugging face ¬∑ ü§ñ modelscope ¬∑ üåê github\n created at: 2025-04-18T13:55:51+00:00\n\nExamples:\n- example user query: I am directing a movie and I want to generate a video scene where Spiderman is surfing.\n  model_name: damo-vilab/text-to-video-ms-1.7b-legacy\n- example user query: Describe the process of generating a video clip with a scene where a dog is playing with a ball in a park.\n  model_name: camenduru/text2-video-zero\n- example user query: The marketing team needs a tool to transform textual descriptions of their products into promotional videos. Build a model that can create videos from text.\n  model_name: chavinlo/TempoFunk\n- example user query: I am writing a script for an educational video on biology. Can you generate a video using the text of my script?\n  model_name: duncan93/video\n- example user query: I want to convey the story of a bear enjoying a beautiful sunset in the forest through an AI-generated video.\n  model_name: nitrosocke/redshift-diffusion\n- example user query: We are presenting a new tool that can convert English text into videos which will have Bruce Lee showcase the given text.\n  model_name: ImRma/Brucelee\n\nUser prompt:\nI'd like to make a visual portrayal capturing Beautiful woman in warm sweater on blue background.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <video_gen>\n\nCandidate Models:\n- model_name: camenduru/text2-video-zero\n  description: This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.\n created at: 2023-03-29\n- model_name: chavinlo/TempoFunk\n  description: A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text.\n created at: 2023-02-07\n- model_name: damo-vilab/text-to-video-ms-1.7b\n  description: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. The overall model parameters are about 1.7 billion. Currently, it only supports English input.\n created at: 2023-03-22\n- model_name: nitrosocke/redshift-diffusion\n  description: Tune-A-Video - Redshift is a text-to-video generation model based on the nitrosocke/redshift-diffusion model. It generates videos based on textual prompts, such as 'a man is skiing' or '(redshift style) spider man is skiing'.\n created at: 2022-11-06\n- model_name: nitrosocke/mo-di-diffusion\n  description: Tune-A-Video is a text-to-video generation model based on the Hugging Face framework. The model generates videos based on textual prompts in a modern Disney style.\n created at: 2022-10-27\n- model_name: ImRma/Brucelee\n  description: A Hugging Face model for converting Persian and English text into video.\n created at: 2023-04-23\n- model_name: damo-vilab/text-to-video-ms-1.7b-legacy\n  description: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.\n created at: 2023-03-22\n- model_name: duncan93/video\n  description: A text-to-video model trained on OpenAssistant/oasst1 dataset.\n created at: 2023-05-02\n- model_name: cerspense/zeroscope_v2_576w\n  description: zeroscope_v2_576w uses 7.9gb of vram when rendering 30 frames at 576x320\n\n### Using it with the 1111 text2video extension\n\n1. Download files in the zs2_576w folder.\n2. Replace the respective files in the 'stable-diffusion-webui\\models\\ModelScope\\t2v' directory.\n\n### Upscaling recommendations\n\nFor upscaling, it's recommended to use [zeroscope_v2_XL](https://huggingface.co/cerspense/zeroscope_v2_XL) via vid2vid in the 1111 extension. It works best at 1024x576 with a denoise strength between 0.66 and 0.85. Remember to use the same prompt that was used to generate the original clip. <br />\n\n### Usage in üß® Diffusers\n\nLet's first install the libraries required:\n\n```bash\n$ pip install diffusers transformers accelerate torch\n```\n\nNow, generate a video:\n\n```py\nimport torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_video\n\npipe = DiffusionPipeline.from_pretrained(\"cerspense/zeroscope_v2_576w\", torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n\nprompt = \"Darth Vader is surfing on waves\"\nvideo_frames = pipe(prompt, num_inference_steps=40, height=320, width=576, num_frames=24).frames\nvideo_path = export_to_video(video_frames)\n```\n\nHere are some results:\n\n<table>\n    <tr>\n        Darth vader is surfing on waves.\n        <br>\n        <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/darthvader_cerpense.gif\"\n            alt=\"Darth vader surfing in waves.\"\n            style=\"width: 576;\" />\n        </center></td>\n    </tr>\n</table>\n\n### Known issues\n\nLower resolutions or fewer frames could lead to suboptimal output. <br />\n\nThanks to [camenduru](https://github.com/camenduru), [kabachuha](https://github.com/kabachuha), [ExponentialML](https://github.com/ExponentialML), [dotsimulate](https://www.instagram.com/dotsimulate/), [VANYA](https://twitter.com/veryVANYA), [polyware](https://twitter.com/polyware_ai), [tin2tin](https://github.com/tin2tin)<br />\n created at: 2023-06-21\n- model_name: Searchium-ai/clip4clip-webvid150k\n  description: A CLIP4Clip video-text retrieval model trained on a subset of the WebVid dataset. \nThe model and training method are described in the paper [\"Clip4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval\"](https://arxiv.org/pdf/2104.08860.pdf) by Lou et el, and implemented in the accompanying [GitHub repository](https://github.com/ArrowLuo/CLIP4Clip).\n\nThe training process utilized the [WebVid Dataset](https://m-bain.github.io/webvid-dataset/), a comprehensive collection of short videos with corresponding textual descriptions sourced from the web. \nFor training purposes, a subset consisting of the first 150,000 video-text pairs from the dataset were used.\n\nThis HF model is based on the [clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32) architecture, with weights trained by Daphna Idelson at [Searchium](https://www.searchium.ai).\n\n\n# How to use\n### Extracting Text Embeddings:\n\n```python\nimport numpy as np\nimport torch\nfrom transformers import CLIPTokenizer, CLIPTextModelWithProjection\n\n\nsearch_sentence = \"a basketball player performing a slam dunk\"\n\nmodel = CLIPTextModelWithProjection.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\ntokenizer = CLIPTokenizer.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\n\ninputs = tokenizer(text=search_sentence , return_tensors=\"pt\")\noutputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n\n# Normalize embeddings for retrieval:\nfinal_output = outputs[0] / outputs[0].norm(dim=-1, keepdim=True)\nfinal_output = final_output.cpu().detach().numpy()\nprint(\"final output: \", final_output)\n```\n\n### Extracting Video Embeddings:\n\nAn additional notebook [\"GSI_VideoRetrieval_VideoEmbedding.ipynb\"](https://huggingface.co/Searchium-ai/clip4clip-webvid150k/blob/main/Notebooks/GSI_VideoRetrieval_VideoEmbedding.ipynb), provides instructions for extracting video embeddings and includes the necessary tools for preprocessing videos.\n created at: 2023-04-17\n- model_name: PAIR/text2video-zero-controlnet-canny-avatar\n  description: [Text2Video-Zero](https://arxiv.org/abs/2303.13439) is a zero-shot text to video generator. It can perform `zero-shot text-to-video generation`, `Video Instruct Pix2Pix` (instruction-guided video editing), \n`text and pose conditional video generation`, `text and canny-edge conditional video generation`, and \n`text, canny-edge and dreambooth conditional video generation`. For more information about this work, \nplease have a look at our [paper](https://arxiv.org/abs/2303.13439) and our demo: [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/PAIR/Text2Video-Zero)\nOur [code](https://github.com/Picsart-AI-Research/Text2Video-Zero) works with any StableDiffusion base model.\n\nThis model provides [DreamBooth](https://arxiv.org/abs/2208.12242) weights for the `Avatar style` to be used with edge guidance (using [ControlNet](https://arxiv.org/abs/2302.05543)) in text2video zero.\n created at: 2023-03-27\n- model_name: Wan-AI/Wan2.1-T2V-1.3B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-01T08:39:07+00:00\n- model_name: bullerwins/Wan2.2-T2V-A14B-GGUF\n  description: you need to download both a high-noise model and a low-noise model. high noise is used for the first steps and the low-noise for the details. place them on comfyui/models/unet you will need from  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T16:29:01+00:00\n- model_name: Wan-AI/Wan2.2-T2V-A14B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T09:04:28+00:00\n- model_name: genmo/mochi-1-preview\n  description: a state of the art video generation model by . mochi 1 preview is an open state-of-the-art video generation model with high-fidelity motion and strong prompt adherence in preliminary evaluation. this model dramatically closes the gap between closed and open video generation systems. we‚Äôre releasing the model under a permissive apache 2.0 license. try this model for free on .\n created at: 2024-10-22T07:36:03+00:00\n- model_name: Wan-AI/Wan2.1-T2V-14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-02-25T07:26:34+00:00\n- model_name: Wan-AI/Wan2.1-T2V-14B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-01T08:39:52+00:00\n- model_name: Wan-AI/Wan2.2-TI2V-5B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T09:03:29+00:00\n- model_name: Skywork/SkyReels-V2-DF-14B-720P\n  description: skyreels v2: infinite-length film generative model üìë technical report ¬∑ üëã playground ¬∑ üí¨ discord ¬∑ ü§ó hugging face ¬∑ ü§ñ modelscope ¬∑ üåê github\n created at: 2025-04-18T13:34:52+00:00\n- model_name: Wan-AI/Wan2.2-TI2V-5B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-18T09:35:44+00:00\n- model_name: Wan-AI/Wan2.2-T2V-A14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-24T15:07:15+00:00\n- model_name: magespace/Wan2.2-I2V-A14B-Lightning-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp üîó lightx2v distilled & optimized wan2.2 for fast, high-quality 480p / 720p image-to-video generation   wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-08-14T20:06:59+00:00\n- model_name: Wan-AI/Wan2.1-T2V-1.3B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-02-25T11:13:48+00:00\n- model_name: QuantStack/Wan2.1_T2V_14B_FusionX-GGUF\n  description: this is a gguf conversion of by . all quantized versions were created from the base t2v fp16 model using the conversion scripts provided by city96, available at the github repository. the model files can be used in with the custom node. place the required models in the following folders:\n created at: 2025-06-07T01:47:02+00:00\n- model_name: Lightricks/LTX-Video-ICLoRA-detailer-13b-0.9.8\n  description: this is a video detailer model on top of trained on custom data. ic lora is a method that enables adding video context into the video generation process. this approach allows for video-to-video control on top of the text-to-video model, providing more precise control over the generated content by conditioning the model on reference video frames during inference. comfy compatible model:\n created at: 2025-07-16T13:41:33+00:00\n- model_name: Lightricks/LTX-Video-0.9.7-distilled\n  description: this model card focuses on the model associated with the ltx-video model, codebase available . ltx-video is the first dit-based video generation model capable of generating high-quality videos in real-time. it produces 30 fps videos at a 1216√ó704 resolution faster than they can be watched. trained on a large-scale dataset of diverse videos, the model generates high-resolution videos with realistic and varied content. we provide a model for both text-to-video as well as image+text-to-video usecases\n created at: 2025-05-14T16:55:19+00:00\n- model_name: Lightricks/LTX-Video-0.9.5\n  description: this model card focuses on the model associated with the ltx-video model, codebase available . ltx-video is the first dit-based video generation model capable of generating high-quality videos in real-time. it produces 30 fps videos at a 1216√ó704 resolution faster than they can be watched. trained on a large-scale dataset of diverse videos, the model generates high-resolution videos with realistic and varied content. we provide a model for both text-to-video as well as image+text-to-video usecases\n created at: 2025-03-16T11:14:42+00:00\n- model_name: Lightricks/LTX-Video-0.9.7-dev\n  description: this model card focuses on the model associated with the ltx-video model, codebase available . ltx-video is the first dit-based video generation model capable of generating high-quality videos in real-time. it produces 30 fps videos at a 1216√ó704 resolution faster than they can be watched. trained on a large-scale dataset of diverse videos, the model generates high-resolution videos with realistic and varied content. we provide a model for both text-to-video as well as image+text-to-video usecases\n created at: 2025-05-13T12:02:56+00:00\n- model_name: tencent/HunyuanVideo\n  description: this repo contains pytorch model definitions, pre-trained weights and inference/sampling code for our paper exploring hunyuanvideo. you can find more visualizations on our .\n created at: 2024-12-01T06:00:34+00:00\n- model_name: Isi99999/Wan2.1-T2V-14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-02T17:37:57+00:00\n- model_name: Skywork/SkyReels-V2-T2V-14B-720P\n  description: skyreels v2: infinite-length film generative model üìë technical report ¬∑ üëã playground ¬∑ üí¨ discord ¬∑ ü§ó hugging face ¬∑ ü§ñ modelscope ¬∑ üåê github\n created at: 2025-04-18T13:55:51+00:00\n\nExamples:\n- example user query: I am directing a movie and I want to generate a video scene where Spiderman is surfing.\n  model_name: damo-vilab/text-to-video-ms-1.7b-legacy\n- example user query: Describe the process of generating a video clip with a scene where a dog is playing with a ball in a park.\n  model_name: camenduru/text2-video-zero\n- example user query: The marketing team needs a tool to transform textual descriptions of their products into promotional videos. Build a model that can create videos from text.\n  model_name: chavinlo/TempoFunk\n- example user query: I am writing a script for an educational video on biology. Can you generate a video using the text of my script?\n  model_name: duncan93/video\n- example user query: I want to convey the story of a bear enjoying a beautiful sunset in the forest through an AI-generated video.\n  model_name: nitrosocke/redshift-diffusion\n- example user query: We are presenting a new tool that can convert English text into videos which will have Bruce Lee showcase the given text.\n  model_name: ImRma/Brucelee\n\nUser prompt:\nCan you create an experimental film on Ravelry: Fort pattern by Jared Flood.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <video_gen>\n\nCandidate Models:\n- model_name: camenduru/text2-video-zero\n  description: This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.\n created at: 2023-03-29\n- model_name: chavinlo/TempoFunk\n  description: A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text.\n created at: 2023-02-07\n- model_name: damo-vilab/text-to-video-ms-1.7b\n  description: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. The overall model parameters are about 1.7 billion. Currently, it only supports English input.\n created at: 2023-03-22\n- model_name: nitrosocke/redshift-diffusion\n  description: Tune-A-Video - Redshift is a text-to-video generation model based on the nitrosocke/redshift-diffusion model. It generates videos based on textual prompts, such as 'a man is skiing' or '(redshift style) spider man is skiing'.\n created at: 2022-11-06\n- model_name: nitrosocke/mo-di-diffusion\n  description: Tune-A-Video is a text-to-video generation model based on the Hugging Face framework. The model generates videos based on textual prompts in a modern Disney style.\n created at: 2022-10-27\n- model_name: ImRma/Brucelee\n  description: A Hugging Face model for converting Persian and English text into video.\n created at: 2023-04-23\n- model_name: damo-vilab/text-to-video-ms-1.7b-legacy\n  description: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.\n created at: 2023-03-22\n- model_name: duncan93/video\n  description: A text-to-video model trained on OpenAssistant/oasst1 dataset.\n created at: 2023-05-02\n- model_name: cerspense/zeroscope_v2_576w\n  description: zeroscope_v2_576w uses 7.9gb of vram when rendering 30 frames at 576x320\n\n### Using it with the 1111 text2video extension\n\n1. Download files in the zs2_576w folder.\n2. Replace the respective files in the 'stable-diffusion-webui\\models\\ModelScope\\t2v' directory.\n\n### Upscaling recommendations\n\nFor upscaling, it's recommended to use [zeroscope_v2_XL](https://huggingface.co/cerspense/zeroscope_v2_XL) via vid2vid in the 1111 extension. It works best at 1024x576 with a denoise strength between 0.66 and 0.85. Remember to use the same prompt that was used to generate the original clip. <br />\n\n### Usage in üß® Diffusers\n\nLet's first install the libraries required:\n\n```bash\n$ pip install diffusers transformers accelerate torch\n```\n\nNow, generate a video:\n\n```py\nimport torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_video\n\npipe = DiffusionPipeline.from_pretrained(\"cerspense/zeroscope_v2_576w\", torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n\nprompt = \"Darth Vader is surfing on waves\"\nvideo_frames = pipe(prompt, num_inference_steps=40, height=320, width=576, num_frames=24).frames\nvideo_path = export_to_video(video_frames)\n```\n\nHere are some results:\n\n<table>\n    <tr>\n        Darth vader is surfing on waves.\n        <br>\n        <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/darthvader_cerpense.gif\"\n            alt=\"Darth vader surfing in waves.\"\n            style=\"width: 576;\" />\n        </center></td>\n    </tr>\n</table>\n\n### Known issues\n\nLower resolutions or fewer frames could lead to suboptimal output. <br />\n\nThanks to [camenduru](https://github.com/camenduru), [kabachuha](https://github.com/kabachuha), [ExponentialML](https://github.com/ExponentialML), [dotsimulate](https://www.instagram.com/dotsimulate/), [VANYA](https://twitter.com/veryVANYA), [polyware](https://twitter.com/polyware_ai), [tin2tin](https://github.com/tin2tin)<br />\n created at: 2023-06-21\n- model_name: Searchium-ai/clip4clip-webvid150k\n  description: A CLIP4Clip video-text retrieval model trained on a subset of the WebVid dataset. \nThe model and training method are described in the paper [\"Clip4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval\"](https://arxiv.org/pdf/2104.08860.pdf) by Lou et el, and implemented in the accompanying [GitHub repository](https://github.com/ArrowLuo/CLIP4Clip).\n\nThe training process utilized the [WebVid Dataset](https://m-bain.github.io/webvid-dataset/), a comprehensive collection of short videos with corresponding textual descriptions sourced from the web. \nFor training purposes, a subset consisting of the first 150,000 video-text pairs from the dataset were used.\n\nThis HF model is based on the [clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32) architecture, with weights trained by Daphna Idelson at [Searchium](https://www.searchium.ai).\n\n\n# How to use\n### Extracting Text Embeddings:\n\n```python\nimport numpy as np\nimport torch\nfrom transformers import CLIPTokenizer, CLIPTextModelWithProjection\n\n\nsearch_sentence = \"a basketball player performing a slam dunk\"\n\nmodel = CLIPTextModelWithProjection.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\ntokenizer = CLIPTokenizer.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\n\ninputs = tokenizer(text=search_sentence , return_tensors=\"pt\")\noutputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n\n# Normalize embeddings for retrieval:\nfinal_output = outputs[0] / outputs[0].norm(dim=-1, keepdim=True)\nfinal_output = final_output.cpu().detach().numpy()\nprint(\"final output: \", final_output)\n```\n\n### Extracting Video Embeddings:\n\nAn additional notebook [\"GSI_VideoRetrieval_VideoEmbedding.ipynb\"](https://huggingface.co/Searchium-ai/clip4clip-webvid150k/blob/main/Notebooks/GSI_VideoRetrieval_VideoEmbedding.ipynb), provides instructions for extracting video embeddings and includes the necessary tools for preprocessing videos.\n created at: 2023-04-17\n- model_name: PAIR/text2video-zero-controlnet-canny-avatar\n  description: [Text2Video-Zero](https://arxiv.org/abs/2303.13439) is a zero-shot text to video generator. It can perform `zero-shot text-to-video generation`, `Video Instruct Pix2Pix` (instruction-guided video editing), \n`text and pose conditional video generation`, `text and canny-edge conditional video generation`, and \n`text, canny-edge and dreambooth conditional video generation`. For more information about this work, \nplease have a look at our [paper](https://arxiv.org/abs/2303.13439) and our demo: [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/PAIR/Text2Video-Zero)\nOur [code](https://github.com/Picsart-AI-Research/Text2Video-Zero) works with any StableDiffusion base model.\n\nThis model provides [DreamBooth](https://arxiv.org/abs/2208.12242) weights for the `Avatar style` to be used with edge guidance (using [ControlNet](https://arxiv.org/abs/2302.05543)) in text2video zero.\n created at: 2023-03-27\n- model_name: Wan-AI/Wan2.1-T2V-1.3B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-01T08:39:07+00:00\n- model_name: bullerwins/Wan2.2-T2V-A14B-GGUF\n  description: you need to download both a high-noise model and a low-noise model. high noise is used for the first steps and the low-noise for the details. place them on comfyui/models/unet you will need from  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T16:29:01+00:00\n- model_name: Wan-AI/Wan2.2-T2V-A14B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T09:04:28+00:00\n- model_name: genmo/mochi-1-preview\n  description: a state of the art video generation model by . mochi 1 preview is an open state-of-the-art video generation model with high-fidelity motion and strong prompt adherence in preliminary evaluation. this model dramatically closes the gap between closed and open video generation systems. we‚Äôre releasing the model under a permissive apache 2.0 license. try this model for free on .\n created at: 2024-10-22T07:36:03+00:00\n- model_name: Wan-AI/Wan2.1-T2V-14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-02-25T07:26:34+00:00\n- model_name: Wan-AI/Wan2.1-T2V-14B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-01T08:39:52+00:00\n- model_name: Wan-AI/Wan2.2-TI2V-5B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T09:03:29+00:00\n- model_name: Skywork/SkyReels-V2-DF-14B-720P\n  description: skyreels v2: infinite-length film generative model üìë technical report ¬∑ üëã playground ¬∑ üí¨ discord ¬∑ ü§ó hugging face ¬∑ ü§ñ modelscope ¬∑ üåê github\n created at: 2025-04-18T13:34:52+00:00\n- model_name: Wan-AI/Wan2.2-TI2V-5B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-18T09:35:44+00:00\n- model_name: Wan-AI/Wan2.2-T2V-A14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-24T15:07:15+00:00\n- model_name: magespace/Wan2.2-I2V-A14B-Lightning-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp üîó lightx2v distilled & optimized wan2.2 for fast, high-quality 480p / 720p image-to-video generation   wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-08-14T20:06:59+00:00\n- model_name: Wan-AI/Wan2.1-T2V-1.3B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-02-25T11:13:48+00:00\n- model_name: QuantStack/Wan2.1_T2V_14B_FusionX-GGUF\n  description: this is a gguf conversion of by . all quantized versions were created from the base t2v fp16 model using the conversion scripts provided by city96, available at the github repository. the model files can be used in with the custom node. place the required models in the following folders:\n created at: 2025-06-07T01:47:02+00:00\n- model_name: Lightricks/LTX-Video-ICLoRA-detailer-13b-0.9.8\n  description: this is a video detailer model on top of trained on custom data. ic lora is a method that enables adding video context into the video generation process. this approach allows for video-to-video control on top of the text-to-video model, providing more precise control over the generated content by conditioning the model on reference video frames during inference. comfy compatible model:\n created at: 2025-07-16T13:41:33+00:00\n- model_name: Lightricks/LTX-Video-0.9.7-distilled\n  description: this model card focuses on the model associated with the ltx-video model, codebase available . ltx-video is the first dit-based video generation model capable of generating high-quality videos in real-time. it produces 30 fps videos at a 1216√ó704 resolution faster than they can be watched. trained on a large-scale dataset of diverse videos, the model generates high-resolution videos with realistic and varied content. we provide a model for both text-to-video as well as image+text-to-video usecases\n created at: 2025-05-14T16:55:19+00:00\n- model_name: Lightricks/LTX-Video-0.9.5\n  description: this model card focuses on the model associated with the ltx-video model, codebase available . ltx-video is the first dit-based video generation model capable of generating high-quality videos in real-time. it produces 30 fps videos at a 1216√ó704 resolution faster than they can be watched. trained on a large-scale dataset of diverse videos, the model generates high-resolution videos with realistic and varied content. we provide a model for both text-to-video as well as image+text-to-video usecases\n created at: 2025-03-16T11:14:42+00:00\n- model_name: Lightricks/LTX-Video-0.9.7-dev\n  description: this model card focuses on the model associated with the ltx-video model, codebase available . ltx-video is the first dit-based video generation model capable of generating high-quality videos in real-time. it produces 30 fps videos at a 1216√ó704 resolution faster than they can be watched. trained on a large-scale dataset of diverse videos, the model generates high-resolution videos with realistic and varied content. we provide a model for both text-to-video as well as image+text-to-video usecases\n created at: 2025-05-13T12:02:56+00:00\n- model_name: tencent/HunyuanVideo\n  description: this repo contains pytorch model definitions, pre-trained weights and inference/sampling code for our paper exploring hunyuanvideo. you can find more visualizations on our .\n created at: 2024-12-01T06:00:34+00:00\n- model_name: Isi99999/Wan2.1-T2V-14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-02T17:37:57+00:00\n- model_name: Skywork/SkyReels-V2-T2V-14B-720P\n  description: skyreels v2: infinite-length film generative model üìë technical report ¬∑ üëã playground ¬∑ üí¨ discord ¬∑ ü§ó hugging face ¬∑ ü§ñ modelscope ¬∑ üåê github\n created at: 2025-04-18T13:55:51+00:00\n\nExamples:\n- example user query: I am directing a movie and I want to generate a video scene where Spiderman is surfing.\n  model_name: damo-vilab/text-to-video-ms-1.7b-legacy\n- example user query: Describe the process of generating a video clip with a scene where a dog is playing with a ball in a park.\n  model_name: camenduru/text2-video-zero\n- example user query: The marketing team needs a tool to transform textual descriptions of their products into promotional videos. Build a model that can create videos from text.\n  model_name: chavinlo/TempoFunk\n- example user query: I am writing a script for an educational video on biology. Can you generate a video using the text of my script?\n  model_name: duncan93/video\n- example user query: I want to convey the story of a bear enjoying a beautiful sunset in the forest through an AI-generated video.\n  model_name: nitrosocke/redshift-diffusion\n- example user query: We are presenting a new tool that can convert English text into videos which will have Bruce Lee showcase the given text.\n  model_name: ImRma/Brucelee\n\nUser prompt:\nDevelop a storytelling visual about Dress made by hand with elegant necklace of fringes and modern-NEW-boho chic borlas-nina chic for girl-fashion kids.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n",
    "You are an AI assistant tasked with selecting the top 3 most suitable models for a user‚Äôs task.\n\nTask type: <video_gen>\n\nCandidate Models:\n- model_name: camenduru/text2-video-zero\n  description: This model is used for generating videos from text inputs. It is based on the Hugging Face framework and can be used with the transformers library. The model is trained on a variety of text and video datasets, and can be used for tasks such as video summarization, video generation from text prompts, and more.\n created at: 2023-03-29\n- model_name: chavinlo/TempoFunk\n  description: A Text-to-Video model using Hugging Face Transformers library. Model is capable of generating video content based on the input text.\n created at: 2023-02-07\n- model_name: damo-vilab/text-to-video-ms-1.7b\n  description: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. The model consists of three sub-networks: text feature extraction model, text feature-to-video latent space diffusion model, and video latent space to video visual space model. The overall model parameters are about 1.7 billion. Currently, it only supports English input.\n created at: 2023-03-22\n- model_name: nitrosocke/redshift-diffusion\n  description: Tune-A-Video - Redshift is a text-to-video generation model based on the nitrosocke/redshift-diffusion model. It generates videos based on textual prompts, such as 'a man is skiing' or '(redshift style) spider man is skiing'.\n created at: 2022-11-06\n- model_name: nitrosocke/mo-di-diffusion\n  description: Tune-A-Video is a text-to-video generation model based on the Hugging Face framework. The model generates videos based on textual prompts in a modern Disney style.\n created at: 2022-10-27\n- model_name: ImRma/Brucelee\n  description: A Hugging Face model for converting Persian and English text into video.\n created at: 2023-04-23\n- model_name: damo-vilab/text-to-video-ms-1.7b-legacy\n  description: This model is based on a multi-stage text-to-video generation diffusion model, which inputs a description text and returns a video that matches the text description. Only English input is supported.\n created at: 2023-03-22\n- model_name: duncan93/video\n  description: A text-to-video model trained on OpenAssistant/oasst1 dataset.\n created at: 2023-05-02\n- model_name: cerspense/zeroscope_v2_576w\n  description: zeroscope_v2_576w uses 7.9gb of vram when rendering 30 frames at 576x320\n\n### Using it with the 1111 text2video extension\n\n1. Download files in the zs2_576w folder.\n2. Replace the respective files in the 'stable-diffusion-webui\\models\\ModelScope\\t2v' directory.\n\n### Upscaling recommendations\n\nFor upscaling, it's recommended to use [zeroscope_v2_XL](https://huggingface.co/cerspense/zeroscope_v2_XL) via vid2vid in the 1111 extension. It works best at 1024x576 with a denoise strength between 0.66 and 0.85. Remember to use the same prompt that was used to generate the original clip. <br />\n\n### Usage in üß® Diffusers\n\nLet's first install the libraries required:\n\n```bash\n$ pip install diffusers transformers accelerate torch\n```\n\nNow, generate a video:\n\n```py\nimport torch\nfrom diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\nfrom diffusers.utils import export_to_video\n\npipe = DiffusionPipeline.from_pretrained(\"cerspense/zeroscope_v2_576w\", torch_dtype=torch.float16)\npipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\npipe.enable_model_cpu_offload()\n\nprompt = \"Darth Vader is surfing on waves\"\nvideo_frames = pipe(prompt, num_inference_steps=40, height=320, width=576, num_frames=24).frames\nvideo_path = export_to_video(video_frames)\n```\n\nHere are some results:\n\n<table>\n    <tr>\n        Darth vader is surfing on waves.\n        <br>\n        <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/darthvader_cerpense.gif\"\n            alt=\"Darth vader surfing in waves.\"\n            style=\"width: 576;\" />\n        </center></td>\n    </tr>\n</table>\n\n### Known issues\n\nLower resolutions or fewer frames could lead to suboptimal output. <br />\n\nThanks to [camenduru](https://github.com/camenduru), [kabachuha](https://github.com/kabachuha), [ExponentialML](https://github.com/ExponentialML), [dotsimulate](https://www.instagram.com/dotsimulate/), [VANYA](https://twitter.com/veryVANYA), [polyware](https://twitter.com/polyware_ai), [tin2tin](https://github.com/tin2tin)<br />\n created at: 2023-06-21\n- model_name: Searchium-ai/clip4clip-webvid150k\n  description: A CLIP4Clip video-text retrieval model trained on a subset of the WebVid dataset. \nThe model and training method are described in the paper [\"Clip4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval\"](https://arxiv.org/pdf/2104.08860.pdf) by Lou et el, and implemented in the accompanying [GitHub repository](https://github.com/ArrowLuo/CLIP4Clip).\n\nThe training process utilized the [WebVid Dataset](https://m-bain.github.io/webvid-dataset/), a comprehensive collection of short videos with corresponding textual descriptions sourced from the web. \nFor training purposes, a subset consisting of the first 150,000 video-text pairs from the dataset were used.\n\nThis HF model is based on the [clip-vit-base-patch32](https://huggingface.co/openai/clip-vit-base-patch32) architecture, with weights trained by Daphna Idelson at [Searchium](https://www.searchium.ai).\n\n\n# How to use\n### Extracting Text Embeddings:\n\n```python\nimport numpy as np\nimport torch\nfrom transformers import CLIPTokenizer, CLIPTextModelWithProjection\n\n\nsearch_sentence = \"a basketball player performing a slam dunk\"\n\nmodel = CLIPTextModelWithProjection.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\ntokenizer = CLIPTokenizer.from_pretrained(\"Searchium-ai/clip4clip-webvid150k\")\n\ninputs = tokenizer(text=search_sentence , return_tensors=\"pt\")\noutputs = model(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n\n# Normalize embeddings for retrieval:\nfinal_output = outputs[0] / outputs[0].norm(dim=-1, keepdim=True)\nfinal_output = final_output.cpu().detach().numpy()\nprint(\"final output: \", final_output)\n```\n\n### Extracting Video Embeddings:\n\nAn additional notebook [\"GSI_VideoRetrieval_VideoEmbedding.ipynb\"](https://huggingface.co/Searchium-ai/clip4clip-webvid150k/blob/main/Notebooks/GSI_VideoRetrieval_VideoEmbedding.ipynb), provides instructions for extracting video embeddings and includes the necessary tools for preprocessing videos.\n created at: 2023-04-17\n- model_name: PAIR/text2video-zero-controlnet-canny-avatar\n  description: [Text2Video-Zero](https://arxiv.org/abs/2303.13439) is a zero-shot text to video generator. It can perform `zero-shot text-to-video generation`, `Video Instruct Pix2Pix` (instruction-guided video editing), \n`text and pose conditional video generation`, `text and canny-edge conditional video generation`, and \n`text, canny-edge and dreambooth conditional video generation`. For more information about this work, \nplease have a look at our [paper](https://arxiv.org/abs/2303.13439) and our demo: [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/PAIR/Text2Video-Zero)\nOur [code](https://github.com/Picsart-AI-Research/Text2Video-Zero) works with any StableDiffusion base model.\n\nThis model provides [DreamBooth](https://arxiv.org/abs/2208.12242) weights for the `Avatar style` to be used with edge guidance (using [ControlNet](https://arxiv.org/abs/2302.05543)) in text2video zero.\n created at: 2023-03-27\n- model_name: Wan-AI/Wan2.1-T2V-1.3B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-01T08:39:07+00:00\n- model_name: bullerwins/Wan2.2-T2V-A14B-GGUF\n  description: you need to download both a high-noise model and a low-noise model. high noise is used for the first steps and the low-noise for the details. place them on comfyui/models/unet you will need from  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T16:29:01+00:00\n- model_name: Wan-AI/Wan2.2-T2V-A14B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T09:04:28+00:00\n- model_name: genmo/mochi-1-preview\n  description: a state of the art video generation model by . mochi 1 preview is an open state-of-the-art video generation model with high-fidelity motion and strong prompt adherence in preliminary evaluation. this model dramatically closes the gap between closed and open video generation systems. we‚Äôre releasing the model under a permissive apache 2.0 license. try this model for free on .\n created at: 2024-10-22T07:36:03+00:00\n- model_name: Wan-AI/Wan2.1-T2V-14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-02-25T07:26:34+00:00\n- model_name: Wan-AI/Wan2.1-T2V-14B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-01T08:39:52+00:00\n- model_name: Wan-AI/Wan2.2-TI2V-5B-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-28T09:03:29+00:00\n- model_name: Skywork/SkyReels-V2-DF-14B-720P\n  description: skyreels v2: infinite-length film generative model üìë technical report ¬∑ üëã playground ¬∑ üí¨ discord ¬∑ ü§ó hugging face ¬∑ ü§ñ modelscope ¬∑ üåê github\n created at: 2025-04-18T13:34:52+00:00\n- model_name: Wan-AI/Wan2.2-TI2V-5B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-18T09:35:44+00:00\n- model_name: Wan-AI/Wan2.2-T2V-A14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  we are excited to introduce wan2.2 , a major upgrade to our foundational video models. with wan2.2 , we have focused on incorporating the following innovations:  wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-07-24T15:07:15+00:00\n- model_name: magespace/Wan2.2-I2V-A14B-Lightning-Diffusers\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë technical report &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp üîó lightx2v distilled & optimized wan2.2 for fast, high-quality 480p / 720p image-to-video generation   wan2.2 builds on the foundation of wan2.1 with notable improvements in generation quality and model capability. this upgrade is driven by a series of key technical innovations, mainly including the mixture-of-experts moe architecture, upgraded training data, and high-compression video generation.\n created at: 2025-08-14T20:06:59+00:00\n- model_name: Wan-AI/Wan2.1-T2V-1.3B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-02-25T11:13:48+00:00\n- model_name: QuantStack/Wan2.1_T2V_14B_FusionX-GGUF\n  description: this is a gguf conversion of by . all quantized versions were created from the base t2v fp16 model using the conversion scripts provided by city96, available at the github repository. the model files can be used in with the custom node. place the required models in the following folders:\n created at: 2025-06-07T01:47:02+00:00\n- model_name: Lightricks/LTX-Video-ICLoRA-detailer-13b-0.9.8\n  description: this is a video detailer model on top of trained on custom data. ic lora is a method that enables adding video context into the video generation process. this approach allows for video-to-video control on top of the text-to-video model, providing more precise control over the generated content by conditioning the model on reference video frames during inference. comfy compatible model:\n created at: 2025-07-16T13:41:33+00:00\n- model_name: Lightricks/LTX-Video-0.9.7-distilled\n  description: this model card focuses on the model associated with the ltx-video model, codebase available . ltx-video is the first dit-based video generation model capable of generating high-quality videos in real-time. it produces 30 fps videos at a 1216√ó704 resolution faster than they can be watched. trained on a large-scale dataset of diverse videos, the model generates high-resolution videos with realistic and varied content. we provide a model for both text-to-video as well as image+text-to-video usecases\n created at: 2025-05-14T16:55:19+00:00\n- model_name: Lightricks/LTX-Video-0.9.5\n  description: this model card focuses on the model associated with the ltx-video model, codebase available . ltx-video is the first dit-based video generation model capable of generating high-quality videos in real-time. it produces 30 fps videos at a 1216√ó704 resolution faster than they can be watched. trained on a large-scale dataset of diverse videos, the model generates high-resolution videos with realistic and varied content. we provide a model for both text-to-video as well as image+text-to-video usecases\n created at: 2025-03-16T11:14:42+00:00\n- model_name: Lightricks/LTX-Video-0.9.7-dev\n  description: this model card focuses on the model associated with the ltx-video model, codebase available . ltx-video is the first dit-based video generation model capable of generating high-quality videos in real-time. it produces 30 fps videos at a 1216√ó704 resolution faster than they can be watched. trained on a large-scale dataset of diverse videos, the model generates high-resolution videos with realistic and varied content. we provide a model for both text-to-video as well as image+text-to-video usecases\n created at: 2025-05-13T12:02:56+00:00\n- model_name: tencent/HunyuanVideo\n  description: this repo contains pytorch model definitions, pre-trained weights and inference/sampling code for our paper exploring hunyuanvideo. you can find more visualizations on our .\n created at: 2024-12-01T06:00:34+00:00\n- model_name: Isi99999/Wan2.1-T2V-14B\n  description: üíú wan &nbsp&nbsp &nbsp&nbsp üñ•Ô∏è github &nbsp&nbsp &nbsp&nbspü§ó hugging face&nbsp&nbsp &nbsp&nbspü§ñ modelscope&nbsp&nbsp &nbsp&nbsp üìë paper coming soon &nbsp&nbsp &nbsp&nbsp üìë blog &nbsp&nbsp &nbsp&nbspüí¨ wechat group&nbsp&nbsp &nbsp&nbsp üìñ discord&nbsp&nbsp  in this repository, we present wan2.1 , a comprehensive and open suite of video foundation models that pushes the boundaries of video generation. wan2.1 offers these key features:  wan2.1 is designed on the mainstream diffusion transformer paradigm, achieving significant advancements in generative capabilities through a series of innovations. these include our novel spatio-temporal variational autoencoder vae, scalable training strategies, large-scale data construction, and automated evaluation metrics. collectively, these contributions enhance the model‚Äôs performance and versatility.\n created at: 2025-03-02T17:37:57+00:00\n- model_name: Skywork/SkyReels-V2-T2V-14B-720P\n  description: skyreels v2: infinite-length film generative model üìë technical report ¬∑ üëã playground ¬∑ üí¨ discord ¬∑ ü§ó hugging face ¬∑ ü§ñ modelscope ¬∑ üåê github\n created at: 2025-04-18T13:55:51+00:00\n\nExamples:\n- example user query: I am directing a movie and I want to generate a video scene where Spiderman is surfing.\n  model_name: damo-vilab/text-to-video-ms-1.7b-legacy\n- example user query: Describe the process of generating a video clip with a scene where a dog is playing with a ball in a park.\n  model_name: camenduru/text2-video-zero\n- example user query: The marketing team needs a tool to transform textual descriptions of their products into promotional videos. Build a model that can create videos from text.\n  model_name: chavinlo/TempoFunk\n- example user query: I am writing a script for an educational video on biology. Can you generate a video using the text of my script?\n  model_name: duncan93/video\n- example user query: I want to convey the story of a bear enjoying a beautiful sunset in the forest through an AI-generated video.\n  model_name: nitrosocke/redshift-diffusion\n- example user query: We are presenting a new tool that can convert English text into videos which will have Bruce Lee showcase the given text.\n  model_name: ImRma/Brucelee\n\nUser prompt:\nCreate a clip displaying Brookfield Barn Wedding Venue With Rustic Styling And Bride In Enzoani With Images From Lemonade Pictures And Film By Kitebox Films.\n\nSTRICT OUTPUT RULES:\n- Evaluate the candidate models for how well they solve the user‚Äôs task.\n- Select the 3 models that best fit the task.\n- Assign each selected model a confidence score between 0 and 1 (higher = more confident).\n- Output ONLY a valid JSON array of 3 objects, each with:\n  {\n    \"model_name\": <string, one of the candidate_models>,\n    \"confidence\": <float between 0 and 1>\n  }\n- Do NOT output any reasoning, explanations, or additional text.\n- Do NOT include <think> tags.\n- Ensure the JSON is valid and contains exactly 3 entries.\n\n"
  ]
}